[
  {
    "chunk_id": 0,
    "paper_filename": "adam_2024_mammography_survey.pdf",
    "paper_title": "Adam 2024 Mammography Survey",
    "chunk_index": 0,
    "total_chunks": 37,
    "text_content": "Citation: Mra\u02c7 cko, A.; Vanov\u02c7 canov\u00e1, L.; Cimr\u00e1k, I. Mammography Datasets for Neural Networks\u2014Survey. J. Imaging 2023 ,9, 95. https:// doi.org/10.3390/jimaging9050095 Academic Editors: Cecilia Di Ruberto, Alessandro Stefano, Albert Comelli, Lorenzo Putzu and Andrea Loddo Received: 7 March 2023 Revised: 2 May 2023 Accepted: 5 May 2023 Published: 10 May 2023 Copyright: \u00a9 2023 by the authors. Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and ",
    "full_text_length": 36865,
    "chunk_length": 1453
  },
  {
    "chunk_id": 1,
    "paper_filename": "adam_2024_mammography_survey.pdf",
    "paper_title": "Adam 2024 Mammography Survey",
    "chunk_index": 1,
    "total_chunks": 37,
    "text_content": "Abstract: Deep neural networks have gained popularity in the \ufb01eld of mammography. Data play an integral role in training these models, as training algorithms requires a large amount of data to capture the general relationship between the model\u2019s input and output. Open-access databases are the most accessible source of mammography data for training neural networks. Our work focuses on conducting a comprehensive survey of mammography databases that contain images with de\ufb01ned abnormal areas of inte",
    "full_text_length": 36865,
    "chunk_length": 1376
  },
  {
    "chunk_id": 2,
    "paper_filename": "adam_2024_mammography_survey.pdf",
    "paper_title": "Adam 2024 Mammography Survey",
    "chunk_index": 2,
    "total_chunks": 37,
    "text_content": "approximately 1842 patients. The number of patients with important \ufb01ndings can be increased to approximately 14,474, depending on the type of agreement with the OPTIMAM team. Furthermore, we provide a description of the annotation process for mammography images to enhance the understanding of the information gained from these datasets. Keywords: mammograms; open-access databases; deep neural networks; arti\ufb01cial intelligence; mammography; machine learning 1. Introduction Breast cancer is the most",
    "full_text_length": 36865,
    "chunk_length": 1307
  },
  {
    "chunk_id": 3,
    "paper_filename": "adam_2024_mammography_survey.pdf",
    "paper_title": "Adam 2024 Mammography Survey",
    "chunk_index": 3,
    "total_chunks": 37,
    "text_content": "the 5-year relative survival rate is 99%. The stage of the disease is the most important factor for treatment selection and prognosis prediction. Therefore, mammographic screening has been established as the best method for detecting breast cancer in its early stages in many countries. The screening is periodically performed and consists of testing healthy individuals to identify those with cancers before any symptoms appear [ 3]. Women in the non-risk group undergo mammography every two years, ",
    "full_text_length": 36865,
    "chunk_length": 1345
  },
  {
    "chunk_id": 4,
    "paper_filename": "adam_2024_mammography_survey.pdf",
    "paper_title": "Adam 2024 Mammography Survey",
    "chunk_index": 4,
    "total_chunks": 37,
    "text_content": "the quality of annotation. If both doctors agree that there is a suspicious J. Imaging 2023 ,9, 95. https://doi.org/10.3390/jimaging9050095 https://www.mdpi.com/journal/jimaging J. Imaging 2023 ,9, 95 2 of 15 \ufb01nding in the breast, a tissue biopsy is necessary. However, many of these biopsies turn out to be benign breast lesions, accounting for 55% to 85% of cases in the USA [ 6]. Mammography is a breast imaging method that uses ionizing radiation (X-rays). In the older method, SFM (screen-\ufb01lm ma",
    "full_text_length": 36865,
    "chunk_length": 1268
  },
  {
    "chunk_id": 5,
    "paper_filename": "adam_2024_mammography_survey.pdf",
    "paper_title": "Adam 2024 Mammography Survey",
    "chunk_index": 5,
    "total_chunks": 37,
    "text_content": "FFDM is even more accurate in women under 50 years old [8] and in the detection rate of interval cancer [9]. The high time burden of mammographic screening creates opportunities for computer diagnostic assistance tools. If these tools are able to achieve results similar to or better than radiologists, then double reading could potentially be conducted with the assistance of a tool and one radiologist. Recently, the most popular methods applied to analyze visual imagery have been deep convolution",
    "full_text_length": 36865,
    "chunk_length": 1318
  },
  {
    "chunk_id": 6,
    "paper_filename": "adam_2024_mammography_survey.pdf",
    "paper_title": "Adam 2024 Mammography Survey",
    "chunk_index": 6,
    "total_chunks": 37,
    "text_content": "process of a radiologist. The standard mammographic examination consists of four images, two projections for each breast: the craniocaudal (CC) view, which is a top-to-bottom view, and the mediolateral oblique (MLO) view, which is an oblique view (Figure 1). The ACR BI-RADS Atlas 2013 [ 10] was designed to standardize breast imaging reporting (Figure 2). For our purposes, breast composition is the \ufb01rst important step. In the older BI-RADS edition, the assignment was based on the four ACR (Americ",
    "full_text_length": 36865,
    "chunk_length": 1203
  },
  {
    "chunk_id": 7,
    "paper_filename": "adam_2024_mammography_survey.pdf",
    "paper_title": "Adam 2024 Mammography Survey",
    "chunk_index": 7,
    "total_chunks": 37,
    "text_content": "[11]. Table 1. ACR standardized categories. Category Fibroglandular Tissue 1 <25% 2 25\u201350% 3 50\u201375% 4 >75% J. Imaging 2023 ,9, 95 3 of 15 Figure 2. Standard reporting system. Figure 3. ACR standardized breast density. Source: [12]. Table 2. New categories from ACR BI-RADS Atlas 2013. Category Description a The breasts are almost entirely fatty b There are scattered areas of \ufb01broglandular density c The breasts are heterogeneously dense, which may obscure small masses d The breasts are extremely d",
    "full_text_length": 36865,
    "chunk_length": 1281
  },
  {
    "chunk_id": 8,
    "paper_filename": "adam_2024_mammography_survey.pdf",
    "paper_title": "Adam 2024 Mammography Survey",
    "chunk_index": 8,
    "total_chunks": 37,
    "text_content": "A mass must be visible in both projections and demonstrate partially or completely convex-outward borders. Masses can vary in shape, margins, and density. J. Imaging 2023 ,9, 95 4 of 15 Figure 4. Different types and margins of masses. Source: [12]. \u2022 Calci\ufb01cations are deposits of calcium salts in the breast [ 15], Figure 5. They are present in approximately 85% of mammograms and can vary in morphology and distribution, see Figure 6. \u2013 Diffuse: Calci\ufb01cations are randomly distributed throughout th",
    "full_text_length": 36865,
    "chunk_length": 1220
  },
  {
    "chunk_id": 9,
    "paper_filename": "adam_2024_mammography_survey.pdf",
    "paper_title": "Adam 2024 Mammography Survey",
    "chunk_index": 9,
    "total_chunks": 37,
    "text_content": "If only one group is present in the breast, the probability of malignancy is increased. \u2013 Linear: Calci\ufb01cations are arranged in a line, corresponding to their location in the ducts, and therefore indicating the malignancy of the \ufb01nding. \u2013 Segmental: Calci\ufb01cations suggest deposits in a duct or ducts and their branches, indicating malignancy. Differentiating between regional and segmental calci\ufb01ca- tions can be problematic. Of the breast cancers detected on mammography due to calci\ufb01cations, about ",
    "full_text_length": 36865,
    "chunk_length": 1325
  },
  {
    "chunk_id": 10,
    "paper_filename": "adam_2024_mammography_survey.pdf",
    "paper_title": "Adam 2024 Mammography Survey",
    "chunk_index": 10,
    "total_chunks": 37,
    "text_content": "is distorted without a de\ufb01nite mass being visible [ 17]. This can include J. Imaging 2023 ,9, 95 5 of 15 thin straight lines or spiculations radiating from a point, as well as focal retraction, distortion, or straightening at the edges of the parenchyma. Figure 7. Architectural distortion examples. Source: [11]. \u2022 Asymmetries represent a range of morphological descriptors for unilateral \ufb01brog- landular density \ufb01ndings seen on one or more mammographic projections that do not meet the criteria for",
    "full_text_length": 36865,
    "chunk_length": 1253
  },
  {
    "chunk_id": 11,
    "paper_filename": "adam_2024_mammography_survey.pdf",
    "paper_title": "Adam 2024 Mammography Survey",
    "chunk_index": 11,
    "total_chunks": 37,
    "text_content": "with age, which may uncover previously unseen \ufb01ndings hidden behind the tissue. The last important step is the BI-RADS assessment, Table 3. Every examination of images should result in one of seven BI-RADS assessment categories. If the \ufb01nal category J. Imaging 2023 ,9, 95 6 of 15 is 4 or 5, then a tissue biopsy for a de\ufb01nitive diagnosis is required. The biopsy result holds the most objective information about the malignancy of the suspicious \ufb01nding. Table 3. BI-RADS assessment categories. Catego",
    "full_text_length": 36865,
    "chunk_length": 1338
  },
  {
    "chunk_id": 12,
    "paper_filename": "adam_2024_mammography_survey.pdf",
    "paper_title": "Adam 2024 Mammography Survey",
    "chunk_index": 12,
    "total_chunks": 37,
    "text_content": "mammography databases that contained information about the boundaries and malignancy of \ufb01ndings. The region of interest (ROI) can be de\ufb01ned in several ways, with the most common method being a binary mask (as shown in Figure 9). The mask is an image with the same resolution as the observed image, where pixels have only two values. The \ufb01rst value (usually 0) represents the background, while the second value (usually 1 or 255) represents the ROI. Alternatively, the ROI can be de\ufb01ned by the coordin",
    "full_text_length": 36865,
    "chunk_length": 1175
  },
  {
    "chunk_id": 13,
    "paper_filename": "adam_2024_mammography_survey.pdf",
    "paper_title": "Adam 2024 Mammography Survey",
    "chunk_index": 13,
    "total_chunks": 37,
    "text_content": "datasets (excluding the OMI-DB, as the amount of data may vary) is shown in Figure 10 and Table 4, while more detailed statistics of the individual datasets are provided in Figures 11\u201313. Figure 9. Binary mask of mass \ufb01nding. Source: [11]. J. Imaging 2023 ,9, 95 7 of 15 Figure 10. Overview of the MIAS, CBIS-DDSM, and INbreast datasets. Figure 11. The MIAS database statistics. Figure 12. The CBIS-DDSM database statistics. J. Imaging 2023 ,9, 95 8 of 15 Table 4. Speci\ufb01c features of the databases w",
    "full_text_length": 36865,
    "chunk_length": 1539
  },
  {
    "chunk_id": 14,
    "paper_filename": "adam_2024_mammography_survey.pdf",
    "paper_title": "Adam 2024 Mammography Survey",
    "chunk_index": 14,
    "total_chunks": 37,
    "text_content": "asymmetriesno no yesrectangle (may contain a combination of abnormalities)FFDM highclassi\ufb01cation, detection J. Imaging 2023 ,9, 95 9 of 15 Figure 13. The INbreast database statistics. 3.1. The Mammographic Image Analysis Society Digital Mammogram Database (MIAS) The dataset [ 19] was released in 1994 and can be accessed at http://peipa.essex.ac.uk/ info/mias.html (accessed on 10 March 2022) without any registration requirement. Each \ufb01nding in the dataset includes details about the breast density",
    "full_text_length": 36865,
    "chunk_length": 1389
  },
  {
    "chunk_id": 15,
    "paper_filename": "adam_2024_mammography_survey.pdf",
    "paper_title": "Adam 2024 Mammography Survey",
    "chunk_index": 15,
    "total_chunks": 37,
    "text_content": "calci\ufb01cations. Each image abnormality is classi\ufb01ed into one of seven categories, including calci\ufb01- cation, well-de\ufb01ned/circumscribed masses, spiculated masses, other ill-de\ufb01ned masses, architectural distortion, asymmetry, and normal. The database comprises only images in the MLO view, with each patient having both a left MLO (LMLO) and a right MLO (RMLO) view, stored in Portable Gray Map (PGM) format. One disadvantage of the dataset is the low resolution of the images, which is only 1024\u00021024 pi",
    "full_text_length": 36865,
    "chunk_length": 1292
  },
  {
    "chunk_id": 16,
    "paper_filename": "adam_2024_mammography_survey.pdf",
    "paper_title": "Adam 2024 Mammography Survey",
    "chunk_index": 16,
    "total_chunks": 37,
    "text_content": "in Figure 15). These elements could have a negative impact during neural network training, and therefore, their removal should be considered. The dataset has been used in various studies, such as the work of Ragab et al. [ 22], where the authors presented a CAD (computer-aided diagnosis) system. They employed a combination of DCNN (deep convolutional neural network) and SVM (support vector machine), where the DCNN was used for deep feature extraction, which was then fed into an SVM classi\ufb01er wit",
    "full_text_length": 36865,
    "chunk_length": 1234
  },
  {
    "chunk_id": 17,
    "paper_filename": "adam_2024_mammography_survey.pdf",
    "paper_title": "Adam 2024 Mammography Survey",
    "chunk_index": 17,
    "total_chunks": 37,
    "text_content": "convolutional architectures. In their research, the VGG16 architecture stood out the most, J. Imaging 2023 ,9, 95 10 of 15 achieving an accuracy of 75.46% in determining whether a whole mammogram belongs to the normal or abnormal category. Figure 14. Artifacts pointed to by arrows, left and middle images without pectoral muscle, and middle and right images do not contain whole breasts. Source: [19]. Figure 15. Redundant elements marked with red squares and arrows. Source: [19]. 3.2.Curated Breas",
    "full_text_length": 36865,
    "chunk_length": 1329
  },
  {
    "chunk_id": 18,
    "paper_filename": "adam_2024_mammography_survey.pdf",
    "paper_title": "Adam 2024 Mammography Survey",
    "chunk_index": 18,
    "total_chunks": 37,
    "text_content": "obtained at https://wiki.cancerimagingarchive. net/pages/viewpage.action?pageId=22516629 (accessed on 10 March 2022). To download the data, an NBIA Data Retriever is required, which is specialized software for opening the TCIA (The Cancer Imaging Archive) manifest \ufb01les from the website. The CBIS-DDSM dataset is divided into two groups based on the type of abnormality, which are calci\ufb01cations and masses. Each abnormality group has a training and a testing set. The dataset provides additional deta",
    "full_text_length": 36865,
    "chunk_length": 1367
  },
  {
    "chunk_id": 19,
    "paper_filename": "adam_2024_mammography_survey.pdf",
    "paper_title": "Adam 2024 Mammography Survey",
    "chunk_index": 19,
    "total_chunks": 37,
    "text_content": "slight variations in resolution among the images. Processing the CBIS-DDSM dataset presented several challenges. Some of the dif\ufb01cul- ties encountered include: 1. Mirrored images: Some images in the dataset were mirrored, where the breast posi- tioning in the right medio-lateral oblique (MLO) image appeared to be the same as that in the left MLO image, and vice versa. 2. Inconsistent \ufb01lenames: The \ufb01lenames of the images did not always correspond with the \ufb01lenames in the accompanying description ",
    "full_text_length": 36865,
    "chunk_length": 1305
  },
  {
    "chunk_id": 20,
    "paper_filename": "adam_2024_mammography_survey.pdf",
    "paper_title": "Adam 2024 Mammography Survey",
    "chunk_index": 20,
    "total_chunks": 37,
    "text_content": "areas of the images. 5. Redundant elements in images: Similar to the MIAS database, images in the CBIS- DDSM dataset may contain redundant elements due to the acquisition with the SFM (scan-\ufb01lm mammography) technology. 6. Artifacts in mammograms: Some mammograms in the dataset may contain artifacts, which could affect the accuracy and reliability of the analysis. These challenges highlight the importance of careful preprocessing and data cleaning steps in working with medical image datasets, to ",
    "full_text_length": 36865,
    "chunk_length": 1282
  },
  {
    "chunk_id": 21,
    "paper_filename": "adam_2024_mammography_survey.pdf",
    "paper_title": "Adam 2024 Mammography Survey",
    "chunk_index": 21,
    "total_chunks": 37,
    "text_content": "indicating the presence or absence of malignant tissue. The best-performing model achieved an AUC (Area Under the ROC Curve) of 0.88 on the dataset. When averaging results from four models, an AUC of 0.91 was achieved, with a sensitivity of 86.1% and a speci\ufb01city of 80.1%. In another publication by Ragab et al. [ 22], which employed a combination of deep features and an SVM classi\ufb01er, the CBIS-DDSM dataset was also used in addition to the MIAS dataset. Experiments on the CBIS-DDSM dataset were c",
    "full_text_length": 36865,
    "chunk_length": 1226
  },
  {
    "chunk_id": 22,
    "paper_filename": "adam_2024_mammography_survey.pdf",
    "paper_title": "Adam 2024 Mammography Survey",
    "chunk_index": 22,
    "total_chunks": 37,
    "text_content": "was no longer available on the of\ufb01cial website. Mammograms in this database are stored in the DICOM format, with a resolution of either 4084 \u00023328 or 3328 \u00022560, depending on the breast size. The database includes images with various abnormalities such as microcalci\ufb01cations, masses, architectural dis- tortions, and asymmetries. Regions of Interest (ROIs) are de\ufb01ned by contour points in an XML (extensible markup language) \ufb01le. Contour annotation of the pectoral muscle is also included (refer to F",
    "full_text_length": 36865,
    "chunk_length": 1321
  },
  {
    "chunk_id": 23,
    "paper_filename": "adam_2024_mammography_survey.pdf",
    "paper_title": "Adam 2024 Mammography Survey",
    "chunk_index": 23,
    "total_chunks": 37,
    "text_content": "were obtained using FFDM technology, which avoids redundant visual information such as view and laterality. However, it is worth noting that the database does not provide any histopathological results for the \ufb01ndings, and only BI-RADS assessment is available. The absence of histopathological results is likely the greatest limitation of the dataset. The usage of the dataset can be observed in the work of Singh et al. [ 25], which focused on generating realistic binary masks using cGAN (conditiona",
    "full_text_length": 36865,
    "chunk_length": 1226
  },
  {
    "chunk_id": 24,
    "paper_filename": "adam_2024_mammography_survey.pdf",
    "paper_title": "Adam 2024 Mammography Survey",
    "chunk_index": 24,
    "total_chunks": 37,
    "text_content": "points of the pectoral muscle. Source: [11]. 3.4. OPTIMAM Medical Image Database (OMI-DB) The OMI-DB database [ 26] stands out from previous databases as it continues to grow in size every year. It collects images and information from several screening centers across the UK, including images without any suspicious \ufb01ndings. Due to the complex structure of the database, it is recommended to use the \u201comidb\u201d Python package to extract and process information from it. While the database is publicly av",
    "full_text_length": 36865,
    "chunk_length": 1316
  },
  {
    "chunk_id": 25,
    "paper_filename": "adam_2024_mammography_survey.pdf",
    "paper_title": "Adam 2024 Mammography Survey",
    "chunk_index": 25,
    "total_chunks": 37,
    "text_content": "need to specify the data they prefer to work with, such as images containing a certain type of \ufb01nding. The OMI-DB database [ 26] currently contains data from 179,326 patients (Table 5). It is important to note that one patient may have multiple malignant or benign episodes. The malignancy of a \ufb01nding is determined by biopsy. The images in the database are stored in DICOM format, and for each patient, there may be ROI information for multiple views (e.g., MLO + CC) described with bounding rectang",
    "full_text_length": 36865,
    "chunk_length": 1217
  },
  {
    "chunk_id": 26,
    "paper_filename": "adam_2024_mammography_survey.pdf",
    "paper_title": "Adam 2024 Mammography Survey",
    "chunk_index": 26,
    "total_chunks": 37,
    "text_content": "Cancer 1160 J. Imaging 2023 ,9, 95 13 of 15 One signi\ufb01cant advantage of the OMI-DB database is that it includes previous mam- mograms from screening exams, taken before the \ufb01nding had developed or was detected. This provides an opportunity to study the evolution of \ufb01ndings over time. Other bene\ufb01ts of the database include the large amount of available data and increased presence of rare \ufb01ndings such as architectural distortions and asymmetries. However, a disadvantage of the database is its compl",
    "full_text_length": 36865,
    "chunk_length": 1285
  },
  {
    "chunk_id": 27,
    "paper_filename": "adam_2024_mammography_survey.pdf",
    "paper_title": "Adam 2024 Mammography Survey",
    "chunk_index": 27,
    "total_chunks": 37,
    "text_content": "be a DCNN with the modern Ef\ufb01cientNet architecture, which achieved an accuracy of up to 85.2% (sensitivity: 74.5%, speci\ufb01city: 91.5%). 4. Conclusions In the \ufb01rst part of our article, we provided a detailed description of the mammography assessment process to provide insights into the information provided by open-access databases. The second part of our article included a comprehensive description and analysis of four databases: INbreast, MIAS, CBIS-DDSM, and OMI-DB. We also discussed the advanta",
    "full_text_length": 36865,
    "chunk_length": 1368
  },
  {
    "chunk_id": 28,
    "paper_filename": "adam_2024_mammography_survey.pdf",
    "paper_title": "Adam 2024 Mammography Survey",
    "chunk_index": 28,
    "total_chunks": 37,
    "text_content": "improve the accuracy of breast cancer diagnosis. In the future, these models have the potential to reduce the time-consuming nature of examinations by replacing the need for a second radiologist in double reading. The combined total of unique images from the described databases, excluding the OMI-DB due to the variations in data depending on the agreement, was 3801, with 4125 annotated \ufb01ndings. The vast majority of these images come from the CBIS-DDSM database. However, this database contains mu",
    "full_text_length": 36865,
    "chunk_length": 1305
  },
  {
    "chunk_id": 29,
    "paper_filename": "adam_2024_mammography_survey.pdf",
    "paper_title": "Adam 2024 Mammography Survey",
    "chunk_index": 29,
    "total_chunks": 37,
    "text_content": "individually described calci\ufb01cations, it is well-suited for addressing various machine learning detection tasks. The OMI-DB is by far the largest database containing high-quality images, but to access them, one needs to undergo a long process of project writing, evaluation, and agreement signing. A signi\ufb01cant advantage of the database is the presence of images from the patient\u2019s previous examinations, which can be used to observe dynamic changes in the breasts. The use of such mammographic data ",
    "full_text_length": 36865,
    "chunk_length": 1322
  },
  {
    "chunk_id": 30,
    "paper_filename": "adam_2024_mammography_survey.pdf",
    "paper_title": "Adam 2024 Mammography Survey",
    "chunk_index": 30,
    "total_chunks": 37,
    "text_content": "can still be achieved even with small-sized datasets. J. Imaging 2023 ,9, 95 14 of 15 Author Contributions: Conceptualization, A.M. and I.C.; methodology, A.M., I.C., and L.V .; re- sources, A.M. and L.V .; data curation, A.M. and L.V .; writing\u2014original draft preparation, A.M.; writing\u2014review and editing, A.M. and I.C.; visualization, A.M.; supervision, I.C. and L.V .; funding acquisition, I.C. All authors have read and agreed to the published version of the manuscript. Funding: This research w",
    "full_text_length": 36865,
    "chunk_length": 1514
  },
  {
    "chunk_id": 31,
    "paper_filename": "adam_2024_mammography_survey.pdf",
    "paper_title": "Adam 2024 Mammography Survey",
    "chunk_index": 31,
    "total_chunks": 37,
    "text_content": "longer available from original source; OMIDB [26]\u2014https://medphys.royalsurrey.nhs.uk/omidb/getting- access/ (accessed on 10 December 2022). Con\ufb02icts of Interest: The authors declare no con\ufb02ict of interest. References 1. Ferlay, J.; Ervik, M.; Lam, F.; Colombet, M.; Mery, L.; Pi\u00f1eros, M.; Znaor, A.; Soerjomataram, I.; Bray, F. Global Cancer Observatory: Cancer Today. 2020. Available online: https://gco.iarc.fr/today (accessed on 31 May 2022). 2. Survival Rates for Breast Cancer. 2022. Available o",
    "full_text_length": 36865,
    "chunk_length": 1636
  },
  {
    "chunk_id": 32,
    "paper_filename": "adam_2024_mammography_survey.pdf",
    "paper_title": "Adam 2024 Mammography Survey",
    "chunk_index": 32,
    "total_chunks": 37,
    "text_content": "[CrossRef] [PubMed] 7. Vinnicombe, S.; Pereira, S.M.P .; McCormack, V .A.; Shiel, S.; Perry, N.; dos Santos Silva, I.M. Full-Field Digital versus Screen-Film Mammography: Comparison within the UK Breast Screening Program and Systematic Review of Published Data. Radiology 2009 , 251, 347\u2013358. [CrossRef] [PubMed] 8. Souza, F.H.; Wendland, E.M.; Rosa, M.I.; Polanczyk, C.A. Is full-\ufb01eld digital mammography more accurate than screen-\ufb01lm mammography in overall population screening? A systematic review",
    "full_text_length": 36865,
    "chunk_length": 1410
  },
  {
    "chunk_id": 33,
    "paper_filename": "adam_2024_mammography_survey.pdf",
    "paper_title": "Adam 2024 Mammography Survey",
    "chunk_index": 33,
    "total_chunks": 37,
    "text_content": ", 5th ed.; American College of Radiology: Reston, VA, USA, 2013. 11. Moreira, I.; Amaral, I.; Domingues, I.; Cardoso, A.; Cardoso, M.; Cardoso, J. INbreast: Toward a Full-\ufb01eld Digital Mammographic Database. Acad. Radiol. 2011 ,19, 236\u2013248. [CrossRef] [PubMed] 12. Lee, R.; Gimenez, F.; Hoogi, A.; Miyake, K.; Gorovoy, M.; Rubin, D. A curated mammography data set for use in computer-aided detection and diagnosis research. Sci. Data 2017 ,4, 170\u2013177. [CrossRef] [PubMed] 13. Walker, H.K.; Hall, W.D.;",
    "full_text_length": 36865,
    "chunk_length": 1626
  },
  {
    "chunk_id": 34,
    "paper_filename": "adam_2024_mammography_survey.pdf",
    "paper_title": "Adam 2024 Mammography Survey",
    "chunk_index": 34,
    "total_chunks": 37,
    "text_content": "Smithuis, R. Bi-RADS for Mammography and Ultrasound 2013. 2014. Available online: https: //radiologyassistant.nl/breast/bi-rads/bi-rads-for-mammography-and-ultrasound-2013 (accessed on 31 May 2022). 18. Weerakkody, Y.; Murphy, A. Asymmetry (Mammography). 2021. Available online: https://radiopaedia.org/articles/asymmetry- mammography?lang=us (accessed on 31 May 2022). 19. Suckling, J. The Mammographic Image Analysis Society Digital Mammogram Database Exerpta Medica. Exerpta Medica Int. Congr. 199",
    "full_text_length": 36865,
    "chunk_length": 1500
  },
  {
    "chunk_id": 35,
    "paper_filename": "adam_2024_mammography_survey.pdf",
    "paper_title": "Adam 2024 Mammography Survey",
    "chunk_index": 35,
    "total_chunks": 37,
    "text_content": "In Applied Information Processing Systems ; Iyer, B., Ghosh, D., Balas, V .E., Eds.; Springer: Singapore, 2022; pp. 121\u2013127. 24. Shen, L.; Margolies, L.R.; Rothstein, J.H.; Fluder, E.; McBride, R.; Sieh, W. Deep learning to improve breast cancer detection on screening mammography. Sci. Rep. 2019 ,9, 12495. [CrossRef] [PubMed] 25. Singh, V .K.; Rashwan, H.A.; Romani, S.; Akram, F.; Pandey, N.; Sarker, M.M.K.; Saleh, A.; Arenas, M.; Arquez, M.; Puig, D.; et al. Breast tumor segmentation and shape ",
    "full_text_length": 36865,
    "chunk_length": 1418
  },
  {
    "chunk_id": 36,
    "paper_filename": "adam_2024_mammography_survey.pdf",
    "paper_title": "Adam 2024 Mammography Survey",
    "chunk_index": 36,
    "total_chunks": 37,
    "text_content": "for Mammography Classi\ufb01cation: An Experimental Study. Sensors 2023 ,23, 1229. [CrossRef] [PubMed] Disclaimer/Publisher\u2019s Note: The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.",
    "full_text_length": 36865,
    "chunk_length": 467
  },
  {
    "chunk_id": 37,
    "paper_filename": "anas_2025_screening_mammography_and_AI_systematic review.pdf",
    "paper_title": "Anas 2025 Screening Mammography And Ai Systematic Review",
    "chunk_index": 0,
    "total_chunks": 36,
    "text_content": "Review began 02/08/2025 Review ended 02/18/2025 Published 02/20/2025 \u00a9 Copyright 2025 Abu Abeelh et al. This is an open access article distributed under the terms of the Creative Commons Attribution License CC- BY 4.0., which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited. DOI: 10.7759/cureus.79353 Screening Mammography and Artificial Intelligence: A Comprehensive Systematic Review Enas Abu Abeelh , Zain Abuabeileh 1. ",
    "full_text_length": 38717,
    "chunk_length": 1530
  },
  {
    "chunk_id": 38,
    "paper_filename": "anas_2025_screening_mammography_and_AI_systematic review.pdf",
    "paper_title": "Anas 2025 Screening Mammography And Ai Systematic Review",
    "chunk_index": 1,
    "total_chunks": 36,
    "text_content": "in screening mammography, focusing on diagnostic performance, reduction of false positives, and support for radiologists in clinical decision-making. A systematic search was conducted across PubMed, Embase, Web of Science, Cochrane Central, and Scopus for studies published between 2013 and 2024, including those evaluating artificial intelligence in mammography screening and reporting outcomes related to cancer detection, sensitivity, specificity, and workflow optimization. A total of 13 studies ",
    "full_text_length": 38717,
    "chunk_length": 1578
  },
  {
    "chunk_id": 39,
    "paper_filename": "anas_2025_screening_mammography_and_AI_systematic review.pdf",
    "paper_title": "Anas 2025 Screening Mammography And Ai Systematic Review",
    "chunk_index": 2,
    "total_chunks": 36,
    "text_content": "the potential to enhance the efficiency and accuracy of breast cancer screening programs, and while it can reduce unnecessary recalls and alleviate radiologists' workloads, issues with false positives and demographic variations in accuracy highlight the need for further research. With ongoing refinement, artificial intelligence could become a valuable tool in routine mammography screening, augmenting radiologists' capabilities and improving patient care. Categories: Radiology, Oncology, Healthca",
    "full_text_length": 38717,
    "chunk_length": 1487
  },
  {
    "chunk_id": 40,
    "paper_filename": "anas_2025_screening_mammography_and_AI_systematic review.pdf",
    "paper_title": "Anas 2025 Screening Mammography And Ai Systematic Review",
    "chunk_index": 3,
    "total_chunks": 36,
    "text_content": "findings are incorrectly flagged as suspicious), specificity (the ability of a test to correctly identify non-cancerous cases), and sensitivity (the ability to correctly detect cancer) are essential for understanding the performance of AI models in this context. In this review, \"AI models\" refer to computer algorithms, often based on deep learning techniques, that are trained to analyze mammography images and detect patterns associated with cancer. AI\u2019s capacity to analyze vast amounts of image ",
    "full_text_length": 38717,
    "chunk_length": 1410
  },
  {
    "chunk_id": 41,
    "paper_filename": "anas_2025_screening_mammography_and_AI_systematic review.pdf",
    "paper_title": "Anas 2025 Screening Mammography And Ai Systematic Review",
    "chunk_index": 4,
    "total_chunks": 36,
    "text_content": "thorough evaluation of AI\u2019s real-world performance. A comprehensive evaluation of AI\u2019s performance in real-world settings, including its sensitivity, specificity, and accuracy, is essential to fully understand its role in enhancing mammography screening [5] . Current literature demonstrates a gap in understanding AI's ability to reduce false positives and unnecessary recalls, as well as its effectiveness in distinguishing between benign and malignant findings. Studies highlight AI\u2019s potential to",
    "full_text_length": 38717,
    "chunk_length": 1451
  },
  {
    "chunk_id": 42,
    "paper_filename": "anas_2025_screening_mammography_and_AI_systematic review.pdf",
    "paper_title": "Anas 2025 Screening Mammography And Ai Systematic Review",
    "chunk_index": 5,
    "total_chunks": 36,
    "text_content": "DOI 10.7759/cureus.79353 The objective of this systematic review is to comprehensively evaluate the application of AI in screening mammography, focusing on its ability to improve diagnostic performance, reduce false positives, and support radiologists in clinical decision-making. To achieve this, we aim to analyze various AI algorithms\u2019 sensitivity, specificity, and overall effectiveness in comparison to traditional radiologist assessments [8] . The methodology will include an exhaustive review ",
    "full_text_length": 38717,
    "chunk_length": 1449
  },
  {
    "chunk_id": 43,
    "paper_filename": "anas_2025_screening_mammography_and_AI_systematic review.pdf",
    "paper_title": "Anas 2025 Screening Mammography And Ai Systematic Review",
    "chunk_index": 6,
    "total_chunks": 36,
    "text_content": "were a combination of terms related to both \"artificial intelligence\", \"mammography\", \"breast cancer screening\", and \"machine learning\". Boolean operators (AND, OR) were applied to ensure a comprehensive search. Eligibility Criteria Studies were included in the review if they were published between January 2013 and December 2024, written in English, focused on the use of artificial intelligence in mammography screening, contained original research (excluding reviews, meta-analyses, or editorials",
    "full_text_length": 38717,
    "chunk_length": 1476
  },
  {
    "chunk_id": 44,
    "paper_filename": "anas_2025_screening_mammography_and_AI_systematic review.pdf",
    "paper_title": "Anas 2025 Screening Mammography And Ai Systematic Review",
    "chunk_index": 7,
    "total_chunks": 36,
    "text_content": "abstracts were screened for relevance by two independent reviewers. Full-text articles of potentially relevant studies were then retrieved and assessed for eligibility. Any disagreements during the selection process were resolved through discussion or consultation with a third reviewer. The search initially was carried out using five databases: PubMed (n = 400), Embase (n = 250), Web of Science (n = 210), Cochrane Central (n = 83), and Scopus (n = 300), yielding a total of 1,243 records. After r",
    "full_text_length": 38717,
    "chunk_length": 1349
  },
  {
    "chunk_id": 45,
    "paper_filename": "anas_2025_screening_mammography_and_AI_systematic review.pdf",
    "paper_title": "Anas 2025 Screening Mammography And Ai Systematic Review",
    "chunk_index": 8,
    "total_chunks": 36,
    "text_content": "Data Extraction Data extraction was performed independently by two reviewers using a standardized extraction form, and the extracted data included study characteristics (e.g., author, year, country), study design (e.g., cohort study, case-control study) where relevant, screening outcomes (e.g., cancer detection rates, recall rates, false-positive rates, specificity, sensitivity), and key findings related to artificial intelligence's performance and its role in improving mammography screening. Da",
    "full_text_length": 38717,
    "chunk_length": 1448
  },
  {
    "chunk_id": 46,
    "paper_filename": "anas_2025_screening_mammography_and_AI_systematic review.pdf",
    "paper_title": "Anas 2025 Screening Mammography And Ai Systematic Review",
    "chunk_index": 9,
    "total_chunks": 36,
    "text_content": "in the PRISMA flow diagram as shown in Figure 1 . 2025 Abu Abeelh et al. Cureus 17(2): e79353. DOI 10.7759/cureus.79353 2 of 10 FIGURE 1: PRISMA Flow Diagram of Study Selection Results The systematic review included 13 studies that comprehensively evaluated the application of AI in screening mammography. These studies assessed AI's performance in different aspects of breast cancer detection, diagnosis, workflow efficiency, and its integration into clinical practice. The included studies varied i",
    "full_text_length": 38717,
    "chunk_length": 1404
  },
  {
    "chunk_id": 47,
    "paper_filename": "anas_2025_screening_mammography_and_AI_systematic review.pdf",
    "paper_title": "Anas 2025 Screening Mammography And Ai Systematic Review",
    "chunk_index": 10,
    "total_chunks": 36,
    "text_content": "single reading was comparable to double reading by two radiologists, demonstrating AI's potential to replace a radiologist in the screening process. 2023 Artificial Intelligence (AI) for Screening Mammography, From the AJR Special Series on AI Applications Lamb et al. [11] American Journal of Roentgenology United States The study reviewed commercial AI algorithms for screening mammography, discussing their clinical applications and potential ethical considerations. The study highlighted the need",
    "full_text_length": 38717,
    "chunk_length": 1450
  },
  {
    "chunk_id": 48,
    "paper_filename": "anas_2025_screening_mammography_and_AI_systematic review.pdf",
    "paper_title": "Anas 2025 Screening Mammography And Ai Systematic Review",
    "chunk_index": 11,
    "total_chunks": 36,
    "text_content": "of the American United Most radiologists expressed interest in using AI if its sensitivity and specificity were balanced. However, radiologists emphasized the importance of using AI 2022 2025 Abu Abeelh et al. Cureus 17(2): e79353. DOI 10.7759/cureus.79353 3 of 10 During Screening Mammography Interpretation al. [13] College of Radiology States tools that complement their work, rather than fully replacing radiologists, to maintain diagnostic accuracy. Use of Artificial Intelligence for Reducing U",
    "full_text_length": 38717,
    "chunk_length": 1464
  },
  {
    "chunk_id": 49,
    "paper_filename": "anas_2025_screening_mammography_and_AI_systematic review.pdf",
    "paper_title": "Anas 2025 Screening Mammography And Ai Systematic Review",
    "chunk_index": 12,
    "total_chunks": 36,
    "text_content": "radiologists with and without AI assistance. However, the AI-CAD system improved specificity and accuracy while reducing recall rates, demonstrating its potential to optimize screening workflows. 2024 AI-Based CAD in Mammographic Interpretation Workflow Yoon et al. [4] European Journal of Radiology Open England AI-CAD detected 17.9% additional cancers that were initially missed by radiologists. However, it increased recall rates and flagged 89.0% of marks as false positives, indicating the need ",
    "full_text_length": 38717,
    "chunk_length": 1421
  },
  {
    "chunk_id": 50,
    "paper_filename": "anas_2025_screening_mammography_and_AI_systematic review.pdf",
    "paper_title": "Anas 2025 Screening Mammography And Ai Systematic Review",
    "chunk_index": 13,
    "total_chunks": 36,
    "text_content": "Mammography Zouzos et al. [17] JMIR Publications Canada The study found that AI systems flagged a higher proportion of women with previous benign biopsy findings, indicating that prior biopsy data should be considered in AI model training to prevent unnecessary recalls in future screenings. 2023 External Validation of AI Algorithms for Automated Interpretation of Screening Mammography Anderson et al. [18] Journal of the American College of Radiology United States Independent validation studies s",
    "full_text_length": 38717,
    "chunk_length": 1411
  },
  {
    "chunk_id": 51,
    "paper_filename": "anas_2025_screening_mammography_and_AI_systematic review.pdf",
    "paper_title": "Anas 2025 Screening Mammography And Ai Systematic Review",
    "chunk_index": 14,
    "total_chunks": 36,
    "text_content": "where AI exhibited higher sensitivity. 2020 Use of Novel AI-Based CAD for Screening Mammography Heywang- Kobrunner et al. [20] Acta Radiologica England The AI system achieved similar cancer detection rates to human readers but had lower specificity. Combining human and AI interpretations increased sensitivity, but required consensus readings for more cases, reducing the time saved by AI automation. 2023 AI for Interval Breast Cancer Detection at Screening Mammography Nanaa et al. [21] Radiology ",
    "full_text_length": 38717,
    "chunk_length": 1370
  },
  {
    "chunk_id": 52,
    "paper_filename": "anas_2025_screening_mammography_and_AI_systematic review.pdf",
    "paper_title": "Anas 2025 Screening Mammography And Ai Systematic Review",
    "chunk_index": 15,
    "total_chunks": 36,
    "text_content": "that AI may enhance the diagnostic performance of screening mammography. Several studies, such as Dembrower et al. (2023) and Kim et al. (2022), reported that AI- assisted readings were associated with reduced false positives, lower recall rates, and improved specificity, which could potentially decrease the workload for radiologists [10,14] . In particular, the study by Kim et al. (2022) observed that AI use in screening mammography was linked to a reduction in unnecessary recalls without compr",
    "full_text_length": 38717,
    "chunk_length": 1370
  },
  {
    "chunk_id": 53,
    "paper_filename": "anas_2025_screening_mammography_and_AI_systematic review.pdf",
    "paper_title": "Anas 2025 Screening Mammography And Ai Systematic Review",
    "chunk_index": 16,
    "total_chunks": 36,
    "text_content": "study Low Well-designed; robust methodology with clear outcome measures and minimal confounding. Lamb et al. (2022) [11] Review of commercial AI algorithms Narrative review Moderate Comprehensive review; however, lacks a formal bias assessment and detailed protocol registration. Ongena et al. (2021) [12] Population survey of women's preferences Survey study Low Clear methodology and sampling; potential self-report bias minimized by large sample size. Hendrix et al. (2022) [13] Radiologist prefer",
    "full_text_length": 38717,
    "chunk_length": 1489
  },
  {
    "chunk_id": 54,
    "paper_filename": "anas_2025_screening_mammography_and_AI_systematic review.pdf",
    "paper_title": "Anas 2025 Screening Mammography And Ai Systematic Review",
    "chunk_index": 17,
    "total_chunks": 36,
    "text_content": "Observational study Moderate Increased recall rates and high false positives; potential bias in patient selection noted. Zeng et al. (2024) [16] Review of AI errors in reading screening mammography Systematic review Moderate Provides a systematic review; however, reporting on certain error types is limited. Zouzos et al. (2023) [17] Effect of benign biopsy findings on AI-based cancer detection Retrospective case- control study Low Clear design with adequate control for confounders; retrospective",
    "full_text_length": 38717,
    "chunk_length": 1471
  },
  {
    "chunk_id": 55,
    "paper_filename": "anas_2025_screening_mammography_and_AI_systematic review.pdf",
    "paper_title": "Anas 2025 Screening Mammography And Ai Systematic Review",
    "chunk_index": 18,
    "total_chunks": 36,
    "text_content": "limitations in consensus reading reported. Nanaa et al. (2024) [21] AI for interval breast cancer detection Observational study Moderate Improved detection in missed cases but limited by suboptimal lesion localization; moderate overall risk. TABLE 2: Risk of Bias Assessment for Included Studies CAD: computer-aided detection A structured risk of bias assessment was conducted for each included study using pre-defined criteria covering factors such as study design, patient selection, performance of",
    "full_text_length": 38717,
    "chunk_length": 1376
  },
  {
    "chunk_id": 56,
    "paper_filename": "anas_2025_screening_mammography_and_AI_systematic review.pdf",
    "paper_title": "Anas 2025 Screening Mammography And Ai Systematic Review",
    "chunk_index": 19,
    "total_chunks": 36,
    "text_content": "(2024) reviewed the types of errors made by AI systems, emphasizing that false positives and false negatives were still significant concerns, particularly at 2025 Abu Abeelh et al. Cureus 17(2): e79353. DOI 10.7759/cureus.79353 6 of 10 lower positivity thresholds [16] . False positives, in particular, increased the number of unnecessary recalls, as evidenced by Yoon et al. (2023), who reported that 89% of AI-detected abnormalities were ultimately benign [4] . Despite these challenges, AI consist",
    "full_text_length": 38717,
    "chunk_length": 1390
  },
  {
    "chunk_id": 57,
    "paper_filename": "anas_2025_screening_mammography_and_AI_systematic review.pdf",
    "paper_title": "Anas 2025 Screening Mammography And Ai Systematic Review",
    "chunk_index": 20,
    "total_chunks": 36,
    "text_content": "particularly those that complement their review process [12,13] . These findings underscore AI's promise in improving the efficiency and accuracy of breast cancer screening but also highlight the need for continued refinement in reducing error rates and addressing public and professional concerns about full AI implementation. Further prospective studies and external validation are required to optimize AI performance and integrate it into routine clinical practice. Discussion The results of this ",
    "full_text_length": 38717,
    "chunk_length": 1490
  },
  {
    "chunk_id": 58,
    "paper_filename": "anas_2025_screening_mammography_and_AI_systematic review.pdf",
    "paper_title": "Anas 2025 Screening Mammography And Ai Systematic Review",
    "chunk_index": 21,
    "total_chunks": 36,
    "text_content": "specificity of breast cancer detection in screening mammography. For instance, McKinney et al. (2020) conducted an extensive international evaluation of an AI system and reported that its sensitivity and specificity were comparable to, and in some settings even exceeded, those of experienced radiologists [3] . Similarly, Lauritzen et al. (2022) found that incorporating AI into the screening process maintained diagnostic accuracy while reducing false-positive rates [7] . These findings are furthe",
    "full_text_length": 38717,
    "chunk_length": 1423
  },
  {
    "chunk_id": 59,
    "paper_filename": "anas_2025_screening_mammography_and_AI_systematic review.pdf",
    "paper_title": "Anas 2025 Screening Mammography And Ai Systematic Review",
    "chunk_index": 22,
    "total_chunks": 36,
    "text_content": "by Ongena et al. (2021), many women expressed a preference for human radiologist involvement due to concerns about trust and communication; they felt that a human expert could better explain uncertainties and offer empathetic support during the screening process [12] . Similarly, radiologists in Hendrix et al. (2022) highlighted that a fully automated system might overlook the nuanced interpretation of imaging findings and individual patient histories, which are critical in complex cases [13] . ",
    "full_text_length": 38717,
    "chunk_length": 1348
  },
  {
    "chunk_id": 60,
    "paper_filename": "anas_2025_screening_mammography_and_AI_systematic review.pdf",
    "paper_title": "Anas 2025 Screening Mammography And Ai Systematic Review",
    "chunk_index": 23,
    "total_chunks": 36,
    "text_content": "In the study by Kim et al. (2022), which involved 793 women recalled for supplemental mammographic views, the reader-averaged recall rate decreased significantly from 60.4% (95% CI, 57.8%-62.9%) to 49.5% (95% CI, 46.5%-52.4%) with AI aid (p < 0.001), while sensitivity for cancer detection remained comparable [14] . In addition, Zouzos et al. (2023) assessed the impact of prior benign biopsy findings on AI performance and found that the AI system flagged 3.5% of healthy women without a benign bio",
    "full_text_length": 38717,
    "chunk_length": 1277
  },
  {
    "chunk_id": 61,
    "paper_filename": "anas_2025_screening_mammography_and_AI_systematic review.pdf",
    "paper_title": "Anas 2025 Screening Mammography And Ai Systematic Review",
    "chunk_index": 24,
    "total_chunks": 36,
    "text_content": "aligned with this trend. Yoon et al. (2023) reported that although AI improved cancer detection rates, it also increased the recall rate significantly [4] . In this study, 89.0% of the artificial intelligence-based computer-aided detection (AI-CAD) marks were observed on negative examinations. This high recall rate can be attributed to the algorithm's prioritization of sensitivity over specificity; the preset threshold (abnormality score \u226510%) led the system to flag subtle findings that radiolog",
    "full_text_length": 38717,
    "chunk_length": 1396
  },
  {
    "chunk_id": 62,
    "paper_filename": "anas_2025_screening_mammography_and_AI_systematic review.pdf",
    "paper_title": "Anas 2025 Screening Mammography And Ai Systematic Review",
    "chunk_index": 25,
    "total_chunks": 36,
    "text_content": "instance, Lee et al. (2024) found that the diagnostic performance of radiologists did not significantly differ when AI assistance was provided, raising questions about AI\u2019s actual value in real-world clinical settings [15] . Similarly, Nanaa et al. (2024) found that while AI could detect interval cancers missed by radiologists, its localization accuracy remained suboptimal, suggesting that AI may require further fine-tuning, particularly in lesion detection and characterization [21] . These disc",
    "full_text_length": 38717,
    "chunk_length": 1410
  },
  {
    "chunk_id": 63,
    "paper_filename": "anas_2025_screening_mammography_and_AI_systematic review.pdf",
    "paper_title": "Anas 2025 Screening Mammography And Ai Systematic Review",
    "chunk_index": 26,
    "total_chunks": 36,
    "text_content": "to significant paradigm shifts in how screening programs are structured. For example, Lauritzen et al. (2022) propose that AI could enable more personalized screening strategies, where the frequency of mammograms is tailored to an individual's risk profile, potentially reducing over-screening and its associated harm [7] . Practical Applications and Future Directions From a practical perspective, AI's role in reducing the workload of radiologists is one of its most promising applications. Several",
    "full_text_length": 38717,
    "chunk_length": 1376
  },
  {
    "chunk_id": 64,
    "paper_filename": "anas_2025_screening_mammography_and_AI_systematic review.pdf",
    "paper_title": "Anas 2025 Screening Mammography And Ai Systematic Review",
    "chunk_index": 27,
    "total_chunks": 36,
    "text_content": "45% to 53% of women at low risk could be safely excluded from further radiologist review [6] . This reduction in workload not only streamlines the screening process but also allows radiologists to focus their expertise on more complex or ambiguous cases, thereby improving diagnostic efficiency and potentially alleviating burnout--a growing concern in the field. Despite its promise, the full clinical adoption of AI in mammography screening is hindered by several practical, regulatory, and ethical",
    "full_text_length": 38717,
    "chunk_length": 1375
  },
  {
    "chunk_id": 65,
    "paper_filename": "anas_2025_screening_mammography_and_AI_systematic review.pdf",
    "paper_title": "Anas 2025 Screening Mammography And Ai Systematic Review",
    "chunk_index": 28,
    "total_chunks": 36,
    "text_content": "potential for algorithmic bias, and the lack of transparency in AI decision-making processes remain significant barriers, as noted by Retson and Eghtedari (2023) [24] . Additionally, as Zeng et al. (2024) pointed out, AI systems are not immune to errors, and accountability remains ambiguous-whether it lies with the AI developer, the healthcare provider, or the radiologist [16] . Together, these challenges highlight the need for comprehensive strategies that address both regulatory and operationa",
    "full_text_length": 38717,
    "chunk_length": 1414
  },
  {
    "chunk_id": 66,
    "paper_filename": "anas_2025_screening_mammography_and_AI_systematic review.pdf",
    "paper_title": "Anas 2025 Screening Mammography And Ai Systematic Review",
    "chunk_index": 29,
    "total_chunks": 36,
    "text_content": "near future, its role as an adjunctive tool appears promising and could lead to substantial improvements in breast cancer screening programs. Future research should focus on refining AI algorithms to reduce false-positive rates, standardizing validation protocols across institutions, and addressing the ethical and practical challenges associated with AI adoption in clinical settings. Additionally, emerging evidence suggests that AI can markedly decrease the number of mammograms requiring radiolo",
    "full_text_length": 38717,
    "chunk_length": 1441
  },
  {
    "chunk_id": 67,
    "paper_filename": "anas_2025_screening_mammography_and_AI_systematic review.pdf",
    "paper_title": "Anas 2025 Screening Mammography And Ai Systematic Review",
    "chunk_index": 30,
    "total_chunks": 36,
    "text_content": "of the manuscript for important intellectual content: Zain Abuabeileh Disclosures Conflicts of interest: In compliance with the ICMJE uniform disclosure form, all authors declare the following: Payment/services info: All authors have declared that no financial support was received from any organization for the submitted work. Financial relationships: All authors have declared that they have no financial relationships at present or within the previous three years with any organizations that might",
    "full_text_length": 38717,
    "chunk_length": 1373
  },
  {
    "chunk_id": 68,
    "paper_filename": "anas_2025_screening_mammography_and_AI_systematic review.pdf",
    "paper_title": "Anas 2025 Screening Mammography And Ai Systematic Review",
    "chunk_index": 31,
    "total_chunks": 36,
    "text_content": "screening mammography: systematic review and meta-analysis . Radiology. 2022, 302:88-104. 10.1148/radiol.2021210391 3 . McKinney SM, Sieniek M, Godbole V, et al.: International evaluation of an AI system for breast cancer screening . Nature. 2020, 577:89-94. 10.1038/s41586-019-1799-6 4 . Yoon JH, Han K, Suh HJ, Youk JH, Lee SE, Kim EK: Artificial intelligence-based computer-assisted detection/diagnosis (AI-CAD) for screening mammography: Outcomes of AI-CAD in the mammographic interpretation work",
    "full_text_length": 38717,
    "chunk_length": 1474
  },
  {
    "chunk_id": 69,
    "paper_filename": "anas_2025_screening_mammography_and_AI_systematic review.pdf",
    "paper_title": "Anas 2025 Screening Mammography And Ai Systematic Review",
    "chunk_index": 32,
    "total_chunks": 36,
    "text_content": "et al.: An artificial intelligence-based mammography screening protocol for breast cancer: Outcome and radiologist workload . Radiology. 2022, 304:41-9. 10.1148/radiol.210948 8 . Strohm L, Hehakaya C, Ranschaert ER, Boon WP, Moors EH: Implementation of artificial intelligence (AI) applications in radiology: hindering and facilitating factors . Eur Radiol. 2020, 30:5525-32. 10.1007/s00330- 020-06946-y 9 . Yue W, Wang Z, Chen H, Payne A, Liu X: Machine learning with applications in breast cancer d",
    "full_text_length": 38717,
    "chunk_length": 1444
  },
  {
    "chunk_id": 70,
    "paper_filename": "anas_2025_screening_mammography_and_AI_systematic review.pdf",
    "paper_title": "Anas 2025 Screening Mammography And Ai Systematic Review",
    "chunk_index": 33,
    "total_chunks": 36,
    "text_content": "Ongena YP, Yakar D, Haan M, Kwee TC: Artificial intelligence in screening mammography: A population survey of women's preferences . J Am Coll Radiol. 2021, 18:79-86. 10.1016/j.jacr.2020.09.042 13 . Hendrix N, Lowry KP, Elmore JG, et al.: Radiologist preferences for artificial intelligence-based decision support during screening mammography interpretation . J Am Coll Radiol. 2022, 19:1098-110. 10.1016/j.jacr.2022.06.019 14 . Kim YS, Jang MJ, Lee SH, et al.: Use of artificial intelligence for redu",
    "full_text_length": 38717,
    "chunk_length": 1428
  },
  {
    "chunk_id": 71,
    "paper_filename": "anas_2025_screening_mammography_and_AI_systematic review.pdf",
    "paper_title": "Anas 2025 Screening Mammography And Ai Systematic Review",
    "chunk_index": 34,
    "total_chunks": 36,
    "text_content": "207:1-13. 10.1007/s10549-024-07353-3 17 . Zouzos A, Milovanovic A, Dembrower K, Strand F: Effect of Benign Biopsy Findings on an Artificial Intelligence-Based Cancer Detector in Screening Mammography: Retrospective Case-Control Study . JMIR AI. 2023, 2:e48123. 10.2196/48123 18 . Anderson AW, Marinovich ML, Houssami N, et al.: Independent external validation of artificial intelligence algorithms for automated interpretation of screening mammography: A systematic review . J Am Coll Radiol. 2022, 1",
    "full_text_length": 38717,
    "chunk_length": 1521
  },
  {
    "chunk_id": 72,
    "paper_filename": "anas_2025_screening_mammography_and_AI_systematic review.pdf",
    "paper_title": "Anas 2025 Screening Mammography And Ai Systematic Review",
    "chunk_index": 35,
    "total_chunks": 36,
    "text_content": "Accuracy of an artificial intelligence system for interval breast cancer detection at screening mammography . Radiology. 2024, 312:e232303. 10.1148/radiol.232303 22 . Braithwaite D, Karanth SD, Divaker J, et al.: Evaluating ChatGPT\u2019s accuracy in providing screening mammography recommendations among older women: Artificial intelligence and cancer communication [PREPRINT] . Res Sq. 2024, 10.21203/rs.3.rs-3911155/v1 23 . Eisemann N, Bunk S, Mukama T, et al.: Nationwide real-world implementation of ",
    "full_text_length": 38717,
    "chunk_length": 940
  },
  {
    "chunk_id": 73,
    "paper_filename": "ankanderia_2024_intelligent_visual_encoder_for_bridging_modalities_in_report_generation.pdf",
    "paper_title": "Ankanderia 2024 Intelligent Visual Encoder For Bridging Modalities In Report Generation",
    "chunk_index": 0,
    "total_chunks": 51,
    "text_content": "InVERGe: Intelligent Visual Encoder for Bridging Modalities in Report Generation Ankan Deria1, Komal Kumar1, Snehashis Chakraborty1, Dwarikanath Mahapatra2, Sudipta Roy1,* 1Artificial Intelligence & Data Science, Jio Institute, Navi Mumbai-410206, India. 2Inception Institute of Artificial Intelligence, UAE. {ankanderia01, dmahapatra }@gmail.com {Komal2.Kumar, Snehashis1.C, sudipta1.roy }@jioinstitute.edu.in Abstract Medical image captioning plays an important role in modern healthcare, improving",
    "full_text_length": 50884,
    "chunk_length": 1510
  },
  {
    "chunk_id": 74,
    "paper_filename": "ankanderia_2024_intelligent_visual_encoder_for_bridging_modalities_in_report_generation.pdf",
    "paper_title": "Ankanderia 2024 Intelligent Visual Encoder For Bridging Modalities In Report Generation",
    "chunk_index": 1,
    "total_chunks": 51,
    "text_content": "Given the high computational demands of end-to-end model training, we introduce a two-step training methodol- ogy with an Intelligent Visual Encoder for Bridging Modal- ities in Report Generation (InVERGe) model. This model incorporates a lightweight transformer known as the Cross- Modal Query Fusion Layer (CMQFL), which utilizes the output from a frozen encoder to identify the most relevant text-grounded image embedding. This layer bridges the gap between the encoder and decoder, significantly ",
    "full_text_length": 50884,
    "chunk_length": 1410
  },
  {
    "chunk_id": 75,
    "paper_filename": "ankanderia_2024_intelligent_visual_encoder_for_bridging_modalities_in_report_generation.pdf",
    "paper_title": "Ankanderia 2024 Intelligent Visual Encoder For Bridging Modalities In Report Generation",
    "chunk_index": 2,
    "total_chunks": 51,
    "text_content": "emerged Figure 1. Illustration of two sets of sample reports produced by the InVERGe model, alongside their corresponding ground truth reports for comparison. The matched text is highlighted in the same color, underscoring the alignment between the predicted and actual reports. as a valuable solution, reducing workload, reducing diag- nostic errors and streamlining clinic workflow. In this pa- per, the main objective is to create clear reports about the image\u2019s content. This work typically follo",
    "full_text_length": 50884,
    "chunk_length": 1344
  },
  {
    "chunk_id": 76,
    "paper_filename": "ankanderia_2024_intelligent_visual_encoder_for_bridging_modalities_in_report_generation.pdf",
    "paper_title": "Ankanderia 2024 Intelligent Visual Encoder For Bridging Modalities In Report Generation",
    "chunk_index": 3,
    "total_chunks": 51,
    "text_content": "the medical images due to slight differences between medi- cal images which make it a challenging task. Also in med- ical imaging [10, 28, 41, 42], capturing pixel-level details such as colour is unnecessary but region, intensity and other details are important. That\u2019s why we need a powerful ViT, trained on medical images that will extract high-quality fea- tures that can help the decoder generate reports. Therefore, we employ a Self-Supervised Joint-Embedding Predictive Architecture like - I-JE",
    "full_text_length": 50884,
    "chunk_length": 1325
  },
  {
    "chunk_id": 77,
    "paper_filename": "ankanderia_2024_intelligent_visual_encoder_for_bridging_modalities_in_report_generation.pdf",
    "paper_title": "Ankanderia 2024 Intelligent Visual Encoder For Bridging Modalities In Report Generation",
    "chunk_index": 4,
    "total_chunks": 51,
    "text_content": "this architecture is to predict various target sections within the image. Since the encoder has not seen texts during their unimodal pretraining and the decoder has also not seen the images during their unimodal pretraining, it becomes challenging to merge them and attain effective alignment between vi- sion and language in our task. To bridge this modality gap requires tighter integration of visual and text representa- tions, We incorporate an intermediary CMQFL layer to ob- tain text-grounded ",
    "full_text_length": 50884,
    "chunk_length": 1324
  },
  {
    "chunk_id": 78,
    "paper_filename": "ankanderia_2024_intelligent_visual_encoder_for_bridging_modalities_in_report_generation.pdf",
    "paper_title": "Ankanderia 2024 Intelligent Visual Encoder For Bridging Modalities In Report Generation",
    "chunk_index": 5,
    "total_chunks": 51,
    "text_content": "enhances its performance, ultimately acquiring the capability to enhance image-based text gener- ation, as demonstrated in Figure 1. The key contributions of this work are summarized as : \u2022 Our image-grounded text generation pre-training em- ploys a self-supervised image representation task to en- hance semantic depth. This involves predicting missing information using pixel reconstruction in an abstract rep- resentation space without reliance on external knowledge or transformations. The improv",
    "full_text_length": 50884,
    "chunk_length": 1427
  },
  {
    "chunk_id": 79,
    "paper_filename": "ankanderia_2024_intelligent_visual_encoder_for_bridging_modalities_in_report_generation.pdf",
    "paper_title": "Ankanderia 2024 Intelligent Visual Encoder For Bridging Modalities In Report Generation",
    "chunk_index": 6,
    "total_chunks": 51,
    "text_content": "we have used a mech- anism that validates image regions corresponding to the report. This involves plotting attention map features from the encoder alongside the CMQFL layer and also plotting the attention maps for individual words. 2. Related work 2.1. Image grounded text generation This task involves generating descriptive sentences for a given image. However, medical report generation is more challenging than image captioning and reports are usually much longer than captions. Several approach",
    "full_text_length": 50884,
    "chunk_length": 1358
  },
  {
    "chunk_id": 80,
    "paper_filename": "ankanderia_2024_intelligent_visual_encoder_for_bridging_modalities_in_report_generation.pdf",
    "paper_title": "Ankanderia 2024 Intelligent Visual Encoder For Bridging Modalities In Report Generation",
    "chunk_index": 7,
    "total_chunks": 51,
    "text_content": "Bootstrap- ping Language-Image Pre-training (BLIP) [30] and Beit [49]. Over the years, several pre-training objectives have been put forth, gradually coalescing around a select few proven approaches. These include image-text contrastive learning [29, 39, 55], image-text matching in Align be- fore fuse [29] and Vlmo [8] and (masked) language mod- elling [29, 49, 57]. BLIP-2 [32] has a Q-Former that uses a frozen visual encoder and then enables zero-shot image-to- text generation through a frozen ",
    "full_text_length": 50884,
    "chunk_length": 1317
  },
  {
    "chunk_id": 81,
    "paper_filename": "ankanderia_2024_intelligent_visual_encoder_for_bridging_modalities_in_report_generation.pdf",
    "paper_title": "Ankanderia 2024 Intelligent Visual Encoder For Bridging Modalities In Report Generation",
    "chunk_index": 8,
    "total_chunks": 51,
    "text_content": "human intelligence, several papers ap- plied the attention mechanism [35, 56]. The paper [20] em- ploys segmentation models like UNet and TorchXRayVi- sion to extract features from segmented regions. These fea- tures are then concatenated to form comprehensive image features. R2Gen [13] utilises a memory-based Transformer architecture, allowing it to remember important informa- tion from earlier in the report and uses a special method to include this memory in the report generation process. A re",
    "full_text_length": 50884,
    "chunk_length": 1367
  },
  {
    "chunk_id": 82,
    "paper_filename": "ankanderia_2024_intelligent_visual_encoder_for_bridging_modalities_in_report_generation.pdf",
    "paper_title": "Ankanderia 2024 Intelligent Visual Encoder For Bridging Modalities In Report Generation",
    "chunk_index": 9,
    "total_chunks": 51,
    "text_content": "can provide valuable support for generating radiology reports. CXR-RePaiR [19] ad- dresses the problem of medically inconsistent information in reports using text-grounded image labels. It labels a report that has the highest cosine similarity in CLIP [39] text embeddings with CLIP image embedding. To calcu- late this similarity score, two pre-trained singular modal- ity encoders are used. The HReMRG-MR [54] model 2029 used reinforcement learning after the decoder to penalize the incorrectly pre",
    "full_text_length": 50884,
    "chunk_length": 1368
  },
  {
    "chunk_id": 83,
    "paper_filename": "ankanderia_2024_intelligent_visual_encoder_for_bridging_modalities_in_report_generation.pdf",
    "paper_title": "Ankanderia 2024 Intelligent Visual Encoder For Bridging Modalities In Report Generation",
    "chunk_index": 10,
    "total_chunks": 51,
    "text_content": "and integrate the subsequent and prior knowledge to create the report. 2.3. Multimodal Task After the evolution of the LLMs, researchers used this for visual language modelling. LLMs have demonstrated the ability to master novel tasks. They exhibit distinct be- haviours and remarkable emergent abilities, like GPT-3\u2019s [9] proficiency in few-shot learning, compared to smaller models like BERT [17] and GPT-2 [38]. Recent LLMs such as GPT-3 [9], PaLM [4, 15], LLaMA [44, 45], Vi- cuna [59], GPT-4 [2]",
    "full_text_length": 50884,
    "chunk_length": 1373
  },
  {
    "chunk_id": 84,
    "paper_filename": "ankanderia_2024_intelligent_visual_encoder_for_bridging_modalities_in_report_generation.pdf",
    "paper_title": "Ankanderia 2024 Intelligent Visual Encoder For Bridging Modalities In Report Generation",
    "chunk_index": 11,
    "total_chunks": 51,
    "text_content": "applications. 2.4. Masked Image Modelling Masked image modelling (MIM) has made significant progress in parallel with masked language modelling (MLM) [9, 12, 22, 46] tasks in NLP, although initially in a less prominent position. Pioneering efforts, such as context encoder methods and Contrastive Predictive Coding (CPC) [22, 46], predict masked areas and missing pixels in images. Modern vision transformers such as ViT [18], and BEiT [7] have revived this approach with innovative design elements i",
    "full_text_length": 50884,
    "chunk_length": 1373
  },
  {
    "chunk_id": 85,
    "paper_filename": "ankanderia_2024_intelligent_visual_encoder_for_bridging_modalities_in_report_generation.pdf",
    "paper_title": "Ankanderia 2024 Intelligent Visual Encoder For Bridging Modalities In Report Generation",
    "chunk_index": 12,
    "total_chunks": 51,
    "text_content": "image embeddings, and our de-coder Vicuna, which draws its inspiration from the LLAMA model.Our model architecture operates in two distinct train- ing stages. Initially, the pretraining stage involves training the encoder once for fine-tuning. In the first stage, we train the CMQFL layer with a frozen image encoder which is trained for different tasks and the frozen decoder performs pre-training using pairs of images and corresponding re- ports. Lastly, we finetune the LLM according to the outpu",
    "full_text_length": 50884,
    "chunk_length": 1264
  },
  {
    "chunk_id": 86,
    "paper_filename": "ankanderia_2024_intelligent_visual_encoder_for_bridging_modalities_in_report_generation.pdf",
    "paper_title": "Ankanderia 2024 Intelligent Visual Encoder For Bridging Modalities In Report Generation",
    "chunk_index": 13,
    "total_chunks": 51,
    "text_content": "of that model. Context EncoderPredictor Target Encoder Masked Image patches Visible Context patches Embedding IB of T arget Encoder Prediction of IB Figure 2. Visual representation of the initial step of training the encoder component of the InVERGe model. In Figure 2, after masking the patches in block-wise, it uses the unmasked visible block to predict the originating of the interested blocks by the TE ( f\u00af\u03b8). The CE ( f\u03b8) is also a ViT that only processes the visible context patches. The TE (",
    "full_text_length": 50884,
    "chunk_length": 1167
  },
  {
    "chunk_id": 87,
    "paper_filename": "ankanderia_2024_intelligent_visual_encoder_for_bridging_modalities_in_report_generation.pdf",
    "paper_title": "Ankanderia 2024 Intelligent Visual Encoder For Bridging Modalities In Report Generation",
    "chunk_index": 14,
    "total_chunks": 51,
    "text_content": "en- coder and the Prediction of those blocks by the Predictor. In this scenario, we employ gradient methods to fine- tune the parameters of both the predictor f\u03d5and the CE (f\u03b8). Simultaneously, the parameters of TE ( f\u00af\u03b8) are con- tinuously adjusted, achieved by applying an exponential moving average (EMA) technique to the parameters of the context encoder. Adopting an EMA strategy for target en- coders is essential in achieving effective training results for joint embedding architectures (JEA) ",
    "full_text_length": 50884,
    "chunk_length": 1324
  },
  {
    "chunk_id": 88,
    "paper_filename": "ankanderia_2024_intelligent_visual_encoder_for_bridging_modalities_in_report_generation.pdf",
    "paper_title": "Ankanderia 2024 Intelligent Visual Encoder For Bridging Modalities In Report Generation",
    "chunk_index": 15,
    "total_chunks": 51,
    "text_content": "pixel information, thereby so- lidifying its pivotal role as an invaluable component within our model\u2019s encoder. After completing the training of the entire model, we adopt the target encoder as our primary encoder for the pro- posed InVERGe model. 3.2. Text Grounded Image Embedding We introduce a CMQFL layer using BERT architecture be- tween the encoder and decoder. This CMQFL layer con- sists of learnable query embeddings, enabling interactions among queries through self-attention and with ima",
    "full_text_length": 50884,
    "chunk_length": 1281
  },
  {
    "chunk_id": 89,
    "paper_filename": "ankanderia_2024_intelligent_visual_encoder_for_bridging_modalities_in_report_generation.pdf",
    "paper_title": "Ankanderia 2024 Intelligent Visual Encoder For Bridging Modalities In Report Generation",
    "chunk_index": 16,
    "total_chunks": 51,
    "text_content": "compared to the frozen image features (e.g., 257\u00d71280 in the encoder). This design, combined with our training three objectives as shown in Figure 4, encourages the query tokens to extract the most relevant visual insights from the image embedding for the text generation. We pre-train text-grounded image embeddings using three key objectives: multimodal contrastive learning (MCL), masked language modelling (MLM), and enhanc- ing multi-modality matching (MMM) through the imple- mentation of batch",
    "full_text_length": 50884,
    "chunk_length": 1388
  },
  {
    "chunk_id": 90,
    "paper_filename": "ankanderia_2024_intelligent_visual_encoder_for_bridging_modalities_in_report_generation.pdf",
    "paper_title": "Ankanderia 2024 Intelligent Visual Encoder For Bridging Modalities In Report Generation",
    "chunk_index": 17,
    "total_chunks": 51,
    "text_content": "Figure 4. Model architecture of CMQFL layer and decoder at first stage training of CMQFL layer. We jointly train the model using three objective functions that enforce the query tokens to extract the most relevant visual information for the text. Since Zencompasses multiple output embeddings, each corresponding to a different query, we evaluate the similar- ity between each query output and tclsby calculating pair- wise similarities. The highest similarity is then identified as the image-text si",
    "full_text_length": 50884,
    "chunk_length": 1338
  },
  {
    "chunk_id": 91,
    "paper_filename": "ankanderia_2024_intelligent_visual_encoder_for_bridging_modalities_in_report_generation.pdf",
    "paper_title": "Ankanderia 2024 Intelligent Visual Encoder For Bridging Modalities In Report Generation",
    "chunk_index": 18,
    "total_chunks": 51,
    "text_content": "to each item within a batch to facilitate the loss calcula- tion. The target vector is specifically designed to match the number of items in the batch, ensuring that each item has a distinct target value. This allows us to calculate losses effi- ciently and accurately during training. The objective func- tion (Lmcl) is defined as the cross-entropy loss ( CE) be- tween Yfand sim : Lmcl=1 2[CE(Yf,sim(Q, T)) +CE(Yf,sim(T, Q))] (3) Masked Language Modelling (MLM) : Due to the CMQFL layer\u2019s architect",
    "full_text_length": 50884,
    "chunk_length": 1270
  },
  {
    "chunk_id": 92,
    "paper_filename": "ankanderia_2024_intelligent_visual_encoder_for_bridging_modalities_in_report_generation.pdf",
    "paper_title": "Ankanderia 2024 Intelligent Visual Encoder For Bridging Modalities In Report Generation",
    "chunk_index": 19,
    "total_chunks": 51,
    "text_content": "tokens. Therefore, the queries are forced to extract visual features that can produce the masked tokens of the text. For that we use cross-entropy loss to improve its text genera- tion ability, ensuring that it can efficiently and accurately generate text autoregressively. Lmlm=\u22121 MMX j=11mask(j) log( pij) (4) Here,Lmlmrepresents the cross-entropy loss for masked language modelling. In this equation, Mis the number of masked tokens, and 1mask(j)is an indicator function that evaluates to 1 if tok",
    "full_text_length": 50884,
    "chunk_length": 1271
  },
  {
    "chunk_id": 93,
    "paper_filename": "ankanderia_2024_intelligent_visual_encoder_for_bridging_modalities_in_report_generation.pdf",
    "paper_title": "Ankanderia 2024 Intelligent Visual Encoder For Bridging Modalities In Report Generation",
    "chunk_index": 20,
    "total_chunks": 51,
    "text_content": "loss function ensures that the model maximizes the likeli- hood of the text tokens, particularly the masked ones, dur- ing training, a vital aspect of autoregressive text generation. Multi-Modality Matching (MMM) : This procedure is designed to determine whether an image and text pair is ei- ther positively matched or not. To achieve this, We leverage a bi-directional self-attention mask where all queries andtexts can appear to each other. As a result, the query em- bedding, denoted as Q, effect",
    "full_text_length": 50884,
    "chunk_length": 1257
  },
  {
    "chunk_id": 94,
    "paper_filename": "ankanderia_2024_intelligent_visual_encoder_for_bridging_modalities_in_report_generation.pdf",
    "paper_title": "Ankanderia 2024 Intelligent Visual Encoder For Bridging Modalities In Report Generation",
    "chunk_index": 21,
    "total_chunks": 51,
    "text_content": "mining strategy inspired by ALBEF [29] to construct close negative pairs. Lmmm=\u22121 BBX i=1CX j=1yijlog(pij) (5) In our approach, we employ the Cross-Entropy Loss (Lmmm) to assess the dissimilarity between predicted and ground truth class probabilities. During the training process, we operate with a batch size denoted as B. Here, irepresents the sample index, jindicates the class index, and C is set to 2, signifying the two classes for matched and unmatched pairs. The variables yijandpijcorrespond",
    "full_text_length": 50884,
    "chunk_length": 1295
  },
  {
    "chunk_id": 95,
    "paper_filename": "ankanderia_2024_intelligent_visual_encoder_for_bridging_modalities_in_report_generation.pdf",
    "paper_title": "Ankanderia 2024 Intelligent Visual Encoder For Bridging Modalities In Report Generation",
    "chunk_index": 22,
    "total_chunks": 51,
    "text_content": "of LLM, we employ a fully-connected layer for linear projection. This projection transforms the out- put embeddings from the CMQFL layer for the input of the decoder (i.e., Vicuna). These newly projected embeddings are then prepended to the input text embeddings, effec- tively acting as soft visual cues that condition the decoder on the visual context received by the CMQFL layer. Be- cause the CMQFL layer is pre-trained to capture language- informative visual features, it acts as an effective in",
    "full_text_length": 50884,
    "chunk_length": 1301
  },
  {
    "chunk_id": 96,
    "paper_filename": "ankanderia_2024_intelligent_visual_encoder_for_bridging_modalities_in_report_generation.pdf",
    "paper_title": "Ankanderia 2024 Intelligent Visual Encoder For Bridging Modalities In Report Generation",
    "chunk_index": 23,
    "total_chunks": 51,
    "text_content": "configuration ensures close integration of visual and textual information. 2032 4. Experiments 4.1. Dataset Description For this experiment, we train the encoder using the NIH dataset [50]. For generating the reports, we utilize three datasets: IU-Xray, MIMIC-CXR and CDD-CESM. MIMIC-CXR: This extensive dataset [25] is commonly used for tasks involving generating reports. It includes 10 folders, comprising a total of 377,110 chest X-ray im- ages and 227,835 corresponding reports. In our research,",
    "full_text_length": 50884,
    "chunk_length": 1372
  },
  {
    "chunk_id": 97,
    "paper_filename": "ankanderia_2024_intelligent_visual_encoder_for_bridging_modalities_in_report_generation.pdf",
    "paper_title": "Ankanderia 2024 Intelligent Visual Encoder For Bridging Modalities In Report Generation",
    "chunk_index": 24,
    "total_chunks": 51,
    "text_content": "images, featuring CC and MLO views for both breasts. This dataset is derived from 326 female patients. In each dataset, we split the dataset into training, valida- tion, and testing sets, with proportions of 75%, 10%, and 15%, respectively. 4.2. Experimental Settings Metrics: Our evaluation employs a set of well-established metrics, namely BLEU [37], METEOR [6] and ROUGE-L [33]. These metrics are computed using the standard eval- uation toolkit. It\u2019s worth noting that BLEU and METEOR were initia",
    "full_text_length": 50884,
    "chunk_length": 1272
  },
  {
    "chunk_id": 98,
    "paper_filename": "ankanderia_2024_intelligent_visual_encoder_for_bridging_modalities_in_report_generation.pdf",
    "paper_title": "Ankanderia 2024 Intelligent Visual Encoder For Bridging Modalities In Report Generation",
    "chunk_index": 25,
    "total_chunks": 51,
    "text_content": "The queries are learned from the CMQFL layer, to extract the most useful information from the image features. This vector is then transformed into the shape required for input into the decoder, which generates the final report. We first train the CMQFL layer while freezing all other parts of the model, and then we fine-tune the decoder for better results. During encoder training, we use the AdamW optimizer with a batch size of 4, and the learning rate is linearly increased from 1.0e-4 to 1.0e-3 ",
    "full_text_length": 50884,
    "chunk_length": 1188
  },
  {
    "chunk_id": 99,
    "paper_filename": "ankanderia_2024_intelligent_visual_encoder_for_bridging_modalities_in_report_generation.pdf",
    "paper_title": "Ankanderia 2024 Intelligent Visual Encoder For Bridging Modalities In Report Generation",
    "chunk_index": 26,
    "total_chunks": 51,
    "text_content": "For both tasks, we utilize the AdamW optimizer with mathematical parameters: \u03b21set to 0.9,\u03b22set to 0.999, and a weight decay of 0.05. Our training process involves 1000 warm-up steps for the IU dataset and 5000 for the MIMIC dataset, with a warm-up learning rate of 1e\u22124. We employ a cosine learning rate decay strategy, beginning with an initial learning rate (lr) of 1e\u22123and gradually decreasing to a final lr of1e\u22127. Throughout both training and evaluation, we consistently use a batch size of 4du",
    "full_text_length": 50884,
    "chunk_length": 1256
  },
  {
    "chunk_id": 100,
    "paper_filename": "ankanderia_2024_intelligent_visual_encoder_for_bridging_modalities_in_report_generation.pdf",
    "paper_title": "Ankanderia 2024 Intelligent Visual Encoder For Bridging Modalities In Report Generation",
    "chunk_index": 27,
    "total_chunks": 51,
    "text_content": "report generation approaches in Table 1. These methods encompass a wide range of tech- niques, including both classic and modern approaches such as Show-tell [48], AdaAtt [35], Att2in [40], Up-down [3], R2Gen [13], M2transformer [16], X-REM [23], BLIP-2 [32] and R2GenGPT [52]. The proposed InVERGe model demonstrates superior performance across nearly all met- rics. In both the MIMIC-CXR and IU datasets, we achieve superior performance compared to the latest R2GenGPT medthod across all metrics ex",
    "full_text_length": 50884,
    "chunk_length": 1301
  },
  {
    "chunk_id": 101,
    "paper_filename": "ankanderia_2024_intelligent_visual_encoder_for_bridging_modalities_in_report_generation.pdf",
    "paper_title": "Ankanderia 2024 Intelligent Visual Encoder For Bridging Modalities In Report Generation",
    "chunk_index": 28,
    "total_chunks": 51,
    "text_content": "decoder cross-attention layer. These attention maps capture the model\u2019s focus on differ- ent regions of the image corresponding to specific words in the input text. The attention maps are then processed to ob- tain a single map per word by taking the maximum attention score across all attention heads. Subsequently, these maps are reshaped to match the dimensions of the image. 5. Ablation studies 5.1. Variour Image Encoder We employ different frozen encoders to extract optimal fea- tures from ima",
    "full_text_length": 50884,
    "chunk_length": 1251
  },
  {
    "chunk_id": 102,
    "paper_filename": "ankanderia_2024_intelligent_visual_encoder_for_bridging_modalities_in_report_generation.pdf",
    "paper_title": "Ankanderia 2024 Intelligent Visual Encoder For Bridging Modalities In Report Generation",
    "chunk_index": 29,
    "total_chunks": 51,
    "text_content": "0.16 0.105 0.079 0.122 0.254 Attn2in [40] 0.331 0.213 0.121 0.081 0.131 0.271 Up-Down [3] 0.318 0.191 0.113 0.071 0.128 0.267 MIMIC-CXR R2Gen [13] 0.311 0.186 0.112 0.077 0.125 0.265 M2 Transformer [16] 0.347 0.211 0.122 0.085 0.140 0.269 X-REM [23] 0.314 0.188 0.112 0.069 0.121 0.266 BLIP-2 [32] 0.377 0.221 0.125 0.088 0.152 0.274 R2GenGPT (Deep) [52] 0.392 0.229 0.129 0.101 0.159 0.283 Proposed (InVERGe) 0.425 0.240 0.132 0.100 0.175 0.309 Show-Tell [48] 0.341 0.203 0.140 0.079 0.123 0.321 Ada",
    "full_text_length": 50884,
    "chunk_length": 1237
  },
  {
    "chunk_id": 103,
    "paper_filename": "ankanderia_2024_intelligent_visual_encoder_for_bridging_modalities_in_report_generation.pdf",
    "paper_title": "Ankanderia 2024 Intelligent Visual Encoder For Bridging Modalities In Report Generation",
    "chunk_index": 30,
    "total_chunks": 51,
    "text_content": "0.324 0.226 0.168 0.195 0.384 Show-Tell [48] 0.284 0.165 0.109 0.079 0.153 0.264 AdaAtt [35] 0.293 0.169 0.116 0.080 0.180 0.269 Attn2in [40] 0.340 0.217 0.124 0.081 0.240 0.306 Up-Down [3] 0.338 0.204 0.123 0.079 0.237 0.310 CDD-CESM R2Gen [13] 0.335 0.199 0.122 0.077 0.213 0.299 M2 Transformer [16] 0.357 0.221 0.125 0.085 0.256 0.315 X-REM [23] 0.333 0.197 0.119 0.074 0.210 0.297 BLIP-2 [32] 0.382 0.235 0.139 0.102 0.301 0.342 R2GenGPT(Deep) [52] 0.417 0.249 0.165 0.129 0.354 0.377 Proposed (I",
    "full_text_length": 50884,
    "chunk_length": 1307
  },
  {
    "chunk_id": 104,
    "paper_filename": "ankanderia_2024_intelligent_visual_encoder_for_bridging_modalities_in_report_generation.pdf",
    "paper_title": "Ankanderia 2024 Intelligent Visual Encoder For Bridging Modalities In Report Generation",
    "chunk_index": 31,
    "total_chunks": 51,
    "text_content": "in- stance, improving the BLEU-2 score from 0.161\u21920.183.5.2. Effect of CMQFL Layer After selecting an effective encoder, we noticed an improve- ment in the accuracy of our base model. However, an ex- amination of the attention maps revealed that the model still faced challenges in identifying abnormal regions to gener- ate accurate reports. To solve this problem, we introduced a CMQFL layer capable of detecting abnormal regions. In Figure 6 we discard 80 % low-value attention weights and then pl",
    "full_text_length": 50884,
    "chunk_length": 1251
  },
  {
    "chunk_id": 105,
    "paper_filename": "ankanderia_2024_intelligent_visual_encoder_for_bridging_modalities_in_report_generation.pdf",
    "paper_title": "Ankanderia 2024 Intelligent Visual Encoder For Bridging Modalities In Report Generation",
    "chunk_index": 32,
    "total_chunks": 51,
    "text_content": "Baseline 0.161 0.124 0.255 MAE + Decoder 0.178 0.060 0.208 CPE + Decoder 0.183 0.117 0.260 CPE + CMQFL + Decoder 0.227 0.163 0.290 2034 Figure 6. Qualitative results of attention maps generated by BLIP and our InVERGe model\u2019s Encoder and CMQFL Layer. The initial stage of representation learning involves the pre-training of CMQFL layers, which capture visual fea- tures relevant to textual content. This process lightens the burden on the LLM when it comes to achieving vision- language alignment. I",
    "full_text_length": 50884,
    "chunk_length": 1265
  },
  {
    "chunk_id": 106,
    "paper_filename": "ankanderia_2024_intelligent_visual_encoder_for_bridging_modalities_in_report_generation.pdf",
    "paper_title": "Ankanderia 2024 Intelligent Visual Encoder For Bridging Modalities In Report Generation",
    "chunk_index": 33,
    "total_chunks": 51,
    "text_content": "notably lower performance. At the CMQFL layer, key image features are identified as the most relevant features, including cardiomegaly, effusion, at- electasis, consolidation, opacities etc. These features are then directed to the corresponding anomalous regions which clearly highlights the performance of the CMQFL layer in capturing anomalous visual regions. As illustrated in Figure 6 chest X-ray, reveals mild left and right pleural effusion, demonstrating the model\u2019s capability to accurately i",
    "full_text_length": 50884,
    "chunk_length": 1351
  },
  {
    "chunk_id": 107,
    "paper_filename": "ankanderia_2024_intelligent_visual_encoder_for_bridging_modalities_in_report_generation.pdf",
    "paper_title": "Ankanderia 2024 Intelligent Visual Encoder For Bridging Modalities In Report Generation",
    "chunk_index": 34,
    "total_chunks": 51,
    "text_content": "functions substantially improve the model\u2019s perfor- mance. To elaborate, in Table 3, we utilize the Masked Language Model (MLM) loss as the initial training objec- tive for the CMQFL layer. This foundational step allows the model to grasp linguistic and contextual understanding. Subsequently, we augment the training method by intro- ducing two additional objectives: Multimodal Contrastive Learning (MCL) and Multi-Modality Matching (MMM).By adding these objectives, the model\u2019s overall capabilitie",
    "full_text_length": 50884,
    "chunk_length": 1411
  },
  {
    "chunk_id": 108,
    "paper_filename": "ankanderia_2024_intelligent_visual_encoder_for_bridging_modalities_in_report_generation.pdf",
    "paper_title": "Ankanderia 2024 Intelligent Visual Encoder For Bridging Modalities In Report Generation",
    "chunk_index": 35,
    "total_chunks": 51,
    "text_content": "3. Evaluation after adding additional objective to train the CMQFL layer. For this, we use MIMIC-CXR dataset. Model BLEU-1 BLEU-2 METEOR Rouge-L MLM 0.410 0.227 0.163 0.290 MLM+MMM 0.416 0.231 0.166 0.30 MLM+MMM+MCL (InVERGe)0.425 0.24 0.175 0.309 The combined effect of these three objective functions leads to a substantial improvement in the performance of the pre-trained model, enhancing its ability to understand and leverage both text and image data effectively. 6. Conclusion In this research",
    "full_text_length": 50884,
    "chunk_length": 1421
  },
  {
    "chunk_id": 109,
    "paper_filename": "ankanderia_2024_intelligent_visual_encoder_for_bridging_modalities_in_report_generation.pdf",
    "paper_title": "Ankanderia 2024 Intelligent Visual Encoder For Bridging Modalities In Report Generation",
    "chunk_index": 36,
    "total_chunks": 51,
    "text_content": "contributes to creating small yet highly informative image embeddings, promoting a more grounded vision and language representation. This ap- proach not only improves the accuracy of the model within a shorter training period but also surpasses previous SOTA models on publicly available datasets, delivering detailed radiology reports and marking a significant advancement in the field. In our work, since the decoder component of our model is pre-trained on natural language, we require the integra",
    "full_text_length": 50884,
    "chunk_length": 1311
  },
  {
    "chunk_id": 110,
    "paper_filename": "ankanderia_2024_intelligent_visual_encoder_for_bridging_modalities_in_report_generation.pdf",
    "paper_title": "Ankanderia 2024 Intelligent Visual Encoder For Bridging Modalities In Report Generation",
    "chunk_index": 37,
    "total_chunks": 51,
    "text_content": "Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen Gould, and Lei Zhang. Bottom-up and top-down attention for image captioning and visual question answering. In Proceedings of the IEEE con- ference on computer vision and pattern recognition , pages 6077\u20136086, 2018. 6, 7 [4] Rohan Anil, Andrew M Dai, Orhan Firat, Melvin John- son, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2 technical report. arXiv preprint arXiv:2305.10",
    "full_text_length": 50884,
    "chunk_length": 1333
  },
  {
    "chunk_id": 111,
    "paper_filename": "ankanderia_2024_intelligent_visual_encoder_for_bridging_modalities_in_report_generation.pdf",
    "paper_title": "Ankanderia 2024 Intelligent Visual Encoder For Bridging Modalities In Report Generation",
    "chunk_index": 38,
    "total_chunks": 51,
    "text_content": "on in- trinsic and extrinsic evaluation measures for machine trans- lation and/or summarization , pages 65\u201372, 2005. 6 [7] Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei. Beit: Bert pre-training of image transformers. arXiv preprint arXiv:2106.08254 , 2021. 3 [8] Hangbo Bao, Wenhui Wang, Li Dong, Qiang Liu, Owais Khan Mohammed, Kriti Aggarwal, Subhojit Som, Songhao Piao, and Furu Wei. Vlmo: Unified vision-language pre-training with mixture-of-modality-experts. Advances in Neural Information Pro",
    "full_text_length": 50884,
    "chunk_length": 1396
  },
  {
    "chunk_id": 112,
    "paper_filename": "ankanderia_2024_intelligent_visual_encoder_for_bridging_modalities_in_report_generation.pdf",
    "paper_title": "Ankanderia 2024 Intelligent Visual Encoder For Bridging Modalities In Report Generation",
    "chunk_index": 39,
    "total_chunks": 51,
    "text_content": "IEEE 24th International Conference on Information Reuse and Integration for Data Science (IRI) , pages 297\u2013302. IEEE, 2023. 1 [11] Fuhai Chen, Rongrong Ji, Chengpeng Dai, Xuri Ge, Shengchuang Zhang, Xiaojing Ma, and Yue Gao. Fac- tored attention and embedding for unstructured-view topic- related ultrasound report generation. arXiv preprint arXiv:2203.06458 , 2022. 2 [12] Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Hee- woo Jun, David Luan, and Ilya Sutskever. Generative pre- training from ",
    "full_text_length": 50884,
    "chunk_length": 1388
  },
  {
    "chunk_id": 113,
    "paper_filename": "ankanderia_2024_intelligent_visual_encoder_for_bridging_modalities_in_report_generation.pdf",
    "paper_title": "Ankanderia 2024 Intelligent Visual Encoder For Bridging Modalities In Report Generation",
    "chunk_index": 40,
    "total_chunks": 51,
    "text_content": "Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311 , 2022. 3 [16] Marcella Cornia, Matteo Stefanini, Lorenzo Baraldi, and Rita Cucchiara. Meshed-memory transformer for image cap- tioning. In Proceedings of the IEEE/CVF conference on com- puter vision and pattern recognition , pages 10578\u201310587, 2020. 2, 6, 7 [17] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutano",
    "full_text_length": 50884,
    "chunk_length": 1409
  },
  {
    "chunk_id": 114,
    "paper_filename": "ankanderia_2024_intelligent_visual_encoder_for_bridging_modalities_in_report_generation.pdf",
    "paper_title": "Ankanderia 2024 Intelligent Visual Encoder For Bridging Modalities In Report Generation",
    "chunk_index": 41,
    "total_chunks": 51,
    "text_content": "pre-trained contrastive language-image model. In Machine Learning for Health , pages 209\u2013219. PMLR, 2021. 2 [20] Esin Darici Haritaoglu, Aleksandr Timashov, Matthew Tan, and Kathy Yu. Chest x-ray report generation from chest-x ray images. 2023. 2 [21] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll\u00b4ar, and Ross Girshick. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages 16000\u2013 16009, 2022. 7",
    "full_text_length": 50884,
    "chunk_length": 1374
  },
  {
    "chunk_id": 115,
    "paper_filename": "ankanderia_2024_intelligent_visual_encoder_for_bridging_modalities_in_report_generation.pdf",
    "paper_title": "Ankanderia 2024 Intelligent Visual Encoder For Bridging Modalities In Report Generation",
    "chunk_index": 42,
    "total_chunks": 51,
    "text_content": "Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representa- tion learning with noisy text supervision. In International conference on machine learning , pages 4904\u20134916. PMLR, 2021. 2 [25] Alistair EW Johnson, Tom J Pollard, Seth J Berkowitz, Nathaniel R Greenbaum, Matthew P Lungren, Chih-ying Deng, Roger G Mark, and Steven Horng. Mimic-cxr, 2036 a de-identified publicly available database of chest radio- graphs with free-text reports. Scientific dat",
    "full_text_length": 50884,
    "chunk_length": 1415
  },
  {
    "chunk_id": 116,
    "paper_filename": "ankanderia_2024_intelligent_visual_encoder_for_bridging_modalities_in_report_generation.pdf",
    "paper_title": "Ankanderia 2024 Intelligent Visual Encoder For Bridging Modalities In Report Generation",
    "chunk_index": 43,
    "total_chunks": 51,
    "text_content": "2 [28] Komal Kumar, Balakrishna Pailla, Kalyan Tadepalli, and Sudipta Roy. Robust msfm learning network for classifi- cation and weakly supervised localization. In Proceedings of the IEEE/CVF International Conference on Computer Vi- sion, pages 2442\u20132451, 2023. 1 [29] Junnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare, Shafiq Joty, Caiming Xiong, and Steven Chu Hong Hoi. Align before fuse: Vision and language representation learn- ing with momentum distillation. Advances in neural infor- mation ",
    "full_text_length": 50884,
    "chunk_length": 1389
  },
  {
    "chunk_id": 117,
    "paper_filename": "ankanderia_2024_intelligent_visual_encoder_for_bridging_modalities_in_report_generation.pdf",
    "paper_title": "Ankanderia 2024 Intelligent Visual Encoder For Bridging Modalities In Report Generation",
    "chunk_index": 44,
    "total_chunks": 51,
    "text_content": "Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597 , 2023. 1, 2, 6, 7 [33] Chin-Yew Lin. Rouge: A package for automatic evaluation of summaries. In Text summarization branches out , pages 74\u201381, 2004. 6 [34] Fenglin Liu, Xian Wu, Shen Ge, Wei Fan, and Yuexian Zou. Exploring and distilling posterior and prior knowl- edge for radiology report generation. In Proceedings of the I",
    "full_text_length": 50884,
    "chunk_length": 1311
  },
  {
    "chunk_id": 118,
    "paper_filename": "ankanderia_2024_intelligent_visual_encoder_for_bridging_modalities_in_report_generation.pdf",
    "paper_title": "Ankanderia 2024 Intelligent Visual Encoder For Bridging Modalities In Report Generation",
    "chunk_index": 45,
    "total_chunks": 51,
    "text_content": "warm starting. Artificial Intelligence in Medicine , 144:102633, 2023. 2 [37] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of theAssociation for Computational Linguistics , pages 311\u2013318, 2002. 6 [38] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsuper- vised multitask learners. 2019. 3 [39] Alec Radford, Jong Wook Kim",
    "full_text_length": 50884,
    "chunk_length": 1372
  },
  {
    "chunk_id": 119,
    "paper_filename": "ankanderia_2024_intelligent_visual_encoder_for_bridging_modalities_in_report_generation.pdf",
    "paper_title": "Ankanderia 2024 Intelligent Visual Encoder For Bridging Modalities In Report Generation",
    "chunk_index": 46,
    "total_chunks": 51,
    "text_content": "7 [41] S Roy, T Meena, and SJ Lim. Demystifying supervised learning in healthcare 4.0: a new reality of transforming di- agnostic medicine. diagnostics 12 (10): 2549, 2022. 1 [42] Sudipta Roy, Debojyoti Pal, and Tanushree Meena. Explain- able artificial intelligence to increase transparency for revo- lutionizing healthcare ecosystem and the road ahead. Net- work Modeling Analysis in Health Informatics and Bioinfor- matics , 13(1):4, 2023. 1 [43] Vivek Tiwari, Krutika Bapat, Kushashwa R Shrimali,",
    "full_text_length": 50884,
    "chunk_length": 1363
  },
  {
    "chunk_id": 120,
    "paper_filename": "ankanderia_2024_intelligent_visual_encoder_for_bridging_modalities_in_report_generation.pdf",
    "paper_title": "Ankanderia 2024 Intelligent Visual Encoder For Bridging Modalities In Report Generation",
    "chunk_index": 47,
    "total_chunks": 51,
    "text_content": "preprint arXiv:2302.13971 , 2023. 3 [45] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288 , 2023. 3 [46] Trieu H Trinh, Minh-Thang Luong, and Quoc V Le. Selfie: Self-supervised pretraining for image embedding. arXiv preprint arXiv:1906.02940 , 2019. 3 [47] Tao Tu, Shekoofeh Azizi, Danny Driess, Mike ",
    "full_text_length": 50884,
    "chunk_length": 1319
  },
  {
    "chunk_id": 121,
    "paper_filename": "ankanderia_2024_intelligent_visual_encoder_for_bridging_modalities_in_report_generation.pdf",
    "paper_title": "Ankanderia 2024 Intelligent Visual Encoder For Bridging Modalities In Report Generation",
    "chunk_index": 48,
    "total_chunks": 51,
    "text_content": "Liu, Kriti Aggarwal, Owais Khan Mo- hammed, Saksham Singhal, Subhojit Som, et al. Image as a foreign language: Beit pretraining for all vision and vision- language tasks. arXiv preprint arXiv:2208.10442 , 2022. 2 2037 [50] Xiaosong Wang, Yifan Peng, Le Lu, Zhiyong Lu, Mo- hammadhadi Bagheri, and Ronald Summers. Chestx-ray8: Hospital-scale chest x-ray database and benchmarks on weakly-supervised classification and localization of com- mon thorax diseases. In 2017 IEEE Conference on Com- puter Vis",
    "full_text_length": 50884,
    "chunk_length": 1372
  },
  {
    "chunk_id": 122,
    "paper_filename": "ankanderia_2024_intelligent_visual_encoder_for_bridging_modalities_in_report_generation.pdf",
    "paper_title": "Ankanderia 2024 Intelligent Visual Encoder For Bridging Modalities In Report Generation",
    "chunk_index": 49,
    "total_chunks": 51,
    "text_content": "Courville, Ruslan Salakhudinov, Rich Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption gen- eration with visual attention. In International conference on machine learning , pages 2048\u20132057. PMLR, 2015. 2 [54] Wenting Xu, Zhenghua Xu, Junyang Chen, Chang Qi, and Thomas Lukasiewicz. Hybrid reinforced medical report gen- eration with m-linear attention and repetition penalty. arXiv preprint arXiv:2210.13729 , 2022. 2 [55] Lewei Yao, Runhui Huang, Lu Hou, Guansong Lu, Minzhe Niu,",
    "full_text_length": 50884,
    "chunk_length": 1381
  },
  {
    "chunk_id": 123,
    "paper_filename": "ankanderia_2024_intelligent_visual_encoder_for_bridging_modalities_in_report_generation.pdf",
    "paper_title": "Ankanderia 2024 Intelligent Visual Encoder For Bridging Modalities In Report Generation",
    "chunk_index": 50,
    "total_chunks": 51,
    "text_content": "models. arXiv preprint arXiv:2205.01917 , 2022. 2 [58] Kai Zhang, Jun Yu, Zhiling Yan, Yixin Liu, Eashan Ad- hikarla, Sunyang Fu, Xun Chen, Chen Chen, Yuyin Zhou, Xiang Li, et al. Biomedgpt: A unified and generalist biomed- ical generative pre-trained transformer for vision, language, and multimodal tasks. arXiv preprint arXiv:2305.17100 , 2023. 3 [59] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuo- han Li, Dacheng Li, Eric Xing, et al. Judgin",
    "full_text_length": 50884,
    "chunk_length": 747
  },
  {
    "chunk_id": 124,
    "paper_filename": "aurora_2024_deeplearning_for_virtual_enhacne_spectral_mammograpy.pdf",
    "paper_title": "Aurora 2024 Deeplearning For Virtual Enhacne Spectral Mammograpy",
    "chunk_index": 0,
    "total_chunks": 58,
    "text_content": "Contents lists available at ScienceDirect Computerized Medical Imaging and Graphics journal homepage: www.elsevier.com/locate/compmedimag A deep learning approach for virtual contrast enhancement in Contrast Enhanced Spectral Mammography Aurora Rofenaa ,1, Valerio Guarrasia ,1, Marina Sarlib, Claudia Lucia Piccolob, Matteo Sammarrab, Bruno Beomonte Zobelb,c, Paolo Sodaa,d ,\u2217 aUnit of Computer Systems & Bioinformatics, Department of Engineering University Campus Bio-Medico, Rome, Italy bDepartmen",
    "full_text_length": 57609,
    "chunk_length": 1494
  },
  {
    "chunk_id": 125,
    "paper_filename": "aurora_2024_deeplearning_for_virtual_enhacne_spectral_mammograpy.pdf",
    "paper_title": "Aurora 2024 Deeplearning For Virtual Enhacne Spectral Mammograpy",
    "chunk_index": 1,
    "total_chunks": 58,
    "text_content": "and a high-energy image. The two scans are combined to get a recombined image showing contrast enhancement. Despite CESM diagnostic advantages for breast cancer diagnosis, the use of contrast medium can cause side effects, and CESM also beams patients with a higher radiation dose compared to standard mammography. To address these limitations, this work proposes using deep generative models for virtual contrast enhancement on CESM, aiming to make CESM contrast- free and reduce the radiation dose.",
    "full_text_length": 57609,
    "chunk_length": 1409
  },
  {
    "chunk_id": 126,
    "paper_filename": "aurora_2024_deeplearning_for_virtual_enhacne_spectral_mammograpy.pdf",
    "paper_title": "Aurora 2024 Deeplearning For Virtual Enhacne Spectral Mammograpy",
    "chunk_index": 2,
    "total_chunks": 58,
    "text_content": "network to generate synthetic recombined images, highlighting the potential of artificial intelligence techniques for virtual contrast enhancement in this field. 1. Introduction Contrast-Enhanced Spectral Mammography (CESM) is a dual-energy breast imaging technique classified as a level II breast diagnostic. Unlike standard mammography, also referred to as full-field digital mammography (FFDM), CESM involves the injection of an iodinated contrast medium that diffuses into the tumor tissue, enhan",
    "full_text_length": 57609,
    "chunk_length": 1444
  },
  {
    "chunk_id": 127,
    "paper_filename": "aurora_2024_deeplearning_for_virtual_enhacne_spectral_mammograpy.pdf",
    "paper_title": "Aurora 2024 Deeplearning For Virtual Enhacne Spectral Mammograpy",
    "chunk_index": 3,
    "total_chunks": 58,
    "text_content": "oblique projections, capturing low-energy (LE) and high-energy (HE) images in rapid succession post-contrast injection. The LE image is \u2217Corresponding author. E-mail address: p.soda@unicampus.it (P. Soda). 1These authors contributed equally to this work.comparable with FFDM and is combined with the HE image through post-processing to generate a dual-energy subtracted (DES) image in which areas of lesion contrast enhancement are highlighted ( Jochelson and Lobbes , 2021 ). However, despite its be",
    "full_text_length": 57609,
    "chunk_length": 1482
  },
  {
    "chunk_id": 128,
    "paper_filename": "aurora_2024_deeplearning_for_virtual_enhacne_spectral_mammograpy.pdf",
    "paper_title": "Aurora 2024 Deeplearning For Virtual Enhacne Spectral Mammograpy",
    "chunk_index": 4,
    "total_chunks": 58,
    "text_content": "Image-to-Image translation tasks in medical imaging, including Virtual Contrast Enhancement (VCE), which involves generating synthetic contrast-enhanced images from non-contrast images. While previous studies have explored VCE https://doi.org/10.1016/j.compmedimag.2024.102398 Received 21 August 2023; Received in revised form 7 May 2024; Accepted 7 May 2024Computerized Medical Imaging and Graphics 116 (2024) 102398 Available online 23 May 2024 0895-6111/\u00a9 2024 The Author(s). Published by Elsevier",
    "full_text_length": 57609,
    "chunk_length": 1314
  },
  {
    "chunk_id": 129,
    "paper_filename": "aurora_2024_deeplearning_for_virtual_enhacne_spectral_mammograpy.pdf",
    "paper_title": "Aurora 2024 Deeplearning For Virtual Enhacne Spectral Mammograpy",
    "chunk_index": 5,
    "total_chunks": 58,
    "text_content": "(CT) im- ages ( Choi et al. , 2021 ; Xie et al. , 2021 ), the application of this approach to CESM remains unexplored. The use of DL techniques to perform VCE on CESM would involve investigating the synthesis of DES images directly from LE images. Given the equivalence be- tween LE images and FFDM images, as supported in Francescone et al. (2014 ), this approach would offer three significant benefits. First, it may eliminate the necessity of administering contrast medium, that may have adverse r",
    "full_text_length": 57609,
    "chunk_length": 1165
  },
  {
    "chunk_id": 130,
    "paper_filename": "aurora_2024_deeplearning_for_virtual_enhacne_spectral_mammograpy.pdf",
    "paper_title": "Aurora 2024 Deeplearning For Virtual Enhacne Spectral Mammograpy",
    "chunk_index": 6,
    "total_chunks": 58,
    "text_content": "to investigate if AI can be used as a possible tool for VCE on CESM, by training and validating established DL models. As part of our efforts to advance research in the CESM field, we release the dataset collected and used in this study. This marks a significant milestone, as it is the first publicly available CESM dataset in DICOM format, with data anonymized to ensure privacy. The dataset consists of 1138 images in DICOM for- mat, comprising 569 LE and 569 DES images, collected from 105 patien",
    "full_text_length": 57609,
    "chunk_length": 1236
  },
  {
    "chunk_id": 131,
    "paper_filename": "aurora_2024_deeplearning_for_virtual_enhacne_spectral_mammograpy.pdf",
    "paper_title": "Aurora 2024 Deeplearning For Virtual Enhacne Spectral Mammograpy",
    "chunk_index": 7,
    "total_chunks": 58,
    "text_content": "images. Through quanti- tative and qualitative assessments, we aim to define the baseline performance for this specific task; \u2022Release of the first dataset of CESM examination images in DI- COM format, complete with information from medical reports and biopsies; \u2022Release of code used in our experiments. The rest of the manuscript is organized as follows: Section 2 presents the background of the study. Section 3 outlines the publicly released 2https://github.com/cosbidev/VCE_CESMdataset used for ",
    "full_text_length": 57609,
    "chunk_length": 1413
  },
  {
    "chunk_id": 132,
    "paper_filename": "aurora_2024_deeplearning_for_virtual_enhacne_spectral_mammograpy.pdf",
    "paper_title": "Aurora 2024 Deeplearning For Virtual Enhacne Spectral Mammograpy",
    "chunk_index": 8,
    "total_chunks": 58,
    "text_content": "VCE across diverse medical imaging modalities, pivotal to our investigation into its application to CESM images in our study. 2.1. CESM technique The CESM technique involves compressing both breasts in cran- iocaudal and mediolateral oblique projections for approximately two minutes after contrast medium injection. During each compression, a LE and HE image are acquired in quick succession, and the exam is completed within 10 min of the start of contrast administration. The LE image is acquired ",
    "full_text_length": 57609,
    "chunk_length": 1282
  },
  {
    "chunk_id": 133,
    "paper_filename": "aurora_2024_deeplearning_for_virtual_enhacne_spectral_mammograpy.pdf",
    "paper_title": "Aurora 2024 Deeplearning For Virtual Enhacne Spectral Mammograpy",
    "chunk_index": 9,
    "total_chunks": 58,
    "text_content": "image or DES image. This DES image suppresses parenchymal tissue, revealing only areas of contrast enhancement, thereby enhancing lesion visibility and improving diagnostic accuracy. Thus, CESM provides radiologists with LE and DES image pairs for diagnosis ( Jochelson and Lobbes , 2021 ). 2.2. CESM clinical applications Because of its characteristics, CESM can be used in several clinical scenarios. It serves as a valuable tool in high-risk screening, further evaluation of extremely dense breast",
    "full_text_length": 57609,
    "chunk_length": 1314
  },
  {
    "chunk_id": 134,
    "paper_filename": "aurora_2024_deeplearning_for_virtual_enhacne_spectral_mammograpy.pdf",
    "paper_title": "Aurora 2024 Deeplearning For Virtual Enhacne Spectral Mammograpy",
    "chunk_index": 10,
    "total_chunks": 58,
    "text_content": "a significant improvement in detecting multifocal-multicentric cancers compared to FFDM. This capability is essential to determine the methods of surgical treatment for the patients, leading CESM to have a crucial influence on surgical decision (Lorek et al., 2021). CESM also plays a crucial role in evaluating the response to neoadjuvant chemotherapy, which is the standard treatment for locally advanced breast carcinomas. Accurate assessment of this response is fundamental to increase the chance",
    "full_text_length": 57609,
    "chunk_length": 1355
  },
  {
    "chunk_id": 135,
    "paper_filename": "aurora_2024_deeplearning_for_virtual_enhacne_spectral_mammograpy.pdf",
    "paper_title": "Aurora 2024 Deeplearning For Virtual Enhacne Spectral Mammograpy",
    "chunk_index": 11,
    "total_chunks": 58,
    "text_content": "well tolerated by patients and feasible due to its fast execution and high success rates (Sammarra et al., 2024; James, 2022). 2.3. CESM limitations Two factors may limit the adoption of CESM as a widespread screening technique. First, although the administration of iodinated contrast medium is essential for the performance of CESM, adverse effects may occur, such as hypersensitivity reactions and the possibility of CIN (Pasternak and Williamson, 2012). The estimated incidence of hypersensitivit",
    "full_text_length": 57609,
    "chunk_length": 1310
  },
  {
    "chunk_id": 136,
    "paper_filename": "aurora_2024_deeplearning_for_virtual_enhacne_spectral_mammograpy.pdf",
    "paper_title": "Aurora 2024 Deeplearning For Virtual Enhacne Spectral Mammograpy",
    "chunk_index": 12,
    "total_chunks": 58,
    "text_content": "a higher radiation dose compared to FFDM (Patel et al., 2018). To overcome these limitations, reducing the administration of the contrast medium and the amount of radiation using AI techniques is a viable solution that can now be investigated, thanks to the recent progress in DL and generative approaches, in particular. 2.4. Image-to-image translation for virtual contrast enhancement In recent years, the use of deep neural networks has become domi- nant in image-to-image translation for medical ",
    "full_text_length": 57609,
    "chunk_length": 1335
  },
  {
    "chunk_id": 137,
    "paper_filename": "aurora_2024_deeplearning_for_virtual_enhacne_spectral_mammograpy.pdf",
    "paper_title": "Aurora 2024 Deeplearning For Virtual Enhacne Spectral Mammograpy",
    "chunk_index": 13,
    "total_chunks": 58,
    "text_content": "images. The aim was to generate synthetic contrast-enhanced images from non-contrast im- ages, with the goal of evaluating the feasibility of reducing contrast media usage while preserving image quality. In this context, Kleesiek et al. (2019) trained a fully convolutional autoencoder with dropout to predict contrast enhancement from non-contrast multiparametric brain MRI scans, with 10 channels used for the model input. Choi et al. (2021) evaluated the 3D implementation of Pix2Pix for generatin",
    "full_text_length": 57609,
    "chunk_length": 1355
  },
  {
    "chunk_id": 138,
    "paper_filename": "aurora_2024_deeplearning_for_virtual_enhacne_spectral_mammograpy.pdf",
    "paper_title": "Aurora 2024 Deeplearning For Virtual Enhacne Spectral Mammograpy",
    "chunk_index": 14,
    "total_chunks": 58,
    "text_content": "energy.3. Materials In this work, we collected and utilized an in-house dataset, named CESM@UCBM in the following, which we make publicly available3 after anonymizing sensitive data. The images are stored in DICOM format, thus including imaging parameters, acquisition details, and annotations, which are useful for the analyses. Table 3.1 summarizes the main characteristics of the dataset. It consists of 1138 images, divided into 569 LE images and 569 DES images. These images were obtained from 1",
    "full_text_length": 57609,
    "chunk_length": 1253
  },
  {
    "chunk_id": 139,
    "paper_filename": "aurora_2024_deeplearning_for_virtual_enhacne_spectral_mammograpy.pdf",
    "paper_title": "Aurora 2024 Deeplearning For Virtual Enhacne Spectral Mammograpy",
    "chunk_index": 15,
    "total_chunks": 58,
    "text_content": "the remaining 154 images have a size of 2294 \u00d71916 pixels. Among the 1138 images in the CESM@UCBM dataset: \u2022284 images show the craniocaudal projection of right breasts, with 142 LE images and 142 DES images; \u2022282 images show the craniocaudal projection of left breasts, with 141 LE images and 141 DES images; \u2022292 images show the mediolateral oblique projection of right breasts, with 146 LE images and 146 DES images; \u2022280 images show the mediolateral oblique projection of left breasts, with 140 L",
    "full_text_length": 57609,
    "chunk_length": 1253
  },
  {
    "chunk_id": 140,
    "paper_filename": "aurora_2024_deeplearning_for_virtual_enhacne_spectral_mammograpy.pdf",
    "paper_title": "Aurora 2024 Deeplearning For Virtual Enhacne Spectral Mammograpy",
    "chunk_index": 16,
    "total_chunks": 58,
    "text_content": "the 4 categories ( a,b,c,d) defined by the fifth edition of the Breast Imaging Reporting and Data System (BI-RADS) released by the American College of Radiology (ACR) (D\u2019orsi et al., 2003). The category aindicates almost entirely fatty breasts and characterizes 130 images, of which 35% show contrast enhancement. The category bidentifies breasts with scattered areas of fibroglandular density and characterizes 360 images, of which 33% show contrast enhancement. The category cindicates heterogeneou",
    "full_text_length": 57609,
    "chunk_length": 1303
  },
  {
    "chunk_id": 141,
    "paper_filename": "aurora_2024_deeplearning_for_virtual_enhacne_spectral_mammograpy.pdf",
    "paper_title": "Aurora 2024 Deeplearning For Virtual Enhacne Spectral Mammograpy",
    "chunk_index": 17,
    "total_chunks": 58,
    "text_content": "6) associated with the probability of malignancy of the lesion and the actions to be taken to conclude the diagnostic\u2013therapeutic management process. Based on biopsies, malignant lesions were identified in 258 images, 129 of which were LE and 129 of which were DES. Benign lesions were identified in 104 images, 52 LE and 52 DES. Borderline lesions were identified in 16 images, 8 LE and 8 DES. The remaining images did not show any tumor-related abnormalities. 4. Methods We design the VCE task on C",
    "full_text_length": 57609,
    "chunk_length": 1308
  },
  {
    "chunk_id": 142,
    "paper_filename": "aurora_2024_deeplearning_for_virtual_enhacne_spectral_mammograpy.pdf",
    "paper_title": "Aurora 2024 Deeplearning For Virtual Enhacne Spectral Mammograpy",
    "chunk_index": 18,
    "total_chunks": 58,
    "text_content": "105 Total images 1138 Image typeLE 569 images DES 569 images Image viewCraniocaudal 566 images mediolateral oblique 572 images Image size2850 \u00d72396 984 images 2294 \u00d71916 154 images Patients\u2019 age<50 years 31 patients 50\u201359 years 37 patients 60\u201369 years 19 patients \u226570 years 18 patients BiopsyMalignant 258 images Benignant 104 images Borderline 16 images ACR categorya 130 images b 360 images c 414 images d 190 images Not reported 44 images BI-RADS descriptors 1\u20136 31 patients Fig. 2. From left to r",
    "full_text_length": 57609,
    "chunk_length": 1270
  },
  {
    "chunk_id": 143,
    "paper_filename": "aurora_2024_deeplearning_for_virtual_enhacne_spectral_mammograpy.pdf",
    "paper_title": "Aurora 2024 Deeplearning For Virtual Enhacne Spectral Mammograpy",
    "chunk_index": 19,
    "total_chunks": 58,
    "text_content": "training and evaluation process. 4.1. Pre-processing To ensure that data are consistent and homogeneous, we first ap- plied zero-padding to make images squared, second we applied contrast stretching to adjust the brightness of the images, third we normalized the pixel values bringing them in the range [0,1], and fourth we resized the images to 256 \u00d7256. 4.2. AI models The AI models used in this work to perform the VCE task on CESM images are an autoencoder and two GANs, the Pix2Pix ( Isola et al",
    "full_text_length": 57609,
    "chunk_length": 1197
  },
  {
    "chunk_id": 144,
    "paper_filename": "aurora_2024_deeplearning_for_virtual_enhacne_spectral_mammograpy.pdf",
    "paper_title": "Aurora 2024 Deeplearning For Virtual Enhacne Spectral Mammograpy",
    "chunk_index": 20,
    "total_chunks": 58,
    "text_content": "\ud835\udc3aconsists of four convolutional en- coder blocks and four convolutional decoder blocks, with bypass con- nection and residual connection. Each block is equipped with three convolutional layers with 3 \u00d73 kernels, each followed by batch nor- malization and ReLU activation function. Only the last decoder block is an exception, as it consists of a convolutional layer with a 1 \u00d71 kernel, followed by a sigmoid activation function. In the first half of the network, each block is followed by a max pooli",
    "full_text_length": 57609,
    "chunk_length": 1253
  },
  {
    "chunk_id": 145,
    "paper_filename": "aurora_2024_deeplearning_for_virtual_enhacne_spectral_mammograpy.pdf",
    "paper_title": "Aurora 2024 Deeplearning For Virtual Enhacne Spectral Mammograpy",
    "chunk_index": 21,
    "total_chunks": 58,
    "text_content": "image\u0302 \ud835\udc66=\ud835\udc3a(\ud835\udc65)and the corresponding target image \ud835\udc66, enabling the optimization of the network parameters as follows: \ud835\udc3a\u2217=argmin \ud835\udc3aE\ud835\udc65,\ud835\udc66[\u2016\ud835\udc66\u2212\ud835\udc3a(\ud835\udc65)\u2016] (1) where G* is the optimal model that minimizes the pixel-wise difference denoted as \u2016.\u2016. 4.2.2. Pix2Pix The Pix2Pix ( Isola et al. , 2017 ) is a conditional GAN specifically designed for image-to-image translation with paired datasets. It com- prises a generator network \ud835\udc3aand a discriminator network \ud835\udc37, as in a conventional GAN. The generator \ud835\udc3alearns the ma",
    "full_text_length": 57609,
    "chunk_length": 1298
  },
  {
    "chunk_id": 146,
    "paper_filename": "aurora_2024_deeplearning_for_virtual_enhacne_spectral_mammograpy.pdf",
    "paper_title": "Aurora 2024 Deeplearning For Virtual Enhacne Spectral Mammograpy",
    "chunk_index": 22,
    "total_chunks": 58,
    "text_content": "(\ud835\udc3a,\ud835\udc37), given by: \ue238\ud835\udc50\ud835\udc3a\ud835\udc34\ud835\udc41 (\ud835\udc3a,\ud835\udc37) =E\ud835\udc65,\ud835\udc66[log(\ud835\udc37(\ud835\udc65,\ud835\udc66))] +E\ud835\udc65[log(1 \u2212\ud835\udc37(\ud835\udc65,\ud835\udc3a(\ud835\udc65)))] (3) The generator tries to minimize this function, learning to fool the discriminator, and the discriminator tries to maximize this function, learning to distinguish real images from synthetic images. The Pix2Pix full objective is formulated as \ud835\udc3a\u2217=argmin \ud835\udc3amax \ud835\udc37[\ue238\ud835\udc50\ud835\udc3a\ud835\udc34\ud835\udc41 (\ud835\udc3a,\ud835\udc37) +\ud835\udf06\ue238\ud835\udc3f1(\ud835\udc3a)] (4) where\ud835\udf06balances the relative contribution of the two loss terms. As presented in Isola et al. (2017 ), both the generator and discrimina- tor ",
    "full_text_length": 57609,
    "chunk_length": 1303
  },
  {
    "chunk_id": 147,
    "paper_filename": "aurora_2024_deeplearning_for_virtual_enhacne_spectral_mammograpy.pdf",
    "paper_title": "Aurora 2024 Deeplearning For Virtual Enhacne Spectral Mammograpy",
    "chunk_index": 23,
    "total_chunks": 58,
    "text_content": "all image\u2019s patches, the overall response of the discriminator is obtained. The total amount of parameters for Pix2Pix is5.7\u22c5107. 4.2.3. CycleGAN The CycleGAN is a type of GAN that learns to translate images from one domain to another without the need for paired samples between the two domains ( Zhu et al. , 2017 ). It comprises two generators, \ud835\udc3aand\ud835\udc39, and two discriminators, \ud835\udc37\ud835\udc66and\ud835\udc37\ud835\udc65. The generator \ud835\udc3alearns the mapping from the\ud835\udc4bdomain to the \ud835\udc4cdomain, while the generator \ud835\udc39learns the mapping from th",
    "full_text_length": 57609,
    "chunk_length": 1292
  },
  {
    "chunk_id": 148,
    "paper_filename": "aurora_2024_deeplearning_for_virtual_enhacne_spectral_mammograpy.pdf",
    "paper_title": "Aurora 2024 Deeplearning For Virtual Enhacne Spectral Mammograpy",
    "chunk_index": 24,
    "total_chunks": 58,
    "text_content": "discriminators. the generated image \u0302\ud835\udc66=\ud835\udc3a(\ud835\udc65). In the same way, the discriminator \ud835\udc37\ud835\udc65 tries to distinguish between the real image \ud835\udc65in the\ud835\udc4bdomain and the generated image \u0302 \ud835\udc65=\ud835\udc39(\ud835\udc66). To minimize the difference between the distributions of real and generated images, the adversarial loss \ue238\ud835\udc3a\ud835\udc34\ud835\udc41(\ud835\udc3a,\ud835\udc37\ud835\udc66)is used to adversarially train the generator \ud835\udc3aand the discriminator \ud835\udc37\ud835\udc66, with\ud835\udc3atrying to maximize \ue238\ud835\udc4e\ud835\udc51\ud835\udc63(\ud835\udc3a,\ud835\udc37\ud835\udc66)to fool\ud835\udc37\ud835\udc66, while\ud835\udc37\ud835\udc66tries to minimize it. It is expressed as: \ue238\ud835\udc3a\ud835\udc34\ud835\udc41(\ud835\udc3a,\ud835\udc37\ud835\udc4c) =E\ud835\udc66[log(\ud835\udc37\ud835\udc4c(\ud835\udc66))] +E\ud835\udc65[log(1 \u2212\ud835\udc37\ud835\udc4c(\ud835\udc3a(\ud835\udc65",
    "full_text_length": 57609,
    "chunk_length": 1368
  },
  {
    "chunk_id": 149,
    "paper_filename": "aurora_2024_deeplearning_for_virtual_enhacne_spectral_mammograpy.pdf",
    "paper_title": "Aurora 2024 Deeplearning For Virtual Enhacne Spectral Mammograpy",
    "chunk_index": 25,
    "total_chunks": 58,
    "text_content": "+E\ud835\udc65[\u2016\ud835\udc39(\ud835\udc3a(\ud835\udc65)) \u2212\ud835\udc65\u20161] (6) Moreover, an identity mapping loss \ue238\ud835\udc56\ud835\udc51(\ud835\udc3a,\ud835\udc39)can be used to ensure that when real samples of the target domain are provided as input to the generator, they are identically mapped to the output. It is formulated as: \ue238\ud835\udc56\ud835\udc51(\ud835\udc3a,\ud835\udc39) =E\ud835\udc66[\u2016\ud835\udc3a(\ud835\udc66) \u2212\ud835\udc66\u20161] +E\ud835\udc65[\u2016\ud835\udc39(\ud835\udc65) \u2212\ud835\udc65\u20161] (7) The CycleGAN full objective can thus be expressed as: \ud835\udc3a\u2217,\ud835\udc39\u2217=argmin \ud835\udc3a,\ud835\udc39max \ud835\udc37\ud835\udc65,\ud835\udc37\ud835\udc66[\ue238\ud835\udc3a\ud835\udc34\ud835\udc41(\ud835\udc3a,\ud835\udc37\ud835\udc66) +\ue238\ud835\udc3a\ud835\udc34\ud835\udc41(\ud835\udc39,\ud835\udc37\ud835\udc65) +\ud835\udf061\ue238\ud835\udc50\ud835\udc66\ud835\udc50(\ud835\udc3a,\ud835\udc39) +\ud835\udf062\ue238\ud835\udc56\ud835\udc51(\ud835\udc3a,\ud835\udc39)] (8) where\ud835\udf061and\ud835\udf062balance the relative contribution of the loss terms. The architecture of t",
    "full_text_length": 57609,
    "chunk_length": 1417
  },
  {
    "chunk_id": 150,
    "paper_filename": "aurora_2024_deeplearning_for_virtual_enhacne_spectral_mammograpy.pdf",
    "paper_title": "Aurora 2024 Deeplearning For Virtual Enhacne Spectral Mammograpy",
    "chunk_index": 26,
    "total_chunks": 58,
    "text_content": "instancenormalization and ReLU activation function. The architecture con- cludes with a single final convolutional layer. On the other hand, the architecture of the discriminators is that of a PatchGAN, examining 70\u00d770 patches to classify the image as real or synthetic. The overall architecture is in line with the one presented in Zhu et al. (2017 ). The total amount of parameters for CycleGAN is 2.8\u22c5107 4.3. Training The Autoencoder, Pix2Pix, and CycleGAN models were trained and evaluated on th",
    "full_text_length": 57609,
    "chunk_length": 1257
  },
  {
    "chunk_id": 151,
    "paper_filename": "aurora_2024_deeplearning_for_virtual_enhacne_spectral_mammograpy.pdf",
    "paper_title": "Aurora 2024 Deeplearning For Virtual Enhacne Spectral Mammograpy",
    "chunk_index": 27,
    "total_chunks": 58,
    "text_content": "Khaled et al. , 2022 ) was performed, following the same pre-processing as described in Section 4.1. To prevent overfitting, we proposed a random data augmentation on the images of the training set in terms of vertical or horizontal shift (by a maximum of \u00b110% of the original dimension), zoom (by a maximum of\u00b110%), horizontal flip, and rotation (by a maximum of \u00b115\u25e6). Each model is trained for a maximum of 200 epochs using an early stopping criterion of 50 epochs following the validation loss. T",
    "full_text_length": 57609,
    "chunk_length": 1163
  },
  {
    "chunk_id": 152,
    "paper_filename": "aurora_2024_deeplearning_for_virtual_enhacne_spectral_mammograpy.pdf",
    "paper_title": "Aurora 2024 Deeplearning For Virtual Enhacne Spectral Mammograpy",
    "chunk_index": 28,
    "total_chunks": 58,
    "text_content": "\ue238\ud835\udc3a\ud835\udc34\ud835\udc41(\ud835\udc3a,\ud835\udc37), and we added the\ud835\udc3f1loss\ue238\ud835\udc3f1(\ud835\udc3a)with a\ud835\udf06factor of 100 for the generator\u2019s training. For both the generator network and the discriminator network, we used the Adam optimizer with a learning rate of 2\u22c510\u22124, a weight decay of 10\u22125, a beta of 0.5 and a momentum of 1. For the CycleGAN, the two generator networks, \ud835\udc3aand\ud835\udc39, and the two discriminator networks, \ud835\udc37\ud835\udc66and\ud835\udc37\ud835\udc65, are trained simultaneously. Typically, CycleGAN can work with unpaired datasets, but in this work, having access to a paired dataset",
    "full_text_length": 57609,
    "chunk_length": 1239
  },
  {
    "chunk_id": 153,
    "paper_filename": "aurora_2024_deeplearning_for_virtual_enhacne_spectral_mammograpy.pdf",
    "paper_title": "Aurora 2024 Deeplearning For Virtual Enhacne Spectral Mammograpy",
    "chunk_index": 29,
    "total_chunks": 58,
    "text_content": "with a\ud835\udf062factor of 5. For both generator and discriminator networks, we used the Adam optimizer with a learning rate of 10\u22125, a weight decay of 10\u22125, a beta of 0.5, and a momentum of 1. For all the models, we did not further investigate any other hyper- parameter configuration since their tuning is out of the scope of this manuscript. Nevertheless, the \u2018\u2018No Free Lunch\u2019\u2019 Theorem for optimiza- tion states that no universal set of hyperparameters will optimize a model\u2019s performance across all possib",
    "full_text_length": 57609,
    "chunk_length": 1258
  },
  {
    "chunk_id": 154,
    "paper_filename": "aurora_2024_deeplearning_for_virtual_enhacne_spectral_mammograpy.pdf",
    "paper_title": "Aurora 2024 Deeplearning For Virtual Enhacne Spectral Mammograpy",
    "chunk_index": 30,
    "total_chunks": 58,
    "text_content": "formulated as: MSE(\ud835\udc66,\u0302 \ud835\udc66) =1 \ud835\udc5a\ud835\udc5b\ud835\udc5a\u2211 \ud835\udc56=1\ud835\udc5b\u2211 \ud835\udc57=1(\ud835\udc66\ud835\udc56\ud835\udc57\u2212\u0302 \ud835\udc66\ud835\udc56\ud835\udc57)2(9) where\ud835\udc5aand\ud835\udc5bare the number of rows and columns in the images, respectively, and \ud835\udc66\ud835\udc56\ud835\udc57and\u0302 \ud835\udc66\ud835\udc56\ud835\udc57represent the pixels elements at the \ud835\udc56th row and\ud835\udc57th column of \ud835\udc66and\u0302 \ud835\udc66, respectively. It varies in the range [0,\u221e]; the lower its value, the higher the quality of the reconstructed image. The PSNR is defined as the ratio of the maximum possible power of a signal to the power of the noise corrupting the signal. In our case, the signal is the target ima",
    "full_text_length": 57609,
    "chunk_length": 1145
  },
  {
    "chunk_id": 155,
    "paper_filename": "aurora_2024_deeplearning_for_virtual_enhacne_spectral_mammograpy.pdf",
    "paper_title": "Aurora 2024 Deeplearning For Virtual Enhacne Spectral Mammograpy",
    "chunk_index": 31,
    "total_chunks": 58,
    "text_content": "commonly expressed as a function of the MSE as follows: PSNR (\ud835\udc66,\u0302 \ud835\udc66) = 10\u22c5log10(max2(\ud835\udc66) MSE(\ud835\udc66,\u0302 \ud835\udc66)) (10) The VIF (Sheikh and Bovik, 2006) evaluates the quality of visual information in the synthetic image \u0302 \ud835\udc66compared to the target image \ud835\udc66. It is derived by modeling the Human-Visual-System in the wavelet domain and is formulated as: VIF(\ud835\udc66,\u0302 \ud835\udc66) =\u2211 \ud835\udc57\u2208\ud835\udc60\ud835\udc62\ud835\udc4f\ud835\udc4f\ud835\udc4e\ud835\udc5b\ud835\udc51\ud835\udc60\ud835\udc3c(\ud835\udc66\ud835\udc57) \u2211 \ud835\udc57\u2208\ud835\udc60\ud835\udc62\ud835\udc4f\ud835\udc4f\ud835\udc4e\ud835\udc5b\ud835\udc51\ud835\udc60\ud835\udc3c(\u0302 \ud835\udc66\ud835\udc57)(11) where\ud835\udc3c(\ud835\udc66\ud835\udc57)and\ud835\udc3c(\u0302 \ud835\udc66\ud835\udc57)represent the information ideally extracted by the brain from a certain subband in the targe",
    "full_text_length": 57609,
    "chunk_length": 1290
  },
  {
    "chunk_id": 156,
    "paper_filename": "aurora_2024_deeplearning_for_virtual_enhacne_spectral_mammograpy.pdf",
    "paper_title": "Aurora 2024 Deeplearning For Virtual Enhacne Spectral Mammograpy",
    "chunk_index": 32,
    "total_chunks": 58,
    "text_content": "\ud835\udc66respectively, while\ud835\udf0e\ud835\udc66and\ud835\udf0e\u0302 \ud835\udc66represent their standard deviations. \ud835\udc361and\ud835\udc362are small constants used for stabilization. The SSIM varies in the range [0,1], the higher its value, the greater the similarity between the two images (Wang et al., 2004). 4.5. Qualitative evaluation The goal of the qualitative analysis is to have a medical assessment of the synthetic DES images generated by the models. It consists of twovisual Turing tests presented to four expert radiologists with experience ranging from",
    "full_text_length": 57609,
    "chunk_length": 1295
  },
  {
    "chunk_id": 157,
    "paper_filename": "aurora_2024_deeplearning_for_virtual_enhacne_spectral_mammograpy.pdf",
    "paper_title": "Aurora 2024 Deeplearning For Virtual Enhacne Spectral Mammograpy",
    "chunk_index": 33,
    "total_chunks": 58,
    "text_content": "synthetic DES images. To this end, we showed radiolo- gists side-by-side synthetic DES images generated by the Autoencoder, Pix2Pix, and CycleGAN for the same LE input image. Then, each radi- ologist had to independently choose the most realistic image without knowing which model generated it. The second visual Turing test aims to study not only if synthetic images are perceived as real ones but also if they can be used to assign the BI-RADS score, a measure indicating patients\u2019 risk of developi",
    "full_text_length": 57609,
    "chunk_length": 1203
  },
  {
    "chunk_id": 158,
    "paper_filename": "aurora_2024_deeplearning_for_virtual_enhacne_spectral_mammograpy.pdf",
    "paper_title": "Aurora 2024 Deeplearning For Virtual Enhacne Spectral Mammograpy",
    "chunk_index": 34,
    "total_chunks": 58,
    "text_content": "provided the four radiol- ogists with both real DES images and these newly generated synthetic DES images. All the images are presented one at a time, in random order: the radiologist specified whether the displayed image is real or synthetic, and he/she scored the BI-RADS. 5. Results and discussion In this Section, we present and discuss the results obtained from the quantitative and the qualitative evaluations, conducted as described in Sections 4.4 and 4.5, respectively. Given that this is th",
    "full_text_length": 57609,
    "chunk_length": 1254
  },
  {
    "chunk_id": 159,
    "paper_filename": "aurora_2024_deeplearning_for_virtual_enhacne_spectral_mammograpy.pdf",
    "paper_title": "Aurora 2024 Deeplearning For Virtual Enhacne Spectral Mammograpy",
    "chunk_index": 35,
    "total_chunks": 58,
    "text_content": "and their corresponding synthetic DES images. The values of these metrics are presented as the mean value \u00b1stan- dard deviation calculated across the different folds. For each metric, the most performing model is highlighted in bold. Furthermore, we performed paired t-tests for each metric to determine any statistical differences in the performances of the models. The results are presented in Table 5.2 with symbols ***, **, or *, indicating p-values lower than 0.001, 0.01, or 0.05, respectively.",
    "full_text_length": 57609,
    "chunk_length": 1309
  },
  {
    "chunk_id": 160,
    "paper_filename": "aurora_2024_deeplearning_for_virtual_enhacne_spectral_mammograpy.pdf",
    "paper_title": "Aurora 2024 Deeplearning For Virtual Enhacne Spectral Mammograpy",
    "chunk_index": 36,
    "total_chunks": 58,
    "text_content": "are in line with expectations, as autoencoders were among the first methods used for image-to-image translation, and it is known that they have the limita- tion of generating blurry images, partly due to the element-wise criteria typically used, such as \ud835\udc3f1loss and\ud835\udc3f2loss (Yoo et al., 2019; Liu et al., 2021b). Referring to Table 5.1, Pix2Pix obtains the worst mean value of VIF, but also the best mean value of SSIM, and both these results are supported by statistical analysis in Table 5.2. On the o",
    "full_text_length": 57609,
    "chunk_length": 1243
  },
  {
    "chunk_id": 161,
    "paper_filename": "aurora_2024_deeplearning_for_virtual_enhacne_spectral_mammograpy.pdf",
    "paper_title": "Aurora 2024 Deeplearning For Virtual Enhacne Spectral Mammograpy",
    "chunk_index": 37,
    "total_chunks": 58,
    "text_content": "Each tabular denotes the mean \u00b1standard deviation from the 10-folds. Model MSE \u2193 PSNR \u2191 VIF \u2191 SSIM \u2191 Autoencoder 0.0083 \u00b10.0036 23.2775 \u00b12.4942 0.1581 \u00b10.0212 0.5365 \u00b10.2104 Pix2Pix 0.0042 \u00b10.0008 26.3476 \u00b10.7640 0.1129 \u00b10.0106 0.8575 \u00b10.0132 CycleGAN 0.0038 \u00b10.0010 26.4866 \u00b10.9206 0.1840 \u00b10.0132 0.8492 \u00b10.0131 Table 5.2 Paired t-test results between all pairs of models for the considered metrics. Symbols ***, **, or *, indicate p-values lower than 0.001, 0.01, or 0.05, respectively. Empty cells",
    "full_text_length": 57609,
    "chunk_length": 1320
  },
  {
    "chunk_id": 162,
    "paper_filename": "aurora_2024_deeplearning_for_virtual_enhacne_spectral_mammograpy.pdf",
    "paper_title": "Aurora 2024 Deeplearning For Virtual Enhacne Spectral Mammograpy",
    "chunk_index": 38,
    "total_chunks": 58,
    "text_content": "lesion, alongside discrepancies in texture compared to the target image. On the contrary, despite statistical analyses not confirming the superiority of CycleGAN, Fig. 5 suggests that CycleGAN is the most promising network for generating synthetic DES images, as it demonstrates superior capability in reconstructing contrast enhance- ment. It is worth noting that such results agree with those reported by Azarfar et al. who implemented VCE on CT (Azarfar et al., 2023): indeed, they proved that the",
    "full_text_length": 57609,
    "chunk_length": 1349
  },
  {
    "chunk_id": 163,
    "paper_filename": "aurora_2024_deeplearning_for_virtual_enhacne_spectral_mammograpy.pdf",
    "paper_title": "Aurora 2024 Deeplearning For Virtual Enhacne Spectral Mammograpy",
    "chunk_index": 39,
    "total_chunks": 58,
    "text_content": "even in the most critical cases, represented by ACR categories c(heterogeneously dense breast tissue) and d(extremely dense breasts). Indeed, heterogeneously or extremely dense breast parenchyma can cause tumor lesions not to be clearly identifiable in images generated by FFDM. Therefore, in these cases, there is a risk that DL models may find it more complex to generate virtual DES images using only LE images, which are analogous to FFDM images. For these reasons, we also compute the MSE, PSNR,",
    "full_text_length": 57609,
    "chunk_length": 1309
  },
  {
    "chunk_id": 164,
    "paper_filename": "aurora_2024_deeplearning_for_virtual_enhacne_spectral_mammograpy.pdf",
    "paper_title": "Aurora 2024 Deeplearning For Virtual Enhacne Spectral Mammograpy",
    "chunk_index": 40,
    "total_chunks": 58,
    "text_content": "analysis indicates that there are no statistically significant differences (Kruskal\u2013Wallis \ud835\udc5d-value>0.05) in the values of the same metric when calculated for the four categories within each model. Additionally, it is worth noting that performance does not deteriorate in the critical cases cand d. Hence, these findings suggest that the implemented models demonstrate robustness across the different mammographic densities. 5.2. Qualitative evaluation In addition to the quantitative analysis, the qu",
    "full_text_length": 57609,
    "chunk_length": 1332
  },
  {
    "chunk_id": 165,
    "paper_filename": "aurora_2024_deeplearning_for_virtual_enhacne_spectral_mammograpy.pdf",
    "paper_title": "Aurora 2024 Deeplearning For Virtual Enhacne Spectral Mammograpy",
    "chunk_index": 41,
    "total_chunks": 58,
    "text_content": "and we conducted the second visual Turing test by using both the real DES images and the CycleGAN-generated DES images. The results are graphically represented in Fig. 6, which shows the True Positive Rate and True Negative Rate obtained by radiologists in distinguishing real images from synthetic images, as well as the Accuracy in associating the BI-RADS descriptor on real and synthetic images. In order to calculate the True Positive Rate and True Negative Rate, we have assigned positive sample",
    "full_text_length": 57609,
    "chunk_length": 1247
  },
  {
    "chunk_id": 166,
    "paper_filename": "aurora_2024_deeplearning_for_virtual_enhacne_spectral_mammograpy.pdf",
    "paper_title": "Aurora 2024 Deeplearning For Virtual Enhacne Spectral Mammograpy",
    "chunk_index": 42,
    "total_chunks": 58,
    "text_content": "that the network generates realistic synthetic DES images that can confuse radiologists into thinking they are real. On the other hand, we evaluated the Accuracy of assigning the BI-RADS descriptor separately for real DES images and synthetic DES images. In both subsets, it was calculated as the ratio of correctly assigned descriptors to the total number of descriptors assigned. There- fore, obtaining a high Accuracy value is desirable for both real and synthetic images. The results show that a ",
    "full_text_length": 57609,
    "chunk_length": 1201
  },
  {
    "chunk_id": 167,
    "paper_filename": "aurora_2024_deeplearning_for_virtual_enhacne_spectral_mammograpy.pdf",
    "paper_title": "Aurora 2024 Deeplearning For Virtual Enhacne Spectral Mammograpy",
    "chunk_index": 43,
    "total_chunks": 58,
    "text_content": "error on real DES images is the same as the error on synthetic DES images. These results suggest that the synthetic DES images have the po- tential to be used for diagnosis, so it is possible to apply the VCE on CESM images by using AI techniques. Thereby, the advantages of CESM, such as better diagnostic accuracy compared to FFDM, especially in patients with dense breast tissue, could be achieved without the requirement of intravenous administration of contrast medium or the need for dual-energ",
    "full_text_length": 57609,
    "chunk_length": 1235
  },
  {
    "chunk_id": 168,
    "paper_filename": "aurora_2024_deeplearning_for_virtual_enhacne_spectral_mammograpy.pdf",
    "paper_title": "Aurora 2024 Deeplearning For Virtual Enhacne Spectral Mammograpy",
    "chunk_index": 44,
    "total_chunks": 58,
    "text_content": "GPU, which has a power usage of 250 W. Based on our experiments, each image translation operation takes 0.1 s. Therefore, the energy consumption per image can be calculated as 250W\u00d7 0.1s= 25W-s= 6.9 \u00d7 10\u22126kWh. On the other hand, estimating the energy consumption of a CESM scan requires considering the power usage of the scanner, reported at 5 kW (as detailed in the datasheet of the Senographe Pristina mammography system). Assuming each CESM scan takes 2 min (Bhimani et al., 2017), the energy con",
    "full_text_length": 57609,
    "chunk_length": 1242
  },
  {
    "chunk_id": 169,
    "paper_filename": "aurora_2024_deeplearning_for_virtual_enhacne_spectral_mammograpy.pdf",
    "paper_title": "Aurora 2024 Deeplearning For Virtual Enhacne Spectral Mammograpy",
    "chunk_index": 45,
    "total_chunks": 58,
    "text_content": "ACR categories a,b,c, and d. Model ACR MSE \u2193 PSNR \u2191 VIF \u2191 SSIM \u2191 Autoencodera 0.0108 \u00b10.0048 21.6093 \u00b12.9097 0.1384 \u00b10.0270 0.5054 \u00b10.2055 b 0.0114 \u00b10.0052 21.7321 \u00b12.9283 0.1676 \u00b10.0249 0.5426 \u00b10.2071 c 0.0065 \u00b10.0031 23.8066 \u00b12.2557 0.1566 \u00b10.0247 0.5358 \u00b10.2110 d 0.0053 \u00b10.0024 24.5332 \u00b12.7948 0.1612 \u00b10.0256 0.5274 \u00b10.2108 Pix2Pixa 0.0034 \u00b10.0014 25.8957 \u00b11.4328 0.1505 \u00b10.0322 0.8170 \u00b10.0529 b 0.0047 \u00b10.0026 26.0185 \u00b12.3233 0.1807 \u00b10.0173 0.8365 \u00b10.0207 c 0.0029 \u00b10.0009 27.1512 \u00b11.4586 0.1738",
    "full_text_length": 57609,
    "chunk_length": 1371
  },
  {
    "chunk_id": 170,
    "paper_filename": "aurora_2024_deeplearning_for_virtual_enhacne_spectral_mammograpy.pdf",
    "paper_title": "Aurora 2024 Deeplearning For Virtual Enhacne Spectral Mammograpy",
    "chunk_index": 46,
    "total_chunks": 58,
    "text_content": "category (from top to bottom). correlates with CO2 emissions, the environmental impact of traditional CESM scans is considerably higher. It is important to note that these considerations are confined to the inference level, without considering model training, nor the production, delivery, installation, and testing of the mammography system. Furthermore, it is worth noting that while the proposed method assumes translation from images without contrast medium to images with contrast medium, the LE",
    "full_text_length": 57609,
    "chunk_length": 1319
  },
  {
    "chunk_id": 171,
    "paper_filename": "aurora_2024_deeplearning_for_virtual_enhacne_spectral_mammograpy.pdf",
    "paper_title": "Aurora 2024 Deeplearning For Virtual Enhacne Spectral Mammograpy",
    "chunk_index": 47,
    "total_chunks": 58,
    "text_content": "easilyseen thanks to an injected iodine contrast medium. In this context, our work offers two contributions. First, this study has demonstrated that VCE can be applied to CESM images avoiding the need for contrast medium administration and minimizing radiation exposure. This claim is supported by experiments using three well-established generative deep models, namely an autoencoder, Pix2Pix, and CycleGAN, which generate DES images solely from LE images. The quantitative analysis results, pertain",
    "full_text_length": 57609,
    "chunk_length": 1374
  },
  {
    "chunk_id": 172,
    "paper_filename": "aurora_2024_deeplearning_for_virtual_enhacne_spectral_mammograpy.pdf",
    "paper_title": "Aurora 2024 Deeplearning For Virtual Enhacne Spectral Mammograpy",
    "chunk_index": 48,
    "total_chunks": 58,
    "text_content": "accuracy equal to using real images. Second, we have presented and made publicly accessible a unique dataset of CESM images. We have taken great care to ensure theComputerized Medical Imaging and Graphics 116 (2024) 102398 8 A. Rofena et al. Fig. 6. Results of the second visual Turing test. True Positive Rate and True Negative Rate are the percentages at which the radiologists correctly label real images \ud835\udc66 and synthetic images \u0302 \ud835\udc66, respectively. The Accuracy is the percentage at which the radiol",
    "full_text_length": 57609,
    "chunk_length": 1232
  },
  {
    "chunk_id": 173,
    "paper_filename": "aurora_2024_deeplearning_for_virtual_enhacne_spectral_mammograpy.pdf",
    "paper_title": "Aurora 2024 Deeplearning For Virtual Enhacne Spectral Mammograpy",
    "chunk_index": 49,
    "total_chunks": 58,
    "text_content": "our dataset provides all the necessary information for conducting CESM analyses, and we extend an invitation to the scientific community to delve deeper into this area. Despite these promising findings, the way to the clinical adoption of VCE technology in clinical practice needs further research and validation, including the development of custom deep architectures that optimize previous ones and the incorporation of loss functions that prioritize lesion reconstruction during training. We are w",
    "full_text_length": 57609,
    "chunk_length": 1490
  },
  {
    "chunk_id": 174,
    "paper_filename": "aurora_2024_deeplearning_for_virtual_enhacne_spectral_mammograpy.pdf",
    "paper_title": "Aurora 2024 Deeplearning For Virtual Enhacne Spectral Mammograpy",
    "chunk_index": 50,
    "total_chunks": 58,
    "text_content": "Visualization, Writing \u2013 original draft, Writing \u2013 review & editing. Valerio Guar- rasi: Conceptualization, Data curation, Formal analysis, Investigation, Methodology, Software, Supervision, Validation, Visualization, Writing \u2013 original draft, Writing \u2013 review & editing. Marina Sarli: Data cura- tion, Resources. Claudia Lucia Piccolo: Conceptualization, Resources. Matteo Sammarra: Conceptualization, Resources. Bruno Beomonte Zobel: Conceptualization, Resources. Paolo Soda: Conceptualization, Fun",
    "full_text_length": 57609,
    "chunk_length": 1515
  },
  {
    "chunk_id": 175,
    "paper_filename": "aurora_2024_deeplearning_for_virtual_enhacne_spectral_mammograpy.pdf",
    "paper_title": "Aurora 2024 Deeplearning For Virtual Enhacne Spectral Mammograpy",
    "chunk_index": 51,
    "total_chunks": 58,
    "text_content": "Imprese e del Made in Italy), un- der the project with CUP B89J23000580005; (ii) PNRR MUR project PE0000013-FAIR; (iii) University Campus Bio-Medico di Roma under the program \u2018\u2018University Strategic Projects\u2019\u2019 within the project \u2018\u2018AI- powered Digital Twin for next-generation lung cancEr cAre (IDEA)\u2019\u2019. Resources are provided by the National Academic Infrastructure for Supercomputing in Sweden (NAISS) and the Swedish National Infras- tructure for Computing (SNIC) at Alvis @ C3SE, partially funded b",
    "full_text_length": 57609,
    "chunk_length": 1397
  },
  {
    "chunk_id": 176,
    "paper_filename": "aurora_2024_deeplearning_for_virtual_enhacne_spectral_mammograpy.pdf",
    "paper_title": "Aurora 2024 Deeplearning For Virtual Enhacne Spectral Mammograpy",
    "chunk_index": 52,
    "total_chunks": 58,
    "text_content": "Choi, Jae Won, Cho, Yeon Jin, Ha, Ji Young, Lee, Seul Bi, Lee, Seunghyun, Choi, Young Hun, Cheon, Jung-Eun, Kim, Woo Sun, 2021. Generating synthetic contrast enhancement from non-contrast chest computed tomography using a generative adversarial network. Sci. Rep. 11 (1), 20403. D\u2019orsi, CJ, Bassett, LWea, Berg, WA, Feig, SA, Jackson, VP, Kopans, DB, et al., 2003. Breast imaging reporting and data system: ACR BI-RADS-mammography. Am. College Radiol. (ACR), Reston 230\u2013234. Elkassas, Hebatalla, El-M",
    "full_text_length": 57609,
    "chunk_length": 1446
  },
  {
    "chunk_id": 177,
    "paper_filename": "aurora_2024_deeplearning_for_virtual_enhacne_spectral_mammograpy.pdf",
    "paper_title": "Aurora 2024 Deeplearning For Virtual Enhacne Spectral Mammograpy",
    "chunk_index": 53,
    "total_chunks": 58,
    "text_content": "full-field digital mammography (FFDM). Eur. J. Radiol. 83 (8), 1350\u20131355. Iotti, Valentina, Ravaioli, Sara, Vacondio, Rita, Coriani, Chiara, Caffarri, Sabrina, Sghe- doni, Roberto, Nitrosi, Andrea, Ragazzi, Moira, Gasparini, Elisa, Masini, Cristina, et al., 2017. Contrast-enhanced spectral mammography in neoadjuvant chemotherapy monitoring: A comparison with breast magnetic resonance imaging. Breast Cancer Res. 19, 1\u201313. Isola, Phillip, Zhu, Jun-Yan, Zhou, Tinghui, Efros, Alexei A, 2017. Image-t",
    "full_text_length": 57609,
    "chunk_length": 1553
  },
  {
    "chunk_id": 178,
    "paper_filename": "aurora_2024_deeplearning_for_virtual_enhacne_spectral_mammograpy.pdf",
    "paper_title": "Aurora 2024 Deeplearning For Virtual Enhacne Spectral Mammograpy",
    "chunk_index": 54,
    "total_chunks": 58,
    "text_content": "BMJ 311 (7001), 376\u2013380. Kaji, Shizuo, Kida, Satoshi, 2019. Overview of image-to-image translation by use of deep neural networks: Denoising, super-resolution, modality conversion, and reconstruction in medical imaging. Radiol. Phys. Technol. 12, 235\u2013248. Khaled, Rana, Helal, Maha, Alfarghaly, Omar, Mokhtar, Omnia, Elkorany, Abeer, El Kas- sas, Hebatalla, Fahmy, Aly, 2022. Categorized contrast enhanced mammography dataset for diagnostic and artificial intelligence research. Sci. Data 9 (1), 1\u201310",
    "full_text_length": 57609,
    "chunk_length": 1505
  },
  {
    "chunk_id": 179,
    "paper_filename": "aurora_2024_deeplearning_for_virtual_enhacne_spectral_mammograpy.pdf",
    "paper_title": "Aurora 2024 Deeplearning For Virtual Enhacne Spectral Mammograpy",
    "chunk_index": 55,
    "total_chunks": 58,
    "text_content": "Jonghye, 2021b. A unified conditional disentanglement framework for multimodal brain mr image translation. In: 2021 IEEE 18th International Symposium on Biomedical Imaging. ISBI, IEEE, pp. 10\u201314.Computerized Medical Imaging and Graphics 116 (2024) 102398 9 A. Rofena et al. Lorek, Andrzej, Steinhof-Radwa\u0144ska, Katarzyna, Barczyk-Gutkowska, Anna, Zar\u0119b- ski, Wojciech, Pale\u0144, Piotr, Szyluk, Karol, Lorek, Joanna, Gra\u017cy\u0144ska, Anna, Niemiec, Pawe\u0142, Gisterek, Iwona, 2021. The usefulness of spectral mammo",
    "full_text_length": 57609,
    "chunk_length": 1526
  },
  {
    "chunk_id": 180,
    "paper_filename": "aurora_2024_deeplearning_for_virtual_enhacne_spectral_mammograpy.pdf",
    "paper_title": "Aurora 2024 Deeplearning For Virtual Enhacne Spectral Mammograpy",
    "chunk_index": 56,
    "total_chunks": 58,
    "text_content": "Carli, M., Battisti, F., 2008. Color image database for evaluation of image quality metrics. In: 2008 IEEE 10th Workshop on Multimedia Signal Processing. pp. 403\u2013408. http://dx.doi.org/10. 1109/MMSP.2008.4665112. Sammarra, Matteo, Piccolo, Claudia Lucia, Sarli, Marina, Stefanucci, Rita, Tom- masiello, Manuela, Orsaria, Paolo, Altomare, Vittorio, Beomonte Zobel, Bruno, 2024. Contrast-enhanced mammography-guided biopsy: Preliminary results of a single-center retrospective experience. J. Clin. Med.",
    "full_text_length": 57609,
    "chunk_length": 1512
  },
  {
    "chunk_id": 181,
    "paper_filename": "aurora_2024_deeplearning_for_virtual_enhacne_spectral_mammograpy.pdf",
    "paper_title": "Aurora 2024 Deeplearning For Virtual Enhacne Spectral Mammograpy",
    "chunk_index": 57,
    "total_chunks": 58,
    "text_content": "Xiaofeng, 2021. Generation of contrast-enhanced CT with residual cycle-consistent generative adversarial network (res-cyclegan). In: Medical Imaging 2021: Physics of Medical Imaging. Vol. 11595, SPIE, pp. 1042\u20131048. Yang, Qingsong, Yan, Pingkun, Kalra, Mannudeep K., Wang, Ge, 2017. CT image denoising with perceptive deep neural networks. arXiv preprint arXiv:1702.07019. Yoo, Jaechang, Eom, Heesong, Choi, Yong Suk, 2019. Image-to-image translation using a cross-domain auto-encoder and decoder. Ap",
    "full_text_length": 57609,
    "chunk_length": 1000
  },
  {
    "chunk_id": 182,
    "paper_filename": "badar_2013_dual_energy_contrast_enhanced_digital_mammography.pdf",
    "paper_title": "Badar 2013 Dual Energy Contrast Enhanced Digital Mammography",
    "chunk_index": 0,
    "total_chunks": 33,
    "text_content": "Diagnostic and Interventional Imaging (2014) 95, 245\u2014258 REVIEW / Breast imaging Dual-energy contrast-enhanced digital mammography in routine clinical practice in 2013 S. Badra,c,\u2217, N. Laurenta,b,c, C. R\u00e9gisa, L. Boulangera,c, S. Lemaillea, E. Ponceleta,b,c aCentre Hospitalier d\u2019Armenti\u00e8res, Centre d\u2019imagerie de la femme, 112, rue Sadi-Carnot, 59280 Armenti\u00e8res, France bCentre Hospitalier de Valenciennes, service d\u2019imagerie de la femme, avenue D\u00e9sandrouin, 59300 Valenciennes, France cCentre Hosp",
    "full_text_length": 36587,
    "chunk_length": 1469
  },
  {
    "chunk_id": 183,
    "paper_filename": "badar_2013_dual_energy_contrast_enhanced_digital_mammography.pdf",
    "paper_title": "Badar 2013 Dual Energy Contrast Enhanced Digital Mammography",
    "chunk_index": 1,
    "total_chunks": 33,
    "text_content": "imaging technique as well as the recent ref- erences, illustrated by clinical reports derived from our everyday practice to focus on the advantages and disadvantages of this new breast exploration. Dual-energy contrast-enhanced mammography is a recent, seemingly promising technique, in the management of breast can- cer . The main advantages consist of its easy installation, the good tolerance and the comfort in the interpretation of dif\ufb01cult to read mammograms. However , the indications and the ",
    "full_text_length": 36587,
    "chunk_length": 1368
  },
  {
    "chunk_id": 184,
    "paper_filename": "badar_2013_dual_energy_contrast_enhanced_digital_mammography.pdf",
    "paper_title": "Badar 2013 Dual Energy Contrast Enhanced Digital Mammography",
    "chunk_index": 2,
    "total_chunks": 33,
    "text_content": "breast lesions. However , problems related to the accessibility of MRI may delay the care of patients with breast cancer . \u2217Corresponding author . Centre Hospitalier d\u2019Armenti\u00e8res, Centre d\u2019imagerie de la femme, 112, rue Sadi-Carnot, 59280 Armenti\u00e8res, France. E-mail address: sammy .badr@etu.univ-lille2.fr (S. Badr). 2211-5684/$ \u2014 see front matter \u00a9 2013 \u00c9ditions fran\u00e7aises de radiologie. Published by Elsevier Masson SAS. All rights reserved. http://dx.doi.org/10.1016/j.diii.2013.10.002 246 S. B",
    "full_text_length": 36587,
    "chunk_length": 1390
  },
  {
    "chunk_id": 185,
    "paper_filename": "badar_2013_dual_energy_contrast_enhanced_digital_mammography.pdf",
    "paper_title": "Badar 2013 Dual Energy Contrast Enhanced Digital Mammography",
    "chunk_index": 3,
    "total_chunks": 33,
    "text_content": "and a review of the lit- erature on contrast-enhanced digital mammography , this article will present different aspects of the dual-energy contrast-enhanced technique that may be encountered in clinical practice. T echnical focus on contrast-enhanced mammography Contrast-enhanced mammography is based on the principle of digital subtraction between 2 images: one image con- taining information about breast vascularisation, the other about its morphology . The post-processing carried out on them re",
    "full_text_length": 36587,
    "chunk_length": 1397
  },
  {
    "chunk_id": 186,
    "paper_filename": "badar_2013_dual_energy_contrast_enhanced_digital_mammography.pdf",
    "paper_title": "Badar 2013 Dual Energy Contrast Enhanced Digital Mammography",
    "chunk_index": 4,
    "total_chunks": 33,
    "text_content": "found with benign lesions [8,9] ). Therefore, the \ufb01rst method of contrast-enhanced mam- mography analysed a single speci\ufb01c incidence at different times after injection. The \ufb01rst image without injection of contrast agent was used as a mask. The opaci\ufb01ed images were then digitally subtracted from the mask to produce the contrast-enhanced mammography images and only under- line the vascularised structures. According to the authors, 4 to 7 acquisitions were carried out each 60\u2014120 s (Fig. 1) [8,10\u20141",
    "full_text_length": 36587,
    "chunk_length": 1322
  },
  {
    "chunk_id": 187,
    "paper_filename": "badar_2013_dual_energy_contrast_enhanced_digital_mammography.pdf",
    "paper_title": "Badar 2013 Dual Energy Contrast Enhanced Digital Mammography",
    "chunk_index": 5,
    "total_chunks": 33,
    "text_content": "saturated by contrast agent, by a \u2018\u2018high energy\u2019\u2019 image (beyond the attenuation coef\ufb01cient of iodine, that is Kiodine = 33.2 keV) and the morphological information by a \u2018\u2018low energy\u2019\u2019 image (below Kiodine ). The digital subtraction of these 2 images only underlines the hyper-vascularised structures (Fig. 2a), as the temporal sub- traction technique does. Value and validity of dual-energy contrast-enhanced mammography No clinical study could demonstrate a signi\ufb01cant differ- ence between the enhan",
    "full_text_length": 36587,
    "chunk_length": 1390
  },
  {
    "chunk_id": 188,
    "paper_filename": "badar_2013_dual_energy_contrast_enhanced_digital_mammography.pdf",
    "paper_title": "Badar 2013 Dual Energy Contrast Enhanced Digital Mammography",
    "chunk_index": 6,
    "total_chunks": 33,
    "text_content": "al., on 120 patients, compared dual-energy contrast-enhanced mammography with mammography alone or with mammog- raphy alongside ultrasound scan in the search for malignant lesions [4]. This work demonstrated a signi\ufb01cant increase in sensitivity for dual-energy contrast-enhanced mammogra- phy (93%) when compared with mammography alone (78%), without a reduction in the speci\ufb01city (assessed at 63%) [4]. In this study , dual-energy contrast-enhanced mammogra- phy did not provide an increase in diagn",
    "full_text_length": 36587,
    "chunk_length": 1422
  },
  {
    "chunk_id": 189,
    "paper_filename": "badar_2013_dual_energy_contrast_enhanced_digital_mammography.pdf",
    "paper_title": "Badar 2013 Dual Energy Contrast Enhanced Digital Mammography",
    "chunk_index": 7,
    "total_chunks": 33,
    "text_content": "a: formation of an image in dual-energy contrast-enhanced mam- mography according to the digital subtraction method. The post-treatment result reveals structures greatly absorbing high energy X-rays, like the vessels saturated with iodine contrast agent. It should be noted that the high energy images are not directly accessible on the consoles; b: dual-energy contrast-enhanced mammography protocol. higher with the help of dual-energy contrast-enhanced mammography: 0.87 vs 0.83, P = 0.045), as we",
    "full_text_length": 36587,
    "chunk_length": 1322
  },
  {
    "chunk_id": 190,
    "paper_filename": "badar_2013_dual_energy_contrast_enhanced_digital_mammography.pdf",
    "paper_title": "Badar 2013 Dual Energy Contrast Enhanced Digital Mammography",
    "chunk_index": 8,
    "total_chunks": 33,
    "text_content": "to be equivalent in terms of the global detection of primary tumours, the MRI would seem to detect additional ipsilat- eral lesions better . However , the speci\ufb01city of dual-energy contrast-enhanced mammography turns to be higher with a lower proportion of false positives. Moreover , the positive predictive value of malignity of an enhanced lesion, in the population, was higher with dual-energy contrast-enhanced mammography than with MRI (97% vs 85%, P < 0.01) [14] .Practical focus on dual-energ",
    "full_text_length": 36587,
    "chunk_length": 1321
  },
  {
    "chunk_id": 191,
    "paper_filename": "badar_2013_dual_energy_contrast_enhanced_digital_mammography.pdf",
    "paper_title": "Badar 2013 Dual Energy Contrast Enhanced Digital Mammography",
    "chunk_index": 9,
    "total_chunks": 33,
    "text_content": "a pause of one minute between images (Fig. 2b). The protocol may be stopped if the dual-energy contrast- enhanced mammography is \u2018\u2018normal\u2019\u2019 (Fig. 3a). If there is an anomaly , the examination may be completed by a latero-medial and/or a spot compression view (Fig. 3b). 248 S. Badr et al. Figure 3. a: normal dual-energy contrast-enhanced mammography (inside left medio-lateral oblique and cranio-caudal view): absence of pathological enhancement. White arrow: vessels; black arrow: diffuse matrix en",
    "full_text_length": 36587,
    "chunk_length": 1375
  },
  {
    "chunk_id": 192,
    "paper_filename": "badar_2013_dual_energy_contrast_enhanced_digital_mammography.pdf",
    "paper_title": "Badar 2013 Dual Energy Contrast Enhanced Digital Mammography",
    "chunk_index": 10,
    "total_chunks": 33,
    "text_content": "not at all enhanced (Fig. 4). When breast cancer is clinically or radiologically sus- pected, dual-energy contrast-enhanced mammography may be useful to detect multiple homo- or contra-lateral can- cers, with the injection of a contrast agent that helps reveal lesions that are not spontaneously visible by standard mammograms (Fig. 5) [5,6] . Dual-energy contrast-enhancedmammography also facilitates the ultrasound study of mul- tiple lesions both to better detect them and also to more easily deci",
    "full_text_length": 36587,
    "chunk_length": 1335
  },
  {
    "chunk_id": 193,
    "paper_filename": "badar_2013_dual_energy_contrast_enhanced_digital_mammography.pdf",
    "paper_title": "Badar 2013 Dual Energy Contrast Enhanced Digital Mammography",
    "chunk_index": 11,
    "total_chunks": 33,
    "text_content": "enhanced mammography and mammography plus breast ultrasound scan (90% sensitivity , P = 0.72; 47% speci\ufb01city , P = 0.08). Dual-energy contrast-enhanced digital mammography 249 Figure 4. A 65-year-old woman with invasive ductal carcinoma initially consulting for a right retroareolar breast mass with permeation nodules. On the right, images by dual-energy contrast-enhanced mammography with cranio-caudal (CC) view show an extensive right retroareolar enhancement opposite a clinical lesion and a sus",
    "full_text_length": 36587,
    "chunk_length": 1347
  },
  {
    "chunk_id": 194,
    "paper_filename": "badar_2013_dual_energy_contrast_enhanced_digital_mammography.pdf",
    "paper_title": "Badar 2013 Dual Energy Contrast Enhanced Digital Mammography",
    "chunk_index": 12,
    "total_chunks": 33,
    "text_content": "nipple due to an architectural disorganisation whose limits are poorly vis- ible, and the presence of a rounded retroareolar breast mass, while the contrast enhancement with dual-energy mammography helps better discern its extension. In this case, it also reassures us about the lack of enhancement ofthe rounded retroareolar mass that, after anatomopatho- logical analysis, corresponds to a galactocele whereas the ultrasound scan might have seemed worrying due to the partially echogenic and irregu",
    "full_text_length": 36587,
    "chunk_length": 1401
  },
  {
    "chunk_id": 195,
    "paper_filename": "badar_2013_dual_energy_contrast_enhanced_digital_mammography.pdf",
    "paper_title": "Badar 2013 Dual Energy Contrast Enhanced Digital Mammography",
    "chunk_index": 13,
    "total_chunks": 33,
    "text_content": "cancer , a contra-lateral lesion was discovered Figure 5. Dual-energy contrast-enhanced mammography for a clinically detected nodule of the left axillary prolongation. Whereas the standard mammogram (top left) shows an architectural disorganisation in the left axillary prolongation (white arrow), dual-energy contrast- enhanced mammography (top right) reveals multiple enhancements and more easily allows for the detection of suspect images in ultrasound scan (at the bottom) and helps orient the bi",
    "full_text_length": 36587,
    "chunk_length": 1407
  },
  {
    "chunk_id": 196,
    "paper_filename": "badar_2013_dual_energy_contrast_enhanced_digital_mammography.pdf",
    "paper_title": "Badar 2013 Dual Energy Contrast Enhanced Digital Mammography",
    "chunk_index": 14,
    "total_chunks": 33,
    "text_content": "translation in the mammography and ultrasound scan. Dual-energy contrast-enhanced mammog- raphy then led to an MRI and then a biopsy under MRI to \ufb01nally conclude to the discovery of a small \ufb01broadenoma in the left breast. If biopsies were possible under dual-energy contrast-enhanced mammography , the patient would have been reassured more quickly and her treatment would not have been as complex. Post-therapeutic monitoring The scar tissue remodelling after breast surgery often appears as archite",
    "full_text_length": 36587,
    "chunk_length": 1387
  },
  {
    "chunk_id": 197,
    "paper_filename": "badar_2013_dual_energy_contrast_enhanced_digital_mammography.pdf",
    "paper_title": "Badar 2013 Dual Energy Contrast Enhanced Digital Mammography",
    "chunk_index": 15,
    "total_chunks": 33,
    "text_content": "\ufb01brotic tissue from a recurrence of breast cancer with an enhancement similar to that found in MRI. Diagnostic assessment of abnormal imaging \ufb01ndings After a classic breast examination (mammograms and ultra- sound scan), probably benign lesions are often classi\ufb01ed as ACR or BI-RADS 3, especially due to probable echogenic cysts. In this situation, dual-energy contrast-enhanced Figure 7. A 38-year-old woman presenting a suspect mass in the right breast. Dual-energy contrast-enhanced mammography , ",
    "full_text_length": 36587,
    "chunk_length": 1384
  },
  {
    "chunk_id": 198,
    "paper_filename": "badar_2013_dual_energy_contrast_enhanced_digital_mammography.pdf",
    "paper_title": "Badar 2013 Dual Energy Contrast Enhanced Digital Mammography",
    "chunk_index": 16,
    "total_chunks": 33,
    "text_content": "due to asymmetric densities. The normal spot compressed images and dual-energy contrast-enhanced mammography reassured the patient. mammography may help distinguish between a purely liquid cyst (Fig. 10) and a cystic lesion containing a tissue portion (Fig. 11). Purely liquid lesions may directly be classi\ufb01ed as benign ACR or BI-RADS 2 lesions without monitoring, and lesions with a solid component may be directly classi\ufb01ed as ACR or BI-RADS 4 in view of a biopsy for a histological examination to",
    "full_text_length": 36587,
    "chunk_length": 1305
  },
  {
    "chunk_id": 199,
    "paper_filename": "badar_2013_dual_energy_contrast_enhanced_digital_mammography.pdf",
    "paper_title": "Badar 2013 Dual Energy Contrast Enhanced Digital Mammography",
    "chunk_index": 17,
    "total_chunks": 33,
    "text_content": "Figure 9. A 68-year-old woman with a personal antecedent of right breast cancer treated by surgery . The standard mammogram (on the left) show dense, dif\ufb01cult to analyse breasts. The dual-energy contrast-enhanced mammography images do not reveal enhancement, which is reassuring. An ultrasound scan is carried out and is normal (no modi\ufb01cation in the radiology procedure even though the dual-energy contrast-enhanced mammography is already reassuring). Among the patients, 32% presented a family hist",
    "full_text_length": 36587,
    "chunk_length": 1297
  },
  {
    "chunk_id": 200,
    "paper_filename": "badar_2013_dual_energy_contrast_enhanced_digital_mammography.pdf",
    "paper_title": "Badar 2013 Dual Energy Contrast Enhanced Digital Mammography",
    "chunk_index": 18,
    "total_chunks": 33,
    "text_content": "were not enhanced, with a histology mainly related to \ufb01broadenomas (n = 7/10). The only false negative in this study corresponded to a lymphangitic carcinomatosis. This data was used to estimate the 95% sensitivity and the 85% speci\ufb01city . The positive and negative predictive values were 67% and 98%, respectively . The main artefact found in the series is matrix enhance- ment in 35% of the cases (n = 26/75). It was pseudo-nodular for 58% (n = 15/26) and diffuse for 42% (n = 11/26) of them. The m",
    "full_text_length": 36587,
    "chunk_length": 1260
  },
  {
    "chunk_id": 201,
    "paper_filename": "badar_2013_dual_energy_contrast_enhanced_digital_mammography.pdf",
    "paper_title": "Badar 2013 Dual Energy Contrast Enhanced Digital Mammography",
    "chunk_index": 19,
    "total_chunks": 33,
    "text_content": "with thick walls, multi-loculated cysts. The exam is classi\ufb01ed as ACR or BI-RADS 3 following the ultrasound scan. The dual-energy contrast-enhanced mammography (top right) does not reveal any intra-cystic enhancement. The exam may therefore be reclassi\ufb01ed as ACR or BI-RADS 2. Dual-energy contrast-enhanced mammography reveals cysts in the form of \u2018\u2018phantom\u2019\u2019 images. Dual-energy contrast-enhanced digital mammography 253 Figure 11. A 52-year-old woman consulting for mobile induration of the right b",
    "full_text_length": 36587,
    "chunk_length": 1526
  },
  {
    "chunk_id": 202,
    "paper_filename": "badar_2013_dual_energy_contrast_enhanced_digital_mammography.pdf",
    "paper_title": "Badar 2013 Dual Energy Contrast Enhanced Digital Mammography",
    "chunk_index": 20,
    "total_chunks": 33,
    "text_content": "be detected. In our series, we only observed one false negative, corresponding to lymphan- gitis carcinomatosis (Fig. 12). T able 1 Absolute and relative distribution of enhance- ment observed in 37 diagnosed lesions. Number Proportion (%) Enhancement 10 27% Fibroadenoma 7* Microcystic area 1 Sclerosing adenosis 1 Lymphangitic carcinomatosis 1 Enhancement of benign lesions 9 24% Fibroadenoma 4 Microcystic area 1 Papilloma 1Angiomatosis 1 Intramammary lymph node 2 Enhancement of malignant lesions",
    "full_text_length": 36587,
    "chunk_length": 1349
  },
  {
    "chunk_id": 203,
    "paper_filename": "badar_2013_dual_energy_contrast_enhanced_digital_mammography.pdf",
    "paper_title": "Badar 2013 Dual Energy Contrast Enhanced Digital Mammography",
    "chunk_index": 21,
    "total_chunks": 33,
    "text_content": ", the in situ study of carcinomas is poorly known. They may also be a source of false negatives in dual-energy contrast-enhanced mammography . False positives in dual-energy contrast-enhanced mammography Our series reported several cases of enhanced benign lesions (33%, n = 9/27). We observed 2 types of \ufb01broadenoma enhancement: a slow pro\ufb01le, homogenising in late images, and a rapid pro\ufb01le, in early images (Fig. 13). This kinetics was also described in MRI [7,8] . A complementary ultrasound scan",
    "full_text_length": 36587,
    "chunk_length": 1283
  },
  {
    "chunk_id": 204,
    "paper_filename": "badar_2013_dual_energy_contrast_enhanced_digital_mammography.pdf",
    "paper_title": "Badar 2013 Dual Energy Contrast Enhanced Digital Mammography",
    "chunk_index": 22,
    "total_chunks": 33,
    "text_content": "may be the cause of false positives due to the appearance that may seem to be sus- pect. In our series, a small number of examinations (n = 3) were potentially masked by matrix enhancement. In these cases, dual-energy contrast-enhanced mammography did not provide any additional information when compared to the mammography and ultrasound scan. Motion blur may also be observed in case of patient\u2019s movement between 254 S. Badr et al. Figure 12. A 55-year-old woman with antecedents of total left mas",
    "full_text_length": 36587,
    "chunk_length": 1295
  },
  {
    "chunk_id": 205,
    "paper_filename": "badar_2013_dual_energy_contrast_enhanced_digital_mammography.pdf",
    "paper_title": "Badar 2013 Dual Energy Contrast Enhanced Digital Mammography",
    "chunk_index": 23,
    "total_chunks": 33,
    "text_content": "displaced with a black edge. In case of motion blur , a cancer may be expressed by a \u2018\u2018black car- cinoma\u2019\u2019: the enhancement of the cancer being masked by the movement artefacts (Fig. 14d). In this case, the diagnosis may be \u2018\u2018corrected\u2019\u2019 on the other dual-energy contrast-enhanced mammography incidences like on the complementary mammography and ultrasound examination. Glandular dose The diagnostic reference level in digital mammography , as de\ufb01ned by Institut de Radioprotection et de S\u00fbret\u00e9 Nucl\u00e9",
    "full_text_length": 36587,
    "chunk_length": 1241
  },
  {
    "chunk_id": 206,
    "paper_filename": "badar_2013_dual_energy_contrast_enhanced_digital_mammography.pdf",
    "paper_title": "Badar 2013 Dual Energy Contrast Enhanced Digital Mammography",
    "chunk_index": 24,
    "total_chunks": 33,
    "text_content": "mGy , and a standard deviation of 0.78 mGy (T able 2). High energy acquisitions account for 25% of the total dose in dual-energy contrast-enhanced mammography with an average MGD of 0.65 mGy ([0.24\u20140.83], standard deviation of 0.23 mGy), vs 2.00 mGy ([0.84\u20143.74], standard deviation of 0.58 mGy) for low energy acquisitions. The exposure of \u2018\u2018standard\u2019\u2019 digital mammograms from the same patient population (360 cranio-caudal and medio- lateral views) was analysed: mean thickness of the breast of57 m",
    "full_text_length": 36587,
    "chunk_length": 1315
  },
  {
    "chunk_id": 207,
    "paper_filename": "badar_2013_dual_energy_contrast_enhanced_digital_mammography.pdf",
    "paper_title": "Badar 2013 Dual Energy Contrast Enhanced Digital Mammography",
    "chunk_index": 25,
    "total_chunks": 33,
    "text_content": "Prospects for dual-energy contrast-enhanced mammography From a technical point of view , one of the main disad- vantages of dual-energy contrast-enhanced mammography is the current inability to carry out biopsies under dual- energy contrast-enhanced mammography due to the lack of a speci\ufb01c system of stereotaxy . Therefore, it is neces- sary to carry out breast MRI and then biopsies under MRI control, of enhancements visible in dual-energy contrast- enhanced mammography and not found in mammogram",
    "full_text_length": 36587,
    "chunk_length": 1367
  },
  {
    "chunk_id": 208,
    "paper_filename": "badar_2013_dual_energy_contrast_enhanced_digital_mammography.pdf",
    "paper_title": "Badar 2013 Dual Energy Contrast Enhanced Digital Mammography",
    "chunk_index": 26,
    "total_chunks": 33,
    "text_content": "MGD Interval (standard deviation) Dual-energy contrast-enhanced mammography High energy (HE) 0.65 0.24\u20140.83 (0.23) Low energy (LE) 2.00 0.84\u20143.74 (0.58) HE + LE (1) 2.65 1.07\u20144.76 (0.78) Standard mammograms (2) 1.72 0.74\u20147.82 (0.96) Ratio (1)/(2) 1.54 Dual-energy contrast-enhanced digital mammography 255 Figure 13. Dual-energy contrast-enhanced mammography image of a \ufb01broadenoma. a: \ufb01broadenoma with slow enhancement. The \ufb01broadenoma (arrow) is better seen in the late dual-energy contrast-enhance",
    "full_text_length": 36587,
    "chunk_length": 1408
  },
  {
    "chunk_id": 209,
    "paper_filename": "badar_2013_dual_energy_contrast_enhanced_digital_mammography.pdf",
    "paper_title": "Badar 2013 Dual Energy Contrast Enhanced Digital Mammography",
    "chunk_index": 27,
    "total_chunks": 33,
    "text_content": "the right side (black arrow), appears black and \u2018\u2018not enhanced\u2019\u2019 in the medio-lateral oblique view due to the movement of the patient: black carcinoma (white arrow). interpretation of the location of the enhancement in MRI. The value of dual-energy contrast-enhanced mammography when compared to MRI is to directly see the correspon- dence between the morphological anomalies detected in mammography on the low energy images and the enhance- ments visible in the recombined images. This advantage of ",
    "full_text_length": 36587,
    "chunk_length": 1369
  },
  {
    "chunk_id": 210,
    "paper_filename": "badar_2013_dual_energy_contrast_enhanced_digital_mammography.pdf",
    "paper_title": "Badar 2013 Dual Energy Contrast Enhanced Digital Mammography",
    "chunk_index": 28,
    "total_chunks": 33,
    "text_content": "Figure 15. Screening of a 50-year-old woman without antecedents with a normal physical examination. Inner architectural distortion visible only on the cranio-caudal side (white arrow). No ultrasound target: an MRI (bottom) is carried out revealing inner enhancement of the left breast (black arrow) biopsied under MRI. The histological examination concluded to angiomatosis. A dual-energy contrast-enhanced mammography carried out after the biopsies under MRI helped \u2018\u2018discover\u2019\u2019 that the biopsied en",
    "full_text_length": 36587,
    "chunk_length": 1395
  },
  {
    "chunk_id": 211,
    "paper_filename": "badar_2013_dual_energy_contrast_enhanced_digital_mammography.pdf",
    "paper_title": "Badar 2013 Dual Energy Contrast Enhanced Digital Mammography",
    "chunk_index": 29,
    "total_chunks": 33,
    "text_content": "breast cancer . This simple technique may be easier and quicker to access than MRI if dual-energy contrast-enhanced mammography spreads. However , additional studies are necessary to better spec- ify the indications, diagnostic performance and its role in the strategy for the screening and care of breast cancer , in particular when compared to MRI. Disclosure of interest The authors declare that they have no con\ufb02icts of interest concerning this article.References [1] Watt AC, Ackerman LV, Shetty",
    "full_text_length": 36587,
    "chunk_length": 1334
  },
  {
    "chunk_id": 212,
    "paper_filename": "badar_2013_dual_energy_contrast_enhanced_digital_mammography.pdf",
    "paper_title": "Badar 2013 Dual Energy Contrast Enhanced Digital Mammography",
    "chunk_index": 30,
    "total_chunks": 33,
    "text_content": "digital mam- mography: initial clinical results of a multireader , multicase study . Breast Cancer Res 2012;14(3):R94. [4] Dromain C, Thibault F, Muller S, Rimareix F, Delaloge S, T ardivon A, et al. Dual-energy contrast-enhanced digital mammography: initial clinical results. Eur Radiol 2011;21(3):565\u201474. [5] Dromain C, Balleyguier C, Adler G, Garbay JR, Delaloge S. Contrast-enhanced digital mammography . Eur J Radiol 2009;69(1):34\u201442. [6] Dromain C, Balleyguier C, Adler G, Garbay J-R, Delaloge ",
    "full_text_length": 36587,
    "chunk_length": 1350
  },
  {
    "chunk_id": 213,
    "paper_filename": "badar_2013_dual_energy_contrast_enhanced_digital_mammography.pdf",
    "paper_title": "Badar 2013 Dual Energy Contrast Enhanced Digital Mammography",
    "chunk_index": 31,
    "total_chunks": 33,
    "text_content": "Radiology 1995;196(3):593\u2014610. [10] Dromain C, Balleyguier C, Muller S, Mathieu MC, Rochard F, Opolon P, et al. Evaluation of tumor angiogenesis of breast car- cinoma using contrast-enhanced digital mammography . AJR Am J Roentgenol 2006;187(5):W528\u201437. [11] Diekmann F, Diekmann S, Jeunehomme F, Muller S, Hamm B, Bick U. Digital mammography using iodine-based con- trast media: initial clinical experience with dynamic contrast medium enhancement. Invest Radiol 2005;40(7):397\u2014404. [12] Diekmann F,",
    "full_text_length": 36587,
    "chunk_length": 1354
  },
  {
    "chunk_id": 214,
    "paper_filename": "badar_2013_dual_energy_contrast_enhanced_digital_mammography.pdf",
    "paper_title": "Badar 2013 Dual Energy Contrast Enhanced Digital Mammography",
    "chunk_index": 32,
    "total_chunks": 33,
    "text_content": "Radiology 2013;266(3): 743\u201451. [15] Berg WA, Gutierrrez L, NessAiver MS, Carter WB, Bhargavan M, Lewis RS, et al. Diagnostic accuracy of mammography , clinical examination, US, and MR imaging in preoperative assessment of breast cancer . Radiology 2004;233:830\u201449.",
    "full_text_length": 36587,
    "chunk_length": 264
  },
  {
    "chunk_id": 215,
    "paper_filename": "barsha_2023_comprehensive_review_on_breast_cancer_detection_classifcation_segmetnaiotn_using_deeplearning.pdf",
    "paper_title": "Barsha 2023 Comprehensive Review On Breast Cancer Detection Classifcation Segmetnaiotn Using Deeplearning",
    "chunk_index": 0,
    "total_chunks": 147,
    "text_content": "See discussions, st ats, and author pr ofiles f or this public ation at : https://www .researchgate.ne t/public ation/372200167 A Comprehensive Review on Breast Cancer Detection, Classi\ufb01cation and Segmentation Using Deep Learning Article in Archiv es of Comput ational Me thods in Engineering \u00b7 July 2023 DOI: 10.1007/s11831-023-09968-z CITATIONS 112READS 3,514 3 author s: Barsha Abhishek a National Instit ute Of T echnolog y Silchar 14 PUBLICA TIONS 223 CITATIONS SEE PROFILE Saroj Kr . Bisw as Na",
    "full_text_length": 144240,
    "chunk_length": 1305
  },
  {
    "chunk_id": 216,
    "paper_filename": "barsha_2023_comprehensive_review_on_breast_cancer_detection_classifcation_segmetnaiotn_using_deeplearning.pdf",
    "paper_title": "Barsha 2023 Comprehensive Review On Breast Cancer Detection Classifcation Segmetnaiotn Using Deeplearning",
    "chunk_index": 1,
    "total_chunks": 147,
    "text_content": "Review on Breast Cancer Detection, Classification and Segmentation Using Deep Learning Barsha Abhisheka1 \u00b7 Saroj Kumar Biswas1 \u00b7 Biswajit Purkayastha1 Received: 16 February 2023 / Accepted: 26 June 2023 \u00a9 The Author(s) under exclusive licence to International Center for Numerical Methods in Engineering (CIMNE) 2023 Abstract The incidence and mortality rate of Breast Cancer (BC) are global problems for women, with over 2.1 million new diagnoses each year worldwide. There is no age range, race, or",
    "full_text_length": 144240,
    "chunk_length": 1318
  },
  {
    "chunk_id": 217,
    "paper_filename": "barsha_2023_comprehensive_review_on_breast_cancer_detection_classifcation_segmetnaiotn_using_deeplearning.pdf",
    "paper_title": "Barsha 2023 Comprehensive Review On Breast Cancer Detection Classifcation Segmetnaiotn Using Deeplearning",
    "chunk_index": 2,
    "total_chunks": 147,
    "text_content": "being employed. However, there is a dearth of review articles that summarize the current research on BC diagnosis. This manuscript addresses the current state of the art in artificial Deep Neural Network (DNN) techniques for BC detection, classification and segmentation using medical imaging. In addition, it emphasizes the working principles, benefits and limitations of imaging modalities used to detect BC, along with a comprehensive analysis of those modalities. The primary purpose of this pape",
    "full_text_length": 144240,
    "chunk_length": 1365
  },
  {
    "chunk_id": 218,
    "paper_filename": "barsha_2023_comprehensive_review_on_breast_cancer_detection_classifcation_segmetnaiotn_using_deeplearning.pdf",
    "paper_title": "Barsha 2023 Comprehensive Review On Breast Cancer Detection Classifcation Segmetnaiotn Using Deeplearning",
    "chunk_index": 3,
    "total_chunks": 147,
    "text_content": "distinct neural network architectures, both shallow and deep, are used to analyze BC images. The CNN is widely employed to develop an efficient BC classification model and several studies either used a pre-trained model or created a new DNN. Lastly, this review addressed 13 significant challenges that are encountered throughout the course of the review for future researchers that aim to improve BC diagnosis models using a wide range of imaging techniques. This paper has the potential to be a hel",
    "full_text_length": 144240,
    "chunk_length": 1241
  },
  {
    "chunk_id": 219,
    "paper_filename": "barsha_2023_comprehensive_review_on_breast_cancer_detection_classifcation_segmetnaiotn_using_deeplearning.pdf",
    "paper_title": "Barsha 2023 Comprehensive Review On Breast Cancer Detection Classifcation Segmetnaiotn Using Deeplearning",
    "chunk_index": 4,
    "total_chunks": 147,
    "text_content": "85% of cases, BC origi- nates in the lining cells (epithelium) of the ducts and 15% in the lobules of breast glandular tissue [1 ]. Initially, the malignant lesion is localized to the duct or lobule, where it normally causes no symptoms and has a low risk of spread- ing (metastasis). These malignancies, if left untreated, can spread to neighboring lymph nodes (regional metastasis) and eventually to other organs in the body (distant metastasis). Figure 1 illustrates a morphological change that oc",
    "full_text_length": 144240,
    "chunk_length": 1262
  },
  {
    "chunk_id": 220,
    "paper_filename": "barsha_2023_comprehensive_review_on_breast_cancer_detection_classifcation_segmetnaiotn_using_deeplearning.pdf",
    "paper_title": "Barsha 2023 Comprehensive Review On Breast Cancer Detection Classifcation Segmetnaiotn Using Deeplearning",
    "chunk_index": 5,
    "total_chunks": 147,
    "text_content": "determined by the Tumor Nodes Metastases (TNM) sys- tem, which was created by the American Joint Committee on Cancer (AJCC) [2 ]. BC is not a contagious or infectious condition. It is estimated that almost 50% of all breast can- cers occur in women who have no recognised breast cancer risk factors other than their gender and age ( age>40years ). In addition, obesity, consumption of alcohol, having a family history of breast cancer, radiation exposure, smoking, and undergoing postmenopausal hormo",
    "full_text_length": 144240,
    "chunk_length": 1188
  },
  {
    "chunk_id": 221,
    "paper_filename": "barsha_2023_comprehensive_review_on_breast_cancer_detection_classifcation_segmetnaiotn_using_deeplearning.pdf",
    "paper_title": "Barsha 2023 Comprehensive Review On Breast Cancer Detection Classifcation Segmetnaiotn Using Deeplearning",
    "chunk_index": 6,
    "total_chunks": 147,
    "text_content": "sig- nificantly higher in middle and low income countries. The World Health Organization (WHO) reports that in the year 2020, 2.3 million women had BC and 68,500 died through this globally [8 ]. In 2022, approximately 287,850 new cases of invasive BC and 51,400 cases of Ductal Carcinoma In Situ (DCIS) have been diagnosed among US women, and 43,250 women died of BC [9 ]. Moreover, according to the National Institute of Cancer Prevention and Research (NICPR) in India, for every two new instances o",
    "full_text_length": 144240,
    "chunk_length": 1215
  },
  {
    "chunk_id": 222,
    "paper_filename": "barsha_2023_comprehensive_review_on_breast_cancer_detection_classifcation_segmetnaiotn_using_deeplearning.pdf",
    "paper_title": "Barsha 2023 Comprehensive Review On Breast Cancer Detection Classifcation Segmetnaiotn Using Deeplearning",
    "chunk_index": 7,
    "total_chunks": 147,
    "text_content": "is a critical condition. Furthermore, inadequate access to specialists and experts in rural areas makes it harder to identify and detect BC early and accurately, which contributes to a higher mortality rate. Early detection of BC aids in providing better quality treat- ment to patients, hence reducing the mortality rate. Figure 2 shows the incidence and mortality rates for the ten most common female malignancies in the year 2020. Medical specialists believe that the best way to improve patients\u2019",
    "full_text_length": 144240,
    "chunk_length": 1244
  },
  {
    "chunk_id": 223,
    "paper_filename": "barsha_2023_comprehensive_review_on_breast_cancer_detection_classifcation_segmetnaiotn_using_deeplearning.pdf",
    "paper_title": "Barsha 2023 Comprehensive Review On Breast Cancer Detection Classifcation Segmetnaiotn Using Deeplearning",
    "chunk_index": 8,
    "total_chunks": 147,
    "text_content": "interest is depicted differently on the basis of the screening method. This fact encouraged us to study several screening approaches in order to choose the most effective screening for detecting and classifying BC. Images gener - ated through screening techniques aid radiologists and doc- tors in detecting diseases, thereby lowering mortality risk by 30\u201370% [14]. The screening techniques that are most often used to detect BC are Digital Mammogram (DM), Breast Ultrasound (BUS), Breast Magnetic Re",
    "full_text_length": 144240,
    "chunk_length": 1359
  },
  {
    "chunk_id": 224,
    "paper_filename": "barsha_2023_comprehensive_review_on_breast_cancer_detection_classifcation_segmetnaiotn_using_deeplearning.pdf",
    "paper_title": "Barsha 2023 Comprehensive Review On Breast Cancer Detection Classifcation Segmetnaiotn Using Deeplearning",
    "chunk_index": 9,
    "total_chunks": 147,
    "text_content": "of its total doctors [15]. In addition, the World Health Organization (WHO) states that India\u2019s doctor-to-patient ratio is 1:1000 Fig. 1 Morphological transition of cancer cells A Comprehensive Review on Breast Cancer Detection, Classification and Segmentation Using\u2026 1 3[16]. Furthermore, due to limited resources and a shortage of expert opinions the process of detecting a vast number of targeted people has become laborious. Thus, because of the constraints of traditional methods such as rigorou",
    "full_text_length": 144240,
    "chunk_length": 1363
  },
  {
    "chunk_id": 225,
    "paper_filename": "barsha_2023_comprehensive_review_on_breast_cancer_detection_classifcation_segmetnaiotn_using_deeplearning.pdf",
    "paper_title": "Barsha 2023 Comprehensive Review On Breast Cancer Detection Classifcation Segmetnaiotn Using Deeplearning",
    "chunk_index": 10,
    "total_chunks": 147,
    "text_content": "evaluation of biomarkers provides a means for medical experts to assess the stage and prognosis of BC. The use of biomarkers [18] has the potential to improve prognosis and treatment outcomes. Proteins, nucleic acids, ribonucleic acids, and metabolites are examples of biomol- ecules that can serve as cancer indicators [19]. By analyzing biomarkers, clinicians can gain a deeper understanding of a patient\u2019s disease status and make better decisions regarding treatment therapies. Various biomarkers ",
    "full_text_length": 144240,
    "chunk_length": 1316
  },
  {
    "chunk_id": 226,
    "paper_filename": "barsha_2023_comprehensive_review_on_breast_cancer_detection_classifcation_segmetnaiotn_using_deeplearning.pdf",
    "paper_title": "Barsha 2023 Comprehensive Review On Breast Cancer Detection Classifcation Segmetnaiotn Using Deeplearning",
    "chunk_index": 11,
    "total_chunks": 147,
    "text_content": "show elevated levels in other non-BC conditions, leading to false positive results. For instance, CA 15-3 bio- marker levels may increase in conditions like liver disease and lung disease, which can result in misdiagnosis. Another challenge is the variability of biomarker expression, which can differ between different subtypes of BC, and even within the same subtype. This inconsistency makes it difficult to identify biomarkers that are reliable for all types of BC. As a result, there is a need f",
    "full_text_length": 144240,
    "chunk_length": 1295
  },
  {
    "chunk_id": 227,
    "paper_filename": "barsha_2023_comprehensive_review_on_breast_cancer_detection_classifcation_segmetnaiotn_using_deeplearning.pdf",
    "paper_title": "Barsha 2023 Comprehensive Review On Breast Cancer Detection Classifcation Segmetnaiotn Using Deeplearning",
    "chunk_index": 12,
    "total_chunks": 147,
    "text_content": "instance, Artificial Intelligence (AI), Computer Vision and Medical Image Processing (MIP). CAD approaches help to minimize the effect of the operator-dependent nature and increase the diagnostic sensitivity and specificity. Identify - ing and classifying subtypes of BC correctly is a significant clinical task. However, it has been observed that 65\u201390% of the biopsies turned out to be benign, therefore, a crucial goal of CAD systems is to distinguish benign and malignant lesions to reduce unnece",
    "full_text_length": 144240,
    "chunk_length": 1265
  },
  {
    "chunk_id": 228,
    "paper_filename": "barsha_2023_comprehensive_review_on_breast_cancer_detection_classifcation_segmetnaiotn_using_deeplearning.pdf",
    "paper_title": "Barsha 2023 Comprehensive Review On Breast Cancer Detection Classifcation Segmetnaiotn Using Deeplearning",
    "chunk_index": 13,
    "total_chunks": 147,
    "text_content": "deaths for the top ten most prevalent cancers among women in 2020 B. Abhisheka et al. 1 3 effective conclusions using Deep Learning (DL) to carry out a variety of clinical tasks. Figure 3 illustrates the workflow of the CAD system. ML has become widely used in several fields, from rec- ommendation systems [24] to health care [25], sentiment analysis [26], and transportation [27]. Amongst all, DL is the frequently used ML algorithm in these applications [28\u201331]. Traditional ML approaches are limi",
    "full_text_length": 144240,
    "chunk_length": 1330
  },
  {
    "chunk_id": 229,
    "paper_filename": "barsha_2023_comprehensive_review_on_breast_cancer_detection_classifcation_segmetnaiotn_using_deeplearning.pdf",
    "paper_title": "Barsha 2023 Comprehensive Review On Breast Cancer Detection Classifcation Segmetnaiotn Using Deeplearning",
    "chunk_index": 14,
    "total_chunks": 147,
    "text_content": "In addition, the manual feature selection approach sometimes fails to adequately extract the desired feature. To address these challenges, the idea of an advanced ML approach such as DL has been introduced, which is robust in extracting the appropriate data from the raw images and effectively differentiating classes. DL models are capable of learning new features on their own through identifying and learning hidden patterns in ROI (Region of Interest) due to their hierarchical design of Deep neu",
    "full_text_length": 144240,
    "chunk_length": 1339
  },
  {
    "chunk_id": 230,
    "paper_filename": "barsha_2023_comprehensive_review_on_breast_cancer_detection_classifcation_segmetnaiotn_using_deeplearning.pdf",
    "paper_title": "Barsha 2023 Comprehensive Review On Breast Cancer Detection Classifcation Segmetnaiotn Using Deeplearning",
    "chunk_index": 15,
    "total_chunks": 147,
    "text_content": "etc. for the early diagnosis of BC. Thus, this manuscript highlights the significant draw - backs and constraints of traditional approaches for BC diag- nosis, which proves the significance of a CAD system. Fur - thermore, it identifies and studies the fundamental concepts, strengths and limitations of different imaging modalities employed for BC detection, classification and segmentation. Besides, it reports the recent advancements in the design of CAD using DL analyses and their limitations. T",
    "full_text_length": 144240,
    "chunk_length": 1289
  },
  {
    "chunk_id": 231,
    "paper_filename": "barsha_2023_comprehensive_review_on_breast_cancer_detection_classifcation_segmetnaiotn_using_deeplearning.pdf",
    "paper_title": "Barsha 2023 Comprehensive Review On Breast Cancer Detection Classifcation Segmetnaiotn Using Deeplearning",
    "chunk_index": 16,
    "total_chunks": 147,
    "text_content": "quality matrices are explained, followed by the chal- lenges encountered, future implications, and a conclusion in Sects. 11 and 12. 2 Research Methodology To conduct the review for this paper, a systematic approach is used to search for relevant literature in multi- ple databases. Initial steps in the analysis involve defining the scope of the review, which included various aspects such as screening modalities, segmentation techniques, DL tools, datasets, quality metrics, and challenges. The su",
    "full_text_length": 144240,
    "chunk_length": 1278
  },
  {
    "chunk_id": 232,
    "paper_filename": "barsha_2023_comprehensive_review_on_breast_cancer_detection_classifcation_segmetnaiotn_using_deeplearning.pdf",
    "paper_title": "Barsha 2023 Comprehensive Review On Breast Cancer Detection Classifcation Segmetnaiotn Using Deeplearning",
    "chunk_index": 17,
    "total_chunks": 147,
    "text_content": "diagnosis and what are their strengths and limita- tions ? Q2. What medical imaging datasets are utilized to develop DL based classification models? Q3. What are the DL techniques employed for BC clas- sification? Q4. How do segmentation techniques improve the classifi - cation results in BC classification? Q5. What performance evaluation metrics are implemented to assess the results of classification models? Q6. What are the current challenges and opportunities in the field of BC? Fig. 3 Workfl",
    "full_text_length": 144240,
    "chunk_length": 1295
  },
  {
    "chunk_id": 233,
    "paper_filename": "barsha_2023_comprehensive_review_on_breast_cancer_detection_classifcation_segmetnaiotn_using_deeplearning.pdf",
    "paper_title": "Barsha 2023 Comprehensive Review On Breast Cancer Detection Classifcation Segmetnaiotn Using Deeplearning",
    "chunk_index": 18,
    "total_chunks": 147,
    "text_content": "classification\u201d, \u201cBC detec- tion\u201d and \u201cBC segmentation\u201d. The use of these keywords leads to the formation of a variety of strings, which are dis- played as follows: \u2022 \u201cBC\u201d+ \u201cDM/BUS/CT/BMRI/HP/TI/PET\u201d \u2022 \u201cBC\u201d+ \u201chistory\u201d + \u201cfirst time applied CNN model\u201d \u2022 \u201cBC\u201d+ \u201cscreening techniques\u201d \u2022 \u201cBC\u201d+ \u201cpublicly available datasets\u201d + DL \u2022 \u201cBC\u201d+ \u201cDL\u201d + \u201chybrid DL\u201d \u2022 \u201cBC\u201d+ \u201csegmentation technique\u201d The papers are searched and collected from credible online database sources like Springer, Wiley, Elsevier, IEEE an",
    "full_text_length": 144240,
    "chunk_length": 1322
  },
  {
    "chunk_id": 234,
    "paper_filename": "barsha_2023_comprehensive_review_on_breast_cancer_detection_classifcation_segmetnaiotn_using_deeplearning.pdf",
    "paper_title": "Barsha 2023 Comprehensive Review On Breast Cancer Detection Classifcation Segmetnaiotn Using Deeplearning",
    "chunk_index": 19,
    "total_chunks": 147,
    "text_content": "that directly relate to the topics. We excluded certain articles from our final collec- tion for several reasons: many articles were eliminated as redundant because the same articles appeared in multiple databases and publications. In addition, some papers were found irrelevant since they dealt with pure medical sci- ence, rather than computer science so those articles were also rejected for inclusion in the final collection. Some were published in languages other than English, others only had a",
    "full_text_length": 144240,
    "chunk_length": 1262
  },
  {
    "chunk_id": 235,
    "paper_filename": "barsha_2023_comprehensive_review_on_breast_cancer_detection_classifcation_segmetnaiotn_using_deeplearning.pdf",
    "paper_title": "Barsha 2023 Comprehensive Review On Breast Cancer Detection Classifcation Segmetnaiotn Using Deeplearning",
    "chunk_index": 20,
    "total_chunks": 147,
    "text_content": "Figure 5 presents the count of research papers retrieved based on their year of publication. Fig. 4 Describing the search terms used for inclusion and exclusion of research papers included in this BC review B. Abhisheka et al. 1 3 3 BC Research Evolution BC research has evolved significantly over the years. Here is a brief overview of the key developments in BC research: 3.1 History of BC Using CNN In the past, early detection of BC relied heavily on manual examination and mammography imaging. A",
    "full_text_length": 144240,
    "chunk_length": 1197
  },
  {
    "chunk_id": 236,
    "paper_filename": "barsha_2023_comprehensive_review_on_breast_cancer_detection_classifcation_segmetnaiotn_using_deeplearning.pdf",
    "paper_title": "Barsha 2023 Comprehensive Review On Breast Cancer Detection Classifcation Segmetnaiotn Using Deeplearning",
    "chunk_index": 21,
    "total_chunks": 147,
    "text_content": "with the use of DM, which produces clearer and more detailed images. In addition, other imaging techniques, such as BMRI and BUS, have been developed and are used in conjunction with mammography to improve the accuracy of early detection. ML and DL algorithms have also been developed to aid in the early detection of BC. Wu et al. [43] have contributed to the classification of microcalcifications for the diagnosis of BC through introducing the first CNN model. Afterwards, many papers presented va",
    "full_text_length": 144240,
    "chunk_length": 1315
  },
  {
    "chunk_id": 237,
    "paper_filename": "barsha_2023_comprehensive_review_on_breast_cancer_detection_classifcation_segmetnaiotn_using_deeplearning.pdf",
    "paper_title": "Barsha 2023 Comprehensive Review On Breast Cancer Detection Classifcation Segmetnaiotn Using Deeplearning",
    "chunk_index": 22,
    "total_chunks": 147,
    "text_content": "BC research and detection have undergone a significant evolution, from manual approaches to machine learning techniques and finally to DL algorithms, which have significantly improved the accuracy and effi - ciency of BC detection. 3.2 Evolution of Segmentation Methods BC segmentation methods have evolved significantly over time, with advances in imaging technologies and DL models. Here are some key developments in BC segmentation meth- ods: In late 1987, the first BC, segmentation was performed",
    "full_text_length": 144240,
    "chunk_length": 1384
  },
  {
    "chunk_id": 238,
    "paper_filename": "barsha_2023_comprehensive_review_on_breast_cancer_detection_classifcation_segmetnaiotn_using_deeplearning.pdf",
    "paper_title": "Barsha 2023 Comprehensive Review On Breast Cancer Detection Classifcation Segmetnaiotn Using Deeplearning",
    "chunk_index": 23,
    "total_chunks": 147,
    "text_content": "CAD tools [49, 50]. These tools helped radiologists segment tumor regions more accurately and efficiently. Furthermore, in the early 2000s, clustering and threshold-based segmentation [51, 52] methods were intro- duced, which used intensity thresholding to identify tumor regions. These methods were more objective than manual segmentation, but suffered from sensitivity to noise and variations in image quality. Besides, region-growing seg- mentation [ 53] methods were introduced, which relied on i",
    "full_text_length": 144240,
    "chunk_length": 1417
  },
  {
    "chunk_id": 239,
    "paper_filename": "barsha_2023_comprehensive_review_on_breast_cancer_detection_classifcation_segmetnaiotn_using_deeplearning.pdf",
    "paper_title": "Barsha 2023 Comprehensive Review On Breast Cancer Detection Classifcation Segmetnaiotn Using Deeplearning",
    "chunk_index": 24,
    "total_chunks": 147,
    "text_content": "illustrates the timeline of segmenta- tion techniques used for BC detection. These methods have shown superior performance com- pared to earlier methods, especially for complex or hetero- geneous tumors. Therefore, BC segmentation methods have evolved from manual to semi-automated to fully automated approaches, with increasing reliance on DL based tech- niques. These evolutions have contributed to more accurate and efficient BC diagnosis and treatment planning, ulti- mately improving patient out",
    "full_text_length": 144240,
    "chunk_length": 1335
  },
  {
    "chunk_id": 240,
    "paper_filename": "barsha_2023_comprehensive_review_on_breast_cancer_detection_classifcation_segmetnaiotn_using_deeplearning.pdf",
    "paper_title": "Barsha 2023 Comprehensive Review On Breast Cancer Detection Classifcation Segmetnaiotn Using Deeplearning",
    "chunk_index": 25,
    "total_chunks": 147,
    "text_content": "diagnosis. The American Cancer Soci- ety (ACS) considers mammograms a standard for the early identification of BC [54]. This screening is conducted on asymptomatic women to locate early and clinically unde- tected BC. In this technique, X-ray imaging is used to pro- duce images of both breasts. For each breast, dual views of a mammogram, the cranio-caudal (top-down view) and the mediolateral oblique (side view at a specific aspect) are obtained by compressing the breast at a nearly vertical plan",
    "full_text_length": 144240,
    "chunk_length": 1248
  },
  {
    "chunk_id": 241,
    "paper_filename": "barsha_2023_comprehensive_review_on_breast_cancer_detection_classifcation_segmetnaiotn_using_deeplearning.pdf",
    "paper_title": "Barsha 2023 Comprehensive Review On Breast Cancer Detection Classifcation Segmetnaiotn Using Deeplearning",
    "chunk_index": 26,
    "total_chunks": 147,
    "text_content": "imaging method for detecting BCs during screening is 84% [62]. The remaining 16% that are not recognised, due to the fact that radiologists are limited to visual acuity [62]. Moreover, DM is limited in a few instances, such as it may miss numerous malignant cases especially, women with dense breasts are more susceptible to false-negative outcomes. As reported Fig. 7 Timeline of segmenta- tion techniques used for BC detection B. Abhisheka et al. 1 3 by the ACS, this procedure overlooks about 1 in",
    "full_text_length": 144240,
    "chunk_length": 1258
  },
  {
    "chunk_id": 242,
    "paper_filename": "barsha_2023_comprehensive_review_on_breast_cancer_detection_classifcation_segmetnaiotn_using_deeplearning.pdf",
    "paper_title": "Barsha 2023 Comprehensive Review On Breast Cancer Detection Classifcation Segmetnaiotn Using Deeplearning",
    "chunk_index": 27,
    "total_chunks": 147,
    "text_content": "potentially effective, noninvasive, safe, economical, and widely acces- sible for detecting BC. Moreover it is more sensitive than mammography for detecting abnormalities in dense breasts, hence, it is valuable for women younger than 35 years of age. Using BUS imaging, detection accuracy of simple cysts can reach 96\u2013100% [65]. The process of ultrasonography involves the transmission of high-frequency sound waves through the breast and transforming the signals that return into images that can be ",
    "full_text_length": 144240,
    "chunk_length": 1254
  },
  {
    "chunk_id": 243,
    "paper_filename": "barsha_2023_comprehensive_review_on_breast_cancer_detection_classifcation_segmetnaiotn_using_deeplearning.pdf",
    "paper_title": "Barsha 2023 Comprehensive Review On Breast Cancer Detection Classifcation Segmetnaiotn Using Deeplearning",
    "chunk_index": 28,
    "total_chunks": 147,
    "text_content": "quality of an image appears primarily depends on the knowledge and expertise of the clinician performing the scan. Therefore, BC diagno- sis is concerned about the aberrant size or shape of lesions; such procedure is significantly influenced by the clinician\u2019s placement of the transducer, like the pressure exerted to the breast. BUS is used in the methods SDAEs and GANs [66], ResNet-GAP [67], NURBS [68], IRDx[69], SUAS [70], DLR [71], HMB-DLGAHA [ 72], ABUS [ 73] and many more. However, BUS has ",
    "full_text_length": 144240,
    "chunk_length": 1253
  },
  {
    "chunk_id": 244,
    "paper_filename": "barsha_2023_comprehensive_review_on_breast_cancer_detection_classifcation_segmetnaiotn_using_deeplearning.pdf",
    "paper_title": "Barsha 2023 Comprehensive Review On Breast Cancer Detection Classifcation Segmetnaiotn Using Deeplearning",
    "chunk_index": 29,
    "total_chunks": 147,
    "text_content": "radio waves to generate detailed images of the breasts [75]. In 1986, BMRI first reported comple- mentary imaging with mammography and BUS [76]. MRI has been incorporated as a preliminary screening of the affected breast in women with recently diagnosed BC, since it discovers new cancer cells that conventional imaging may have missed. Additionally, it is particularly used for screen- ing the abnormalities in women at high risk for having BC, evaluating the staging period, genetic mutation, assis",
    "full_text_length": 144240,
    "chunk_length": 1328
  },
  {
    "chunk_id": 245,
    "paper_filename": "barsha_2023_comprehensive_review_on_breast_cancer_detection_classifcation_segmetnaiotn_using_deeplearning.pdf",
    "paper_title": "Barsha 2023 Comprehensive Review On Breast Cancer Detection Classifcation Segmetnaiotn Using Deeplearning",
    "chunk_index": 30,
    "total_chunks": 147,
    "text_content": "DWI [87] and so on. However, the process has several significant advantages but it may also have some drawbacks, for instance it is time-consuming, expensive and Positive Predictive Value (PPV) is diminished by the high rate of false-positive reports, which may lead to needless breast biopsies. Furthermore, microcalcifications are beyond the scope of its detection abilities. Due to the use of a power - ful magnet and a contrast substance, pregnant women are not advised to undergo this examinatio",
    "full_text_length": 144240,
    "chunk_length": 1212
  },
  {
    "chunk_id": 246,
    "paper_filename": "barsha_2023_comprehensive_review_on_breast_cancer_detection_classifcation_segmetnaiotn_using_deeplearning.pdf",
    "paper_title": "Barsha 2023 Comprehensive Review On Breast Cancer Detection Classifcation Segmetnaiotn Using Deeplearning",
    "chunk_index": 31,
    "total_chunks": 147,
    "text_content": "diagnosed, to assist doctors in course of treatment. The precise assessment of BC staging is crucial because it may affect the decision to proceed with axillary dissection or not [92, 93]. Patients with symptoms that could be caused by metastasis to the lung or liver, cancer cells in the lymph nodes under the axilla, or tumor that is larger than 5 cm are tend to undergo this procedure. Typically, CT has low contrast, thus contrast dye such as iodine based compounds, barium-sulfate, gadolinium, a",
    "full_text_length": 144240,
    "chunk_length": 1245
  },
  {
    "chunk_id": 247,
    "paper_filename": "barsha_2023_comprehensive_review_on_breast_cancer_detection_classifcation_segmetnaiotn_using_deeplearning.pdf",
    "paper_title": "Barsha 2023 Comprehensive Review On Breast Cancer Detection Classifcation Segmetnaiotn Using Deeplearning",
    "chunk_index": 32,
    "total_chunks": 147,
    "text_content": "Comprehensive Review on Breast Cancer Detection, Classification and Segmentation Using\u2026 1 3addition, it has a lower detection threshold for microcal- cification compared to DM. However, the development of spiral CT [97\u2013 99] has shown significant results for address- ing challenges in breast lesion diagnosis and providing bet- ter information about the spread of BC. In the preoperative evaluation of BC, it has the possibility of being used as an alternative to 3D MRI. Several strategies have been",
    "full_text_length": 144240,
    "chunk_length": 1208
  },
  {
    "chunk_id": 248,
    "paper_filename": "barsha_2023_comprehensive_review_on_breast_cancer_detection_classifcation_segmetnaiotn_using_deeplearning.pdf",
    "paper_title": "Barsha 2023 Comprehensive Review On Breast Cancer Detection Classifcation Segmetnaiotn Using Deeplearning",
    "chunk_index": 33,
    "total_chunks": 147,
    "text_content": "and physiologic infor - mation of tissues. In order to do a PET scan a little amount of radiotracer substance is injected into a patient\u2019s vein, typi - cally on the inside of the elbow or in a small vein in the hand [104, 105]. Fluorodeoxyglucose (FDG), a form of sugar, is frequently used as a substance for this procedure, because it is significant for anticipating neoadjuvant treatment [106, 107]. The drug is inserted intravenously to a patient, circu- lates throughout the blood and accumulates",
    "full_text_length": 144240,
    "chunk_length": 1216
  },
  {
    "chunk_id": 249,
    "paper_filename": "barsha_2023_comprehensive_review_on_breast_cancer_detection_classifcation_segmetnaiotn_using_deeplearning.pdf",
    "paper_title": "Barsha 2023 Comprehensive Review On Breast Cancer Detection Classifcation Segmetnaiotn Using Deeplearning",
    "chunk_index": 34,
    "total_chunks": 147,
    "text_content": "therefore, FDG PET is not applied for early BC diagnosis. The primary use of PET is in the assessment of metastatic progression. In addition, PET is not typically employed independently in the detection of BC, therefore, fusing PET and CT together leading to more reliable results, thus increasing accuracy [110, 111]. using this modality several methods have been proposed such as [112, 113]. 4.6 Histopathological (HP) Images The term histopathology is used to describe the process of removing a sp",
    "full_text_length": 144240,
    "chunk_length": 1199
  },
  {
    "chunk_id": 250,
    "paper_filename": "barsha_2023_comprehensive_review_on_breast_cancer_detection_classifcation_segmetnaiotn_using_deeplearning.pdf",
    "paper_title": "Barsha 2023 Comprehensive Review On Breast Cancer Detection Classifcation Segmetnaiotn Using Deeplearning",
    "chunk_index": 35,
    "total_chunks": 147,
    "text_content": "E) before being examined under microscope [115]. The objective of H & E is to generate coloured HP images for clear vision and depth examination of the tissues [116, 117]. Moreover, the ability to recognise color transitions in stained images is crucial for making a correct diagnosis of BC. Hence, it provides a high level of authenticity in BC classification; particularly in subtype classification. How - ever, it suffers from some drawbacks as well, for instance, biopsies are invasive procedures",
    "full_text_length": 144240,
    "chunk_length": 1305
  },
  {
    "chunk_id": 251,
    "paper_filename": "barsha_2023_comprehensive_review_on_breast_cancer_detection_classifcation_segmetnaiotn_using_deeplearning.pdf",
    "paper_title": "Barsha 2023 Comprehensive Review On Breast Cancer Detection Classifcation Segmetnaiotn Using Deeplearning",
    "chunk_index": 36,
    "total_chunks": 147,
    "text_content": "effective, painless, and radi- ation-free technique used to detect and classify BC [123, 124]. The clinical interpretation of TI depends primarily on color i.e., lower thermal levels are represented by the blue color (healthy), whereas red, orange, or yellow (depending on the operator settings) patches signify a deformity [125]. In thermography, infrared cameras are used to capture tem- perature maps of the breast. The fundamental idea behind thermography is that cells that proliferate uncontrol",
    "full_text_length": 144240,
    "chunk_length": 1319
  },
  {
    "chunk_id": 252,
    "paper_filename": "barsha_2023_comprehensive_review_on_breast_cancer_detection_classifcation_segmetnaiotn_using_deeplearning.pdf",
    "paper_title": "Barsha 2023 Comprehensive Review On Breast Cancer Detection Classifcation Segmetnaiotn Using Deeplearning",
    "chunk_index": 37,
    "total_chunks": 147,
    "text_content": "deep informa- tion cannot be obtained. Figure 9 provides a comprehensive overview of the seven BC screening methods, highlighting their strengths and limitations for a better understanding of each method\u2019s effectiveness. 5 Observations on Different Imaging Modalities Based on breast composition: Currently, a wide range of digital image processing modalities are available for the detection of BC. A female breast structure is divided into 4 distinct categories based on its composition [128]: (i) a",
    "full_text_length": 144240,
    "chunk_length": 1311
  },
  {
    "chunk_id": 253,
    "paper_filename": "barsha_2023_comprehensive_review_on_breast_cancer_detection_classifcation_segmetnaiotn_using_deeplearning.pdf",
    "paper_title": "Barsha 2023 Comprehensive Review On Breast Cancer Detection Classifcation Segmetnaiotn Using Deeplearning",
    "chunk_index": 38,
    "total_chunks": 147,
    "text_content": "sound waves at the tissue surface that can lead to the better detection of BC with dense breasts, thus increasing accuracy. However, the effectiveness of this technique is highly reliant on the experience and expertise of the treating physician. These two imaging modalities are initial and adequate to detect BC at an early phase [132]. Based on cost/affordability: In BC diagnostic proce- dure, multiple imaging modalities are employed and the cost of each modality can be influenced by a variety o",
    "full_text_length": 144240,
    "chunk_length": 1314
  },
  {
    "chunk_id": 254,
    "paper_filename": "barsha_2023_comprehensive_review_on_breast_cancer_detection_classifcation_segmetnaiotn_using_deeplearning.pdf",
    "paper_title": "Barsha 2023 Comprehensive Review On Breast Cancer Detection Classifcation Segmetnaiotn Using Deeplearning",
    "chunk_index": 39,
    "total_chunks": 147,
    "text_content": "modern equipment is required including intensity modulated radiation therapy (IMRT) and proton therapy [135, 136]. In addition to the psychological and physical difficulties, this expense can be a substantial financial hardship for a BC diagnosis. Besides, using hybrid imaging modalities for screening BC contribute to more accurate detection. Therefore, the cost of a hybrid device will be high; nevertheless, it is possible to design a sys- tem that can detect the key patterns from hybrid modalit",
    "full_text_length": 144240,
    "chunk_length": 1249
  },
  {
    "chunk_id": 255,
    "paper_filename": "barsha_2023_comprehensive_review_on_breast_cancer_detection_classifcation_segmetnaiotn_using_deeplearning.pdf",
    "paper_title": "Barsha 2023 Comprehensive Review On Breast Cancer Detection Classifcation Segmetnaiotn Using Deeplearning",
    "chunk_index": 40,
    "total_chunks": 147,
    "text_content": "of BC: Staging is a term in which medi- cal experts determine whether or not the disease has spread, and if so, how far it has spread from the original tumor. Moreover, in newly diagnosed BC, it is crucial to precisely determine the level of localized and metastatic malignancy to improve treatment strategies and outcomes. It is observed that MRI, CT and PET are recognised to be among the advanced imaging modalities used in the staging of BC [140, 141]. Among all, MRI is often regarded as the gol",
    "full_text_length": 144240,
    "chunk_length": 1238
  },
  {
    "chunk_id": 256,
    "paper_filename": "barsha_2023_comprehensive_review_on_breast_cancer_detection_classifcation_segmetnaiotn_using_deeplearning.pdf",
    "paper_title": "Barsha 2023 Comprehensive Review On Breast Cancer Detection Classifcation Segmetnaiotn Using Deeplearning",
    "chunk_index": 41,
    "total_chunks": 147,
    "text_content": "of metastatic BC. A comprehensive analysis of the existing lit- erature has shown that, in case of bone metastases detection, PET/MRI are significantly better than CT in the patients with newly diagnosed BC [142, 143]. Based on facilities/expertise available: In order to achieve the best possible results from BC treatment, an early diagnosis and a well-coordinated, interdisciplinary strategy are essential. However, efficient healthcare management approaches can be challenging to implement in rur",
    "full_text_length": 144240,
    "chunk_length": 1335
  },
  {
    "chunk_id": 257,
    "paper_filename": "barsha_2023_comprehensive_review_on_breast_cancer_detection_classifcation_segmetnaiotn_using_deeplearning.pdf",
    "paper_title": "Barsha 2023 Comprehensive Review On Breast Cancer Detection Classifcation Segmetnaiotn Using Deeplearning",
    "chunk_index": 42,
    "total_chunks": 147,
    "text_content": "Using\u2026 1 3minimize BC deaths. Yet, the research on mammography\u2019s effectiveness in developing nations is limited despite LTM income countries having lower BC incidence rates, younger populations, and less reliable access to good-quality care [145]. The incidence of advanced BC rates are common in rural areas due to a lack of access to BC facilities; however, the overall mortality rate is lower in urban areas because of the higher percentage of BC facilities serving the popu- lation there [146, 14",
    "full_text_length": 144240,
    "chunk_length": 1328
  },
  {
    "chunk_id": 258,
    "paper_filename": "barsha_2023_comprehensive_review_on_breast_cancer_detection_classifcation_segmetnaiotn_using_deeplearning.pdf",
    "paper_title": "Barsha 2023 Comprehensive Review On Breast Cancer Detection Classifcation Segmetnaiotn Using Deeplearning",
    "chunk_index": 43,
    "total_chunks": 147,
    "text_content": "alternatives [148]. Therefore, emphasizing the significance of annual Clinical Mammography (CM), enlightening on the signs and symptoms of BC, organiz- ing BC awareness programs, modifying health care systems might be cost-effective and feasible BC control measures for treatable cancers, thus, improve BC survival rates. Modalities either for diagnosis or treatment: There are two imaging techniques used to diagnose BC: radiogra- phy and histology. The field of radiology is concerned with the capt",
    "full_text_length": 144240,
    "chunk_length": 1287
  },
  {
    "chunk_id": 259,
    "paper_filename": "barsha_2023_comprehensive_review_on_breast_cancer_detection_classifcation_segmetnaiotn_using_deeplearning.pdf",
    "paper_title": "Barsha 2023 Comprehensive Review On Breast Cancer Detection Classifcation Segmetnaiotn Using Deeplearning",
    "chunk_index": 44,
    "total_chunks": 147,
    "text_content": "test for a BC diagnosis, is a histopathological imaging. Though there are several types of biopsies available depending on the patient\u2019s state at the time of the procedure. Each of the imaging modalities mentioned above is employed as a part of the BC diagnosis tool. Once a tumor and its nature have been accurately detected using imaging techniques, the next step is to select the most appropriate treatment option. BC effective treatments include surgery, radia - tion therapy and chemotherapy. In",
    "full_text_length": 144240,
    "chunk_length": 1250
  },
  {
    "chunk_id": 260,
    "paper_filename": "barsha_2023_comprehensive_review_on_breast_cancer_detection_classifcation_segmetnaiotn_using_deeplearning.pdf",
    "paper_title": "Barsha 2023 Comprehensive Review On Breast Cancer Detection Classifcation Segmetnaiotn Using Deeplearning",
    "chunk_index": 45,
    "total_chunks": 147,
    "text_content": "study shows that, MRI is the most effec- tive imaging technique for detecting cancer recurrence following tumor excision, since it can detect the presence of new cancer cells in its original locations [ 149, 150]. Therefore, treatments for BC are painful and expensive, thus, early detection is required to avoid all invasive opera- tions, which is feasible through employing a reliable CAD system. 6 Dataset Availability This section provides an overview of both public and pri- vate datasets used i",
    "full_text_length": 144240,
    "chunk_length": 1295
  },
  {
    "chunk_id": 261,
    "paper_filename": "barsha_2023_comprehensive_review_on_breast_cancer_detection_classifcation_segmetnaiotn_using_deeplearning.pdf",
    "paper_title": "Barsha 2023 Comprehensive Review On Breast Cancer Detection Classifcation Segmetnaiotn Using Deeplearning",
    "chunk_index": 46,
    "total_chunks": 147,
    "text_content": "classification papers. The BreaKHis dataset comprises sev - eral breast biopsy samples, with a total of 7909 HP images. The samples are sourced from the \u201cPathological Anatomy and Cytopathology\" Lab located in Brazil, and are obtained from 82 anonymous patients. It is the most exhaustive col- lection of such samples currently available. For BMRI and TI, the Duke-Breast-Cancer-MRI and DMI-IR datasets are publicly available. PET/CT scans are not included as they are more effective in detecting dist",
    "full_text_length": 144240,
    "chunk_length": 1337
  },
  {
    "chunk_id": 262,
    "paper_filename": "barsha_2023_comprehensive_review_on_breast_cancer_detection_classifcation_segmetnaiotn_using_deeplearning.pdf",
    "paper_title": "Barsha 2023 Comprehensive Review On Breast Cancer Detection Classifcation Segmetnaiotn Using Deeplearning",
    "chunk_index": 47,
    "total_chunks": 147,
    "text_content": "reported in various articles [155\u2013 163]. In these articles, private datasets are addressed while keeping to the constraint of being unable to disclose their data due to confidentiality restrictions. Moreover, a significant observation that emerged from this analysis is that researchers experienced a higher prob- ability to achieve better outcomes when employing datasets that are publicly accessible, whereas their own datasets that are not public generate less favorable results. This can be attri",
    "full_text_length": 144240,
    "chunk_length": 1336
  },
  {
    "chunk_id": 263,
    "paper_filename": "barsha_2023_comprehensive_review_on_breast_cancer_detection_classifcation_segmetnaiotn_using_deeplearning.pdf",
    "paper_title": "Barsha 2023 Comprehensive Review On Breast Cancer Detection Classifcation Segmetnaiotn Using Deeplearning",
    "chunk_index": 48,
    "total_chunks": 147,
    "text_content": "basic features. Some recent models on diagnosis of BC through CNN technique are highlighted in this section. This section provides an over - view of currently existing CNN-based CAD models for BC detection, classification and segmentation. Ding et al. [66] have used a ResNEtGAP network to clas- sify and localize the tumor simultaneously. The network uses B-mode ultrasound (BUS) images and elastography ultra- sound (EUS) images across different channels for training. 264 patients were used for th",
    "full_text_length": 144240,
    "chunk_length": 1297
  },
  {
    "chunk_id": 264,
    "paper_filename": "barsha_2023_comprehensive_review_on_breast_cancer_detection_classifcation_segmetnaiotn_using_deeplearning.pdf",
    "paper_title": "Barsha 2023 Comprehensive Review On Breast Cancer Detection Classifcation Segmetnaiotn Using Deeplearning",
    "chunk_index": 49,
    "total_chunks": 147,
    "text_content": "the duration required to complete the complex task. In addi- tion, data augmentation and preprocessing are applied to enhance the results and preserve the originality of the input image. Therefore, it is observed that among the three DCNN classifiers, ResNet is the most precise and effective classifier, yielding the highest average accuracy ranging from 97.81 to 99.70% for both binary and multiclass classification. To reduce the problems of dataset constraints, misclassi- fication and wrong pred",
    "full_text_length": 144240,
    "chunk_length": 1351
  },
  {
    "chunk_id": 265,
    "paper_filename": "barsha_2023_comprehensive_review_on_breast_cancer_detection_classifcation_segmetnaiotn_using_deeplearning.pdf",
    "paper_title": "Barsha 2023 Comprehensive Review On Breast Cancer Detection Classifcation Segmetnaiotn Using Deeplearning",
    "chunk_index": 50,
    "total_chunks": 147,
    "text_content": "to classify the data. The proposed framework has attained an accuracy of 99.1%, which is outperforms compared to recent DL techniques. Maqsood et al. [177] have introduced a transferable tex- ture convolutional neural network (TTCNN) to accurately detect and classify BC in less computation time using DM images. Histogram Equalization is utilized to enhance the sharpness of the contours in DM images.Then, the preproc- essed images are given to TTCNN for classification. Moreo- ver, an energy layer",
    "full_text_length": 144240,
    "chunk_length": 1272
  },
  {
    "chunk_id": 266,
    "paper_filename": "barsha_2023_comprehensive_review_on_breast_cancer_detection_classifcation_segmetnaiotn_using_deeplearning.pdf",
    "paper_title": "Barsha 2023 Comprehensive Review On Breast Cancer Detection Classifcation Segmetnaiotn Using Deeplearning",
    "chunk_index": 51,
    "total_chunks": 147,
    "text_content": "DM Public MIAS https:// www. mammo image. org/ datab ases/ [164] DM Public DDSM http:// www. eng. usf. edu/ cvprg/ mammo graphy/ datab ase. html [152] DM Public INbreast https:// pubmed. ncbi. nlm. nih. gov/ 22078 258/ [165] DM Restricted Hologic digital mammography systems https:// link. sprin ger. com/ chapt er/ 10. 1007/ 978-3- 319- 41546-8_5 [166] DM Restricted Department of Radiology at the Univer - sity of Chicagohttps:// www. scien cedir ect. com/ scien ce/ artic le/ pii/ S0031 32031 8300",
    "full_text_length": 144240,
    "chunk_length": 1198
  },
  {
    "chunk_id": 267,
    "paper_filename": "barsha_2023_comprehensive_review_on_breast_cancer_detection_classifcation_segmetnaiotn_using_deeplearning.pdf",
    "paper_title": "Barsha 2023 Comprehensive Review On Breast Cancer Detection Classifcation Segmetnaiotn Using Deeplearning",
    "chunk_index": 52,
    "total_chunks": 147,
    "text_content": "nis- 2017- 003 [171] HP Public BreakHis https:// web. inf. ufpr. br/ vri/ datab ases/ breast- cancer- histo patho logic al- datab ase- break his/ [172] TI Public DMI-IR https:// sci- hub. hkvisa. net/ 10. 1166/ jmihi. 2014. 1226 [173] BUS Public OASBUD https:// pubmed. ncbi. nlm. nih. gov/ 28859 252/ [174] BUS Public UDIAT https:// sci- hub. hkvisa. net/ 10. 1109/ JBHI. 2017. 27318 73 [175] BMRI Public DCE-MRI https:// ieeex plore. ieee. org/ stamp/ stamp. jsp? tp= & arnum ber= 87133 52 A Compre",
    "full_text_length": 144240,
    "chunk_length": 1302
  },
  {
    "chunk_id": 268,
    "paper_filename": "barsha_2023_comprehensive_review_on_breast_cancer_detection_classifcation_segmetnaiotn_using_deeplearning.pdf",
    "paper_title": "Barsha 2023 Comprehensive Review On Breast Cancer Detection Classifcation Segmetnaiotn Using Deeplearning",
    "chunk_index": 53,
    "total_chunks": 147,
    "text_content": "DNN are employed for BC multi-classification using HP images. In addition, data augmentation, denoising, and segmentation are performed in the preprocessing step to optimize clas- sification performance and counteract the issue of over - fitting. Moreover, it is observed that augmenting the data significantly contributes to enhancing the classification accuracy. According to the results, using handcrafted tech- niques as feature extractors and DNN classifiers yielded the best results in multi-cl",
    "full_text_length": 144240,
    "chunk_length": 1390
  },
  {
    "chunk_id": 269,
    "paper_filename": "barsha_2023_comprehensive_review_on_breast_cancer_detection_classifcation_segmetnaiotn_using_deeplearning.pdf",
    "paper_title": "Barsha 2023 Comprehensive Review On Breast Cancer Detection Classifcation Segmetnaiotn Using Deeplearning",
    "chunk_index": 54,
    "total_chunks": 147,
    "text_content": "a final decision. An experiment is carried out to assess the merits and drawbacks of various approaches using both Magnification-specific binary classification (MSB) and Magnification-independent binary classification (MIB). The findings indicated that the DenseNet201 architecture out- performed the other methods in the MSB scenario. The out- comes are evaluated in comparison with current techniques, and in every instance, the proposed systems exhibited better performance. Luo et al. [180] have ",
    "full_text_length": 144240,
    "chunk_length": 1427
  },
  {
    "chunk_id": 270,
    "paper_filename": "barsha_2023_comprehensive_review_on_breast_cancer_detection_classifcation_segmetnaiotn_using_deeplearning.pdf",
    "paper_title": "Barsha 2023 Comprehensive Review On Breast Cancer Detection Classifcation Segmetnaiotn Using Deeplearning",
    "chunk_index": 55,
    "total_chunks": 147,
    "text_content": "single classifier is the application of an ensemble clas- sifier. Podda et al. [181] have presented a multi-layer and fully automated pipeline for identifying and categorizing breast lesions that are linked to an increased risk of can- cer, using BUS images. Ensemble-based classifiers utilize various techniques to combine the predictions of a single classifier, resulting in improved overall predictions that are more accurate than those made by a single classifier. In the segmentation task, the p",
    "full_text_length": 144240,
    "chunk_length": 1352
  },
  {
    "chunk_id": 271,
    "paper_filename": "barsha_2023_comprehensive_review_on_breast_cancer_detection_classifcation_segmetnaiotn_using_deeplearning.pdf",
    "paper_title": "Barsha 2023 Comprehensive Review On Breast Cancer Detection Classifcation Segmetnaiotn Using Deeplearning",
    "chunk_index": 56,
    "total_chunks": 147,
    "text_content": "the collection, which can be masked to exclude particular regions during processing. The sensitivity, accuracy, AUC and specificity of the suggested model are 89.47%, 90.50%, 0.901 \u00b1 0.0314 and 90.71% respectively. respectively. Hossain et al. [182] have proposed an automated method that segments micro-calcification regions from mammogram images. The proposed method applies preprocessing tech- niques to enhance the image quality and segment the breast region from the pectoral region. Fuzzy C-mea",
    "full_text_length": 144240,
    "chunk_length": 1369
  },
  {
    "chunk_id": 272,
    "paper_filename": "barsha_2023_comprehensive_review_on_breast_cancer_detection_classifcation_segmetnaiotn_using_deeplearning.pdf",
    "paper_title": "Barsha 2023 Comprehensive Review On Breast Cancer Detection Classifcation Segmetnaiotn Using Deeplearning",
    "chunk_index": 57,
    "total_chunks": 147,
    "text_content": "CAD system for accurately and timely classifying BC as benign and malig- nant through BUS images. This study employed a combina- tion of bicluster-based ensemble learning and the AdaBoost algorithm to identify consistent patterns within columns of training data. In addition, the patterns that frequently occurred in tumors with the same label were considered as potential diagnostic rules. Since BUS imaging relies entirely on the operator, thus, a new approach has been introduced to score features",
    "full_text_length": 144240,
    "chunk_length": 1328
  },
  {
    "chunk_id": 273,
    "paper_filename": "barsha_2023_comprehensive_review_on_breast_cancer_detection_classifcation_segmetnaiotn_using_deeplearning.pdf",
    "paper_title": "Barsha 2023 Comprehensive Review On Breast Cancer Detection Classifcation Segmetnaiotn Using Deeplearning",
    "chunk_index": 58,
    "total_chunks": 147,
    "text_content": "and the use of CAD methods to reduce the number of unnecessary biopsies. The authors applied DL using CNN to classify abnormali- ties as benign or malignant in mammogram images, using two different databases: mini-MIAS and BCDR. The initial results showed accuracy, precision, recall, and f-score val- ues between around 60% and 72%. The authors then used B. Abhisheka et al. 1 3 preprocessing methods, including cropping, augmentation, and balancing image data, to improve the results. They created ",
    "full_text_length": 144240,
    "chunk_length": 1296
  },
  {
    "chunk_id": 274,
    "paper_filename": "barsha_2023_comprehensive_review_on_breast_cancer_detection_classifcation_segmetnaiotn_using_deeplearning.pdf",
    "paper_title": "Barsha 2023 Comprehensive Review On Breast Cancer Detection Classifcation Segmetnaiotn Using Deeplearning",
    "chunk_index": 59,
    "total_chunks": 147,
    "text_content": "images. The suggested hybrid model retains the advantages of both networks and demonstrates superior performance compared to the individual base classifiers. Moreover, the efficient and precise operation of the sys - tem depends significantly on the use of thresholding and probability-based elements. The proposed system is tested using datasets from two distinct BC modalities: BUS and DM. The proposed model produces superior outcomes com- pared to the current leading methods by incorporating the",
    "full_text_length": 144240,
    "chunk_length": 1374
  },
  {
    "chunk_id": 275,
    "paper_filename": "barsha_2023_comprehensive_review_on_breast_cancer_detection_classifcation_segmetnaiotn_using_deeplearning.pdf",
    "paper_title": "Barsha 2023 Comprehensive Review On Breast Cancer Detection Classifcation Segmetnaiotn Using Deeplearning",
    "chunk_index": 60,
    "total_chunks": 147,
    "text_content": "model, the decision scores of the underlying classification methods are adaptively integrated to produce final predictions, and the Gompertz function is employed to generate fuzzy rankings of these methods. Additionally, pre-trained DCNN models are used to extract features and classify mammography images using dense and softmax layers. Initially, a number of pre-trained DCNN models, such as VGG-11, ResNet164, DenseNet121, and Inception V4, are used to diagnose cancer in mammography images. Nonet",
    "full_text_length": 144240,
    "chunk_length": 1304
  },
  {
    "chunk_id": 276,
    "paper_filename": "barsha_2023_comprehensive_review_on_breast_cancer_detection_classifcation_segmetnaiotn_using_deeplearning.pdf",
    "paper_title": "Barsha 2023 Comprehensive Review On Breast Cancer Detection Classifcation Segmetnaiotn Using Deeplearning",
    "chunk_index": 61,
    "total_chunks": 147,
    "text_content": "Machine (SVM) is imple- mented. MIP based architecture is evaluated and compared with state of arts based on Area Under Curve (AUC) and DeLong test, it achieved high performance of 91% and 93% respectively, for the independent test set. Mohammed et al. [187] have used several types of DL models that are evaluated on HP dataset, in order to reduce the risk of misdiagnosis. Among all, Densenet169, Resnet50, and Resnet101 are performing best. In this work data are treated in two different manner, f",
    "full_text_length": 144240,
    "chunk_length": 1301
  },
  {
    "chunk_id": 277,
    "paper_filename": "barsha_2023_comprehensive_review_on_breast_cancer_detection_classifcation_segmetnaiotn_using_deeplearning.pdf",
    "paper_title": "Barsha 2023 Comprehensive Review On Breast Cancer Detection Classifcation Segmetnaiotn Using Deeplearning",
    "chunk_index": 62,
    "total_chunks": 147,
    "text_content": "can accurately detect BC on screening mammograms, while eliminating the reliance on rarely available lesion annotations. In addition, DL models can accurately classify screening mammograms with only clinical ROI annotations used in the initial stage, and can be fine-tuned using addi- tional datasets without ROI annotations. The study found that the quality of the patch classifiers is critical to the accu- racy of the whole image classifiers, and that using larger or more patches can improve accu",
    "full_text_length": 144240,
    "chunk_length": 1345
  },
  {
    "chunk_id": 278,
    "paper_filename": "barsha_2023_comprehensive_review_on_breast_cancer_detection_classifcation_segmetnaiotn_using_deeplearning.pdf",
    "paper_title": "Barsha 2023 Comprehensive Review On Breast Cancer Detection Classifcation Segmetnaiotn Using Deeplearning",
    "chunk_index": 63,
    "total_chunks": 147,
    "text_content": "density measurement, such as differences in images from different devices and the lack of an objective gold standard. Specifically, the study reports that the auto- mated framework achieved a DICE score of 0.77, which is the same as the DICE score achieved by the two radiologists when their segmentations were compared. Wang et al. [72] have demonstrated the effectiveness of novel DL models with the addition of Automatic Segmenta- tion Network (ASN) for morphological analysis and deter - mined th",
    "full_text_length": 144240,
    "chunk_length": 1256
  },
  {
    "chunk_id": 279,
    "paper_filename": "barsha_2023_comprehensive_review_on_breast_cancer_detection_classifcation_segmetnaiotn_using_deeplearning.pdf",
    "paper_title": "Barsha 2023 Comprehensive Review On Breast Cancer Detection Classifcation Segmetnaiotn Using Deeplearning",
    "chunk_index": 64,
    "total_chunks": 147,
    "text_content": "higher accuracy (78.11%) and FPR (72.86%), and the ResNet101 v2 model has higher sensitivity (85.00%). Moreover, based on the AUCs and APs, the proposed ResNet101 v2 model pro- duced the best result 85% and 90% respectively, compared with the remaining five D L models. Nagalakshmi [190] has employed DCNN with K-means clustering and a multiclass SVM model to establish a CAD A Comprehensive Review on Breast Cancer Detection, Classification and Segmentation Using\u2026 1 3system that boosts the quality ",
    "full_text_length": 144240,
    "chunk_length": 1269
  },
  {
    "chunk_id": 280,
    "paper_filename": "barsha_2023_comprehensive_review_on_breast_cancer_detection_classifcation_segmetnaiotn_using_deeplearning.pdf",
    "paper_title": "Barsha 2023 Comprehensive Review On Breast Cancer Detection Classifcation Segmetnaiotn Using Deeplearning",
    "chunk_index": 65,
    "total_chunks": 147,
    "text_content": "type of BC. The proposed model outperforms the other classical classifiers with an accuracy of 96.72%. Raaj et al. [191] have designed a CAD system for the classification of tumors of DM based on the descriptors of the texture in the early stage. In addition, the proposed system incorporates a radon transform, an enhanced data module, and a hybrid CNN structure. To boost the detec- tion rate of the proposed approach, the spatial image of the source DM is transformed using the radon transform int",
    "full_text_length": 144240,
    "chunk_length": 1246
  },
  {
    "chunk_id": 281,
    "paper_filename": "barsha_2023_comprehensive_review_on_breast_cancer_detection_classifcation_segmetnaiotn_using_deeplearning.pdf",
    "paper_title": "Barsha 2023 Comprehensive Review On Breast Cancer Detection Classifcation Segmetnaiotn Using Deeplearning",
    "chunk_index": 66,
    "total_chunks": 147,
    "text_content": "the use of a differ - ence filter as a novel and efficient preprocessing step to be employed in DL algorithms. Furthermore, the classification process is carried out using Yolov4 and R-CNN DL models, in order to detect the appropriate regions by differentiat- ing between the noises and microcalcification-containing regions, thus increasing the ratio of accuracy. It is noted that the combination of difference filter and DL models is the first method to effectively classify microcalcification. Tab",
    "full_text_length": 144240,
    "chunk_length": 1250
  },
  {
    "chunk_id": 282,
    "paper_filename": "barsha_2023_comprehensive_review_on_breast_cancer_detection_classifcation_segmetnaiotn_using_deeplearning.pdf",
    "paper_title": "Barsha 2023 Comprehensive Review On Breast Cancer Detection Classifcation Segmetnaiotn Using Deeplearning",
    "chunk_index": 67,
    "total_chunks": 147,
    "text_content": "stage through a reliable CAD system using BUS images. Due to differences in the level of clinical skills among medical experts, BUS image analysis is highly incon- sistent and insensitive. To address this problem and improve the quality of diagnoses, an automated BC diagnosis system is developed. The novelty of this work is deploying images on mobile phones and performing diagnosis on each image. The developed system consists of three subsystems: (i) to remove noise from BUS images stacked denoi",
    "full_text_length": 144240,
    "chunk_length": 1267
  },
  {
    "chunk_id": 283,
    "paper_filename": "barsha_2023_comprehensive_review_on_breast_cancer_detection_classifcation_segmetnaiotn_using_deeplearning.pdf",
    "paper_title": "Barsha 2023 Comprehensive Review On Breast Cancer Detection Classifcation Segmetnaiotn Using Deeplearning",
    "chunk_index": 68,
    "total_chunks": 147,
    "text_content": "Bidirectional Long Short Term Memories (BiLSTMs), which extract features based on temporal cor - relations. Furthermore, the preprocessing is performed on each image, and then the preprocessed images are fed to the proposed networks. In terms of classifying accuracy, it is observed that the first one achieves a 97.50% accuracy in classification, while the second one achieves a 98.56%. However, the imaging approach requires high computational capacity to preprocess the images. Demir [194] has pro",
    "full_text_length": 144240,
    "chunk_length": 1329
  },
  {
    "chunk_id": 284,
    "paper_filename": "barsha_2023_comprehensive_review_on_breast_cancer_detection_classifcation_segmetnaiotn_using_deeplearning.pdf",
    "paper_title": "Barsha 2023 Comprehensive Review On Breast Cancer Detection Classifcation Segmetnaiotn Using Deeplearning",
    "chunk_index": 69,
    "total_chunks": 147,
    "text_content": "classifier performs poorly for the malignant class despite showing consistently high results across all the benign class. Hence, the optimized SVM exceeded the softmax classifier with a 100% accuracy across all magni - fication factors. Patil1 et al. [ 195] have presented a framework that uses two different DL architectures termed as CNN and recur - rent neural network (RNN) to process DM images. This framework comprises four stages, including Pre-processing, segmenting the tumor, extracting fea",
    "full_text_length": 144240,
    "chunk_length": 1335
  },
  {
    "chunk_id": 285,
    "paper_filename": "barsha_2023_comprehensive_review_on_breast_cancer_detection_classifcation_segmetnaiotn_using_deeplearning.pdf",
    "paper_title": "Barsha 2023 Comprehensive Review On Breast Cancer Detection Classifcation Segmetnaiotn Using Deeplearning",
    "chunk_index": 70,
    "total_chunks": 147,
    "text_content": "into RNN. Moreover, to integrate the data generated by the two distinct neural networks are united through a natural language pro- cessing (NLP)-inspired perceptron attention method. There- fore, a novel, normalizing approach is introduced to improve classification accuracy. Accurate segmentation of breast masses on medical images is a tremendous assistance in diagnosing and treating B. Abhisheka et al. 1 3Table 2 Some latest BC classification, detection and segmentation models using CNN are lis",
    "full_text_length": 144240,
    "chunk_length": 1290
  },
  {
    "chunk_id": 286,
    "paper_filename": "barsha_2023_comprehensive_review_on_breast_cancer_detection_classifcation_segmetnaiotn_using_deeplearning.pdf",
    "paper_title": "Barsha 2023 Comprehensive Review On Breast Cancer Detection Classifcation Segmetnaiotn Using Deeplearning",
    "chunk_index": 71,
    "total_chunks": 147,
    "text_content": "high and used the breakHis dataset, which includes four distinct types of images but only benign and malignant ones were put to use in this study [131] Jabeen et al., MDPI sensors (2022)133 N, 210 M and 487 B Transfer learning BUS Acc 99.1 Lack of explainability and interpretability of the model in terms of disease detection [177] Maqsood et al., MDPI applied sciences (2022)373 N and 223 M TTCNN DM Acc 97.49 Clip Limit for CLAHE has been arbitrarily decided, no metricused to assess the improveme",
    "full_text_length": 144240,
    "chunk_length": 1250
  },
  {
    "chunk_id": 287,
    "paper_filename": "barsha_2023_comprehensive_review_on_breast_cancer_detection_classifcation_segmetnaiotn_using_deeplearning.pdf",
    "paper_title": "Barsha 2023 Comprehensive Review On Breast Cancer Detection Classifcation Segmetnaiotn Using Deeplearning",
    "chunk_index": 72,
    "total_chunks": 147,
    "text_content": "CNN models and hyperparameter optimization is not consid- ered [180] Luo et al., Pattern Recogni- tion (2022)Dataset A: 160 benign and 132 malignant Dataset B: 786 benign and 916 malignantSBANet and channel atten- tionBUS Acc 90.78, Sensitivity 91.18, Specificity 90.44, F1-score 91.46, and AUC 95.49Classification method has two stages, increases complexity and training time and it lacks end-to-end functionality [181] Podda et al., Journal of Com- putational Science (2022)437 B, 210 M and 133 N E",
    "full_text_length": 144240,
    "chunk_length": 1288
  },
  {
    "chunk_id": 288,
    "paper_filename": "barsha_2023_comprehensive_review_on_breast_cancer_detection_classifcation_segmetnaiotn_using_deeplearning.pdf",
    "paper_title": "Barsha 2023 Comprehensive Review On Breast Cancer Detection Classifcation Segmetnaiotn Using Deeplearning",
    "chunk_index": 73,
    "total_chunks": 147,
    "text_content": "mentationDM Acc 98.2, F-measure 98.5 and 97.8 Dice scoreSkilled radiologist is needed to carry out a qualitative analy - sis due to the substandard performance on the MIAS A Comprehensive Review on Breast Cancer Detection, Classification and Segmentation Using\u2026 1 3 Table 2 (continued) Paper Authors, Publisher No. of images Technique used Type of image sequence Performance(%) Challenges and limitations [183] Huang et al., IEEE TRANS- ACTIONS ON KNOWL - EDGE AND DATA ENGI- NEERING(2020)418 B and 6",
    "full_text_length": 144240,
    "chunk_length": 1203
  },
  {
    "chunk_id": 289,
    "paper_filename": "barsha_2023_comprehensive_review_on_breast_cancer_detection_classifcation_segmetnaiotn_using_deeplearning.pdf",
    "paper_title": "Barsha 2023 Comprehensive Review On Breast Cancer Detection Classifcation Segmetnaiotn Using Deeplearning",
    "chunk_index": 74,
    "total_chunks": 147,
    "text_content": "and BCDR: 840 M, 864 BCNN DM Acc on MIAS 87 and on BCDR 88The effectiveness of calcifica- tion for BCDR has decreased [132] Sahu et al.,Biomedical Signal Processing and Con- trol(2023)BUS:B 487, M 210, N 133 DM: N 2728, M 3596, B 3360Hybrid CNN BUS and DM Acc 99.17 and 98.00 with DDSM dataset. 96.52, 93.18 with BUSI datasetThe combined outcomes of parse learning and transfer learning are not reported [185] Altameem et al., MDPI Diag- nostics (2022)11145 N 1145 B 1145 M Gompertz function and fuzz",
    "full_text_length": 144240,
    "chunk_length": 1242
  },
  {
    "chunk_id": 290,
    "paper_filename": "barsha_2023_comprehensive_review_on_breast_cancer_detection_classifcation_segmetnaiotn_using_deeplearning.pdf",
    "paper_title": "Barsha 2023 Comprehensive Review On Breast Cancer Detection Classifcation Segmetnaiotn Using Deeplearning",
    "chunk_index": 75,
    "total_chunks": 147,
    "text_content": "invasive carci- nomas 100 B 100 in situ carcinomasEnsemble learning HP Acc 92.5 The effectiveness of the model is reduced when BACH is used instead of BreakHis [188] Shen et al. Nature Publishing Group UK London (2019)2478 images from 1249 patientsResnet-50 and VGG16 DM AUC of 0.88,sensitivity: 86.1%, specificity: 80.1%Mammograms are downsized to fit available GPU memory, and the datasets used are not nationally representative samples [189] P\u00e9rez-Benito et al., Computer Methods and Programs in B",
    "full_text_length": 144240,
    "chunk_length": 1349
  },
  {
    "chunk_id": 291,
    "paper_filename": "barsha_2023_comprehensive_review_on_breast_cancer_detection_classifcation_segmetnaiotn_using_deeplearning.pdf",
    "paper_title": "Barsha 2023 Comprehensive Review On Breast Cancer Detection Classifcation Segmetnaiotn Using Deeplearning",
    "chunk_index": 76,
    "total_chunks": 147,
    "text_content": "52 M Ensemble-Net DM Acc 96.72 Neglected clinical features in favour of feature extraction for tumor detection B. Abhisheka et al. 1 3 BC. Pan et al. [197] have developed a tumor segmentation model titled Spatial-Channel Fully Convolutional Network (SC-FCN), which uses BUS images. In addition, to improve classification process, fully convolutional networks are incorporated with bi-directional long short-term memory (BLSTM) and spatial-channel attention modules. The pro- posed method is evaluated",
    "full_text_length": 144240,
    "chunk_length": 1312
  },
  {
    "chunk_id": 292,
    "paper_filename": "barsha_2023_comprehensive_review_on_breast_cancer_detection_classifcation_segmetnaiotn_using_deeplearning.pdf",
    "paper_title": "Barsha 2023 Comprehensive Review On Breast Cancer Detection Classifcation Segmetnaiotn Using Deeplearning",
    "chunk_index": 77,
    "total_chunks": 147,
    "text_content": "clas- sifying types of BC, which led to a more precise classifica- tion of BC. The obtained results show that the proposed method outperforms the existing DL methods by providing a higher accuracy and lower error rate of 99.6% and 0.12% respectively. Saleh et al. [199] have proposed a hybrid model using RNN and the Keras-Tuner optimization technique for BC diagnosis. The optimized deep RNN has an input layer, five hidden layers, five dropout layers, and the output layer. The proposed model has t",
    "full_text_length": 144240,
    "chunk_length": 1204
  },
  {
    "chunk_id": 293,
    "paper_filename": "barsha_2023_comprehensive_review_on_breast_cancer_detection_classifcation_segmetnaiotn_using_deeplearning.pdf",
    "paper_title": "Barsha 2023 Comprehensive Review On Breast Cancer Detection Classifcation Segmetnaiotn Using Deeplearning",
    "chunk_index": 78,
    "total_chunks": 147,
    "text_content": "segmentation. 9 Comparison with Existing Works From previous published papers, it has been observed that a very limited amount of work has been done on the survey of BC classification. Hassan et al. [129] have focused on the current state of DL and machine learning CAD systems for detecting and classifying masses using mammography imaging modality only. They have not covered all screen- ing modalities currently available. Roslidar et al. [200] have presented a survey of the potential use of ther",
    "full_text_length": 144240,
    "chunk_length": 1283
  },
  {
    "chunk_id": 294,
    "paper_filename": "barsha_2023_comprehensive_review_on_breast_cancer_detection_classifcation_segmetnaiotn_using_deeplearning.pdf",
    "paper_title": "Barsha 2023 Comprehensive Review On Breast Cancer Detection Classifcation Segmetnaiotn Using Deeplearning",
    "chunk_index": 79,
    "total_chunks": 147,
    "text_content": "used Type of image sequence Performance(%) Challenges and limitations [191] Raaj et al.,Biomedical Signal Processing and Control (2023)For DDSM=N:1600 M: 1024 MIAS= N: 175 M: 147Hybride CNN DM 98.44 Acc on DDSM dataset 99.17 Acc on MIAS datasetThe morphological segmen- tation approach used for detecting and segmenting the interior cancer pixels, which reduces the accuracy of cancer pixel detection in abnormal mammogram images [192] Yurdusev et al., Biomedi- cal Signal Processing and Control (202",
    "full_text_length": 144240,
    "chunk_length": 1352
  },
  {
    "chunk_id": 295,
    "paper_filename": "barsha_2023_comprehensive_review_on_breast_cancer_detection_classifcation_segmetnaiotn_using_deeplearning.pdf",
    "paper_title": "Barsha 2023 Comprehensive Review On Breast Cancer Detection Classifcation Segmetnaiotn Using Deeplearning",
    "chunk_index": 80,
    "total_chunks": 147,
    "text_content": "9979 Non M CNN, GAN and denoising autoencodersBUS Acc, Sensitivity and Specificity 89.34, 87.71 and 90.68The model is limited to detecting potentially harmful changes in the breast lump, but the identi- fication of solid nodules is also medically significant [193] Aslan M. F., Computers and Electrical Engineering (2023)220 B, 67 N and 49 M CNN and BiLSTM DM Acc 97.60 and 98.56 Prioritized determining the type of cancer over identifying the loca- tion of the tumor [194] Demir.F., Biocybernetics a",
    "full_text_length": 144240,
    "chunk_length": 1235
  },
  {
    "chunk_id": 296,
    "paper_filename": "barsha_2023_comprehensive_review_on_breast_cancer_detection_classifcation_segmetnaiotn_using_deeplearning.pdf",
    "paper_title": "Barsha 2023 Comprehensive Review On Breast Cancer Detection Classifcation Segmetnaiotn Using Deeplearning",
    "chunk_index": 81,
    "total_chunks": 147,
    "text_content": "Cancers (2019) 400 training,100 test image CNN and RNN HP Acc 97.5 Computationally expensive [197] Pan. et al., Ultrasonics (2021) 124 patients SC-FCN-BLST BUS DSC, recall, PR and HD are 81.78, 80.67, 82.92 and 11.13A problem is present in sensitivity, resulting in the failure to detect certain lesions [198] Dewangan et al., Multimedia Tools and Applications (2022)735 M 265 B BPBRW and HKH-ABO MRI Acc 99.6 The BPBRW has a complicated design [199] Saleh et al., Computational Intelligence and Neur",
    "full_text_length": 144240,
    "chunk_length": 1248
  },
  {
    "chunk_id": 297,
    "paper_filename": "barsha_2023_comprehensive_review_on_breast_cancer_detection_classifcation_segmetnaiotn_using_deeplearning.pdf",
    "paper_title": "Barsha 2023 Comprehensive Review On Breast Cancer Detection Classifcation Segmetnaiotn Using Deeplearning",
    "chunk_index": 82,
    "total_chunks": 147,
    "text_content": "Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) to investigate differ - ent DL-based methods for BC detection. The paper lacks a discussion on the ideal imaging modality to be used for each stage and an in-depth analysis of the screening modality for detecting BC. To a certain extent, the performed surveys are restricted in terms of imaging approaches, analyzed components, seg- mentation, classification methods and challenges. No sur - veys have been reported that explicitly fo",
    "full_text_length": 144240,
    "chunk_length": 1324
  },
  {
    "chunk_id": 298,
    "paper_filename": "barsha_2023_comprehensive_review_on_breast_cancer_detection_classifcation_segmetnaiotn_using_deeplearning.pdf",
    "paper_title": "Barsha 2023 Comprehensive Review On Breast Cancer Detection Classifcation Segmetnaiotn Using Deeplearning",
    "chunk_index": 83,
    "total_chunks": 147,
    "text_content": "able resource for upcoming researchers. Overall, this survey is intended to contribute to the ongoing efforts to improve the understanding of BC and develop more effective strate- gies for its prevention and treatment. Table 4 highlights the comparison between our proposed survey and the existing survey. 10 Quality Metrics for Evaluating Performance DL algorithms are assessed based on several metrics such as accuracy, F1 score, and AUC. Below is a description of these evaluation parameters. The ",
    "full_text_length": 144240,
    "chunk_length": 1234
  },
  {
    "chunk_id": 299,
    "paper_filename": "barsha_2023_comprehensive_review_on_breast_cancer_detection_classifcation_segmetnaiotn_using_deeplearning.pdf",
    "paper_title": "Barsha 2023 Comprehensive Review On Breast Cancer Detection Classifcation Segmetnaiotn Using Deeplearning",
    "chunk_index": 84,
    "total_chunks": 147,
    "text_content": "in terms of quality and performance. These studies [206\u2013 213]have proposed methods for DL models that rely on metrics such as accuracy, F1 score, and AUC. To compute the accuracy and F1 score equ-1 and equ-2 are employed. In the equations, the terms TP, TN, FP, and FN represent true positives, true negatives, false positives, and false negatives. In the process of evaluation, analyz- ing and interpreting data is an indispensable component, Table 4 Comparison of proposed survey with existing surv",
    "full_text_length": 144240,
    "chunk_length": 981
  },
  {
    "chunk_id": 300,
    "paper_filename": "barsha_2023_comprehensive_review_on_breast_cancer_detection_classifcation_segmetnaiotn_using_deeplearning.pdf",
    "paper_title": "Barsha 2023 Comprehensive Review On Breast Cancer Detection Classifcation Segmetnaiotn Using Deeplearning",
    "chunk_index": 85,
    "total_chunks": 147,
    "text_content": "\u2713 \u2717 \u2713 \u2713 \u2717 \u2717 \u2713 \u2713 Yassin et al. [205] \u2713 \u2713 \u2713 \u2713 \u2713 \u2717 \u2717 \u2713 \u2717 Hassan et al. [129] \u2713 \u2713 \u2713 \u2717 \u2717 \u2713 \u2713 \u2717 \u2717 Roslidar et al.[200] \u2713 \u2713 \u2713 \u2717 \u2717 \u2713 \u2713 \u2717 \u2717 Proposed survey \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713Table 5 shows a comparison of DL approaches in terms of the obtained Accuracy, AUC and F1 score Ref Accuracy (%) F1 Score (%) AUC (%) [180] 90.78 91.46 95.49 [214] 98.87 97.99 98.88 [215] 91.27 84.17 93 [216] 95.58 89 85 [217] 98.60 93.50 99.40 [218] 97.98 95.97 98.46 [219] 99.41 98.08 97.61 [206] 96.70 96 98.30 [207] 98.96 97.66 99.5",
    "full_text_length": 144240,
    "chunk_length": 975
  },
  {
    "chunk_id": 301,
    "paper_filename": "barsha_2023_comprehensive_review_on_breast_cancer_detection_classifcation_segmetnaiotn_using_deeplearning.pdf",
    "paper_title": "Barsha 2023 Comprehensive Review On Breast Cancer Detection Classifcation Segmetnaiotn Using Deeplearning",
    "chunk_index": 86,
    "total_chunks": 147,
    "text_content": "and Segmentation Using\u2026 1 3as it yields clear and comprehensible results that can be utilized and enhanced. \u2022 Accuracy refers to the ability of the model to correctly classify images or other types of data and is calculated using Eq. 1. \u2022 AUC curve is a graphical representation of the perfor - mance of a binary classification model. It measures the performance of the model over all possible classification thresholds. \u2022 F1 score is a weighted harmonic mean of the precision and recall, where the F",
    "full_text_length": 144240,
    "chunk_length": 1240
  },
  {
    "chunk_id": 302,
    "paper_filename": "barsha_2023_comprehensive_review_on_breast_cancer_detection_classifcation_segmetnaiotn_using_deeplearning.pdf",
    "paper_title": "Barsha 2023 Comprehensive Review On Breast Cancer Detection Classifcation Segmetnaiotn Using Deeplearning",
    "chunk_index": 87,
    "total_chunks": 147,
    "text_content": ")have been employed to provide a more comprehensive understanding of BC. Nevertheless, manually analyzing various imaging modalities with a large number of images is an inefficient and challenging task that might lead to mis- diagnosis and an elevated false-detection rate. Therefore, an automated strategy is required to address these issues. In order to early detection of BC, medical image analysis employing CAD has been shown to be the most efficient method. The purpose of CAD systems is to aid",
    "full_text_length": 144240,
    "chunk_length": 1303
  },
  {
    "chunk_id": 303,
    "paper_filename": "barsha_2023_comprehensive_review_on_breast_cancer_detection_classifcation_segmetnaiotn_using_deeplearning.pdf",
    "paper_title": "Barsha 2023 Comprehensive Review On Breast Cancer Detection Classifcation Segmetnaiotn Using Deeplearning",
    "chunk_index": 88,
    "total_chunks": 147,
    "text_content": "of instances that radiologists can analyze is constrained. However, the recent developments in AI, particularly DL-based approaches, have the potential to significantly accelerate the image analysis process, assisting radiologists in making earlier diagnoses of BC. The research revealed that DL-based CAD systems can produce satisfactory outcomes in the field of medical image analysis. But there are still several obstacles that prevent these approaches from being used in a clinical context. In sp",
    "full_text_length": 144240,
    "chunk_length": 1278
  },
  {
    "chunk_id": 304,
    "paper_filename": "barsha_2023_comprehensive_review_on_breast_cancer_detection_classifcation_segmetnaiotn_using_deeplearning.pdf",
    "paper_title": "Barsha 2023 Comprehensive Review On Breast Cancer Detection Classifcation Segmetnaiotn Using Deeplearning",
    "chunk_index": 89,
    "total_chunks": 147,
    "text_content": "challenge is the scarcity of a comprehen- sive dataset to train DL algorithms for medical imaging, due to their reliance on large and high-quality training datasets, an adequate dataset is needed to operate effec- tively. In addition, the creation of an adequate dataset is difficult, since the annotation of medical images is tedi- ous, time-consuming, and entails considerable effort to eliminate human error. Since there are less anomalous instances relative to normal ones, collecting adequate da",
    "full_text_length": 144240,
    "chunk_length": 1341
  },
  {
    "chunk_id": 305,
    "paper_filename": "barsha_2023_comprehensive_review_on_breast_cancer_detection_classifcation_segmetnaiotn_using_deeplearning.pdf",
    "paper_title": "Barsha 2023 Comprehensive Review On Breast Cancer Detection Classifcation Segmetnaiotn Using Deeplearning",
    "chunk_index": 90,
    "total_chunks": 147,
    "text_content": "than any other modalitiy. Therefore, one approach to generating a large dataset for several imaging modalities is a multi-institutional alli- ance. The dataset gathered through these collaborations comprises a diverse population of patients in terms of demographics, clinical history, imaging modality, and treatment procedure. The use of these datasets improves the robustness, accuracy and reliability of DL algorithms. \u2022 In most cases, the existing imaging datasets have a lim- ited range of data.",
    "full_text_length": 144240,
    "chunk_length": 1298
  },
  {
    "chunk_id": 306,
    "paper_filename": "barsha_2023_comprehensive_review_on_breast_cancer_detection_classifcation_segmetnaiotn_using_deeplearning.pdf",
    "paper_title": "Barsha 2023 Comprehensive Review On Breast Cancer Detection Classifcation Segmetnaiotn Using Deeplearning",
    "chunk_index": 91,
    "total_chunks": 147,
    "text_content": "another approach to this issue; in this approach, the algorithm is trained on datasets region- ally, but it must also move across hubs so that it can be updated on datasets in each location. However, the feder - ated learning algorithms have not yet gained widespread adoption. \u2022 The majority of studies relied on private datasets, which are considered confidential, for their analyses. There - fore, it is difficult to compare the efficacy of such mod- els across studies. Several analyses employed ",
    "full_text_length": 144240,
    "chunk_length": 1263
  },
  {
    "chunk_id": 307,
    "paper_filename": "barsha_2023_comprehensive_review_on_breast_cancer_detection_classifcation_segmetnaiotn_using_deeplearning.pdf",
    "paper_title": "Barsha 2023 Comprehensive Review On Breast Cancer Detection Classifcation Segmetnaiotn Using Deeplearning",
    "chunk_index": 92,
    "total_chunks": 147,
    "text_content": "be effective, however, the effectiveness of the target model is heavily depend- ent on the differences between the features of the source dataset and the target dataset. To boost prediction results, some studies have used data augmentation to artificially enlarge the dataset. However, it has to be noted that, unlike new independent images, data augmentation fails to contribute a significant amount of additional informa - tion to the DL model. \u2022 DL-based CAD systems also struggle with a lack of c",
    "full_text_length": 144240,
    "chunk_length": 1254
  },
  {
    "chunk_id": 308,
    "paper_filename": "barsha_2023_comprehensive_review_on_breast_cancer_detection_classifcation_segmetnaiotn_using_deeplearning.pdf",
    "paper_title": "Barsha 2023 Comprehensive Review On Breast Cancer Detection Classifcation Segmetnaiotn Using Deeplearning",
    "chunk_index": 93,
    "total_chunks": 147,
    "text_content": "have been less reliable than those from supervised techniques. \u2022 The issue of class imbalance poses a significant obstacle for DL models. Therefore, there is a possibility that the results may be skewed in favour of the majority class. Even in the multiclass classification, there is an uneven distribution at both the patient and image level. Thus, the absence of diverse and ethnic datasets is another issue that must be tackled. \u2022 A further significant difficulty for DL algorithms is the need to ",
    "full_text_length": 144240,
    "chunk_length": 1242
  },
  {
    "chunk_id": 309,
    "paper_filename": "barsha_2023_comprehensive_review_on_breast_cancer_detection_classifcation_segmetnaiotn_using_deeplearning.pdf",
    "paper_title": "Barsha 2023 Comprehensive Review On Breast Cancer Detection Classifcation Segmetnaiotn Using Deeplearning",
    "chunk_index": 94,
    "total_chunks": 147,
    "text_content": "the imaging data. \u2022 One such problem that DL models have to deal with is label noise. During the growth of BC, it spreads from one part of the breast to other breast tissues. Therefore, a sin- gle image may be annotated with various stages of BC, since specific regions of the same breast may be affected differently. These images may cause confusion for the DL model when it comes to the multi-classification scenario. \u2022 In terms of DL techniques, there has not been extensive research on the potent",
    "full_text_length": 144240,
    "chunk_length": 1189
  },
  {
    "chunk_id": 310,
    "paper_filename": "barsha_2023_comprehensive_review_on_breast_cancer_detection_classifcation_segmetnaiotn_using_deeplearning.pdf",
    "paper_title": "Barsha 2023 Comprehensive Review On Breast Cancer Detection Classifcation Segmetnaiotn Using Deeplearning",
    "chunk_index": 95,
    "total_chunks": 147,
    "text_content": "1 3actual meaning of the decision being made by the algo- rithms and what aspects of an image are extremely dis- criminatory, thus, they do not favour these opaque DL algorithms. Therefore, to boost the level of trust and reliability on the actions taken by DL tools, adoption of interpretable methodologies with appropriate explana - tions of DL algorithms is essential, hence, ensure the safe and effective use of DL. \u2022 Moreover, few studies indicate that employing omics [222] data rather than ima",
    "full_text_length": 144240,
    "chunk_length": 1221
  },
  {
    "chunk_id": 311,
    "paper_filename": "barsha_2023_comprehensive_review_on_breast_cancer_detection_classifcation_segmetnaiotn_using_deeplearning.pdf",
    "paper_title": "Barsha 2023 Comprehensive Review On Breast Cancer Detection Classifcation Segmetnaiotn Using Deeplearning",
    "chunk_index": 96,
    "total_chunks": 147,
    "text_content": "of different techniques and imaging data are widely available, whereas it is not in the case of omics data. \u2022 During the study of the relevant literature, it is observed that the DL-based CAD approaches overlook the corre- lation that exists between the two vision tasks of tumor region segmentation and classification. The network is fed only breast tumor images based on the defined ROI without any prior information about the tumor\u2019s outlines. The clinical parameters associated with tumor segment",
    "full_text_length": 144240,
    "chunk_length": 1350
  },
  {
    "chunk_id": 312,
    "paper_filename": "barsha_2023_comprehensive_review_on_breast_cancer_detection_classifcation_segmetnaiotn_using_deeplearning.pdf",
    "paper_title": "Barsha 2023 Comprehensive Review On Breast Cancer Detection Classifcation Segmetnaiotn Using Deeplearning",
    "chunk_index": 97,
    "total_chunks": 147,
    "text_content": "performance. If the DL model is trained to focus more on these parameters then the classification of breast tumors will improve tremendously. Therefore, it is crucial to design the system that improves the features including the segmentation information based on clinical diagnosis criteria. \u2022 In some papers [ 196, 223] it is observed that the authors train models on a slice by slice basis at time, which pro- vides higher accuracy, but there is considerable data leakage. This can be reduced by se",
    "full_text_length": 144240,
    "chunk_length": 1229
  },
  {
    "chunk_id": 313,
    "paper_filename": "barsha_2023_comprehensive_review_on_breast_cancer_detection_classifcation_segmetnaiotn_using_deeplearning.pdf",
    "paper_title": "Barsha 2023 Comprehensive Review On Breast Cancer Detection Classifcation Segmetnaiotn Using Deeplearning",
    "chunk_index": 98,
    "total_chunks": 147,
    "text_content": "the computing time grows propor -tionally with the size of the images used, thus, neces - sitates the use of high-performance hardware. 12 Conclusions BC survival rates can be increased with early diagnosis. Since, technology has reached the point where early detec- tion of BC is feasible. Moreover, AI based techniques in medical image analysis have allowed for the automatic extraction of crucial features from large datasets, enabling a better BC diagnosis. This study aims to analyze the state- ",
    "full_text_length": 144240,
    "chunk_length": 1323
  },
  {
    "chunk_id": 314,
    "paper_filename": "barsha_2023_comprehensive_review_on_breast_cancer_detection_classifcation_segmetnaiotn_using_deeplearning.pdf",
    "paper_title": "Barsha 2023 Comprehensive Review On Breast Cancer Detection Classifcation Segmetnaiotn Using Deeplearning",
    "chunk_index": 99,
    "total_chunks": 147,
    "text_content": "concerns regarding the explainability and inter - pretability of these systems need to be taken into considera- tion. Therefore, this work highlights potential future research directions and challenges in adopting AI-based approaches for BC diagnosis based on a variety of MIM. This analysis indicates that there is a significant need for an integrated, fully-automated framework that can reliably identify BC with minimum intervention. Data Availibility Data sharing is not applicable to this articl",
    "full_text_length": 144240,
    "chunk_length": 1383
  },
  {
    "chunk_id": 315,
    "paper_filename": "barsha_2023_comprehensive_review_on_breast_cancer_detection_classifcation_segmetnaiotn_using_deeplearning.pdf",
    "paper_title": "Barsha 2023 Comprehensive Review On Breast Cancer Detection Classifcation Segmetnaiotn Using Deeplearning",
    "chunk_index": 100,
    "total_chunks": 147,
    "text_content": "of- breast- cancer. html 3. Momenimovahed Z, Salehiniya H (2019) Epidemiological characteristics of and risk factors for breast cancer in the world. Breast Cancer Targets Ther 11:151\u2013164 4. Ellington TD, Henley SJ, Wilson RJ, Miller JW, Wu M, Richard- son LC (2023) Trends in breast cancer mortality by race/ethnic- ity, age, and us census region, United States\u20131999-2020. Cancer 129(1):32\u201338 B. Abhisheka et al. 1 3 5. Giaquinto AN, Sung H, Miller KD, Kramer JL, Newman LA, Minihan A, Jemal A, Siege",
    "full_text_length": 144240,
    "chunk_length": 1253
  },
  {
    "chunk_id": 316,
    "paper_filename": "barsha_2023_comprehensive_review_on_breast_cancer_detection_classifcation_segmetnaiotn_using_deeplearning.pdf",
    "paper_title": "Barsha 2023 Comprehensive Review On Breast Cancer Detection Classifcation Segmetnaiotn Using Deeplearning",
    "chunk_index": 101,
    "total_chunks": 147,
    "text_content": "fact- sheets/ detail/ breast- cancer 9. Hirschman J, Whitman S, Ansell D (2007) The black: white dis- parity in breast cancer mortality: the example of Chicago. Cancer Causes Control 18:323\u2013333 10. National Institute of Cancer Prevention and Research (2020). http:// cance rindia. org. in/ cancer- stati stics/ 11. Mehrotra R, Yadav K (2022) Breast cancer in India: present sce- nario and the challenges ahead. World J Clin Oncol 13(3):209 12. Dhillon A, Singh A (2020) ebrecap: extreme learning-base",
    "full_text_length": 144240,
    "chunk_length": 1301
  },
  {
    "chunk_id": 317,
    "paper_filename": "barsha_2023_comprehensive_review_on_breast_cancer_detection_classifcation_segmetnaiotn_using_deeplearning.pdf",
    "paper_title": "Barsha 2023 Comprehensive Review On Breast Cancer Detection Classifcation Segmetnaiotn Using Deeplearning",
    "chunk_index": 102,
    "total_chunks": 147,
    "text_content": "Johnson J, Geraghty K, Riley R, Zhou A, Panagopoulou E, Chew-Graham CA, Peters D, Esmail A et al (2022) Associations of physician burnout with career engage- ment and quality of patient care: systematic review and meta- analysis. BMJ 378:e070442 16. Scroll.in (2021). https:// scroll. in/ artic le/ 10297 66/ how- true- is- the- health- minis ters- claim- that- indias- doctor- popul ation- ratio- excee ds- who- guide lines 17. Graham LJ, Shupe MP, Schneble EJ, Flynt FL, Clemenshaw MN, Kirkpatrick ",
    "full_text_length": 144240,
    "chunk_length": 1275
  },
  {
    "chunk_id": 318,
    "paper_filename": "barsha_2023_comprehensive_review_on_breast_cancer_detection_classifcation_segmetnaiotn_using_deeplearning.pdf",
    "paper_title": "Barsha 2023 Comprehensive Review On Breast Cancer Detection Classifcation Segmetnaiotn Using Deeplearning",
    "chunk_index": 103,
    "total_chunks": 147,
    "text_content": "prognosis in multi-omics: from computational needs to machine learning and deep learning. Arch Comput Methods Eng 30(2):917\u2013949 20. Toss A, Cristofanilli M (2015) Molecular characterization and targeted therapeutic approaches in breast cancer. Breast Cancer Res 17(1):1\u201311 21. Oloomi M, Moazzezy N, Bouzari S (2020) Comparing blood ver - sus tissue-based biomarkers expression in breast cancer patients. Heliyon 6(4):03728 22. Joseph C, Papadaki A, Althobiti M, Alsaleem M, Aleskanda- rany MA, Rakha ",
    "full_text_length": 144240,
    "chunk_length": 1366
  },
  {
    "chunk_id": 319,
    "paper_filename": "barsha_2023_comprehensive_review_on_breast_cancer_detection_classifcation_segmetnaiotn_using_deeplearning.pdf",
    "paper_title": "Barsha 2023 Comprehensive Review On Breast Cancer Detection Classifcation Segmetnaiotn Using Deeplearning",
    "chunk_index": 104,
    "total_chunks": 147,
    "text_content": "massive open online courses using machine learning approaches. Expert Syst Appl 199:117092 25. Alanazi A (2022) Using machine learning for healthcare chal- lenges and opportunities. Inform Med Unlocked 30:100924 26. Rodrigues AP, Fernandes R, Shetty A, Lakshmanna K, Shafi RM et al (2022) Real-time twitter spam detection and sentiment analysis using machine learning and deep learning techniques. Comput Intell Neurosci. https:// doi. org/ 10. 1155/ 2022/ 52119 49 27. Mohanta BK, Jena D, Mohapatra ",
    "full_text_length": 144240,
    "chunk_length": 1318
  },
  {
    "chunk_id": 320,
    "paper_filename": "barsha_2023_comprehensive_review_on_breast_cancer_detection_classifcation_segmetnaiotn_using_deeplearning.pdf",
    "paper_title": "Barsha 2023 Comprehensive Review On Breast Cancer Detection Classifcation Segmetnaiotn Using Deeplearning",
    "chunk_index": 105,
    "total_chunks": 147,
    "text_content": "Grandhi S (2022) A deep learning approach for sentiment analysis of covid-19 reviews. Appl Sci 12(8):3709 31. Ravi C, Tigga A, Reddy GT, Hakak S, Alazab M (2022) Driver identification using optimized deep learning model in smart transportation. ACM Trans Int Technol 22(4):1\u201317 32. Majumdar S, Pramanik P, Sarkar R (2023) Gamma function based ensemble of CNN models for breast cancer detection in histopathology images. Expert Syst Appl 213:119022 33. Shen T, Wang J, Gou C, Wang F-Y (2020) Hierarchi",
    "full_text_length": 144240,
    "chunk_length": 1354
  },
  {
    "chunk_id": 321,
    "paper_filename": "barsha_2023_comprehensive_review_on_breast_cancer_detection_classifcation_segmetnaiotn_using_deeplearning.pdf",
    "paper_title": "Barsha 2023 Comprehensive Review On Breast Cancer Detection Classifcation Segmetnaiotn Using Deeplearning",
    "chunk_index": 106,
    "total_chunks": 147,
    "text_content": "predators algorithm. Neural Comput Appl 34(20):18015\u201318033 36. Desai M, Shah M (2021) An anatomization on breast cancer detection and diagnosis employing multi-layer perceptron neu- ral network (mlp) and convolutional neural network (cnn). Clin eHealth 4:1\u201311 37. Sannasi Chakravarthy S, Bharanidharan N, Rajaguru H (2022) Multi-deep CNN based experimentations for early diagnosis of breast cancer. IETE J Res 2022:1\u201316 38. Bie C, Li Y, Zhou Y, Bhujwalla ZM, Song X, Liu G, van Zijl PC, Yadav NN (202",
    "full_text_length": 144240,
    "chunk_length": 1347
  },
  {
    "chunk_id": 322,
    "paper_filename": "barsha_2023_comprehensive_review_on_breast_cancer_detection_classifcation_segmetnaiotn_using_deeplearning.pdf",
    "paper_title": "Barsha 2023 Comprehensive Review On Breast Cancer Detection Classifcation Segmetnaiotn Using Deeplearning",
    "chunk_index": 107,
    "total_chunks": 147,
    "text_content": "selection with deep learning algorithm. Res Biomed Eng 2023:1\u201313 41. Mokni R, Haoues M (2022) Cadnet157 model: fine-tuned resnet152 model for breast cancer diagnosis from mammography images. Neural Comput Appl 2022:1\u201324 42. Picard J (1998) History of mammography. Bulletin de l\u2019 Academie nationale de medecine 182(8):1613\u20131620 43. Wu YC, Freedman MT, Hasegawa A, Zuurbier RA, Lo S-CB, Mun SK (1995) Classification of microcalcifications in A Comprehensive Review on Breast Cancer Detection, Classific",
    "full_text_length": 144240,
    "chunk_length": 1372
  },
  {
    "chunk_id": 323,
    "paper_filename": "barsha_2023_comprehensive_review_on_breast_cancer_detection_classifcation_segmetnaiotn_using_deeplearning.pdf",
    "paper_title": "Barsha 2023 Comprehensive Review On Breast Cancer Detection Classifcation Segmetnaiotn Using Deeplearning",
    "chunk_index": 108,
    "total_chunks": 147,
    "text_content": "on bioinformatics and biomedicine (BIBM). IEEE, pp. 700\u2013704 46. Kassani SH, Kassani PH, Wesolowski MJ, Schneider KA, Deters R (2019) Breast cancer diagnosis with transfer learning and global pooling. In: 2019 international conference on informa- tion and communication technology convergence (ICTC). IEEE, pp 519\u2013524 47. Suh YJ, Jung J, Cho B-J (2020) Automated breast cancer detec- tion in digital mammograms of various densities via deep learn- ing. J Person Med 10(4):211 48. Chan H-P, Doi K, Galh",
    "full_text_length": 144240,
    "chunk_length": 1343
  },
  {
    "chunk_id": 324,
    "paper_filename": "barsha_2023_comprehensive_review_on_breast_cancer_detection_classifcation_segmetnaiotn_using_deeplearning.pdf",
    "paper_title": "Barsha 2023 Comprehensive Review On Breast Cancer Detection Classifcation Segmetnaiotn Using Deeplearning",
    "chunk_index": 109,
    "total_chunks": 147,
    "text_content": "N, Baskurt A (1998) Multires- olution texture based adaptive clustering algorithm for breast lesion segmentation. Eur J Ultrasound 8(2):135\u2013144 52. Guliato D, Rangayyan RM, Carnielli WA, Zuffo J, Desautels J (1998) Segmentation of breast tumors in mammograms by fuzzy region growing. In: Proceedings of the 20th annual international conference of the IEEE engineering in medicine and biology society, vol 20 biomedical engineering towards the year 2000 and beyond (Cat. No. 98CH36286). IEEE, vol 2, p",
    "full_text_length": 144240,
    "chunk_length": 1328
  },
  {
    "chunk_id": 325,
    "paper_filename": "barsha_2023_comprehensive_review_on_breast_cancer_detection_classifcation_segmetnaiotn_using_deeplearning.pdf",
    "paper_title": "Barsha 2023 Comprehensive Review On Breast Cancer Detection Classifcation Segmetnaiotn Using Deeplearning",
    "chunk_index": 110,
    "total_chunks": 147,
    "text_content": "auto- matic breast cancer assessment using digital mammography. IEEE Trans Instrum Meas 71:1\u201313 56. Nguyen HT, Tran SB, Nguyen DB, Pham HH, Nguyen HQ (2022) A novel multi-view deep learning approach for bi-rads and density assessment of mammograms. In: 2022 44th annual international conference of the IEEE engineering in medicine & biology society (EMBC). IEEE, pp 2144\u20132148 57. Jiang J, Peng J, Hu C, Jian W, Wang X, Liu W (2022) Breast cancer detection and classification in mammogram using a thre",
    "full_text_length": 144240,
    "chunk_length": 1253
  },
  {
    "chunk_id": 326,
    "paper_filename": "barsha_2023_comprehensive_review_on_breast_cancer_detection_classifcation_segmetnaiotn_using_deeplearning.pdf",
    "paper_title": "Barsha 2023 Comprehensive Review On Breast Cancer Detection Classifcation Segmetnaiotn Using Deeplearning",
    "chunk_index": 111,
    "total_chunks": 147,
    "text_content": "L, Olszewski P, Osowski S, Ko\u0142odziej M (2021) Deep neural system for supporting tumor recognition of mammograms using modified gan. Expert Syst Appl 164:113968 61. Singh VK, Rashwan HA, Romani S, Akram F, Pandey N, Sarker MMK, Saleh A, Arenas M, Arquez M, Puig D et al (2020) Breast tumor segmentation and shape classification in mammograms using generative adversarial and convolutional neural network. Expert Syst Appl 139:112855 62. Trister AD, Buist DS, Lee CI (2017) Will machine learn- ing tip ",
    "full_text_length": 144240,
    "chunk_length": 1205
  },
  {
    "chunk_id": 327,
    "paper_filename": "barsha_2023_comprehensive_review_on_breast_cancer_detection_classifcation_segmetnaiotn_using_deeplearning.pdf",
    "paper_title": "Barsha 2023 Comprehensive Review On Breast Cancer Detection Classifcation Segmetnaiotn Using Deeplearning",
    "chunk_index": 112,
    "total_chunks": 147,
    "text_content": "breast masses on 3d us volumetric images: effect of computer-aided diagnosis on radiologist accuracy. Radiology 242(3):716\u2013724 65. Qi X, Yi F, Zhang L, Chen Y, Pi Y, Chen Y, Guo J, Wang J, Guo Q, Li J et al (2022) Computer-aided diagnosis of breast cancer in ultrasonography images by deep learning. Neurocom - puting 472:152\u2013165 66. Ding W, Wang J, Zhou W, Zhou S, Chang C, Shi J (2022) Joint localization and classification of breast cancer in b-mode ultra- sound imaging via collaborative learning",
    "full_text_length": 144240,
    "chunk_length": 1159
  },
  {
    "chunk_id": 328,
    "paper_filename": "barsha_2023_comprehensive_review_on_breast_cancer_detection_classifcation_segmetnaiotn_using_deeplearning.pdf",
    "paper_title": "Barsha 2023 Comprehensive Review On Breast Cancer Detection Classifcation Segmetnaiotn Using Deeplearning",
    "chunk_index": 113,
    "total_chunks": 147,
    "text_content": "ir and ultrasound attenuation system for differentiating breast cancer from adjacent normal tissue. IEEE Trans Biomed Eng 68(12):3554\u20133563 69. Wu L, Ye W, Liu Y, Chen D, Wang Y, Cui Y, Li Z, Li P, Li Z, Liu Z et al (2022) An integrated deep learning model for the prediction of pathological complete response to neoadjuvant chemotherapy with serial ultrasonography in breast cancer patients: a multicentre, retrospective study. Breast Cancer Res 24(1):81 70. Gu J, Tong T, He C, Xu M, Yang X, Tian J,",
    "full_text_length": 144240,
    "chunk_length": 1224
  },
  {
    "chunk_id": 329,
    "paper_filename": "barsha_2023_comprehensive_review_on_breast_cancer_detection_classifcation_segmetnaiotn_using_deeplearning.pdf",
    "paper_title": "Barsha 2023 Comprehensive Review On Breast Cancer Detection Classifcation Segmetnaiotn Using Deeplearning",
    "chunk_index": 114,
    "total_chunks": 147,
    "text_content": "H, Luo G, Li B, Shang H, Shao H, Sun S, Wang Z, Wang K, Cheng W (2022) Performance of novel deep learn- ing network with the incorporation of the automatic segmenta- tion network for diagnosis of breast cancer in automated breast ultrasound. Eur Radiol 32(10):7163\u20137172 73. Matsumoto Y, Katsumura A, Miki N (2022) Pressure-controlled ultrasound probe for reliable imaging in breast cancer diagnosis. Jpn J Appl Phys 61:1035 74. Prasad S, Almekkawy M (2022) Deepuct: complex cascaded deep learning net",
    "full_text_length": 144240,
    "chunk_length": 1229
  },
  {
    "chunk_id": 330,
    "paper_filename": "barsha_2023_comprehensive_review_on_breast_cancer_detection_classifcation_segmetnaiotn_using_deeplearning.pdf",
    "paper_title": "Barsha 2023 Comprehensive Review On Breast Cancer Detection Classifcation Segmetnaiotn Using Deeplearning",
    "chunk_index": 115,
    "total_chunks": 147,
    "text_content": "3 77. Houssami N, Hayes DF (2009) Review of preoperative mag- netic resonance imaging (MRI) in breast cancer: should MRI be performed on all women with newly diagnosed, early stage breast cancer? CA Cancer J Clin 59(5):290\u2013302 78. Meyer A, Chlebus G, Rak M, Schindele D, Schostak M, van Ginneken B, Schenk A, Meine H, Hahn HK, Schreiber A et al (2021) Anisotropic 3d multi-stream CNN for accurate prostate segmentation from multi-planar MRI. Comput Methods Pro- grams Biomed 200:105821 79. Piantadosi",
    "full_text_length": 144240,
    "chunk_length": 1213
  },
  {
    "chunk_id": 331,
    "paper_filename": "barsha_2023_comprehensive_review_on_breast_cancer_detection_classifcation_segmetnaiotn_using_deeplearning.pdf",
    "paper_title": "Barsha 2023 Comprehensive Review On Breast Cancer Detection Classifcation Segmetnaiotn Using Deeplearning",
    "chunk_index": 116,
    "total_chunks": 147,
    "text_content": "LM, Carru- thers C, Carter WB, Ciocca R, Sabol J, Frazier TG et al (2022) Outcomes of abbreviated MRI (ab-MRI) for women of any breast cancer risk and breast density in a community academic setting. Ann Surg Oncol 29(10):6215\u20136221 82. Galati F, Rizzo V, Moffa G, Caramanico C, Kripa E, Cerbelli B, D\u2019 Amati G, Pediconi F (2022) Radiologic-pathologic cor - relation in breast cancer: do MRI biomarkers correlate with pathologic features and molecular subtypes? Eur Radiol Exp 6(1):39 83. Mann RM, Atha",
    "full_text_length": 144240,
    "chunk_length": 1228
  },
  {
    "chunk_id": 332,
    "paper_filename": "barsha_2023_comprehensive_review_on_breast_cancer_detection_classifcation_segmetnaiotn_using_deeplearning.pdf",
    "paper_title": "Barsha 2023 Comprehensive Review On Breast Cancer Detection Classifcation Segmetnaiotn Using Deeplearning",
    "chunk_index": 117,
    "total_chunks": 147,
    "text_content": "subtypes of breast cancer from multi-scale lesion images of DCE-MRI with transfer learning technique. Comput Biol Med 150:106147 85. Ren T, Lin S, Huang P, Duong TQ (2022) Convolutional neu - ral network of multiparametric MRI accurately detects axillary lymph node metastasis in breast cancer patients with pre neoad- juvant chemotherapy. Clin Breast Cancer 22(2):170\u2013177 86. Kang BJ, Kim MJ, Shin HJ, Moon WK (2022) Acquisition and interpretation guidelines of breast diffusion-weighted MRI (DW- MR",
    "full_text_length": 144240,
    "chunk_length": 1318
  },
  {
    "chunk_id": 333,
    "paper_filename": "barsha_2023_comprehensive_review_on_breast_cancer_detection_classifcation_segmetnaiotn_using_deeplearning.pdf",
    "paper_title": "Barsha 2023 Comprehensive Review On Breast Cancer Detection Classifcation Segmetnaiotn Using Deeplearning",
    "chunk_index": 118,
    "total_chunks": 147,
    "text_content": "Imaging 13(1):71 90. Hermena S, Young M (2022) CT-scan image production proce- dures. StatPearls, Tampa 91. Desperito E, Schwartz L, Capaccione KM, Collins BT, Jama- bawalikar S, Peng B, Patrizio R, Salvatore MM (2022) Chest CT for breast cancer diagnosis. Life 12(11):1699 92. Volterrani L, Gentili F, Fausto A, Pelini V, Megha T, Sardanelli F, Mazzei MA (2020) Dual-energy CT for locoregional stag- ing of breast cancer: preliminary results. Am J Roentgenol 214(3):707\u2013714 93. Yang X, Wu L, Ye W, Z",
    "full_text_length": 144240,
    "chunk_length": 1202
  },
  {
    "chunk_id": 334,
    "paper_filename": "barsha_2023_comprehensive_review_on_breast_cancer_detection_classifcation_segmetnaiotn_using_deeplearning.pdf",
    "paper_title": "Barsha 2023 Comprehensive Review On Breast Cancer Detection Classifcation Segmetnaiotn Using Deeplearning",
    "chunk_index": 119,
    "total_chunks": 147,
    "text_content": "pp 149\u2013168 95. Yeh BM, FitzGerald PF, Edic PM, Lambert JW, Colborn RE, Marino ME, Evans PM, Roberts JC, Wang ZJ, Wong MJ et al (2017) Opportunities for new CT contrast agents to maximize the diagnostic potential of emerging spectral CT technologies. Adv Drug Deliv Rev 113:201\u2013222 96. Nicolas E, Khalifa N, Laporte C, Bouhroum S, Kirova Y (2021) Safety margins for the delineation of the left anterior descending artery in patients treated for breast cancer. Int J Radiat Oncol Biol Phys 109(1):267\u20132",
    "full_text_length": 144240,
    "chunk_length": 1215
  },
  {
    "chunk_id": 335,
    "paper_filename": "barsha_2023_comprehensive_review_on_breast_cancer_detection_classifcation_segmetnaiotn_using_deeplearning.pdf",
    "paper_title": "Barsha 2023 Comprehensive Review On Breast Cancer Detection Classifcation Segmetnaiotn Using Deeplearning",
    "chunk_index": 120,
    "total_chunks": 147,
    "text_content": "imaging in a large cohort of patients. Med Phys 50:2417 99. Shim S, Cester D, Ruby L, Bluethgen C, Marcon M, Berger N, Unkelbach J, Boss A (2022) Fully automated breast segmentation on spiral breast computed tomography images. J Appl Clin Med Phys 23(10):13726 100. Hadebe B, Harry L, Ebrahim T, Pillay V, Vorster M (2023) The role of PET/CT in breast cancer. Diagnostics 13(4):597 101. Koh J, Yoon Y, Kim S, Han K, Kim E-K (2022) Deep learning for the detection of breast cancers on chest computed t",
    "full_text_length": 144240,
    "chunk_length": 1221
  },
  {
    "chunk_id": 336,
    "paper_filename": "barsha_2023_comprehensive_review_on_breast_cancer_detection_classifcation_segmetnaiotn_using_deeplearning.pdf",
    "paper_title": "Barsha 2023 Comprehensive Review On Breast Cancer Detection Classifcation Segmetnaiotn Using Deeplearning",
    "chunk_index": 121,
    "total_chunks": 147,
    "text_content": "and development of 16 /u1D6FC-[18f] fluoroestradiol (fes), a PET radi- otracer for the estrogen receptor, a historical review. Nucl Med Biol 92:24\u201337 104. Volpe A, Lang C, Lim L, Man F, Kurtys E, Ashmore-Harris C, Johnson P, Skourti E, de Rosales RT, Fruhwirth GO (2020) Spatiotemporal PET imaging reveals differences in car-t tumor retention in triple-negative breast cancer models. Mol Ther 28(10):2271\u20132285 105. Mankoff DA, Sellmyer MA (2022) PET of fibroblast-activation protein for breast cancer",
    "full_text_length": 144240,
    "chunk_length": 1294
  },
  {
    "chunk_id": 337,
    "paper_filename": "barsha_2023_comprehensive_review_on_breast_cancer_detection_classifcation_segmetnaiotn_using_deeplearning.pdf",
    "paper_title": "Barsha 2023 Comprehensive Review On Breast Cancer Detection Classifcation Segmetnaiotn Using Deeplearning",
    "chunk_index": 122,
    "total_chunks": 147,
    "text_content": "Nucl Med 52:520 108. Simsek A, Kutluturk K, Comak A, Akatli A, Kekilli E, Unal B (2021) Factors affecting the accuracy of 18 f-FDG PET/CT in detecting additional tumor foci in breast cancer. Arch Hell Med/ Arheia Ellenikes Iatrikes 38(2):63\u201368 109. Kwon Y (2019) Positron Emission Tomography (PET) of Breast cancer heterogeneous for HER2 and EGFR using bispecific radioimmunoconjugates 110. Ming Y, Wu N, Qian T, Li X, Wan DQ, Li C, Li Y, Wu Z, Wang X, Liu J et al (2020) Progress and future trends i",
    "full_text_length": 144240,
    "chunk_length": 1207
  },
  {
    "chunk_id": 338,
    "paper_filename": "barsha_2023_comprehensive_review_on_breast_cancer_detection_classifcation_segmetnaiotn_using_deeplearning.pdf",
    "paper_title": "Barsha 2023 Comprehensive Review On Breast Cancer Detection Classifcation Segmetnaiotn Using Deeplearning",
    "chunk_index": 123,
    "total_chunks": 147,
    "text_content": "A, Karanikas G et al (2021) Breast tumor characterization using [18f] fdg-pet/ CT imaging combined with data preprocessing and radiomics. Cancers 13(6):1249 113. Antunovic L, De Sanctis R, Cozzi L, Kirienko M, Sagona A, Tor - risi R, Tinterri C, Santoro A, Chiti A, Zelic R et al (2019) PET/ CT radiomics in breast cancer: promising tool for prediction of pathological response to neoadjuvant chemotherapy. Eur J Nucl Med Mol Imaging 46:1468\u20131477 114. Zhou X, Li C, Rahaman MM, Yao Y, Ai S, Sun C, Wa",
    "full_text_length": 144240,
    "chunk_length": 1246
  },
  {
    "chunk_id": 339,
    "paper_filename": "barsha_2023_comprehensive_review_on_breast_cancer_detection_classifcation_segmetnaiotn_using_deeplearning.pdf",
    "paper_title": "Barsha 2023 Comprehensive Review On Breast Cancer Detection Classifcation Segmetnaiotn Using Deeplearning",
    "chunk_index": 124,
    "total_chunks": 147,
    "text_content": "(2020) Pix2pix-based stain-to-stain trans- lation: a solution for robust stain normalization in histopathology images analysis. In: 2020 international conference on machine vision and image processing (MVIP). IEEE, pp 1\u20137 117. Boschman J, Farahani H, Darbandsari A, Ahmadvand P, Van Spankeren A, Farnell D, Levine AB, Naso JR, Churg A, Jones SJ et al (2022) The utility of color normalization for AI-based diagnosis of hematoxylin and eosin-stained pathology images. J Pathol 256(1):15\u201324 118. Singh ",
    "full_text_length": 144240,
    "chunk_length": 1431
  },
  {
    "chunk_id": 340,
    "paper_filename": "barsha_2023_comprehensive_review_on_breast_cancer_detection_classifcation_segmetnaiotn_using_deeplearning.pdf",
    "paper_title": "Barsha 2023 Comprehensive Review On Breast Cancer Detection Classifcation Segmetnaiotn Using Deeplearning",
    "chunk_index": 125,
    "total_chunks": 147,
    "text_content": "A (2020) Breast cancer diagnosis in histopathological images using resnet-50 convolutional neu- ral network. In: 2020 IEEE international IOT, electronics and mechatronics conference (IEMTRONICS). IEEE, pp 1\u20137 122. Karthik R, Menaka R, Siddharth M (2022) Classification of breast cancer from histopathology images using an ensemble of deep multiscale networks. Biocybern Biomed Eng 42(3):963\u2013976 123. Mashekova A, Zhao Y, Ng EY, Zarikas V, Fok SC, Mukhmetov O (2022) Early detection of the breast canc",
    "full_text_length": 144240,
    "chunk_length": 1396
  },
  {
    "chunk_id": 341,
    "paper_filename": "barsha_2023_comprehensive_review_on_breast_cancer_detection_classifcation_segmetnaiotn_using_deeplearning.pdf",
    "paper_title": "Barsha 2023 Comprehensive Review On Breast Cancer Detection Classifcation Segmetnaiotn Using Deeplearning",
    "chunk_index": 126,
    "total_chunks": 147,
    "text_content": "cancer detec- tion in thermograms using a hybrid of GA and GWO based deep feature selection method. Expert Syst Appl 219:119643 127. Torres-Galv\u00e1n JC, Guevara E, Kolosovas-Machuca ES, Oceguera-Villanueva A, Flores JL, Gonz\u00e1lez FJ (2022) Deep convolutional neural networks for classifying breast cancer using infrared thermography. Quant InfraRed Thermogr J 19(4):283\u2013294 128. Na SP, Houserkovaa D (2007) The role of various modalities in breast imaging. Biomed Pap Med Fac Univ Palacky Olomouc Czech ",
    "full_text_length": 144240,
    "chunk_length": 1351
  },
  {
    "chunk_id": 342,
    "paper_filename": "barsha_2023_comprehensive_review_on_breast_cancer_detection_classifcation_segmetnaiotn_using_deeplearning.pdf",
    "paper_title": "Barsha 2023 Comprehensive Review On Breast Cancer Detection Classifcation Segmetnaiotn Using Deeplearning",
    "chunk_index": 127,
    "total_chunks": 147,
    "text_content": "U, Zhang Y-D, Hamza A, Mickus A, Dama\u0161evi\u010dius R (2022) Breast cancer classification from ultrasound images using probability-based optimal deep learning feature fusion. Sensors 22(3):807 132. Sahu A, Das PK, Meher S (2023) High accuracy hybrid CNN classifiers for breast cancer detection using mammogram and ultrasound datasets. Biomed Signal Process Control 80:104292 133. Chen LW, Cao Y, D\u2019Rummo K, Shen X (2022) Estimation of patient out-of-pocket cost for radiation therapy by insurance type and ",
    "full_text_length": 144240,
    "chunk_length": 1325
  },
  {
    "chunk_id": 343,
    "paper_filename": "barsha_2023_comprehensive_review_on_breast_cancer_detection_classifcation_segmetnaiotn_using_deeplearning.pdf",
    "paper_title": "Barsha 2023 Comprehensive Review On Breast Cancer Detection Classifcation Segmetnaiotn Using Deeplearning",
    "chunk_index": 128,
    "total_chunks": 147,
    "text_content": "devel- opment of the national indication protocol for proton therapy and first clinical experiences. Clin Oncol 34(4):247\u2013257 136. Mu\u00f1oz-Montecinos C, Gonz\u00e1lez-Browne C, Maza F, Carre\u00f1o- Leiton D, Gonz\u00e1lez P, Chahuan B, Quirland C (2022) Cost-effec- tiveness of intraoperative radiation therapy versus intensity-mod- ulated radiation therapy for the treatment of early breast cancer: a disinvestment analysis 137. Madani M, Behzadi MM, Nabavi S (2022) The role of deep learning in advancing breast ca",
    "full_text_length": 144240,
    "chunk_length": 1334
  },
  {
    "chunk_id": 344,
    "paper_filename": "barsha_2023_comprehensive_review_on_breast_cancer_detection_classifcation_segmetnaiotn_using_deeplearning.pdf",
    "paper_title": "Barsha 2023 Comprehensive Review On Breast Cancer Detection Classifcation Segmetnaiotn Using Deeplearning",
    "chunk_index": 129,
    "total_chunks": 147,
    "text_content": "R (2020) The role of staging computed tomography on detection of occult metastasis in asymptomatic breast cancer patients. Cancer Rep 3(3):1247 141. Han S, Choi JY (2021) Impact of 18f-fdg PET, PET/CT, and PET/MRI on staging and management as an initial staging modality in breast cancer: a systematic review and meta-analysis. Clin Nucl Med 46(4):271 142. Ruan D, Sun L (2022) Diagnostic performance of PET/MRI in breast cancer: a systematic review and Bayesian bivariate meta- analysis. Clin Breast",
    "full_text_length": 144240,
    "chunk_length": 1308
  },
  {
    "chunk_id": 345,
    "paper_filename": "barsha_2023_comprehensive_review_on_breast_cancer_detection_classifcation_segmetnaiotn_using_deeplearning.pdf",
    "paper_title": "Barsha 2023 Comprehensive Review On Breast Cancer Detection Classifcation Segmetnaiotn Using Deeplearning",
    "chunk_index": 130,
    "total_chunks": 147,
    "text_content": "145. Petrova D, Garrido D, \u0160pac\u00edrov\u00e1 Z, Fern\u00e1ndez-Mart\u00ednez NF, Ivanova G, Rodr\u00edguez-Barranco M, Poll\u00e1n M, Barrios-Rod- r\u00edguez R, S\u00e1nchez MJ (2022) Duration of the patient interval in breast cancer and factors associated with longer delays in B. Abhisheka et al. 1 3 low-and middle-income countries: a systematic review with meta-analysis. Psycho-Oncology 32:13\u201324 146. Roh S, Lee Y-S (2023) Developing culturally tailored mobile web app education to promote breast cancer screening: knowl - edge, bar",
    "full_text_length": 144240,
    "chunk_length": 1310
  },
  {
    "chunk_id": 346,
    "paper_filename": "barsha_2023_comprehensive_review_on_breast_cancer_detection_classifcation_segmetnaiotn_using_deeplearning.pdf",
    "paper_title": "Barsha 2023 Comprehensive Review On Breast Cancer Detection Classifcation Segmetnaiotn Using Deeplearning",
    "chunk_index": 131,
    "total_chunks": 147,
    "text_content": "149. Lee J, Kang BJ, Park GE, Kim SH (2022) The usefulness of magnetic resonance imaging (MRI) for the detection of local recurrence after mastectomy with reconstructive surgery in breast cancer patients. Diagnostics 12(9):2203 150. Thawani R, Gao L, Mohinani A, Tudorica A, Li X, Mitri Z, Huang W (2022) Quantitative DCE-MRI prediction of breast cancer recurrence following neoadjuvant chemotherapy: a pre - liminary study. BMC Med Imaging 22(1):1\u201311 151. Bowyer K, Kopans D, Kegelmeyer W, Moore R, ",
    "full_text_length": 144240,
    "chunk_length": 1313
  },
  {
    "chunk_id": 347,
    "paper_filename": "barsha_2023_comprehensive_review_on_breast_cancer_detection_classifcation_segmetnaiotn_using_deeplearning.pdf",
    "paper_title": "Barsha 2023 Comprehensive Review On Breast Cancer Detection Classifcation Segmetnaiotn Using Deeplearning",
    "chunk_index": 132,
    "total_chunks": 147,
    "text_content": "Toward a standard reference database for computer- aided mammography. In: Medical imaging 2008: computer- aided diagnosis. SPIE, vol 6915, pp 606\u2013614 155. Kooi T, Litjens G, Van Ginneken B, Gubern-M\u00e9rida A, S\u00e1nchez CI, Mann R, den Heeten A, Karssemeijer N (2017) Large scale deep learning for computer aided detection of mammographic lesions. Med Image Anal 35:303\u2013312 156. Zhang Q, Xiao Y, Dai W, Suo J, Wang C, Shi J, Zheng H (2016) Deep learning based classification of breast tumors with shear-wa",
    "full_text_length": 144240,
    "chunk_length": 1304
  },
  {
    "chunk_id": 348,
    "paper_filename": "barsha_2023_comprehensive_review_on_breast_cancer_detection_classifcation_segmetnaiotn_using_deeplearning.pdf",
    "paper_title": "Barsha 2023 Comprehensive Review On Breast Cancer Detection Classifcation Segmetnaiotn Using Deeplearning",
    "chunk_index": 133,
    "total_chunks": 147,
    "text_content": "K, Sugita N, Kawasumi Y, Ishibashi T, Yoshizawa M (2016) Mass detection using deep convolutional neural network for mammographic computer- aided diagnosis. In: 2016 55th annual conference of the society of instrument and control engineers of Japan (SICE). IEEE, pp 1382\u20131386 160. de la Rosa RS, Lamard M, Cazuguel G, Coatrieux G, Cozic M, Quellec G (2015) Multiple-instance learning for breast cancer detection in mammograms. In: 2015 37th annual international conference of the IEEE engineering in m",
    "full_text_length": 144240,
    "chunk_length": 1286
  },
  {
    "chunk_id": 349,
    "paper_filename": "barsha_2023_comprehensive_review_on_breast_cancer_detection_classifcation_segmetnaiotn_using_deeplearning.pdf",
    "paper_title": "Barsha 2023 Comprehensive Review On Breast Cancer Detection Classifcation Segmetnaiotn Using Deeplearning",
    "chunk_index": 134,
    "total_chunks": 147,
    "text_content": "163. Mehra R et al (2018) Breast cancer histology images classifi - cation: training from scratch or transfer learning? ICT Express 4(4):247\u2013254 164. PUB MH, Bowyer K, Kopans D, Moore R, Kegelmeyer P (1996) The digital database for screening mammography. In: Proceed- ings of the third international workshop on digital mammogra- phy, Chicago, pp 9\u201312 165. Mordang J-J, Janssen T, Bria A, Kooi T, Gubern-M\u00e9rida A, Karssemeijer N (2016) Automatic microcalcification detec - tion in multi-vendor mammog",
    "full_text_length": 144240,
    "chunk_length": 1336
  },
  {
    "chunk_id": 350,
    "paper_filename": "barsha_2023_comprehensive_review_on_breast_cancer_detection_classifcation_segmetnaiotn_using_deeplearning.pdf",
    "paper_title": "Barsha 2023 Comprehensive Review On Breast Cancer Detection Classifcation Segmetnaiotn Using Deeplearning",
    "chunk_index": 135,
    "total_chunks": 147,
    "text_content": "tumour segmentation using modified automatic seeded region growing based on particle swarm optimization image clustering. In: Soft computing in industrial applications: proceedings of the 17th online world conference on soft computing in industrial applications. Springer, pp 49\u201360 169. Saha A, Harowicz MR, Grimm LJ, Kim CE, Ghate SV, Walsh R, Mazurowski MA (2018) A machine learning approach to radiogenomics of breast cancer: a study of 922 subjects and 529 dce-MRI features. Br J Cancer 119(4):50",
    "full_text_length": 144240,
    "chunk_length": 1306
  },
  {
    "chunk_id": 351,
    "paper_filename": "barsha_2023_comprehensive_review_on_breast_cancer_detection_classifcation_segmetnaiotn_using_deeplearning.pdf",
    "paper_title": "Barsha 2023 Comprehensive Review On Breast Cancer Detection Classifcation Segmetnaiotn Using Deeplearning",
    "chunk_index": 136,
    "total_chunks": 147,
    "text_content": "breast research with infrared image. J Med Imaging Health Inform 4(1):92\u2013100 173. Piotrzkowska-Wr\u00f3blewska H, Dobruch-Sobczak K, Byra M, Nowicki A (2017) Open access database of raw ultrasonic signals acquired from malignant and benign breast lesions. Med Phys 44(11):6105\u20136109 174. Yap MH, Pons G, Marti J, Ganau S, Sentis M, Zwiggelaar R, Davison AK, Marti R (2017) Automated breast ultrasound lesions detection using convolutional neural networks. IEEE J Biomed Health Inform 22(4):1218\u20131226 175. B",
    "full_text_length": 144240,
    "chunk_length": 1413
  },
  {
    "chunk_id": 352,
    "paper_filename": "barsha_2023_comprehensive_review_on_breast_cancer_detection_classifcation_segmetnaiotn_using_deeplearning.pdf",
    "paper_title": "Barsha 2023 Comprehensive Review On Breast Cancer Detection Classifcation Segmetnaiotn Using Deeplearning",
    "chunk_index": 137,
    "total_chunks": 147,
    "text_content": "and classification towards computer-aided diagnosis using digital mammography in early stages. Appl Sci 12(7):3273 178. Joseph AA, Abdullahi M, Junaidu SB, Ibrahim HH, Chiroma H (2022) Improved multi-classification of breast cancer histo- pathological images using handcrafted features and deep neural network (dense layer). Intell Syst Appl 14:200066 A Comprehensive Review on Breast Cancer Detection, Classification and Segmentation Using\u2026 1 3 179. Taheri S, Golrizkhatami Z (2022) Magnification-sp",
    "full_text_length": 144240,
    "chunk_length": 1398
  },
  {
    "chunk_id": 353,
    "paper_filename": "barsha_2023_comprehensive_review_on_breast_cancer_detection_classifcation_segmetnaiotn_using_deeplearning.pdf",
    "paper_title": "Barsha 2023 Comprehensive Review On Breast Cancer Detection Classifcation Segmetnaiotn Using Deeplearning",
    "chunk_index": 138,
    "total_chunks": 147,
    "text_content": "segmentation network from mammogram images. J King Saud Univ Comput Inf Sci 34(2):86\u201394 183. Huang Q, Chen Y, Liu L, Tao D, Li X (2019) On combining biclustering mining and adaboost for breast tumor classifica- tion. IEEE Trans Knowl Data Eng 32(4):728\u2013738 184. Hepsa\u011f PU, \u00d6zel SA, Yaz\u0131c\u0131 A (2017) Using deep learning for mammography classification. In: 2017 international confer - ence on computer science and engineering (UBMK). IEEE, pp 418\u2013423 185. Altameem A, Mahanty C, Poonia RC, Saudagar AKJ,",
    "full_text_length": 144240,
    "chunk_length": 1254
  },
  {
    "chunk_id": 354,
    "paper_filename": "barsha_2023_comprehensive_review_on_breast_cancer_detection_classifcation_segmetnaiotn_using_deeplearning.pdf",
    "paper_title": "Barsha 2023 Comprehensive Review On Breast Cancer Detection Classifcation Segmetnaiotn Using Deeplearning",
    "chunk_index": 139,
    "total_chunks": 147,
    "text_content": "M, Elmasry N, Adnan GT et al (2022) The impact of data processing and ensemble on breast cancer detection using deep learning. J Comput Com- mun 1(1):27\u201337 188. Shen L, Margolies LR, Rothstein JH, Fluder E, McBride R, Sieh W (2019) Deep learning to improve breast cancer detec- tion on screening mammography. Sci Rep 9(1):12495 189. P\u00e9rez-Benito FJ, Signol F, Perez-Cortes J-C, Fuster-Baggetto A, Pollan M, P\u00e9rez-G\u00f3mez B, Salas-Trejo D, Casals M, Mar - t\u00ednez I, LLobert R (2020) A deep learning syste",
    "full_text_length": 144240,
    "chunk_length": 1326
  },
  {
    "chunk_id": 355,
    "paper_filename": "barsha_2023_comprehensive_review_on_breast_cancer_detection_classifcation_segmetnaiotn_using_deeplearning.pdf",
    "paper_title": "Barsha 2023 Comprehensive Review On Breast Cancer Detection Classifcation Segmetnaiotn Using Deeplearning",
    "chunk_index": 140,
    "total_chunks": 147,
    "text_content": "K, Hekim M (2023) Detection and clas- sification of microcalcifications in mammograms images using difference filter and yolov4 deep learning model. Biomed Sig- nal Process Control 80:104360 193. Aslan MF (2023) A hybrid end-to-end learning approach for breast cancer diagnosis: convolutional recurrent network. Com- put Electr Eng 105:108562 194. Demir F (2021) Deepbreastnet: a novel and robust approach for automated breast cancer detection from histopathological images. Biocybern Biomed Eng 41(3",
    "full_text_length": 144240,
    "chunk_length": 1365
  },
  {
    "chunk_id": 356,
    "paper_filename": "barsha_2023_comprehensive_review_on_breast_cancer_detection_classifcation_segmetnaiotn_using_deeplearning.pdf",
    "paper_title": "Barsha 2023 Comprehensive Review On Breast Cancer Detection Classifcation Segmetnaiotn Using Deeplearning",
    "chunk_index": 141,
    "total_chunks": 147,
    "text_content": "lSTM neural network and attention mechanism. Ultrasonics 110:106271 198. Dewangan KK, Dewangan DK, Sahu SP, Janghel R (2022) Breast cancer diagnosis in an early stage using novel deep learning with hybrid optimization technique. Multimed Tools Appl 81(10):13935\u201313960 199. Saleh H, Alyami H, Alosaimi W et al (2022) Predicting breast cancer based on optimized deep learning approach. Comput Intell Neurosci 202:1820777 200. Roslidar R, Rahman A, Muharar R, Syahputra MR, Arnia F, Syukri M, Pradhan B,",
    "full_text_length": 144240,
    "chunk_length": 1308
  },
  {
    "chunk_id": 357,
    "paper_filename": "barsha_2023_comprehensive_review_on_breast_cancer_detection_classifcation_segmetnaiotn_using_deeplearning.pdf",
    "paper_title": "Barsha 2023 Comprehensive Review On Breast Cancer Detection Classifcation Segmetnaiotn Using Deeplearning",
    "chunk_index": 142,
    "total_chunks": 147,
    "text_content": "future direc- tion. Diagnostics 13(1):161 203. Nassif AB, Talib MA, Nasir Q, Afadar Y, Elgendy O (2022) Breast cancer detection using artificial intelligence techniques: a systematic literature review. Artif Intell Med 127:102276 204. ElOuassif B, Idri A, Hosni M, Abran A (2021) Classification techniques in breast cancer diagnosis: a systematic literature review. Comput Methods Biomech Biomed Eng 9(1):50\u201377 205. Yassin NI, Omran S, El Houby EM, Allam H (2018) Machine learning techniques for brea",
    "full_text_length": 144240,
    "chunk_length": 1387
  },
  {
    "chunk_id": 358,
    "paper_filename": "barsha_2023_comprehensive_review_on_breast_cancer_detection_classifcation_segmetnaiotn_using_deeplearning.pdf",
    "paper_title": "Barsha 2023 Comprehensive Review On Breast Cancer Detection Classifcation Segmetnaiotn Using Deeplearning",
    "chunk_index": 143,
    "total_chunks": 147,
    "text_content": "208. Parvin F, Hasan MAM (2020) A comparative study of differ - ent types of convolutional neural networks for breast cancer histopathological image classification. In: 2020 IEEE region 10 symposium (TENSYMP). IEEE, pp 945\u2013948 209. Castro-Tapia S, Casta\u00f1eda-Miranda CL, Olvera-Olvera CA, Guerrero-Osuna HA, Ortiz-Rodriguez JM, Mart\u00ednez-Blanco M, D\u00edaz-Florez G, Mendiola-Santiba\u00f1ez JD, Sol\u00eds-S\u00e1nchez LO et al (2021) Classification of breast cancer in mammograms with deep learning adding a fifth class",
    "full_text_length": 144240,
    "chunk_length": 1385
  },
  {
    "chunk_id": 359,
    "paper_filename": "barsha_2023_comprehensive_review_on_breast_cancer_detection_classifcation_segmetnaiotn_using_deeplearning.pdf",
    "paper_title": "Barsha 2023 Comprehensive Review On Breast Cancer Detection Classifcation Segmetnaiotn Using Deeplearning",
    "chunk_index": 144,
    "total_chunks": 147,
    "text_content": "J, Wang F, Duong TQ (2020) Prediction of pathological com- plete response to neoadjuvant chemotherapy in breast cancer using deep learning with integrative imaging, molecular and demographic data. In: Proceedings of medical image comput- ing and computer assisted intervention\u2013MICCAI 2020: 23rd international conference, Lima, Peru, October 4\u20138. Springer, Part II 23, pp 242\u2013252 B. Abhisheka et al. 1 3 213. Chen X, Men K, Chen B, Tang Y, Zhang T, Wang S, Li Y, Dai J (2020) CNN-based quality assuran",
    "full_text_length": 144240,
    "chunk_length": 1293
  },
  {
    "chunk_id": 360,
    "paper_filename": "barsha_2023_comprehensive_review_on_breast_cancer_detection_classifcation_segmetnaiotn_using_deeplearning.pdf",
    "paper_title": "Barsha 2023 Comprehensive Review On Breast Cancer Detection Classifcation Segmetnaiotn Using Deeplearning",
    "chunk_index": 145,
    "total_chunks": 147,
    "text_content": "Wadhwa G, Kaur A (2020) A deep cnn technique for detection of breast cancer using histopathology images. In: 2020 advanced computing and communication technologies for high perfor - mance applications (ACCTHPA). IEEE, pp 179\u2013185 217. Krithiga R, Geetha P (2020) Deep learning based breast can- cer detection and classification using fuzzy merging techniques. Mach Vis Appl 31:1\u201318 218. Salama WM, Elbagoury AM, Aly MH (2020) Novel breast can- cer classification framework based on deep learning. IET ",
    "full_text_length": 144240,
    "chunk_length": 1341
  },
  {
    "chunk_id": 361,
    "paper_filename": "barsha_2023_comprehensive_review_on_breast_cancer_detection_classifcation_segmetnaiotn_using_deeplearning.pdf",
    "paper_title": "Barsha 2023 Comprehensive Review On Breast Cancer Detection Classifcation Segmetnaiotn Using Deeplearning",
    "chunk_index": 146,
    "total_chunks": 147,
    "text_content": "for breast cancer histopathological image classification. Electronics 11(22):3767 222. Tong L, Mitchel J, Chatlin K, Wang MD (2020) Deep learning based feature-level integration of multi-omics data for breast cancer patients survival analysis. BMC Med Inform Decis Mak 20:1\u201312 223. Debelee TG, Schwenker F, Ibenthal A, Yohannes D (2020) Sur - vey of deep learning in breast cancer image analysis. Evol Syst 11:143\u2013163 Publisher's Note Springer Nature remains neutral with regard to jurisdictional cla",
    "full_text_length": 144240,
    "chunk_length": 909
  },
  {
    "chunk_id": 362,
    "paper_filename": "batuhan_2020_improving_clinical_outcome_predictions_convlutional_over_medical_entities_with_multimdal_learning.pdf",
    "paper_title": "Batuhan 2020 Improving Clinical Outcome Predictions Convlutional Over Medical Entities With Multimdal Learning",
    "chunk_index": 0,
    "total_chunks": 46,
    "text_content": "Improving Clinical Outcome Predictions Using Convolution over Medical Entities with Multimodal Learning Batuhan Bardak, Mehmet Tan* Department of Computer Engineering TOBB University of Economics and Technology Ankara, Turkey Abstract Early prediction of mortality and length of stay(LOS) of a patient is vital for saving a pa- tient's life and management of hospital resources. Availability of electronic health records(EHR) makes a huge impact on the healthcare domain and there has seen several wo",
    "full_text_length": 46612,
    "chunk_length": 1382
  },
  {
    "chunk_id": 363,
    "paper_filename": "batuhan_2020_improving_clinical_outcome_predictions_convlutional_over_medical_entities_with_multimdal_learning.pdf",
    "paper_title": "Batuhan 2020 Improving Clinical Outcome Predictions Convlutional Over Medical Entities With Multimdal Learning",
    "chunk_index": 1,
    "total_chunks": 46,
    "text_content": "ect of di erent embedding techniques such as Word2vec, FastText on medical entities. In the experiments, our proposed method robustly outperforms all other baseline models including di erent multimodal architectures for all clinical tasks. The code for the proposed method is available at https://github.com/tanlab/ConvolutionMedicalNer . Keywords: deep learning; healthcare; ehr; ner; multimodal 1 Introduction Electronic Health Record (EHR) data collected from patients who have been admitted into ",
    "full_text_length": 46612,
    "chunk_length": 1384
  },
  {
    "chunk_id": 364,
    "paper_filename": "batuhan_2020_improving_clinical_outcome_predictions_convlutional_over_medical_entities_with_multimdal_learning.pdf",
    "paper_title": "Batuhan 2020 Improving Clinical Outcome Predictions Convlutional Over Medical Entities With Multimdal Learning",
    "chunk_index": 2,
    "total_chunks": 46,
    "text_content": "data set and deep learning models to predict di erent clinical outcomes [2, 3, 4]. Understanding the health condition of the patient by observing the clinical measure- ments, laboratory test results and predicting the condition of patients during their ICU stay is a vital problem. In this paper, we focus on two di erent common risk prediction tasks, mor- tality (in-hospital & in-ICU) and length of ICU stay (LOS). Both are very important clinical outcomes for determining treatment methods, planni",
    "full_text_length": 46612,
    "chunk_length": 1283
  },
  {
    "chunk_id": 365,
    "paper_filename": "batuhan_2020_improving_clinical_outcome_predictions_convlutional_over_medical_entities_with_multimdal_learning.pdf",
    "paper_title": "Batuhan 2020 Improving Clinical Outcome Predictions Convlutional Over Medical Entities With Multimdal Learning",
    "chunk_index": 3,
    "total_chunks": 46,
    "text_content": "contains quite detailed information about patients, projecting the knowledge and inference of doctors and even critical details about patient health status for many cases. As per the importance of the clinical notes, researchers want to take advantage of the rich content in clinical notes. Moreover, the recent developments in Natural Language Processing (NLP), there has been increasing interest in using clinical notes to make clinical model predictions [10, 11]. Although it may be possible to le",
    "full_text_length": 46612,
    "chunk_length": 1294
  },
  {
    "chunk_id": 366,
    "paper_filename": "batuhan_2020_improving_clinical_outcome_predictions_convlutional_over_medical_entities_with_multimdal_learning.pdf",
    "paper_title": "Batuhan 2020 Improving Clinical Outcome Predictions Convlutional Over Medical Entities With Multimdal Learning",
    "chunk_index": 4,
    "total_chunks": 46,
    "text_content": "NLP that focuses on informa- tion extraction aiming to extract entities in a text and classify them into prede ned classes. These classes can be locations, people, or organizations in general NER algorithms [12, 13]. There can be various NER models for di erent domains like cybersecurity [14] or medicine [15]. Recently, several deep learning algorithms were applied to clinical texts to train clinical named entity recog- nition models. These clinical NER models generally try to extract medical in",
    "full_text_length": 46612,
    "chunk_length": 1298
  },
  {
    "chunk_id": 367,
    "paper_filename": "batuhan_2020_improving_clinical_outcome_predictions_convlutional_over_medical_entities_with_multimdal_learning.pdf",
    "paper_title": "Batuhan 2020 Improving Clinical Outcome Predictions Convlutional Over Medical Entities With Multimdal Learning",
    "chunk_index": 5,
    "total_chunks": 46,
    "text_content": "and medical entity features. Then we apply 2 # of Patient # of hospital admission # of ICU admission MIMIC-III ( >15 years old) 38,597 49,785 53,423 MIMIC-Extract 34,472 34,472 34,472 MIMIC-Extract (at least 24+6 (gap) hours patient) 23,937 23,937 23,937 Final Cohort (After clinical note elimination) 21,080 21,080 21,080 Table 1: Summary statistics of the original MIMIC-III dataset, and the nal cohort that is used in this study. multimodal approach and use these features together in several ways",
    "full_text_length": 46612,
    "chunk_length": 1267
  },
  {
    "chunk_id": 368,
    "paper_filename": "batuhan_2020_improving_clinical_outcome_predictions_convlutional_over_medical_entities_with_multimdal_learning.pdf",
    "paper_title": "Batuhan 2020 Improving Clinical Outcome Predictions Convlutional Over Medical Entities With Multimdal Learning",
    "chunk_index": 6,
    "total_chunks": 46,
    "text_content": "models used in this study. Finally, we report experimental results and conclude the paper by our ndings and conclusion. 2 Related Work With the rapid development of deep learning algorithms in the last decade, the number of deep learning models increased substantially for various clinical predictions. Several studies have explored EHRs to solve clinical problems, e.g., [18] used 13 di erent vital measurements to classify 128 diagnoses using Long Short Term Memory (LSTM) and DoctorAI [5] used Gat",
    "full_text_length": 46612,
    "chunk_length": 1295
  },
  {
    "chunk_id": 369,
    "paper_filename": "batuhan_2020_improving_clinical_outcome_predictions_convlutional_over_medical_entities_with_multimdal_learning.pdf",
    "paper_title": "Batuhan 2020 Improving Clinical Outcome Predictions Convlutional Over Medical Entities With Multimdal Learning",
    "chunk_index": 7,
    "total_chunks": 46,
    "text_content": "[24], and SOFA [25]. Nowadays with the progress on deep learning, di erent architectures have been applied on EHR data to predict this kind of problems. [26] used ensemble learning to make an early mortality prediction and [27] proposed a method to predict mortality using 12 features extracted from the vital signals in the rst hour of ICU admission. Darabi et al. [28] used Convolutional neural network to predict long-term mortality risk on the MIMIC-III dataset. More recent work[8] includes atte",
    "full_text_length": 46612,
    "chunk_length": 1293
  },
  {
    "chunk_id": 370,
    "paper_filename": "batuhan_2020_improving_clinical_outcome_predictions_convlutional_over_medical_entities_with_multimdal_learning.pdf",
    "paper_title": "Batuhan 2020 Improving Clinical Outcome Predictions Convlutional Over Medical Entities With Multimdal Learning",
    "chunk_index": 8,
    "total_chunks": 46,
    "text_content": "transforming raw structured data into usable hourly time series data. In order to solve this problem, [31, 32, 33] carried out a comprehensive benchmark on MIMIC-III for various tasks such as mortality, LOS, readmission, phenotyping and make their code publicly available. Purushotham et. al. [33] extracts 17 features from the MIMIC-III and works on hospital mortality, LOS and ICD-9 code group predictions. They compared their proposed super learner method with feedforward and recurrent neural net",
    "full_text_length": 46612,
    "chunk_length": 1372
  },
  {
    "chunk_id": 371,
    "paper_filename": "batuhan_2020_improving_clinical_outcome_predictions_convlutional_over_medical_entities_with_multimdal_learning.pdf",
    "paper_title": "Batuhan 2020 Improving Clinical Outcome Predictions Convlutional Over Medical Entities With Multimdal Learning",
    "chunk_index": 9,
    "total_chunks": 46,
    "text_content": "preprocessing steps such as unit conversion, outlier handling, imputing missing data. In this study, to increase reproducibility, we used MIMIC-Extract pipeline to featurize MIMIC-III data. We also use medical entities which are extracted from clinical notes to improve our model predictions. Clinical natural language processing and information extraction has been widely studied in recent years on clinical notes. [34, 35] proposed a deep learning based multi- task learning to make clinical predic",
    "full_text_length": 46612,
    "chunk_length": 1330
  },
  {
    "chunk_id": 372,
    "paper_filename": "batuhan_2020_improving_clinical_outcome_predictions_convlutional_over_medical_entities_with_multimdal_learning.pdf",
    "paper_title": "Batuhan 2020 Improving Clinical Outcome Predictions Convlutional Over Medical Entities With Multimdal Learning",
    "chunk_index": 10,
    "total_chunks": 46,
    "text_content": "they take these clinical notes which makes hard to process directly. There are a number of studies in the eld of clinical NLP which try to extract medical entities in clinical notes [40, 41, 42]. In this work, we use med7 [15] which is developed for free-text electronic health record. Then, we combine these medical entities with structured data to bene t from multimodal approach. For a detailed overview on deep learning for natural language processing in the clinical domain, readers can refer to",
    "full_text_length": 46612,
    "chunk_length": 1274
  },
  {
    "chunk_id": 373,
    "paper_filename": "batuhan_2020_improving_clinical_outcome_predictions_convlutional_over_medical_entities_with_multimdal_learning.pdf",
    "paper_title": "Batuhan 2020 Improving Clinical Outcome Predictions Convlutional Over Medical Entities With Multimdal Learning",
    "chunk_index": 11,
    "total_chunks": 46,
    "text_content": "to explore how physiological time series data and clinical notes can be integrated. The study by Jin. et al[50] is the closest to our work in terms of motivation. They made hospital mortality prediction by combining clinical notes and time series data. Clinical notes are represented with Doc2VecC [51] algorithm in two di erent ways. First, they directly combine clinical notes with time series data, second, they use neural network based clinical NER service to extract ve types of medical entities",
    "full_text_length": 46612,
    "chunk_length": 1246
  },
  {
    "chunk_id": 374,
    "paper_filename": "batuhan_2020_improving_clinical_outcome_predictions_convlutional_over_medical_entities_with_multimdal_learning.pdf",
    "paper_title": "Batuhan 2020 Improving Clinical Outcome Predictions Convlutional Over Medical Entities With Multimdal Learning",
    "chunk_index": 12,
    "total_chunks": 46,
    "text_content": "rather than just in-hospital mortality. \u2022We compare di erent types of word embedding methods (Word2Vec, FastText, Concatena- tion), and discuss the e ect these methods on medical entities. \u2022We propose a convolutional based deep learning model for combining clinical NER features with time series ICU features. We compare our proposed model with several benchmarks. 3 Materials and Methods In this section, we begin by describing our dataset. The details of baselines and clinical NER model are explai",
    "full_text_length": 46612,
    "chunk_length": 1279
  },
  {
    "chunk_id": 375,
    "paper_filename": "batuhan_2020_improving_clinical_outcome_predictions_convlutional_over_medical_entities_with_multimdal_learning.pdf",
    "paper_title": "Batuhan 2020 Improving Clinical Outcome Predictions Convlutional Over Medical Entities With Multimdal Learning",
    "chunk_index": 13,
    "total_chunks": 46,
    "text_content": "patient's rst ICU visit with some patient inclusion criteria. They eliminate data from patients younger than 15 years old and where the LOS are not between 12 hours and 10 days. This pipeline produces a cohort of 34,472 patients and 104 clinically 5 aggregated time-series variables. In all of our experiments, we use the rst 24 hours of patient's data after ICU admission and only consider the patients with at least 30 hours of present data like MIMIC-Extract. In our multimodal approach we combine",
    "full_text_length": 46612,
    "chunk_length": 1199
  },
  {
    "chunk_id": 376,
    "paper_filename": "batuhan_2020_improving_clinical_outcome_predictions_convlutional_over_medical_entities_with_multimdal_learning.pdf",
    "paper_title": "Batuhan 2020 Improving Clinical Outcome Predictions Convlutional Over Medical Entities With Multimdal Learning",
    "chunk_index": 14,
    "total_chunks": 46,
    "text_content": "split, for all clinical tasks, we split the data based on class distribution with 70%/10%/20% ratio. Statistics of the nal cohort and the others are summarized in Table 1. Problem De nition. We mainly focus on two vital clinical prediction tasks, mortality(in- hospital & in-ICU) and LOS( >3 &>7) at ICU. We use the same de nitions of the benchmark tasks de ned by MIMIC-Extract as the following four binary classi cation tasks. The explanation of these tasks and the class distributions are as follo",
    "full_text_length": 46612,
    "chunk_length": 1248
  },
  {
    "chunk_id": 377,
    "paper_filename": "batuhan_2020_improving_clinical_outcome_predictions_convlutional_over_medical_entities_with_multimdal_learning.pdf",
    "paper_title": "Batuhan 2020 Improving Clinical Outcome Predictions Convlutional Over Medical Entities With Multimdal Learning",
    "chunk_index": 15,
    "total_chunks": 46,
    "text_content": "Baseline Models In this subsection, we discuss our time-series baseline modal that we evaluate on each of our four benchmark tasks. Further, we explain clinical NER model, embedding approaches to represent medical entities and the multimodal baselines used in this study . 3.2.1 Time Series Model We employ both Long Short Term Memory (LSTM) [52] and Gated Recurrent Units (GRU) [53] networks to capture the temporal information between the patient features. As a result of time-series baseline exper",
    "full_text_length": 46612,
    "chunk_length": 1246
  },
  {
    "chunk_id": 378,
    "paper_filename": "batuhan_2020_improving_clinical_outcome_predictions_convlutional_over_medical_entities_with_multimdal_learning.pdf",
    "paper_title": "Batuhan 2020 Improving Clinical Outcome Predictions Convlutional Over Medical Entities With Multimdal Learning",
    "chunk_index": 16,
    "total_chunks": 46,
    "text_content": "number of unique entity number. The last column shows the output of med7 for example sentence given from clinical notes. LSTM up to %0 :5 - %1, while using a simpler architecture. Therefore, we use GRU for all of the multimodal architectures. In general, GRU cell has two gates, a reset gate rand an update gate z. With these gates, GRU can handle the vanishing gradient problem. We can iterate the mathematical formulation of GRU modal as follows: zt=\u001b(Wzxt+Uzht\u00001+bz) rt=\u001b(Wrxt+Urht\u00001+br) ^ht= tanh",
    "full_text_length": 46612,
    "chunk_length": 1333
  },
  {
    "chunk_id": 379,
    "paper_filename": "batuhan_2020_improving_clinical_outcome_predictions_convlutional_over_medical_entities_with_multimdal_learning.pdf",
    "paper_title": "Batuhan 2020 Improving Clinical Outcome Predictions Convlutional Over Medical Entities With Multimdal Learning",
    "chunk_index": 17,
    "total_chunks": 46,
    "text_content": "embedded into continuous word vectors. Then, we take the mean of these learned entity representations. (2) The words are removed from clinical notes if they are not belong to any medical entity category. Then, we train Doc2Vec on the preprocessed clinical notes to learn low dimensional representation of medical entities. whereztandrtrespectively represent the update gate and the reset gate, ^htthe candidate activation unit, htthe current activation, and \u000erepresents element-wise multiplication. F",
    "full_text_length": 46612,
    "chunk_length": 1336
  },
  {
    "chunk_id": 380,
    "paper_filename": "batuhan_2020_improving_clinical_outcome_predictions_convlutional_over_medical_entities_with_multimdal_learning.pdf",
    "paper_title": "Batuhan 2020 Improving Clinical Outcome Predictions Convlutional Over Medical Entities With Multimdal Learning",
    "chunk_index": 18,
    "total_chunks": 46,
    "text_content": "[54, 55, 15]. We use a pre-trained clinical NER model, med7 [15], which uses the same dataset that we use in our experiments, MIMIC-III. This clinical NER model extracts seven di erent named entities such as 'Drug', 'Strength', 'Duration', 'Route', 'Form', 'Dosage', 'Frequency'. To represent the patient's medical entities we try two di erent embedding methods, word embedding and document em- bedding. First, we use three di erent word embedding algorithms to represent the each clinical NER model ",
    "full_text_length": 46612,
    "chunk_length": 1295
  },
  {
    "chunk_id": 381,
    "paper_filename": "batuhan_2020_improving_clinical_outcome_predictions_convlutional_over_medical_entities_with_multimdal_learning.pdf",
    "paper_title": "Batuhan 2020 Improving Clinical Outcome Predictions Convlutional Over Medical Entities With Multimdal Learning",
    "chunk_index": 19,
    "total_chunks": 46,
    "text_content": "Conv 1DGlobal Max PoolFigure 2: Overview of Proposed multimodel architecture for predicting the In-Hospital Mortality, In-ICU Mortality, LOS >3, and LOS >7. To extract timeseries features, we use MIMIC- EXTRACT pipeline and fed these features through GRU. We also preprocess the clinical notes and use med7 to extract medical entities. 1D CNN is applied to extract features from medical entity representations. In the nal layer, we concatenate features that extracted from timeseries and medical enti",
    "full_text_length": 46612,
    "chunk_length": 1283
  },
  {
    "chunk_id": 382,
    "paper_filename": "batuhan_2020_improving_clinical_outcome_predictions_convlutional_over_medical_entities_with_multimdal_learning.pdf",
    "paper_title": "Batuhan 2020 Improving Clinical Outcome Predictions Convlutional Over Medical Entities With Multimdal Learning",
    "chunk_index": 20,
    "total_chunks": 46,
    "text_content": "concatenation of Word2Vec & FastText embeddings. Word2Vec [16] is a two-layer neural network that learns the representations of words in the given text with two ways: as a continuous bag-of-words (CBOW) and as a skip-gram. FastText [17] is an extension of the skip-gram model implemented by Facebook's AI Research (FAIR) lab which can handle out-of-vocabulary (OOV) words, and can learn better representations for rare words using several n-grams for words. We use pre-trained word2vec ( wi2R100) and",
    "full_text_length": 46612,
    "chunk_length": 1309
  },
  {
    "chunk_id": 383,
    "paper_filename": "batuhan_2020_improving_clinical_outcome_predictions_convlutional_over_medical_entities_with_multimdal_learning.pdf",
    "paper_title": "Batuhan 2020 Improving Clinical Outcome Predictions Convlutional Over Medical Entities With Multimdal Learning",
    "chunk_index": 21,
    "total_chunks": 46,
    "text_content": "learning document level representations, we combine the rst 24 hours of patient's clinical notes and apply clinical NER algorithm to keep only medical related keywords in the clinical notes. When training Doc2Vec, we use context window size of 5 words. This algorithm produces the xed-length feature vector ( di2R100) for each patient. We present two di erent baseline multimodal approaches with word and document embeddings that combine time-series data and medical entities. Multimodal with Average",
    "full_text_length": 46612,
    "chunk_length": 1299
  },
  {
    "chunk_id": 384,
    "paper_filename": "batuhan_2020_improving_clinical_outcome_predictions_convlutional_over_medical_entities_with_multimdal_learning.pdf",
    "paper_title": "Batuhan 2020 Improving Clinical Outcome Predictions Convlutional Over Medical Entities With Multimdal Learning",
    "chunk_index": 22,
    "total_chunks": 46,
    "text_content": "through one layer GRU layer with 256 hidden units as explained in Section 3.2.1. Averaged representations of medical entities are combined with time-series feature maps that are learned via GRU. In the end, these merged feature representations are fed into fully connected layer with 256 neurons, and a sigmoid classi er is added to the model. Multimodal with Doc2Vec Representation. In this multimodal approach, instead of av- eraging medical entities, we apply Doc2Vec algorithm to obtain the xed-l",
    "full_text_length": 46612,
    "chunk_length": 1306
  },
  {
    "chunk_id": 385,
    "paper_filename": "batuhan_2020_improving_clinical_outcome_predictions_convlutional_over_medical_entities_with_multimdal_learning.pdf",
    "paper_title": "Batuhan 2020 Improving Clinical Outcome Predictions Convlutional Over Medical Entities With Multimdal Learning",
    "chunk_index": 23,
    "total_chunks": 46,
    "text_content": "convo- lutional layers as a feature extractor on medical entities. Applying 1D Convolutional Neural Networks(CNN) on text learns the combination of adjacent words and shows successful results for various NLP problems [57]. In our model, Kmedical entities were extracted from Nclinical notes from each patient. These Kmedical entities are rst represented as a sequence of word embeddings with di erent word representation techniques such as Word2vec, FastText, and a combination of them. These entitie",
    "full_text_length": 46612,
    "chunk_length": 1284
  },
  {
    "chunk_id": 386,
    "paper_filename": "batuhan_2020_improving_clinical_outcome_predictions_convlutional_over_medical_entities_with_multimdal_learning.pdf",
    "paper_title": "Batuhan 2020 Improving Clinical Outcome Predictions Convlutional Over Medical Entities With Multimdal Learning",
    "chunk_index": 24,
    "total_chunks": 46,
    "text_content": "layers with lter size 32, 64, and 96. The kernel size is same for three convolutional layer. The output of the last convolutional layer is followed by the max-pooling layer. The nal features of the max-pooling layers are concatenated with the features from one layer GRU with 256 hidden units and fed through one fully-connected layer with 512 hidden units. 4 Experimental Results In this section, we report the results of our baseline and multimodel experiments, the metrics we used for the evaluati",
    "full_text_length": 46612,
    "chunk_length": 1252
  },
  {
    "chunk_id": 387,
    "paper_filename": "batuhan_2020_improving_clinical_outcome_predictions_convlutional_over_medical_entities_with_multimdal_learning.pdf",
    "paper_title": "Batuhan 2020 Improving Clinical Outcome Predictions Convlutional Over Medical Entities With Multimdal Learning",
    "chunk_index": 25,
    "total_chunks": 46,
    "text_content": "learning rate of 0.001. All models are trained to minimize the binary crossentropy loss and we independently tune the hyperparameters - number of hidden layers, hidden units, convolutional lters, lter-size, learning rate, dropout rates and regularization parameters on the validation set. Each model is trained for 50 epochs and early stopping is used on the validation loss. We train each model 10 times with di erent initialization seed and report the average performance. Evaluation metrics. The c",
    "full_text_length": 46612,
    "chunk_length": 1291
  },
  {
    "chunk_id": 388,
    "paper_filename": "batuhan_2020_improving_clinical_outcome_predictions_convlutional_over_medical_entities_with_multimdal_learning.pdf",
    "paper_title": "Batuhan 2020 Improving Clinical Outcome Predictions Convlutional Over Medical Entities With Multimdal Learning",
    "chunk_index": 26,
    "total_chunks": 46,
    "text_content": "metric which calculates the harmonic mean of precision and recall. Implementation Details. The aforementioned deep learning algorithms are implemented using Keras [61], which runs Tensor ow [62] on its backend. med7 is used for extracting clinical related 11 Task Baseline Modal Embedding AUROC AUPRC F1 In-Hospital MortalityGRU - 85.04 \u00060.004 52.15\u00060.009 42.29\u00060.016 Doc2Vec Multimodal Doc2Vec 85.96 \u00060.002 54.17\u00060.004 46.60\u00060.016 Word2Vec 86.42\u00060.004 54.22\u00060.008 45.42\u00060.013 Averaged Multimodal Fas",
    "full_text_length": 46612,
    "chunk_length": 1632
  },
  {
    "chunk_id": 389,
    "paper_filename": "batuhan_2020_improving_clinical_outcome_predictions_convlutional_over_medical_entities_with_multimdal_learning.pdf",
    "paper_title": "Batuhan 2020 Improving Clinical Outcome Predictions Convlutional Over Medical Entities With Multimdal Learning",
    "chunk_index": 27,
    "total_chunks": 46,
    "text_content": "\u00060.005 17.91\u00060.006 1.35\u00060.008 Averaged Multimodal FastText 71.31 \u00060.008 17.57\u00060.007 1.02\u00060.008 Concat 71.59 \u00060.007 17.67\u00060.007 1.37\u00060.013 Table 3: Performance comparison of baseline methods. For all four clinical tasks, we report both AUC, AUPRC and F1 scores and the standard deviations. entities from clinical notes. All experiments experiments were performed on a computer with NVIDIA Tesla K80 GPU with 24GB of VRAM, 378 GB of ram and Intel Xeon E5 2683 processor. The full code of this work is a",
    "full_text_length": 46612,
    "chunk_length": 1570
  },
  {
    "chunk_id": 390,
    "paper_filename": "batuhan_2020_improving_clinical_outcome_predictions_convlutional_over_medical_entities_with_multimdal_learning.pdf",
    "paper_title": "Batuhan 2020 Improving Clinical Outcome Predictions Convlutional Over Medical Entities With Multimdal Learning",
    "chunk_index": 28,
    "total_chunks": 46,
    "text_content": "Model FastText 69.61 \u00060.003 62.55\u00060.003 55.87\u00060.017 Concat 69.93\u00060.001 62.77\u00060.002 55.82\u00060.008 LOS>7 DaysBest Baseline - 71.63 \u00060.005 17.91\u00060.006 2.33\u00060.012 Word2Vec 72.55\u00060.005 18.78\u00060.006 1.58\u00060.001 Proposed Model FastText 71.81 \u00060.004 18.01\u00060.004 1.08\u00060.008 Concat 71.92 \u00060.007 18.25\u00060.006 1.38\u00060.009 Table 4: Proposed model performance comparison with best baseline model. We select the highest score for each metric and each clinical task from baseline methods. 4.2 Results 4.2.1 Baseline Modal ",
    "full_text_length": 46612,
    "chunk_length": 1369
  },
  {
    "chunk_id": 391,
    "paper_filename": "batuhan_2020_improving_clinical_outcome_predictions_convlutional_over_medical_entities_with_multimdal_learning.pdf",
    "paper_title": "Batuhan 2020 Improving Clinical Outcome Predictions Convlutional Over Medical Entities With Multimdal Learning",
    "chunk_index": 29,
    "total_chunks": 46,
    "text_content": "and AUPRC and %7 for F1 score. Multimodal approach also improves 13 the performance of predictions tasks in LOS problem. Both in LOS >3 and LOS >7, all metrics are improved around %1.5. For all experiments, time-series GRU modal only get better F1 score for LOS>7 problem. 4.2.2 Proposed Modal Results In this section, we compare the result of our proposed model against the best scores taken from baseline models. All results for the proposed model against best baseline scores are provided in Table",
    "full_text_length": 46612,
    "chunk_length": 1259
  },
  {
    "chunk_id": 392,
    "paper_filename": "batuhan_2020_improving_clinical_outcome_predictions_convlutional_over_medical_entities_with_multimdal_learning.pdf",
    "paper_title": "Batuhan 2020 Improving Clinical Outcome Predictions Convlutional Over Medical Entities With Multimdal Learning",
    "chunk_index": 30,
    "total_chunks": 46,
    "text_content": "features improve the prediction performance on all clinical tasks. As shown in Table 3, multimodal baseline modals increase all metrics performance which indicates the bene t of using medical entities for predicting mortality and LOS. These experiments also provide an opportunity to compare the medical entity representation methods. Although there is no certain winner for all tasks, in the baseline models, the results show us for mortality prediction tasks, representing the medical entities with",
    "full_text_length": 46612,
    "chunk_length": 1353
  },
  {
    "chunk_id": 393,
    "paper_filename": "batuhan_2020_improving_clinical_outcome_predictions_convlutional_over_medical_entities_with_multimdal_learning.pdf",
    "paper_title": "Batuhan 2020 Improving Clinical Outcome Predictions Convlutional Over Medical Entities With Multimdal Learning",
    "chunk_index": 31,
    "total_chunks": 46,
    "text_content": "experiments and comparisons, our main motivation is nding an e\u000ecient way to combine time-series features with medical entities. Even though both baseline multimodals improve the prediction results compared to timeseries baseline, to make better fea- ture extraction on medical entities, we want to take the advantage of 1D CNN. In the literature, there have been several studies that use 1D CNN in NLP. We stack three 1D convolution oper- ation to extract the features, and then apply 1D max pooling ",
    "full_text_length": 46612,
    "chunk_length": 1278
  },
  {
    "chunk_id": 394,
    "paper_filename": "batuhan_2020_improving_clinical_outcome_predictions_convlutional_over_medical_entities_with_multimdal_learning.pdf",
    "paper_title": "Batuhan 2020 Improving Clinical Outcome Predictions Convlutional Over Medical Entities With Multimdal Learning",
    "chunk_index": 32,
    "total_chunks": 46,
    "text_content": "all tasks) compared to the timeseries and multimodal, so we do not report these results. 6 Conclusion Over the past decade, there has been increased attention to improve mortality and LOS predic- tion performance. Predicting any complications and saving patient's life is an important task for healthcare system which motivates us to work on mortality prediction. LOS is another important clinical problem to improve hospital performance and better healthcare resource utilisation. In this work, we p",
    "full_text_length": 46612,
    "chunk_length": 1329
  },
  {
    "chunk_id": 395,
    "paper_filename": "batuhan_2020_improving_clinical_outcome_predictions_convlutional_over_medical_entities_with_multimdal_learning.pdf",
    "paper_title": "Batuhan 2020 Improving Clinical Outcome Predictions Convlutional Over Medical Entities With Multimdal Learning",
    "chunk_index": 33,
    "total_chunks": 46,
    "text_content": "be extended in multiple directions. First, we can involve more features associated with patient such as prescription data and diagnosis codes to improve the prediction performance. Second, using di erent word embedding especially transformer based techniques can be used for learning the entity representations. Another thing we may consider in the future is to use more advanced deep learning architectures with attention based will be useful for clinical tasks. References [1] Alistair EW Johnson, ",
    "full_text_length": 46612,
    "chunk_length": 1377
  },
  {
    "chunk_id": 396,
    "paper_filename": "batuhan_2020_improving_clinical_outcome_predictions_convlutional_over_medical_entities_with_multimdal_learning.pdf",
    "paper_title": "Batuhan 2020 Improving Clinical Outcome Predictions Convlutional Over Medical Entities With Multimdal Learning",
    "chunk_index": 34,
    "total_chunks": 46,
    "text_content": "Tristan Naumann, Nathan Hunt, Harini Suresh, Pe- ter Szolovits, and Marzyeh Ghassemi. Semi-supervised biomedical translation with cycle 15 wasserstein regression gans. In Thirty-Second AAAI Conference on Arti cial Intelligence , 2018. [4] Christopher Barton, Uli Chettipally, Yifan Zhou, Zirui Jiang, Anna Lynn-Palevsky, Sidney Le, Jacob Calvert, and Ritankar Das. Evaluation of a machine learning algorithm for up to 48-hour advance prediction of sepsis using six vital signs. Computers in biology a",
    "full_text_length": 46612,
    "chunk_length": 1399
  },
  {
    "chunk_id": 397,
    "paper_filename": "batuhan_2020_improving_clinical_outcome_predictions_convlutional_over_medical_entities_with_multimdal_learning.pdf",
    "paper_title": "Batuhan 2020 Improving Clinical Outcome Predictions Convlutional Over Medical Entities With Multimdal Learning",
    "chunk_index": 35,
    "total_chunks": 46,
    "text_content": "Barajas and Ram Akella. Dynamically modeling patient's health state from electronic medical records: A time series approach. In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , pages 69{78, 2015. [8] Huan Song, Deepta Rajan, Jayaraman J Thiagarajan, and Andreas Spanias. Attend and diagnose: Clinical time series analysis using attention models. In Thirty-second AAAI con- ference on arti cial intelligence , 2018. [9] Harini Suresh, Jen J Gong, an",
    "full_text_length": 46612,
    "chunk_length": 1356
  },
  {
    "chunk_id": 398,
    "paper_filename": "batuhan_2020_improving_clinical_outcome_predictions_convlutional_over_medical_entities_with_multimdal_learning.pdf",
    "paper_title": "Batuhan 2020 Improving Clinical Outcome Predictions Convlutional Over Medical Entities With Multimdal Learning",
    "chunk_index": 36,
    "total_chunks": 46,
    "text_content": "packing predictive value in clinical note representations. AMIA Summits on Translational Science Proceedings , 2018:26, 2018. [12] Christopher D Manning, Mihai Surdeanu, John Bauer, Jenny Rose Finkel, Steven Bethard, and David McClosky. The stanford corenlp natural language processing toolkit. In Pro- 16 ceedings of 52nd annual meeting of the association for computational linguistics: system demonstrations , pages 55{60, 2014. [13] Matthew Honnibal and Mark Johnson. An improved non-monotonic tra",
    "full_text_length": 46612,
    "chunk_length": 1446
  },
  {
    "chunk_id": 399,
    "paper_filename": "batuhan_2020_improving_clinical_outcome_predictions_convlutional_over_medical_entities_with_multimdal_learning.pdf",
    "paper_title": "Batuhan 2020 Improving Clinical Outcome Predictions Convlutional Over Medical Entities With Multimdal Learning",
    "chunk_index": 37,
    "total_chunks": 46,
    "text_content": "Chen, Greg Corrado, and Je rey Dean. E\u000ecient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781 , 2013. [17] Armand Joulin, Edouard Grave, Piotr Bojanowski, and Tomas Mikolov. Bag of tricks for e\u000ecient text classi cation. arXiv preprint arXiv:1607.01759 , 2016. [18] Zachary C Lipton, David C Kale, Charles Elkan, and Randall Wetzel. Learning to diagnose with lstm recurrent neural networks. arXiv preprint arXiv:1511.03677 , 2015. [19] Edward Choi, Andy Schuetz, Walt",
    "full_text_length": 46612,
    "chunk_length": 1367
  },
  {
    "chunk_id": 400,
    "paper_filename": "batuhan_2020_improving_clinical_outcome_predictions_convlutional_over_medical_entities_with_multimdal_learning.pdf",
    "paper_title": "Batuhan 2020 Improving Clinical Outcome Predictions Convlutional Over Medical Entities With Multimdal Learning",
    "chunk_index": 38,
    "total_chunks": 46,
    "text_content": "using arti cial neural network synthesised by genetic algorithm. The Lancet , 347(9009):1146{1150, 1996. [22] Leo Anthony Celi, Sean Galvin, Guido Davidzon, Joon Lee, Daniel Scott, and Roger Mark. A database-driven decision support system: customized mortality prediction. Journal of personalized medicine , 2(4):138{148, 2012. 17 [23] William A Knaus, Jack E Zimmerman, Douglas P Wagner, Elizabeth A Draper, and Diane E Lawrence. Apache-acute physiology and chronic health evaluation: a physiologica",
    "full_text_length": 46612,
    "chunk_length": 1430
  },
  {
    "chunk_id": 401,
    "paper_filename": "batuhan_2020_improving_clinical_outcome_predictions_convlutional_over_medical_entities_with_multimdal_learning.pdf",
    "paper_title": "Batuhan 2020 Improving Clinical Outcome Predictions Convlutional Over Medical Entities With Multimdal Learning",
    "chunk_index": 39,
    "total_chunks": 46,
    "text_content": "James McNicholas, and Jim Briggs. Early hospital mortality prediction of intensive care unit patients using an ensemble learning approach. International journal of medical informatics , 108:185{195, 2017. [27] Reza Sadeghi, Tanvi Banerjee, and William Romine. Early hospital mortality prediction using vital signals. Smart Health , 9:265{274, 2018. [28] Hamid R Darabi, Daniel Tsinis, Kevin Zecchini, Winthrop F Whitcomb, and Alexander Liss. Forecasting mortality risk for patients admitted to intens",
    "full_text_length": 46612,
    "chunk_length": 1455
  },
  {
    "chunk_id": 402,
    "paper_filename": "batuhan_2020_improving_clinical_outcome_predictions_convlutional_over_medical_entities_with_multimdal_learning.pdf",
    "paper_title": "Batuhan 2020 Improving Clinical Outcome Predictions Convlutional Over Medical Entities With Multimdal Learning",
    "chunk_index": 40,
    "total_chunks": 46,
    "text_content": "Ver Steeg, and Aram Galstyan. Multitask learning and benchmarking with clinical time series data. Scienti c data , 6(1):1{ 18, 2019. [32] Shirly Wang, Matthew BA McDermott, Geeticka Chauhan, Marzyeh Ghassemi, Michael C Hughes, and Tristan Naumann. Mimic-extract: A data extraction, preprocessing, and repre- sentation pipeline for mimic-iii. In Proceedings of the ACM Conference on Health, Inference, and Learning , pages 222{235, 2020. 18 [33] Sanjay Purushotham, Chuizheng Meng, Zhengping Che, and ",
    "full_text_length": 46612,
    "chunk_length": 1441
  },
  {
    "chunk_id": 403,
    "paper_filename": "batuhan_2020_improving_clinical_outcome_predictions_convlutional_over_medical_entities_with_multimdal_learning.pdf",
    "paper_title": "Batuhan 2020 Improving Clinical Outcome Predictions Convlutional Over Medical Entities With Multimdal Learning",
    "chunk_index": 41,
    "total_chunks": 46,
    "text_content": "of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 , 2018. [37] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V Le. Xlnet: Generalized autoregressive pretraining for language understanding. In Advances in neural information processing systems , pages 5754{5764, 2019. [38] Kexin Huang, Jaan Altosaar, and Rajesh Ranganath. Clinicalbert: Modeling clinical notes and predicting hospital readmission. arXiv preprint arX",
    "full_text_length": 46612,
    "chunk_length": 1467
  },
  {
    "chunk_id": 404,
    "paper_filename": "batuhan_2020_improving_clinical_outcome_predictions_convlutional_over_medical_entities_with_multimdal_learning.pdf",
    "paper_title": "Batuhan 2020 Improving Clinical Outcome Predictions Convlutional Over Medical Entities With Multimdal Learning",
    "chunk_index": 42,
    "total_chunks": 46,
    "text_content": "Bruijn, Muqun Li, Astha LaPlante, and Khal- doun Zine El Abidine. Extracting umls concepts from medical text using general and domain- speci c deep learning models. arXiv preprint arXiv:1910.01274 , 2019. [43] Stephen Wu, Kirk Roberts, Surabhi Datta, Jingcheng Du, Zongcheng Ji, Yuqi Si, Sarvesh Soni, Qiong Wang, Qiang Wei, Yang Xiang, et al. Deep learning in clinical natural language processing: a methodical review. Journal of the American Medical Informatics Association , 27(3):457{470, 2020. 1",
    "full_text_length": 46612,
    "chunk_length": 1365
  },
  {
    "chunk_id": 405,
    "paper_filename": "batuhan_2020_improving_clinical_outcome_predictions_convlutional_over_medical_entities_with_multimdal_learning.pdf",
    "paper_title": "Batuhan 2020 Improving Clinical Outcome Predictions Convlutional Over Medical Entities With Multimdal Learning",
    "chunk_index": 43,
    "total_chunks": 46,
    "text_content": "Youssef Mroueh, Etienne Marcheret, and Vaibhava Goel. Deep multimodal learning for audio-visual speech recognition. In 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) , pages 2130{2134. IEEE, 2015. [48] Swaraj Khadanga, Karan Aggarwal, Sha q Joty, and Jaideep Srivastava. Using clinical notes with time series data for icu management. arXiv preprint arXiv:1909.09702 , 2019. [49] Satya Narayan Shukla and Benjamin M Marlin. Integrating physiological time series",
    "full_text_length": 46612,
    "chunk_length": 1478
  },
  {
    "chunk_id": 406,
    "paper_filename": "batuhan_2020_improving_clinical_outcome_predictions_convlutional_over_medical_entities_with_multimdal_learning.pdf",
    "paper_title": "Batuhan 2020 Improving Clinical Outcome Predictions Convlutional Over Medical Entities With Multimdal Learning",
    "chunk_index": 44,
    "total_chunks": 46,
    "text_content": "memory. Neural computation , 9(8):1735{1780, 1997. [53] Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. Empirical evaluation of gated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555 , 2014. [54] Mark Neumann, Daniel King, Iz Beltagy, and Waleed Ammar. Scispacy: Fast and robust models for biomedical natural language processing. arXiv preprint arXiv:1902.07669 , 2019. 20 [55] Andriy Mulyar, Darshini Mahendran, Luke Ma ey, Amy Olex, Grant Matteo, Ne",
    "full_text_length": 46612,
    "chunk_length": 1458
  },
  {
    "chunk_id": 407,
    "paper_filename": "batuhan_2020_improving_clinical_outcome_predictions_convlutional_over_medical_entities_with_multimdal_learning.pdf",
    "paper_title": "Batuhan 2020 Improving Clinical Outcome Predictions Convlutional Over Medical Entities With Multimdal Learning",
    "chunk_index": 45,
    "total_chunks": 46,
    "text_content": "Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980 , 2014. [60] Jesse Davis and Mark Goadrich. The relationship between precision-recall and roc curves. In Proceedings of the 23rd international conference on Machine learning , pages 233{240, 2006. [61] Fran\u0018 cois Chollet. keras. https://github.com/fchollet/keras , 2015. [62] Mart\u0013 \u0010n Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S. Corrado, Andy Davis, Je rey Dean, Matthieu D",
    "full_text_length": 46612,
    "chunk_length": 1084
  },
  {
    "chunk_id": 408,
    "paper_filename": "Christine_2016_mammographic_breast_density_assessment_using_BIRADS_catagorization.pdf",
    "paper_title": "Christine 2016 Mammographic Breast Density Assessment Using Birads Catagorization",
    "chunk_index": 0,
    "total_chunks": 38,
    "text_content": "Original Investigation Mammographic Breast Density Assessment Using Automated Volumetric Software and Breast Imaging Reporting and Data System (BIRADS) Categorization by Expert Radiologists Christine N. Damases , MTech Radiography, Patrick C. Brennan , PhD,Claudia Mello-Thoms , PhD, Mark F. McEntee , BSc (Hons) Radiography, PhD Rationale and Objectives: To investigate agreement on mammographic breast density (MD) assessment between automated volu- metric software and Breast Imaging Reporting and",
    "full_text_length": 36199,
    "chunk_length": 1437
  },
  {
    "chunk_id": 409,
    "paper_filename": "Christine_2016_mammographic_breast_density_assessment_using_BIRADS_catagorization.pdf",
    "paper_title": "Christine 2016 Mammographic Breast Density Assessment Using Birads Catagorization",
    "chunk_index": 1,
    "total_chunks": 38,
    "text_content": "also demonstrated moderate agreement ( \u03ba=0.565; 95% CI =0.519\u2013 0.610) ranging from 0.328 to 0.669. Radiologists\u2019 average BIRADS was lower than average VDG scores by 0.33, with their mean being 2.13, whereas the mean VDG was 2.48 (U =\u22123.742;P<0.001). VDG and BIRADS showed a very strong positive correlation ( \u03c1=0.91; P<0.001) as did BIRADS and average volumetric breast density percentage ( \u03c1=0.94;P<0.001). Conclusions: Automated volumetric breast density assessment shows moderate agreement and ver",
    "full_text_length": 36199,
    "chunk_length": 1352
  },
  {
    "chunk_id": 410,
    "paper_filename": "Christine_2016_mammographic_breast_density_assessment_using_BIRADS_catagorization.pdf",
    "paper_title": "Christine 2016 Mammographic Breast Density Assessment Using Birads Catagorization",
    "chunk_index": 2,
    "total_chunks": 38,
    "text_content": "breasts (1). Mammographic density (MD), the most common measure of breast density, is de\ufb01ned by the relativeamount of fat and \ufb01broglandular tissue in the breast as seen on a mammogram. This is usually expressed as a percentage, where MD is the proportion of the breast area on a mammogram that is radiodense or opaque (2). However, area-based, two- dimensional measures of MD such as semiautomated Cumulus do not take the volume of density into account. It has been proposed that MD might be used to ",
    "full_text_length": 36199,
    "chunk_length": 1282
  },
  {
    "chunk_id": 411,
    "paper_filename": "Christine_2016_mammographic_breast_density_assessment_using_BIRADS_catagorization.pdf",
    "paper_title": "Christine 2016 Mammographic Breast Density Assessment Using Birads Catagorization",
    "chunk_index": 3,
    "total_chunks": 38,
    "text_content": "(6). There is no gold standard method for measuring actual breast density, although attempts to quantify this feature using mam- mography are well reported. The most common (conventional)Acad Radiol 2016; 23:70\u201377 From the Faculty of Health Sciences, Discipline of Medical Radiation Sciences and Brain and Mind Research Institute, M205, Cumberland Campus, The University of Sydney, 75 East St, Room M205, Lidcombe, Sydney, NSW 2141, Australia (C.N.D., P.C.B., C.M-T., M.F.M.E.); Faculty of Health Sci",
    "full_text_length": 36199,
    "chunk_length": 1435
  },
  {
    "chunk_id": 412,
    "paper_filename": "Christine_2016_mammographic_breast_density_assessment_using_BIRADS_catagorization.pdf",
    "paper_title": "Christine 2016 Mammographic Breast Density Assessment Using Birads Catagorization",
    "chunk_index": 4,
    "total_chunks": 38,
    "text_content": "(BIRADS) (7,8), which de\ufb01nes MD from I (describing an entirely fatty breast) to IV (representing an extremely dense breast) (9). The recent update in the BIRADS standard changed numbered categories to letters and removed percentage descriptors from the four categories (7\u20139). BIRADS assessment of MD has formed the basis for the majority of studies evaluating the importance of MD on both mammographic sensitivity and breast cancer risk (10\u201312) . However, this method is subjective and not without it",
    "full_text_length": 36199,
    "chunk_length": 1348
  },
  {
    "chunk_id": 413,
    "paper_filename": "Christine_2016_mammographic_breast_density_assessment_using_BIRADS_catagorization.pdf",
    "paper_title": "Christine 2016 Mammographic Breast Density Assessment Using Birads Catagorization",
    "chunk_index": 5,
    "total_chunks": 38,
    "text_content": "(18,19) , such as VolparaDensity (Volpara Solutions Limited, New Zealand) and Quantra (Hologic Inc., Bedford, MA), calculate the volume of dense tissue based on measurement and modeling, do not require manual intervention, and have been shown to be more re- producible than BIRADS (20). Recently, Alonzo-Proulx et al. presented data from a comparison of several fully automated volumetric methods, and showed that Volpara was the most reliable (20). Volpara automatically measures volumetric MD and t",
    "full_text_length": 36199,
    "chunk_length": 1332
  },
  {
    "chunk_id": 414,
    "paper_filename": "Christine_2016_mammographic_breast_density_assessment_using_BIRADS_catagorization.pdf",
    "paper_title": "Christine 2016 Mammographic Breast Density Assessment Using Birads Catagorization",
    "chunk_index": 6,
    "total_chunks": 38,
    "text_content": "visual assessment. To date, data available comparing volumetric MD esti- mates to BIRADS assessment have used either a single radiologist (21)or less than \ufb01ve radiologists (22,23) . Given the reported intra- and interreader variation, a larger sample of readers is required to determine the range of agreement that can be expected between Volpara scores and expert BIRADS assessment. The current work will address this de\ufb01ciency, by comparing the two methods in terms of the density scores produced a",
    "full_text_length": 36199,
    "chunk_length": 1317
  },
  {
    "chunk_id": 415,
    "paper_filename": "Christine_2016_mammographic_breast_density_assessment_using_BIRADS_catagorization.pdf",
    "paper_title": "Christine 2016 Mammographic Breast Density Assessment Using Birads Catagorization",
    "chunk_index": 7,
    "total_chunks": 38,
    "text_content": "of these studies have previously been reported (24,25) . The results of previ- ous work (24)demonstrated that the effect of using two mammographic imaging systems on volumetric MD mea- surement was negligible. The focus of that article was to investigate the impact of mammography systems on MD as- sessment, whereas the current article is aimed at investigating agreement between automated volumetric software and BIRADS categorization by expert radiologists in mammo- graphic breast density assessm",
    "full_text_length": 36199,
    "chunk_length": 1272
  },
  {
    "chunk_id": 416,
    "paper_filename": "Christine_2016_mammographic_breast_density_assessment_using_BIRADS_catagorization.pdf",
    "paper_title": "Christine 2016 Mammographic Breast Density Assessment Using Birads Catagorization",
    "chunk_index": 8,
    "total_chunks": 38,
    "text_content": "spectrum of age ranges normally encountered in mammog- raphy. Mammograms were acquired 1 year apart on a GE Healthcare Senographe Essential (or DS) and a Hologic Lorad Selenia. The 40 cases each contained three images: a left craniocaudal (LCC), a left mediolateral oblique (LMLO), and a combined image of the LCC and the LMLO. To ensure observers could evaluate the images in an acceptable time frame, only the left breast images were used for this study. For each breast density assessment method, ",
    "full_text_length": 36199,
    "chunk_length": 1226
  },
  {
    "chunk_id": 417,
    "paper_filename": "Christine_2016_mammographic_breast_density_assessment_using_BIRADS_catagorization.pdf",
    "paper_title": "Christine 2016 Mammographic Breast Density Assessment Using Birads Catagorization",
    "chunk_index": 9,
    "total_chunks": 38,
    "text_content": "a single \ufb01ve-megapixel diagnostic quality monitor (EIZO, Japan) using ViewDEX 2.0 (27). During evaluation of the images, the ambient lighting was kept between 25 and 35 lux as con\ufb01rmed by a calibrated pho- tometer (model 07\u2013621, Nuclear Associates) (28). Radiologists had the ability to adjust the window width and level, as well as pan and zoom the image. For each case, there were three images: \ufb01rst the LCC, followed by the LMLO, and \ufb01nally the combined LCC and LMLO presented together. A score on",
    "full_text_length": 36199,
    "chunk_length": 1253
  },
  {
    "chunk_id": 418,
    "paper_filename": "Christine_2016_mammographic_breast_density_assessment_using_BIRADS_catagorization.pdf",
    "paper_title": "Christine 2016 Mammographic Breast Density Assessment Using Birads Catagorization",
    "chunk_index": 10,
    "total_chunks": 38,
    "text_content": "71 tissue is extremely dense. Denser breasts may lower the sen- sitivity of mammography (7\u20139). To replicate clinical practice, radiologists made one overall judgment from the combined image of LCC and LMLO for each of the 40 cases. A total of 800 MD judgments were made (20 readers \u00d740 cases) (Fig 1 ). The breast densities were then grouped as low and high or binary classi\ufb01cation; low including BIRADS I and II and high including BIRADS III and IV. Breast Density Quantification Using Volpara Autom",
    "full_text_length": 36199,
    "chunk_length": 1178
  },
  {
    "chunk_id": 419,
    "paper_filename": "Christine_2016_mammographic_breast_density_assessment_using_BIRADS_catagorization.pdf",
    "paper_title": "Christine 2016 Mammographic Breast Density Assessment Using Birads Catagorization",
    "chunk_index": 11,
    "total_chunks": 38,
    "text_content": "40 cases of MD measured using Volpara, nine were in VDG 1, 13 in VDG 2, eight in VDG 3, and 10 in VDG 4. Agreement Between BIRADS and Volpara (VDG and AvBD%) VDG thresholds were established based on the BIRADS density assessment of one radiologist (30). Although using a single expert has advantages such as no interreader variability, the \ufb01nd- ings of a single expert may not represent the population of radiologists. Therefore, data from the current study were re- analyzed to examine the effect of",
    "full_text_length": 36199,
    "chunk_length": 1237
  },
  {
    "chunk_id": 420,
    "paper_filename": "Christine_2016_mammographic_breast_density_assessment_using_BIRADS_catagorization.pdf",
    "paper_title": "Christine 2016 Mammographic Breast Density Assessment Using Birads Catagorization",
    "chunk_index": 12,
    "total_chunks": 38,
    "text_content": "readers was compared and expressed as Cohen\u2019s kappa ( \u03ba) with the use of a 20 \u00d720 matrix. Agreement was examined using the four-point scale and binary classi\ufb01cations, respectively. Results were considered to be statistically signi\ufb01cant at P<0.05. One- way analysis of variance with a Bonferroni post hoc test was used to determine whether there were any signi\ufb01cant differ- ences in reading times spent on cases in each of the four BIRADS categories. RESULTS All 20 participants were specialized in br",
    "full_text_length": 36199,
    "chunk_length": 1198
  },
  {
    "chunk_id": 421,
    "paper_filename": "Christine_2016_mammographic_breast_density_assessment_using_BIRADS_catagorization.pdf",
    "paper_title": "Christine 2016 Mammographic Breast Density Assessment Using Birads Catagorization",
    "chunk_index": 13,
    "total_chunks": 38,
    "text_content": "=31%, BIRADS IV=9%, whereas VDG allocated the images as VDG 1=22.5%, VDG 2 =32.5%, VDG 3 =20%, and VDG 4 =25%. Radiologists ranked density using BIRADS signi\ufb01cantly lower than VDG with a mean of 2.13 versus 2.48 (Z =\u22123.742; P<0.001). Correlation Between BIRADS and Volpara (VDG and AvBD%) Very strong positive correlations were found between four- point scale BIRADS with VDG ( \u03c1=0.91; P<0.001) and AvBD% ( \u03c1=0.94; P<0.001), as well as between binary scale BIRADS and VDG ( \u03c1=0.90; P<0.001). No signi",
    "full_text_length": 36199,
    "chunk_length": 1255
  },
  {
    "chunk_id": 422,
    "paper_filename": "Christine_2016_mammographic_breast_density_assessment_using_BIRADS_catagorization.pdf",
    "paper_title": "Christine 2016 Mammographic Breast Density Assessment Using Birads Catagorization",
    "chunk_index": 14,
    "total_chunks": 38,
    "text_content": "almost perfect agree- ment ( \u03ba=0.898; 95% CI =0.701\u20131.000). Agreement between four-point scale BIRADS and VDG increased from moderate ( \u03ba=0.537) to substantial ( \u03ba=0.659) (31)by changing Volpara preset thresholds for VDG 1 from 0%\u20134.5% to 0%\u20134.9%; for VDG 2 from 4.5%\u20137.5% to Figure 1. Schematic overview of image selection.DAMASES ET AL Academic Radiology, Vol 23, No 1, January 2016 72 4.9%\u20138.7%; for VDG 3 from 7.5%\u201315.5% to 8.7%\u201317.1%; and for VDG 4 from >15.5% to >17.1%. Interreader Agreement o",
    "full_text_length": 36199,
    "chunk_length": 1257
  },
  {
    "chunk_id": 423,
    "paper_filename": "Christine_2016_mammographic_breast_density_assessment_using_BIRADS_catagorization.pdf",
    "paper_title": "Christine 2016 Mammographic Breast Density Assessment Using Birads Catagorization",
    "chunk_index": 15,
    "total_chunks": 38,
    "text_content": "took signi\ufb01cantly less time for radiologists to rate than BIRADS III and IV at F=15.517; P<0.001 as shown in Table 1 . Although there was agreement between BIRADS and VDG in 26 of 40 cases, the current study demonstrates that VDG generally provides a higher assessment of MD than BIRADS. For instance, there were \ufb01ve cases rated as BIRADS I that were given a VDG score of 2 by Volpara, whereas there were seven cases that were rated as BIRADS III that were given a VDG 4 by Volpara ( Fig 4 ). Overall",
    "full_text_length": 36199,
    "chunk_length": 1120
  },
  {
    "chunk_id": 424,
    "paper_filename": "Christine_2016_mammographic_breast_density_assessment_using_BIRADS_catagorization.pdf",
    "paper_title": "Christine 2016 Mammographic Breast Density Assessment Using Birads Catagorization",
    "chunk_index": 16,
    "total_chunks": 38,
    "text_content": "by Volpara as: VDG 1=22.5%, VDG 2 =32.5%, VDG 3 =20%, and VDG 4 =25%. (Color version of figure available online). TABLE 1. Mean Difference in Time Spent on Each BIRADS Category Assessment by Radiologists BIRADS BIRADS Mean Difference Std. Error PValue95% Confidence Interval Lower Bound Upper Bound II I \u22120.55317 .51747 1.000 \u22121.9218 .8155 III \u22122.54092* .49325 .000 \u22123.8455 \u22121.2363 IV \u22123.96609* .74457 .000 \u22125.9354 \u22121.9968 II III \u22121.98775* .52820 .001 \u22123.3848 \u2212.5907 IV \u22123.41293* .76817 .000 \u22125.4446 ",
    "full_text_length": 36199,
    "chunk_length": 1290
  },
  {
    "chunk_id": 425,
    "paper_filename": "Christine_2016_mammographic_breast_density_assessment_using_BIRADS_catagorization.pdf",
    "paper_title": "Christine 2016 Mammographic Breast Density Assessment Using Birads Catagorization",
    "chunk_index": 17,
    "total_chunks": 38,
    "text_content": "a wide range of interreader agreement is seen on a four- point scale ( \u03ba=0.33\u20130.67). On a binary scale, BIRADS expert radiologists had an almost perfect average interreader agree- ment with a range of \u03ba=0.656\u20130.901. Martin et al. also previously demonstrated that there is a range of interreader agreement for MD assessment, although their range was smaller at 0.61\u20130.76 (32). Ciatto et al. examined interreader vari- ability during MD assessment using BIRADS and demonstrated a wide range of kappa v",
    "full_text_length": 36199,
    "chunk_length": 1303
  },
  {
    "chunk_id": 426,
    "paper_filename": "Christine_2016_mammographic_breast_density_assessment_using_BIRADS_catagorization.pdf",
    "paper_title": "Christine 2016 Mammographic Breast Density Assessment Using Birads Catagorization",
    "chunk_index": 18,
    "total_chunks": 38,
    "text_content": "more reproduc- ible(20). Comparison Between BIRADS and Volpara MD Assessments Wide ranges of AvBD% for each BIRADS category dem- onstrated that a single BIRADS category is likely to contain AvBD% that extend beyond the thresholds assigned by Volpara (Fig 3). For example, BIRADS I had AvBD% ranging from 2.5% to 5%. Volpara indicates that a BIRADS I should range from 0% to 4.5%; therefore, the range of AvBD% seen for the radiologists\u2019 BIRADS assessments exceeds this range. The same observation was",
    "full_text_length": 36199,
    "chunk_length": 1194
  },
  {
    "chunk_id": 427,
    "paper_filename": "Christine_2016_mammographic_breast_density_assessment_using_BIRADS_catagorization.pdf",
    "paper_title": "Christine 2016 Mammographic Breast Density Assessment Using Birads Catagorization",
    "chunk_index": 19,
    "total_chunks": 38,
    "text_content": "( Fig 3). This demonstrates that there are multiple AvBD% (as classi\ufb01ed by the software) in every category of BIRADS, as classi\ufb01ed by the experts. For the same amount or volume of dense tissue in the breast, radiologists Figure 4. Examples of mammograms showing discrepancy between BIRADS and VDG. These mammograms demonstrate the variations in breast density between BIRADS and Volpara. ( a) Majority of the radiologists rated this case as BIRADS I, whereas Volpara rated the same case as VDG 2. ( b",
    "full_text_length": 36199,
    "chunk_length": 1053
  },
  {
    "chunk_id": 428,
    "paper_filename": "Christine_2016_mammographic_breast_density_assessment_using_BIRADS_catagorization.pdf",
    "paper_title": "Christine 2016 Mammographic Breast Density Assessment Using Birads Catagorization",
    "chunk_index": 20,
    "total_chunks": 38,
    "text_content": "4 as well as the Number of Images on which BIRADS and VGD Agreed for Each Category (Shown in Squares) VDG Total 1234 BIRADS I 9 5 0 0 14 II 08 201 0 III 0 0 6 7 13 IV 00 03 3 Total 9 13 8 10 40DAMASES ET AL Academic Radiology, Vol 23, No 1, January 2016 74 are assigning a range of BIRADS scores. Although there was 65% agreement between BIRADS and VDG in assessment of 40 cases, the current study demonstrates that VDG gen- erally provides a higher assessment of MD than BIRADS ( Fig 4). Martin et a",
    "full_text_length": 36199,
    "chunk_length": 1108
  },
  {
    "chunk_id": 429,
    "paper_filename": "Christine_2016_mammographic_breast_density_assessment_using_BIRADS_catagorization.pdf",
    "paper_title": "Christine 2016 Mammographic Breast Density Assessment Using Birads Catagorization",
    "chunk_index": 21,
    "total_chunks": 38,
    "text_content": "largest range from 8% to 60%; and BIRADS IV had a range from 20% to 82%. Although MDEST is a different method of MD assessment and uses dif- ferent percentage categorizations, it demonstrates a similar pattern of large variations by radiologists across percentages indicated by automated MD categorization systems. The pattern of a wide range of quantitative percentage den- sities in each BIRADS category was also demonstrated by Jeffreys et al. (35). The authors compared BIRADS MD clas- si\ufb01cations",
    "full_text_length": 36199,
    "chunk_length": 1231
  },
  {
    "chunk_id": 430,
    "paper_filename": "Christine_2016_mammographic_breast_density_assessment_using_BIRADS_catagorization.pdf",
    "paper_title": "Christine 2016 Mammographic Breast Density Assessment Using Birads Catagorization",
    "chunk_index": 22,
    "total_chunks": 38,
    "text_content": "\ufb01nding that a single BIRADS cate- gory is likely to contain AvBD% that extends beyond the thresholds assigned by Volpara and that there are wide interreader variations in the assessment of BIRADS MD. Variability in assessment of MD can potentially in\ufb02uence the individualized breast cancer screening pathway of a woman. This variability becomes crucial when it affects the classi\ufb01- cations that separate \u201clow\u201d from \u201chigh\u201d density, that is, those that could fall in either BIRADS I and II or BIRADS II",
    "full_text_length": 36199,
    "chunk_length": 1271
  },
  {
    "chunk_id": 431,
    "paper_filename": "Christine_2016_mammographic_breast_density_assessment_using_BIRADS_catagorization.pdf",
    "paper_title": "Christine 2016 Mammographic Breast Density Assessment Using Birads Catagorization",
    "chunk_index": 23,
    "total_chunks": 38,
    "text_content": "a range from 0.5% to 19.2% for fatty breasts (BIRADS I), 1.2%\u201352.7% for scattered densities (BIRADS II), 15.9%\u201382.2% for heterogeneously dense (BIRADS III), and 60.1%\u201387.9% for extremely dense breasts (BIRADS IV) (12). The authors reported that the reader- assigned percent MD ranges for fatty (BIRADS I) ( r=2.4\u2013 13.1) and extremely dense breasts (BIRADS IV) ( r=70.5\u2013 87.2) correlated well with BIRADS de\ufb01nitions, whereas the ranges of densities in the scattered (BIRADS II) ( r=5.8\u2013 32.2) and hete",
    "full_text_length": 36199,
    "chunk_length": 1296
  },
  {
    "chunk_id": 432,
    "paper_filename": "Christine_2016_mammographic_breast_density_assessment_using_BIRADS_catagorization.pdf",
    "paper_title": "Christine 2016 Mammographic Breast Density Assessment Using Birads Catagorization",
    "chunk_index": 24,
    "total_chunks": 38,
    "text_content": "was assessed, they demonstrated an almost perfect agree- ment with a kappa of 0.898 (95% CI =0.701\u20131.000). The moderate agreement between BIRADS and VDG and the apparent higher density ratings with VDG might be ad- dressed through adjusting the thresholds used to classify VDG categories. Improving agreement between BIRADS and VDG through adjusting Volpara preset thresholds based on feed- back from a larger number of expert readers is a novel and important \ufb01nding of this work. Improved agreement ",
    "full_text_length": 36199,
    "chunk_length": 1299
  },
  {
    "chunk_id": 433,
    "paper_filename": "Christine_2016_mammographic_breast_density_assessment_using_BIRADS_catagorization.pdf",
    "paper_title": "Christine 2016 Mammographic Breast Density Assessment Using Birads Catagorization",
    "chunk_index": 25,
    "total_chunks": 38,
    "text_content": "of 0.60 ( P<.001) and 0.63 ( P<.001) for Quantra and Volpara, respectively,\u201d and they suggested that this could be because radiologists use a semivolumetric approach to measuring visual density (39). Modeling the effect of changes in thresholds on the agreement between BIRADS and VDG may lead to greater improvements; however, these improvements may be pop- ulation and reader speci\ufb01c, and may need to be adjusted for the population of radiologists and breast densities in each country. Our previous",
    "full_text_length": 36199,
    "chunk_length": 1215
  },
  {
    "chunk_id": 434,
    "paper_filename": "Christine_2016_mammographic_breast_density_assessment_using_BIRADS_catagorization.pdf",
    "paper_title": "Christine 2016 Mammographic Breast Density Assessment Using Birads Catagorization",
    "chunk_index": 26,
    "total_chunks": 38,
    "text_content": "cases. A study by Holland et al. (40)dem- onstrated high reproducibility of MD measurement with Volpara compared to BIRADS over a mean screening interval period of 22.65 months between their two studies. The authors re- ported that in 89.7% of the cases, MD remained in the same category. In 3.2% of the pairs, an increase in percentage density was actually reported, resulting in a change from nondense to dense category. The authors report that this effect may have been due to differences in the b",
    "full_text_length": 36199,
    "chunk_length": 1172
  },
  {
    "chunk_id": 435,
    "paper_filename": "Christine_2016_mammographic_breast_density_assessment_using_BIRADS_catagorization.pdf",
    "paper_title": "Christine 2016 Mammographic Breast Density Assessment Using Birads Catagorization",
    "chunk_index": 27,
    "total_chunks": 38,
    "text_content": "American radiologists. Future work might ascertain the need for further assessment as an outcome rather than a density measurement itself; for example, does the patient need ultrasound or further imaging as a result of MD. Knowledge of a woman\u2019s MD can be used to predict her risk to cancer and personalize her imaging pathway.Academic Radiology, Vol 23, No 1, January 2016 COMPARING EXPERTS TO AUTOMATED DENSITY ASSESSMENT 75 However, measurement of MD has proven to be trouble- some with wide varia",
    "full_text_length": 36199,
    "chunk_length": 1276
  },
  {
    "chunk_id": 436,
    "paper_filename": "Christine_2016_mammographic_breast_density_assessment_using_BIRADS_catagorization.pdf",
    "paper_title": "Christine 2016 Mammographic Breast Density Assessment Using Birads Catagorization",
    "chunk_index": 28,
    "total_chunks": 38,
    "text_content": "scale of the variation differs from country to country. Previous studies on MD assessment have used smaller numbers of radiologists, typically 10 or less (41,42) . Thus, a strength of the current work is the consensus result of 20 radiologists. This study has several limitations. Firstly, there is no widely accepted ground truth for MD measurement and thus a ref- erence standard to evaluate MD does not exist. Secondly, Volpara reference thresholds have been calibrated based on one ra- diologist ",
    "full_text_length": 36199,
    "chunk_length": 1192
  },
  {
    "chunk_id": 437,
    "paper_filename": "Christine_2016_mammographic_breast_density_assessment_using_BIRADS_catagorization.pdf",
    "paper_title": "Christine 2016 Mammographic Breast Density Assessment Using Birads Catagorization",
    "chunk_index": 29,
    "total_chunks": 38,
    "text_content": "for the current study was 40 images, this results in a lower power of 0.51 at an alpha of 0.05. Finally, the radiologists may not have been familiar with the presentation state of the images and may be used to a different presentation, and this may have affected their conclusions on density. CONCLUSIONS This work demonstrates that expert radiologists differ in their MD assessment on a four-point scale and less so on a binary scale. Reduction in the variability of MD assessment may require a wide",
    "full_text_length": 36199,
    "chunk_length": 1318
  },
  {
    "chunk_id": 438,
    "paper_filename": "Christine_2016_mammographic_breast_density_assessment_using_BIRADS_catagorization.pdf",
    "paper_title": "Christine 2016 Mammographic Breast Density Assessment Using Birads Catagorization",
    "chunk_index": 30,
    "total_chunks": 38,
    "text_content": "support for this study. The authors thank Professor Steve Hillis, University of Iowa, Patrick Kasi, Uni- versity of Western Sydney, and Ziba Gandomkar, University of Sydney for their assistance with statistical support. REFERENCES 1.McCormack VA, dos Santos Silva I. Breast density and parenchymal pat- terns as markers of breast cancer risk: a meta-analysis. Cancer Epidemiol Biomarkers Prev 2006; 15:1159\u20131169.2.Ursin GQS. Mammographic density\u2014a useful biomarker for the breast cancer risk in epide",
    "full_text_length": 36199,
    "chunk_length": 1533
  },
  {
    "chunk_id": 439,
    "paper_filename": "Christine_2016_mammographic_breast_density_assessment_using_BIRADS_catagorization.pdf",
    "paper_title": "Christine 2016 Mammographic Breast Density Assessment Using Birads Catagorization",
    "chunk_index": 31,
    "total_chunks": 38,
    "text_content": "mammography. In: D\u2019Orsi CJ, Mendelson EB, Ikeda DM, et al., eds. Breast imaging reporting and data system: ACR BI-RADS\u2014breast imaging atlas. 4th ed. Reston, VA: American College of Radiology, 2003:61\u2013128. 8.Radiology ACo. The American College of radiology BIRADS ATLAS and MQSA: frequently asked questions. Available at: http://www.acr.org/ ~/media/ACR/Documents/PDF/QualitySafety/Resources/BIRADS/ BIRADSFAQs.pdf . Accessed September 25, 2012. 9.Radiology ACo. BI-RADS mammography 2013-ACR BI-RADS a",
    "full_text_length": 36199,
    "chunk_length": 1470
  },
  {
    "chunk_id": 440,
    "paper_filename": "Christine_2016_mammographic_breast_density_assessment_using_BIRADS_catagorization.pdf",
    "paper_title": "Christine 2016 Mammographic Breast Density Assessment Using Birads Catagorization",
    "chunk_index": 32,
    "total_chunks": 38,
    "text_content": "definitions. Acad Radiol 2006; 13:1143\u2013 1149. 13.Zhou C, Chan HP, Petrick N, et al. Computerized image analysis: esti- mation of breast density on mammograms. Med Phys 2001; 28:1056\u2013 1069. 14.McCormack VA, Highnam R, Perry N, et al. Comparison of a new and existing method of mammographic density measurement: intramethod reliability and associations with known risk factors. Cancer Epidemiol Biomarkers Prev 2007; 16:1148\u20131154. 15.Ciatto S, Bernardi D, Calabrese M, et al. A first evaluation of brea",
    "full_text_length": 36199,
    "chunk_length": 1381
  },
  {
    "chunk_id": 441,
    "paper_filename": "Christine_2016_mammographic_breast_density_assessment_using_BIRADS_catagorization.pdf",
    "paper_title": "Christine 2016 Mammographic Breast Density Assessment Using Birads Catagorization",
    "chunk_index": 33,
    "total_chunks": 38,
    "text_content": "among BI-RADS categories, a semi-automated software and a fully automated one. Breast 2009; 18:35\u201340. 19.Byng JW, Boyd NF, Fishell E, et al. Automated analysis of mammo- graphic densities. Phys Med Biol 1996; 41:909\u2013923. 20.Alonzo-Proulx O, Mawdsley GE, Patrie JT, et al. Reliability of automated breast density measurement. Radiology 2015; 275:366\u2013 376. 21.Sauber N, Chan A, Highnam R. BI-RADS breast density classification\u2014 an international standard. ECR; 2013. 22.Wang K, Chan A, Highnam R. Robust",
    "full_text_length": 36199,
    "chunk_length": 1389
  },
  {
    "chunk_id": 442,
    "paper_filename": "Christine_2016_mammographic_breast_density_assessment_using_BIRADS_catagorization.pdf",
    "paper_title": "Christine 2016 Mammographic Breast Density Assessment Using Birads Catagorization",
    "chunk_index": 34,
    "total_chunks": 38,
    "text_content": "to BIRADS. Med Imag 2014: Image Percept, Obs Perform, Technol Assess 2014; 9037:8.DAMASES ET AL Academic Radiology, Vol 23, No 1, January 2016 76 26.Wanders JO, Holland K, Veldhuis WB, et al. Effect of volumetric mam- mographic density on performance of a breast cancer screening program using full-field digital mammography. ECR 2015. 27.B\u00f6rjesson S, H\u00e5kansson M, B\u00e5th M, et al. A software tool for increased efficiency in observer performance studies in radiology. Radiat Prot Do- simetry 2005; 114",
    "full_text_length": 36199,
    "chunk_length": 1414
  },
  {
    "chunk_id": 443,
    "paper_filename": "Christine_2016_mammographic_breast_density_assessment_using_BIRADS_catagorization.pdf",
    "paper_title": "Christine 2016 Mammographic Breast Density Assessment Using Birads Catagorization",
    "chunk_index": 35,
    "total_chunks": 38,
    "text_content": "J Roentgenol 2000; 174:1769\u20131777. 31.Viera AJ, Garrett JM. Understanding interobserver agreement: the kappa statistic. Fam Med 2005; 37:360\u2013363. 32.Martin KE, Helvie MA, Zhou C, et al. Mammographic density measured with quantitative computer-aided method: comparison with radiolo- gists\u2019 estimates and BI-RADS categories. Radiology 2006; 240:656\u2013 665. 33.Ciatto S, Houssami N, Apruzzese A, et al. Categorizing breast mammo- graphic density: intra- and interobserver reproducibility of BI-RADS density",
    "full_text_length": 36199,
    "chunk_length": 1421
  },
  {
    "chunk_id": 444,
    "paper_filename": "Christine_2016_mammographic_breast_density_assessment_using_BIRADS_catagorization.pdf",
    "paper_title": "Christine 2016 Mammographic Breast Density Assessment Using Birads Catagorization",
    "chunk_index": 36,
    "total_chunks": 38,
    "text_content": "Phys Med Biol 1994; 39:1629\u20131638. 37.Byng JW, Yaffe MJ, Jong RA, et al. Analysis of mammographic density and breast cancer risk from digitized mammograms. Radiographics 1998; 18:1587\u20131598. 38.Boyd NF, Byng JW, Jong RA, et al. Quantitative classification of mam- mographic densities and breast cancer risk: results from the Canadian National Breast Screening Study. J Natl Cancer Inst 1995; 87:670\u2013675. 39.Morris OWE, Tucker L, Black R, et al. Mammographic breast density: comparison of methods for qu",
    "full_text_length": 36199,
    "chunk_length": 1237
  },
  {
    "chunk_id": 445,
    "paper_filename": "Christine_2016_mammographic_breast_density_assessment_using_BIRADS_catagorization.pdf",
    "paper_title": "Christine 2016 Mammographic Breast Density Assessment Using Birads Catagorization",
    "chunk_index": 37,
    "total_chunks": 38,
    "text_content": "al. Variability in radiologists\u2019 interpre- tations of mammograms. NEJM 1994; 331:1493\u20131499.Academic Radiology, Vol 23, No 1, January 2016 COMPARING EXPERTS TO AUTOMATED DENSITY ASSESSMENT 77",
    "full_text_length": 36199,
    "chunk_length": 190
  },
  {
    "chunk_id": 446,
    "paper_filename": "cristiana_towards_an_interpretable_breast_cancer_detection_and_diagnosis_system.pdf",
    "paper_title": "Cristiana Towards An Interpretable Breast Cancer Detection And Diagnosis System",
    "chunk_index": 0,
    "total_chunks": 83,
    "text_content": "Contents lists available at ScienceDirect Computers in Biology and Medicine journal homepage: www.elsevier.com/locate/compbiomed Towards an interpretable breast cancer detection and diagnosis system Cristiana Moroz-Dubenco \u2217,Ad\u00e9l Bajcsi ,Anca Andreica ,Camelia Chira Babe\u015f\u2013Bolyai University, Mihail Kog\u0103lniceanu 1, Cluj-Napoca, 400084, Cluj, Romania A R T I C L E I N F O Keywords: Digital mammogram processing Breast tissue segmentation Lesion classification InterpretabilityA B S T R A C T Accordin",
    "full_text_length": 84216,
    "chunk_length": 1485
  },
  {
    "chunk_id": 447,
    "paper_filename": "cristiana_towards_an_interpretable_breast_cancer_detection_and_diagnosis_system.pdf",
    "paper_title": "Cristiana Towards An Interpretable Breast Cancer Detection And Diagnosis System",
    "chunk_index": 1,
    "total_chunks": 83,
    "text_content": "five steps. After a robust pre-processing and an unsupervised segmentation, we analyze four feature extraction techniques, both textural and shape-based, and three methods for feature selection. To facilitate interpretation, we employ the Decision Tree algorithm for benign/malignant classification and experiment with different methods to avoid overfitting: pre-pruning, post-pruning, and ensemble-based (Random Forest classifier). Our system reaches a maximum accuracy of 95% and 100% precision and",
    "full_text_length": 84216,
    "chunk_length": 1352
  },
  {
    "chunk_id": 448,
    "paper_filename": "cristiana_towards_an_interpretable_breast_cancer_detection_and_diagnosis_system.pdf",
    "paper_title": "Cristiana Towards An Interpretable Breast Cancer Detection And Diagnosis System",
    "chunk_index": 2,
    "total_chunks": 83,
    "text_content": "cancer can be curable by surgery, radiation therapy, and chemotherapy, if detected in its early stages. When de- tected later, the probability of metastasis increases significantly, which can lead to a lethal outcome. Therefore, regular screening is crucial for early detection and a better chance of successful treatment. Mammography is the most frequently used technique for detecting early signs of breast cancer. However, its interpretation can be chal- lenging, even for experienced specialists.",
    "full_text_length": 84216,
    "chunk_length": 1363
  },
  {
    "chunk_id": 449,
    "paper_filename": "cristiana_towards_an_interpretable_breast_cancer_detection_and_diagnosis_system.pdf",
    "paper_title": "Cristiana Towards An Interpretable Breast Cancer Detection And Diagnosis System",
    "chunk_index": 3,
    "total_chunks": 83,
    "text_content": "refers to a system\u2019s ability to present the internal processes in a manner that is easily understandable to the end user. The field of interpretable \u2217Corresponding author. E-mail address: cristiana.moroz@ubbcluj.ro (C. Moroz-Dubenco) .AI is relatively new and rapidly emerging, with only a limited number of studies addressing the need for transparency and understanding in AI-driven decision-making systems. In general, CAD systems utilize image processing and AI techniques to automatically detect ",
    "full_text_length": 84216,
    "chunk_length": 1352
  },
  {
    "chunk_id": 450,
    "paper_filename": "cristiana_towards_an_interpretable_breast_cancer_detection_and_diagnosis_system.pdf",
    "paper_title": "Cristiana Towards An Interpretable Breast Cancer Detection And Diagnosis System",
    "chunk_index": 4,
    "total_chunks": 83,
    "text_content": "systems is their interpretability [2]. The transparency of the system ensures that radiologists can understand and interpret the steps that the CAD system undertakes to reach its diagnosis. The purpose of the current study is to build a fully automated CAD system that detects possible lesions and classifies them into benign and malignant classes with the ability to visually explain the result of every step. To achieve our objective, we propose the use of an unsupervised segmentation method to de",
    "full_text_length": 84216,
    "chunk_length": 1387
  },
  {
    "chunk_id": 451,
    "paper_filename": "cristiana_towards_an_interpretable_breast_cancer_detection_and_diagnosis_system.pdf",
    "paper_title": "Cristiana Towards An Interpretable Breast Cancer Detection And Diagnosis System",
    "chunk_index": 5,
    "total_chunks": 83,
    "text_content": "). C. Moroz-Dubenco et al. Decision Tree models are preferred over artificial neural networks (ANNs) in applications requiring interpretability due to their inher- ently transparent structure. Unlike ANNs, which operate as \u2018\u2018black-box\u2019\u2019 models with complex layers of interconnected neurons, DTs offer clear rule-based paths that can be easily visualized and understood by hu- mans. Although there are initiatives to improve the explainability of ANNs [3,4], they remain challenging to validate, as th",
    "full_text_length": 84216,
    "chunk_length": 1384
  },
  {
    "chunk_id": 452,
    "paper_filename": "cristiana_towards_an_interpretable_breast_cancer_detection_and_diagnosis_system.pdf",
    "paper_title": "Cristiana Towards An Interpretable Breast Cancer Detection And Diagnosis System",
    "chunk_index": 6,
    "total_chunks": 83,
    "text_content": "a more complex, often black-box model (such as deep learning) arrives at a specific decision or prediction, typically through post-hoc methods like feature importance or visual heatmaps. Simply put, interpretability is centered on the inner workings of a model, while explainability, on the decisions made. In the current work, we are focusing on interpretability, which provides a greater level of detail. For pre-processing, we employ the technique proposed in [5,6], while segmentation is done usi",
    "full_text_length": 84216,
    "chunk_length": 1391
  },
  {
    "chunk_id": 453,
    "paper_filename": "cristiana_towards_an_interpretable_breast_cancer_detection_and_diagnosis_system.pdf",
    "paper_title": "Cristiana Towards An Interpretable Breast Cancer Detection And Diagnosis System",
    "chunk_index": 7,
    "total_chunks": 83,
    "text_content": "the possible pa- rameter combinations on images from the mini-MIAS (Mammographic Image Analysis Society) dataset [9], and choose the one that produces the best results. Additionally, the resulting system is tested on the mini- DDSM dataset [10] in order to prove its ability of generalization and robustness. The novelty of the proposed system lies in its interpretability, which is achieved by visually explaining the outcome of each step in the process. In healthcare, datasets are often limited in",
    "full_text_length": 84216,
    "chunk_length": 1302
  },
  {
    "chunk_id": 454,
    "paper_filename": "cristiana_towards_an_interpretable_breast_cancer_detection_and_diagnosis_system.pdf",
    "paper_title": "Cristiana Towards An Interpretable Breast Cancer Detection And Diagnosis System",
    "chunk_index": 8,
    "total_chunks": 83,
    "text_content": "of 100% on the mini-MIAS dataset [9]. 1.1. Background Various systems for the detection and diagnosis of breast can- cer have been proposed so far. In the following, we highlight key contributions related to breast cancer classification presented in the literature. The authors of [11] introduce a CAD system for the classification of benign/malignant lesions in2018 . Pre-processing is done through median filtering, and the suspicious region is cropped and segmented using Otsu\u2019s thresholding metho",
    "full_text_length": 84216,
    "chunk_length": 1323
  },
  {
    "chunk_id": 455,
    "paper_filename": "cristiana_towards_an_interpretable_breast_cancer_detection_and_diagnosis_system.pdf",
    "paper_title": "Cristiana Towards An Interpretable Breast Cancer Detection And Diagnosis System",
    "chunk_index": 9,
    "total_chunks": 83,
    "text_content": "image is further seg- mented with the Fuzzy C-Means clustering method and post-processedusing morphological techniques in order to determine the boundary of the lesion. Statistical and textural features are extracted from the ROIs and classified with the Random Forest algorithm. The proposed approach was tested on 100 images from MIAS and reached an accuracy of 90.47%. Boudraa et al. (2020) [13] proposed a CAD system for breast cancer classification based on a two-step approach: feature extracti",
    "full_text_length": 84216,
    "chunk_length": 1338
  },
  {
    "chunk_id": 456,
    "paper_filename": "cristiana_towards_an_interpretable_breast_cancer_detection_and_diagnosis_system.pdf",
    "paper_title": "Cristiana Towards An Interpretable Breast Cancer Detection And Diagnosis System",
    "chunk_index": 10,
    "total_chunks": 83,
    "text_content": "from the MIAS dataset, while the Random Forest-based approach achieved an accuracy of 94.6% and a 94.7% sensitivity score. [14] (from 2021 ) presents a breast cancer diagnosis system which relays on a nature-inspired algorithm for feature extraction and em- ploys both DT and Gradient Boosting algorithms for classification. The features are extracted through the Particle Swarm Optimization tech- nique, using a validation set\u2019s accuracy as fitness function. Afterward, the features are fed to the c",
    "full_text_length": 84216,
    "chunk_length": 1315
  },
  {
    "chunk_id": 457,
    "paper_filename": "cristiana_towards_an_interpretable_breast_cancer_detection_and_diagnosis_system.pdf",
    "paper_title": "Cristiana Towards An Interpretable Breast Cancer Detection And Diagnosis System",
    "chunk_index": 11,
    "total_chunks": 83,
    "text_content": "pixel with the highest intensity as the initial seed. Subsequently, multiple types of feature are extracted from the segmented region, both geo- metrical and textural, from which the top twenty most exclusionary (based on their discriminating power) are selected using the Relief-F algorithm. These features are then classified with nine algorithms: K- Nearest Neighbor, Support Vector Machine, Gaussian Mixture Model, Multi-class Support Vector Machine, Decision Tree, Discriminate Anal- ysis, Naive",
    "full_text_length": 84216,
    "chunk_length": 1312
  },
  {
    "chunk_id": 458,
    "paper_filename": "cristiana_towards_an_interpretable_breast_cancer_detection_and_diagnosis_system.pdf",
    "paper_title": "Cristiana Towards An Interpretable Breast Cancer Detection And Diagnosis System",
    "chunk_index": 12,
    "total_chunks": 83,
    "text_content": "mammography and published their results in2023 . The proposed methodology is tested on images from the MIAS dataset. After rotating and cropping the images to a ratio of 7:5, contrast stretching and Gaussian filtering are applied. Textural features are extracted from the GLCM. The dataset is then split into training and testing sets using different ratios, and classification is performed using the RF method with various numbers of trees. The best results were obtained for the 80:20 training-test",
    "full_text_length": 84216,
    "chunk_length": 1313
  },
  {
    "chunk_id": 459,
    "paper_filename": "cristiana_towards_an_interpretable_breast_cancer_detection_and_diagnosis_system.pdf",
    "paper_title": "Cristiana Towards An Interpretable Breast Cancer Detection And Diagnosis System",
    "chunk_index": 13,
    "total_chunks": 83,
    "text_content": "predictions. The authors concluded that Grad-CAM is the most time efficient and also generates the most accurate heatmaps. Lampour et al. [20] trained five frequently used CNNs (VGG16, VGG19, ResNet50, Xception, and MobileNet) on Chinese Mammogram Dataset [21] and reported 80.02%, 84.79%, 78.15%, 79.09% and 74.95% accuracy, respectively. Additionally, the authors applied Grad-CAM [3] to generate heatmaps for the predic- tions and concluded that the attention-maps generated for XceptionComputers ",
    "full_text_length": 84216,
    "chunk_length": 1332
  },
  {
    "chunk_id": 460,
    "paper_filename": "cristiana_towards_an_interpretable_breast_cancer_detection_and_diagnosis_system.pdf",
    "paper_title": "Cristiana Towards An Interpretable Breast Cancer Detection And Diagnosis System",
    "chunk_index": 14,
    "total_chunks": 83,
    "text_content": "the development of self-explainable models. Chen et al. [4] proposed a patch-based self- explainable model named ProtoPNet. In [24], the authors trained ProtoPNet on the CBIS-DDSM dataset [25] and reported a test accuracy of 68.5%. In this study, we propose a fully automated and interpretable CAD system for the detection of breast cancer. The methods used are selected such that they can be visualized by the user for validation. The mam- mograms from the mini-MIAS dataset are used and a comprehen",
    "full_text_length": 84216,
    "chunk_length": 1254
  },
  {
    "chunk_id": 461,
    "paper_filename": "cristiana_towards_an_interpretable_breast_cancer_detection_and_diagnosis_system.pdf",
    "paper_title": "Cristiana Towards An Interpretable Breast Cancer Detection And Diagnosis System",
    "chunk_index": 15,
    "total_chunks": 83,
    "text_content": "the conclusions and future work in Section 5. 2. Materials and methods In the current paper, our aim is to create a CAD system to help doc- tors in the diagnosis of breast cancer in an early stage. As mentioned in Section 1, support systems generally consist of five steps. Fig. 1details the structure of the proposed system. . In the following subsections, the methods used are detailed for every step. 2.1. Pre-processing Mammograms are X-ray images of breast tissue. These images usu- ally have lo",
    "full_text_length": 84216,
    "chunk_length": 1239
  },
  {
    "chunk_id": 462,
    "paper_filename": "cristiana_towards_an_interpretable_breast_cancer_detection_and_diagnosis_system.pdf",
    "paper_title": "Cristiana Towards An Interpretable Breast Cancer Detection And Diagnosis System",
    "chunk_index": 16,
    "total_chunks": 83,
    "text_content": "tissue structures as presented in [8]. First, labels are discarded using thresholding. After the preliminary experiments presented in [26], we decided to set the threshold to50for each image. The result of the thresholding is presented inFig. 1aI. The largest component of the binary image is used as the breast mask, as shown inFig. 1aII. In the next step, the pectoral muscle is removed. In [26], various methods were tested, and the Seeded Region Growing (SRG) algorithm produced the best results.",
    "full_text_length": 84216,
    "chunk_length": 1309
  },
  {
    "chunk_id": 463,
    "paper_filename": "cristiana_towards_an_interpretable_breast_cancer_detection_and_diagnosis_system.pdf",
    "paper_title": "Cristiana Towards An Interpretable Breast Cancer Detection And Diagnosis System",
    "chunk_index": 17,
    "total_chunks": 83,
    "text_content": "1aIV). We highlight the fact that, for better understanding, the pre-processing technique can be displayed step by step, as shown in Fig. 1a.2.2. Segmentation In order to identify the region of interest from the pre-processed mammograms, we use the Threshold-based GrowCut (TbGC) segmenta- tion method proposed in our previous work [7,8], which is an improved and unsupervised version of the GrowCut algorithm [27]. In the fol- lowing paragraphs, we detail the steps of the segmentation, which are il",
    "full_text_length": 84216,
    "chunk_length": 1331
  },
  {
    "chunk_id": 464,
    "paper_filename": "cristiana_towards_an_interpretable_breast_cancer_detection_and_diagnosis_system.pdf",
    "paper_title": "Cristiana Towards An Interpretable Breast Cancer Detection And Diagnosis System",
    "chunk_index": 18,
    "total_chunks": 83,
    "text_content": "2.Foreground seeds selection \u2014 because abnormal tissue appears brighter on mammograms, the 25-pixel radius (as experimen- tally chosen in [28]) circle with the highest sum of pixels\u2019 intensities is identified, and all the pixels within this circle are used as foreground seeds, completely removing the need for human intervention. 3.Computational time \u2014 the algorithm uses Cellular Automation and, thus, yields a result when the automation converges. Taking into consideration the fact that medical i",
    "full_text_length": 84216,
    "chunk_length": 1277
  },
  {
    "chunk_id": 465,
    "paper_filename": "cristiana_towards_an_interpretable_breast_cancer_detection_and_diagnosis_system.pdf",
    "paper_title": "Cristiana Towards An Interpretable Breast Cancer Detection And Diagnosis System",
    "chunk_index": 19,
    "total_chunks": 83,
    "text_content": "since a complete run leads to more false positives, as shown in [7,29]. 4.Performance \u2014 to maintain a high level of accuracy despite all the changes made to the original GrowCut algorithm, the cell evolution rule is modified such that a pixel\u2019s label is updated only if its new strength exceeds a certain threshold (\ud835\udf03\ud835\udc61). From the experiments presented in [7], it can be concluded that this value should be equal to 0.5. As detailed in [7,8], TbGC converts the semi-supervised GrowCut (requiring human",
    "full_text_length": 84216,
    "chunk_length": 1221
  },
  {
    "chunk_id": 466,
    "paper_filename": "cristiana_towards_an_interpretable_breast_cancer_detection_and_diagnosis_system.pdf",
    "paper_title": "Cristiana Towards An Interpretable Breast Cancer Detection And Diagnosis System",
    "chunk_index": 20,
    "total_chunks": 83,
    "text_content": "selection \u2013 using the darkest pixels from the image \u2013 can be easily identified using a pre-processed mammogram, where all the pixels aside from the breast are turned into black. The foreground seeds selection, on the other hand, can be seen as an iterative process, where the image is parsed pixel-by-pixel, and each pixel is considered the center of a circle with a radius of 25 pixels. The brightness of this circle is computed and compared to the previous maximum, and, if it is higher, it becomes",
    "full_text_length": 84216,
    "chunk_length": 1227
  },
  {
    "chunk_id": 467,
    "paper_filename": "cristiana_towards_an_interpretable_breast_cancer_detection_and_diagnosis_system.pdf",
    "paper_title": "Cristiana Towards An Interpretable Breast Cancer Detection And Diagnosis System",
    "chunk_index": 21,
    "total_chunks": 83,
    "text_content": "of iterations, the result of each iteration can be displayed so that the users can follow and understand the process. Fig. 3holds the results yielded by TbGC at each iteration. Although the segmentation is performed on the pre- processed images, we chose to output its results on the original imagesComputers in Biology and Medicine 185 (2025) 109520 3 C. Moroz-Dubenco et al. Fig. 1.The structure of the proposed system. The input is a mammogram, which is pre-processed, segmented, and features are ",
    "full_text_length": 84216,
    "chunk_length": 1283
  },
  {
    "chunk_id": 468,
    "paper_filename": "cristiana_towards_an_interpretable_breast_cancer_detection_and_diagnosis_system.pdf",
    "paper_title": "Cristiana Towards An Interpretable Breast Cancer Detection And Diagnosis System",
    "chunk_index": 22,
    "total_chunks": 83,
    "text_content": "it a suitable choice for our interpretable CAD system. Moreover, on the mini-MIAS dataset, it yielded an accuracy of 98.52% and a precision score of almost 70% [8], which is comparable to other unsupervised, yet more difficult to understand, segmentation techniques. Therefore, our choice of algorithm was motivated by the robustness, time efficiency, and interpretability of the Threshold-based GrowCut segmentation method. 2.3. Feature extraction Feature extraction plays a key role in an automated",
    "full_text_length": 84216,
    "chunk_length": 1332
  },
  {
    "chunk_id": 469,
    "paper_filename": "cristiana_towards_an_interpretable_breast_cancer_detection_and_diagnosis_system.pdf",
    "paper_title": "Cristiana Towards An Interpretable Breast Cancer Detection And Diagnosis System",
    "chunk_index": 23,
    "total_chunks": 83,
    "text_content": "also for validating each step sepa- rately, such that, if a decision of the system is not in alignment with the medical expert\u2019s opinion, they can clearly tell which step of the process caused the difference and decide which outcome (theirs or the system\u2019s) is more likely to be correct. In the following paragraphs, the extracted features (also listed inFig. 1c) are detailed. 2.3.1. Local Binary Pattern Local Binary Pattern (LBP) [30] compares the intensity of each pixel with its neighboring pixe",
    "full_text_length": 84216,
    "chunk_length": 1273
  },
  {
    "chunk_id": 470,
    "paper_filename": "cristiana_towards_an_interpretable_breast_cancer_detection_and_diagnosis_system.pdf",
    "paper_title": "Cristiana Towards An Interpretable Breast Cancer Detection And Diagnosis System",
    "chunk_index": 24,
    "total_chunks": 83,
    "text_content": "vector that describes the texture of the image.Computers in Biology and Medicine 185 (2025) 109520 4 C. Moroz-Dubenco et al. Fig. 2.Step-by-step selection of foreground seeds. Fig. 3.Step-by-step segmentation. Table 1 Variants of LBP. Name Grayscale invariant Rotation invariant default [30] \u2713 ror[31] \u2713 \u2713 uniform [32] \u2713 \u2713 nri_uniform [33] \u2713 var[34] \u2713 Various extensions to the original LBP method have been intro- duced over the years, introducing rotation invariance, uniform patterns, and neighbor",
    "full_text_length": 84216,
    "chunk_length": 1263
  },
  {
    "chunk_id": 471,
    "paper_filename": "cristiana_towards_an_interpretable_breast_cancer_detection_and_diagnosis_system.pdf",
    "paper_title": "Cristiana Towards An Interpretable Breast Cancer Detection And Diagnosis System",
    "chunk_index": 25,
    "total_chunks": 83,
    "text_content": "range of21\u2212 24 as multipliers. As mentioned in Section 1and detailed in Section 2.5, we employ the Decision Tree algorithm with three types of pruning for classification, and therefore we need to find the best combination ofTable 2 LBP parameters for classification methods. Pruning Method Radius Multiplier Pre default 9 2 Post default 9 2 Ensemble default 7 16 parameters for each of these variants. As a means of this, we analyze all possible variants, totaling 5 (methods)\u22c54 (radii)\u22c55 (multiplier",
    "full_text_length": 84216,
    "chunk_length": 1269
  },
  {
    "chunk_id": 472,
    "paper_filename": "cristiana_towards_an_interpretable_breast_cancer_detection_and_diagnosis_system.pdf",
    "paper_title": "Cristiana Towards An Interpretable Breast Cancer Detection And Diagnosis System",
    "chunk_index": 26,
    "total_chunks": 83,
    "text_content": "ensemble-based pruning \u2014 meaning that the output of multiple DTs is combined towards the final classification - a slightly lower radius is employed, while the multiplier is considerably higher, thus leading to more neighbors being considered for each pixel. As will be furtherComputers in Biology and Medicine 185 (2025) 109520 5 C. Moroz-Dubenco et al. Table 3 Best PCA parameters for classification methods. Extraction Pruning Selection No. of Alpha Kernel Gamma Degree components LBPPre Sparse PCA",
    "full_text_length": 84216,
    "chunk_length": 1102
  },
  {
    "chunk_id": 473,
    "paper_filename": "cristiana_towards_an_interpretable_breast_cancer_detection_and_diagnosis_system.pdf",
    "paper_title": "Cristiana Towards An Interpretable Breast Cancer Detection And Diagnosis System",
    "chunk_index": 27,
    "total_chunks": 83,
    "text_content": "\u2013 \u2013 \u2013 Post Incremental PCA 2 \u2013 \u2013 \u2013 \u2013 Ensemble Sparse PCA 4 \u2013 \u2013 \u2013 \u2013 Fig. 4.LBP features extracted. explained in detail in Section 3, the increased number of neighbors proves to be beneficial to the overall performance of the system. The features extracted with LBP can be displayed to be analyzed by the user, as shown inFig. 4for the cropped ROI (presented inFig. 3(f)), using the parameters from the last row ofTable 2, in alignment with our efforts towards an interpretable system. 2.3.2. Gray-Leve",
    "full_text_length": 84216,
    "chunk_length": 1186
  },
  {
    "chunk_id": 474,
    "paper_filename": "cristiana_towards_an_interpretable_breast_cancer_detection_and_diagnosis_system.pdf",
    "paper_title": "Cristiana Towards An Interpretable Breast Cancer Detection And Diagnosis System",
    "chunk_index": 28,
    "total_chunks": 83,
    "text_content": "diagonal, respectively: 0\u25e6, 45\u25e6, 90\u25e6, and 135\u25e6. The values of the features are computed separately for each angle and then the mean is calculated. As we apply the feature extraction only to the region of interest resulting from the segmentation process, which is the brightest area of the breast, we presume that normalizing the pixels to a smaller interval can benefit the classification output. We experiment with seven different levels, ranging from 21to28(the maximum gray level for 8-bit deep im",
    "full_text_length": 84216,
    "chunk_length": 1314
  },
  {
    "chunk_id": 475,
    "paper_filename": "cristiana_towards_an_interpretable_breast_cancer_detection_and_diagnosis_system.pdf",
    "paper_title": "Cristiana Towards An Interpretable Breast Cancer Detection And Diagnosis System",
    "chunk_index": 29,
    "total_chunks": 83,
    "text_content": "Non-Uniformity (GLN) =1 \ud835\udc41\ud835\udc5f\ud835\udc62\ud835\udc5b\ud835\udc60\u2211\ud835\udc41\ud835\udc54 \ud835\udc57=1[\ud835\udc5d\ud835\udc54(\ud835\udc57)]2, 4.Run Length Non-Uniformity (RLN) =1 \ud835\udc41\ud835\udc5f\ud835\udc62\ud835\udc5b\ud835\udc60\u2211\ud835\udc41\ud835\udc54 \ud835\udc57=1[\ud835\udc5d\ud835\udc5f(\ud835\udc57)]2and 5.Run Percentage (RP) =\ud835\udc41\ud835\udc5f\ud835\udc62\ud835\udc5b\ud835\udc60 \ud835\udc41\ud835\udc5d\ud835\udc56\ud835\udc65\ud835\udc52\ud835\udc59 \ud835\udc60. Although GLRLM features cannot be analyzed in a visual manner, the matrix can be inspected, and given the formula for each of the features, the process of feature extraction can be easily understood.2.3.3. Geometrical The shape features are extracted from the mask of the lesion, inde- pendently of the intensity of the pixels. First, geometrical feature",
    "full_text_length": 84216,
    "chunk_length": 1336
  },
  {
    "chunk_id": 476,
    "paper_filename": "cristiana_towards_an_interpretable_breast_cancer_detection_and_diagnosis_system.pdf",
    "paper_title": "Cristiana Towards An Interpretable Breast Cancer Detection And Diagnosis System",
    "chunk_index": 30,
    "total_chunks": 83,
    "text_content": "shape characteristics by fitting an ellipse to the tumor as shown inFig. 5(b). After the ellipse is defined, the difference between the border of the lesion and the ellipse is calculated (\ud835\udee5\ud835\udc51, presented in Fig. 5(a)). Generally, benign lesions have more regular borders, result- ing in lower differences, while malignant lesions have more irregular borders, resulting in higher differences. The final features are extracted as follows: 1.Root Mean Square Roughness =\u221a \u27e8\ud835\udee5\ud835\udc512\u27e9\u2212\u27e8\ud835\udee5\ud835\udc51\u27e92 2.Root Mean Square Sl",
    "full_text_length": 84216,
    "chunk_length": 1328
  },
  {
    "chunk_id": 477,
    "paper_filename": "cristiana_towards_an_interpretable_breast_cancer_detection_and_diagnosis_system.pdf",
    "paper_title": "Cristiana Towards An Interpretable Breast Cancer Detection And Diagnosis System",
    "chunk_index": 31,
    "total_chunks": 83,
    "text_content": "as shown inFig. 5. To fa- cilitate analysis of the characteristics, the segments can also be shown. 2.4. Feature selection Feature selection aims to identify the most relevant features from the set of extracted features obtained from the segmented mammo- graphic images. The selected features are subsequently used for the classification and identification of potential malignancies, which can aid radiologists in accurately diagnosing breast cancer. We analyze and experiment with three techniques (",
    "full_text_length": 84216,
    "chunk_length": 1299
  },
  {
    "chunk_id": 478,
    "paper_filename": "cristiana_towards_an_interpretable_breast_cancer_detection_and_diagnosis_system.pdf",
    "paper_title": "Cristiana Towards An Interpretable Breast Cancer Detection And Diagnosis System",
    "chunk_index": 32,
    "total_chunks": 83,
    "text_content": "comes to GA, the experiments presented in [5] clearly show that PCAComputers in Biology and Medicine 185 (2025) 109520 6 C. Moroz-Dubenco et al. Fig. 5.Computing featured from the contour of the lesion. outperforms GA when applied on the mini-MIAS dataset, and therefore we decided not to analyze it further. PCA reduces dimensionality by centering the data and applying singular value decomposition (SVD) to it. To assess the importance of data centering, we also conduct experiments using SVD as fe",
    "full_text_length": 84216,
    "chunk_length": 1315
  },
  {
    "chunk_id": 479,
    "paper_filename": "cristiana_towards_an_interpretable_breast_cancer_detection_and_diagnosis_system.pdf",
    "paper_title": "Cristiana Towards An Interpretable Breast Cancer Detection And Diagnosis System",
    "chunk_index": 33,
    "total_chunks": 83,
    "text_content": "matrix, and only the features with the highest eigenvalues are kept. In this manner, PCA increases the interpretability of the data, while preserving only the essential information. In our experiments, we also employ three variants of PCA: Kernel PCA, which utilizes kernels to achieve non-linear dimensionality reduc- tion, Sparse PCA, which finds a group of sparse constituents that can efficiently reconstruct the data, andIncremental PCA, which performs linear dimensionality reduction using Sing",
    "full_text_length": 84216,
    "chunk_length": 1298
  },
  {
    "chunk_id": 480,
    "paper_filename": "cristiana_towards_an_interpretable_breast_cancer_detection_and_diagnosis_system.pdf",
    "paper_title": "Cristiana Towards An Interpretable Breast Cancer Detection And Diagnosis System",
    "chunk_index": 34,
    "total_chunks": 83,
    "text_content": "different types of kernels, namely linear ,polynomial , sigmoid andcosine , different values in the range of5\u22c510\u22124\u22121 \ud835\udc5b\ud835\udc5c. \ud835\udc5c\ud835\udc53 \ud835\udc53 \ud835\udc52\ud835\udc4e\ud835\udc61\ud835\udc62\ud835\udc5f\ud835\udc52\ud835\udc60 for thepolynomial andsigmoid kernels coefficient (denoted as gamma) and with degrees in the interval of[1 \u2212 5]for thepolynomial kernel. Table 3presents the combinations for which the highest metrics values were achieved. For a better understanding of the feature selection process, the ratio of variance\ud835\udc52\ud835\udc56\ud835\udc54 \ud835\udc52\ud835\udc5b\ud835\udc63\ud835\udc4e\ud835\udc59 \ud835\udc62\ud835\udc52 \ud835\udc61\ud835\udc5c\ud835\udc61\ud835\udc4e\ud835\udc59 \ud835\udc52\ud835\udc56\ud835\udc54 \ud835\udc52\ud835\udc5b\ud835\udc63\ud835\udc4e\ud835\udc59 \ud835\udc62\ud835\udc52\ud835\udc60can be examined. Fig. 6holds th",
    "full_text_length": 84216,
    "chunk_length": 1366
  },
  {
    "chunk_id": 481,
    "paper_filename": "cristiana_towards_an_interpretable_breast_cancer_detection_and_diagnosis_system.pdf",
    "paper_title": "Cristiana Towards An Interpretable Breast Cancer Detection And Diagnosis System",
    "chunk_index": 35,
    "total_chunks": 83,
    "text_content": "Singular Value Decomposition (SVD) is a factorization of a matrix that comes from the field of linear algebra. Specifically, a matrix of order\ud835\udc5a\u00d7\ud835\udc5bis represented as the product of three matrices:\ud835\udc34[\ud835\udc5a\u00d7\ud835\udc5b]= \ud835\udc48[\ud835\udc5a\u00d7\ud835\udc5f]\ud835\udef4[\ud835\udc5f\u00d7\ud835\udc5f](\ud835\udc49[\ud835\udc5b\u00d7\ud835\udc5f])\ud835\udc47, where\ud835\udc5fis the rank. In our experiments, we employ a variant of the SVD, namely Trun- cated SVD, which finds a reduced rank approximation by setting all the singular values to 0, except from the\ud835\udc58largest ones. In order to find the best suited\ud835\udc58value for our needs, we experiment w",
    "full_text_length": 84216,
    "chunk_length": 1273
  },
  {
    "chunk_id": 482,
    "paper_filename": "cristiana_towards_an_interpretable_breast_cancer_detection_and_diagnosis_system.pdf",
    "paper_title": "Cristiana Towards An Interpretable Breast Cancer Detection And Diagnosis System",
    "chunk_index": 36,
    "total_chunks": 83,
    "text_content": "matrix formed by the matrices of between-class variance and data covariance. The highest eigenvalues correspond to a projection of the data onto a lower-dimensional space, which maximizes the separation between the classes. Since the maximum number of components for LDA is equal to\ud835\udc5a\ud835\udc56\ud835\udc5b(\ud835\udc5b\ud835\udc5c. \ud835\udc5c\ud835\udc53 \ud835\udc50 \ud835\udc59 \ud835\udc4e\ud835\udc60\ud835\udc60\ud835\udc52\ud835\udc60 \u2212 1, \ud835\udc5b\ud835\udc5c. \ud835\udc5c\ud835\udc53 \ud835\udc53 \ud835\udc52\ud835\udc4e\ud835\udc61\ud835\udc62\ud835\udc5f\ud835\udc52\ud835\udc60 )and, for our goal, we only have two classes, benign and malignant, the data will be reduced to 1 dimension.Computers in Biology and Medicine 185 (2025) 109520 7 C. Moroz-Dube",
    "full_text_length": 84216,
    "chunk_length": 1256
  },
  {
    "chunk_id": 483,
    "paper_filename": "cristiana_towards_an_interpretable_breast_cancer_detection_and_diagnosis_system.pdf",
    "paper_title": "Cristiana Towards An Interpretable Breast Cancer Detection And Diagnosis System",
    "chunk_index": 37,
    "total_chunks": 83,
    "text_content": "1e) are used due to their comprehensibility by humans. DTs are rule-based models that provide intuitive decision-making criteria and can be easily interpreted (as shown inFig. 7). In each node, a feature is selected and a criteria is established to split the dataset. The disadvantage of using DTs is their tendency to overfit the data, which can lead to poor generalization. To overcome this issue we propose three methods: (1) pre-pruning, (2) post-pruning and (3) ensemble (RF). Once the DT is bui",
    "full_text_length": 84216,
    "chunk_length": 1220
  },
  {
    "chunk_id": 484,
    "paper_filename": "cristiana_towards_an_interpretable_breast_cancer_detection_and_diagnosis_system.pdf",
    "paper_title": "Cristiana Towards An Interpretable Breast Cancer Detection And Diagnosis System",
    "chunk_index": 38,
    "total_chunks": 83,
    "text_content": "greater the representation of the associated class. In each node, the first row presents the selected feature (in case of geometrical features: area, perimeter, and compactness) and the split value, the second row indicates the Gini impurity of the node, while the last two rows mark the total number of samples in the node and their distribution.Table 5 Best DT hyper-parameters. Features Depth Samples-split Samples-leaf LBP 5 2 1 GLRLM 5 2 2 Geometrical 6 2 3 Contour-based 2 2 1 2.5.1. Pre-prunin",
    "full_text_length": 84216,
    "chunk_length": 1189
  },
  {
    "chunk_id": 485,
    "paper_filename": "cristiana_towards_an_interpretable_breast_cancer_detection_and_diagnosis_system.pdf",
    "paper_title": "Cristiana Towards An Interpretable Breast Cancer Detection And Diagnosis System",
    "chunk_index": 39,
    "total_chunks": 83,
    "text_content": "to split an internal node (samples-split) and the minimum number of samples required to be at a leaf node (samples-leaf), should be tuned. As a means to finding the best set of hyper-parameters, we experi- ment with values in the range of[1 \u2212 9]for the maximum depth, also considering the variant of not limiting the depth (denoted asNone), and with values in the range of[1 \u2212 5]for samples-split and samples-leaf. The results of the experiments are presented inTable 5. 2.5.2. Post-pruning Post-prun",
    "full_text_length": 84216,
    "chunk_length": 1308
  },
  {
    "chunk_id": 486,
    "paper_filename": "cristiana_towards_an_interpretable_breast_cancer_detection_and_diagnosis_system.pdf",
    "paper_title": "Cristiana Towards An Interpretable Breast Cancer Detection And Diagnosis System",
    "chunk_index": 40,
    "total_chunks": 83,
    "text_content": "the values used in our experiments are presented inTable 6. 2.5.3. Ensemble Ensemble-based methods involve combining multiple models. Ran- dom Forests (RFs) are ensembles of decision trees that combine the output of multiple trees to improve classification performance and minimize overfitting. To make the final decision, the class with the highest number of votes is selected.Computers in Biology and Medicine 185 (2025) 109520 8 C. Moroz-Dubenco et al. Table 6 Best DT cost-complexity parameter. F",
    "full_text_length": 84216,
    "chunk_length": 1291
  },
  {
    "chunk_id": 487,
    "paper_filename": "cristiana_towards_an_interpretable_breast_cancer_detection_and_diagnosis_system.pdf",
    "paper_title": "Cristiana Towards An Interpretable Breast Cancer Detection And Diagnosis System",
    "chunk_index": 41,
    "total_chunks": 83,
    "text_content": "that RFs, although comprising multiple DTs, do not lose the interpretability characteristic \u2014 not only can all the trees be displayed similar to the one inFig. 7, but they can also be grouped according to the predicted class, such that, once the diagnosis is set based on the majority votes, a single tree from this majority can be displayed and examined by the user. 3. Experimental results The goal of the current paper is the development of a fully auto- mated CAD system to help radiologists in t",
    "full_text_length": 84216,
    "chunk_length": 1275
  },
  {
    "chunk_id": 488,
    "paper_filename": "cristiana_towards_an_interpretable_breast_cancer_detection_and_diagnosis_system.pdf",
    "paper_title": "Cristiana Towards An Interpretable Breast Cancer Detection And Diagnosis System",
    "chunk_index": 42,
    "total_chunks": 83,
    "text_content": "ods (LBP, GLRLM, Geometrical and Contour-based), three feature se- lection methods (PCA, SVD and LDA) and three pruning techniques (pre-pruning, post-pruning and ensemble-based) to avoid overfitting. Experiments were carried out for every combination of these methods. Considering only the methods, their combinations result in 36 exper- iments. When parameters are included, the number of combinations reaches into millions. Therefore, some parameters were selected based on preliminary experiments,",
    "full_text_length": 84216,
    "chunk_length": 1318
  },
  {
    "chunk_id": 489,
    "paper_filename": "cristiana_towards_an_interpretable_breast_cancer_detection_and_diagnosis_system.pdf",
    "paper_title": "Cristiana Towards An Interpretable Breast Cancer Detection And Diagnosis System",
    "chunk_index": 43,
    "total_chunks": 83,
    "text_content": "will use images that contain a single lesion. As a result, 57 images are used, 31 with malignant lesion, and 26 with benign lesions. To properly evaluate and compare the variants, the dataset is divided according to a 70%\u201330% train-test ratio and only the test results are taken into account; that is, the results obtained for new data. Once the best combination is chosen, the same test results are compared with results from the literature.3.3. Metrics The performance of the proposed approach is m",
    "full_text_length": 84216,
    "chunk_length": 1235
  },
  {
    "chunk_id": 490,
    "paper_filename": "cristiana_towards_an_interpretable_breast_cancer_detection_and_diagnosis_system.pdf",
    "paper_title": "Cristiana Towards An Interpretable Breast Cancer Detection And Diagnosis System",
    "chunk_index": 44,
    "total_chunks": 83,
    "text_content": "of benign lesions correctly labeled, pre- sented in Eq. (4) Specificity =\ud835\udc47 \ud835\udc41 \ud835\udc47 \ud835\udc41+\ud835\udc39 \ud835\udc43(4) where\ud835\udc47 \ud835\udc43is the number of malignant lesions classified correctly,\ud835\udc47 \ud835\udc41 is the number of benign lesions classified correctly,\ud835\udc39 \ud835\udc43is the number of misclassified benign lesions as malignant, and\ud835\udc39 \ud835\udc41is the number of misclassified malignant lesions as benign. 3.4. Results Table 8details the results achieved with the different feature ex- traction and feature selection combinations. In the presented approach, two types ",
    "full_text_length": 84216,
    "chunk_length": 1289
  },
  {
    "chunk_id": 491,
    "paper_filename": "cristiana_towards_an_interpretable_breast_cancer_detection_and_diagnosis_system.pdf",
    "paper_title": "Cristiana Towards An Interpretable Breast Cancer Detection And Diagnosis System",
    "chunk_index": 45,
    "total_chunks": 83,
    "text_content": "with pre-pruning and RF (ensemble-based pruning), as well as SVD and pre-pruning. However, for these cases, the specificity score achieved with GLRLM is much lower than the one obtained with LBP features, with a difference of at least 30%, while the gain in sensitivity is of 10%. This suggests that using GLRLM features lead to a more imbalanced sensitivity- specificity trade-off. A GLRLM-based system will correctly determine more malignant tumors, but will generate more false positives, which, i",
    "full_text_length": 84216,
    "chunk_length": 1306
  },
  {
    "chunk_id": 492,
    "paper_filename": "cristiana_towards_an_interpretable_breast_cancer_detection_and_diagnosis_system.pdf",
    "paper_title": "Cristiana Towards An Interpretable Breast Cancer Detection And Diagnosis System",
    "chunk_index": 46,
    "total_chunks": 83,
    "text_content": "completely identical with the actual lesion and, thus, the contours may slightly differ. To reduce the size of the DTs it is essential to decrease the number of input features. Therefore, feature selection is applied to keep the most significant characteristics extracted from the ROI. From the findings outlined inTable 8arises that PCA outperforms both SVD and LDA. In our experiments, different pruning methods are used to overcome overfitting of the built models, as mentioned in Section 2.5. Bas",
    "full_text_length": 84216,
    "chunk_length": 1319
  },
  {
    "chunk_id": 493,
    "paper_filename": "cristiana_towards_an_interpretable_breast_cancer_detection_and_diagnosis_system.pdf",
    "paper_title": "Cristiana Towards An Interpretable Breast Cancer Detection And Diagnosis System",
    "chunk_index": 47,
    "total_chunks": 83,
    "text_content": "109520 9 C. Moroz-Dubenco et al. Table 8 Evaluation metrics for different combinations. Extraction Selection Pruning Accuracy Precision Sensitivity Specificity LBPPCAPre 0.85 0.89 0.80 0.90 Post 0.85 0.82 0.90 0.80 Ensemble 0.95 1.00 0.90 1.00 SVDPre 0.80 0.88 0.70 0.90 Post 0.80 1.00 0.60 1.00 Ensemble 0.80 0.88 0.70 0.90 LDAPre 0.75 1.00 0.50 1.00 Post 0.70 0.83 0.50 0.90 Ensemble 0.45 0.44 0.40 0.50 GLRMPCAPre 0.75 0.69 0.90 0.60 Post 0.70 0.67 0.80 0.60 Ensemble 0.80 0.70 1.00 0.60 SVDPre 0.",
    "full_text_length": 84216,
    "chunk_length": 1161
  },
  {
    "chunk_id": 494,
    "paper_filename": "cristiana_towards_an_interpretable_breast_cancer_detection_and_diagnosis_system.pdf",
    "paper_title": "Cristiana Towards An Interpretable Breast Cancer Detection And Diagnosis System",
    "chunk_index": 48,
    "total_chunks": 83,
    "text_content": "Ensemble 0.50 0.50 0.50 0.50 Contour-basedPCAPre 0.75 0.75 0.75 0.70 Post 0.75 0.75 0.75 0.70 Ensemble 0.85 0.88 0.85 1.0 SVDPre 0.75 0.75 0.75 0.80 Post 0.80 0.80 0.80 0.80 Ensemble 0.85 0.85 0.85 0.90 LDAPre 0.80 0.85 0.80 0.60 Post 0.80 0.85 0.80 0.60 Ensemble 0.90 0.91 0.90 0.80 Fig. 8.Confusion matrix for the best-performing configuration on the test images. B stands for benign andMstands for malignant. 4. Discussion The paper introduces an interpretable and fully automated CAD system for b",
    "full_text_length": 84216,
    "chunk_length": 1250
  },
  {
    "chunk_id": 495,
    "paper_filename": "cristiana_towards_an_interpretable_breast_cancer_detection_and_diagnosis_system.pdf",
    "paper_title": "Cristiana Towards An Interpretable Breast Cancer Detection And Diagnosis System",
    "chunk_index": 49,
    "total_chunks": 83,
    "text_content": "of this study is to develop an interpretable CAD system. As detailed in Sections 2and3, the proposed system is capable of providing a visual explanation for every step of the system while achieving an accuracy of 95% (with only one misclassification due to the limited number of inputs). There is a trade-off between a system\u2019sperformance and interpretability. While artificial neural networks typ- ically offer superior performance, their decision-making process is not transparent, which is why the",
    "full_text_length": 84216,
    "chunk_length": 1285
  },
  {
    "chunk_id": 496,
    "paper_filename": "cristiana_towards_an_interpretable_breast_cancer_detection_and_diagnosis_system.pdf",
    "paper_title": "Cristiana Towards An Interpretable Breast Cancer Detection And Diagnosis System",
    "chunk_index": 50,
    "total_chunks": 83,
    "text_content": "we can conclude that our system can still be improved. It is important to mention that, given the data employed in our experiments, the decrease in sensitivity is due to only one malignant lesion being incorrectly labeled. From Table 9, we can also note that the usage of multiple trees (i.e. ensemble-based pruning) does not guarantee an increased perfor- mance, as the accuracy obtained in [11] using a single DT, equal to the one achieved with our proposed system, is higher than the values ob- ta",
    "full_text_length": 84216,
    "chunk_length": 1236
  },
  {
    "chunk_id": 497,
    "paper_filename": "cristiana_towards_an_interpretable_breast_cancer_detection_and_diagnosis_system.pdf",
    "paper_title": "Cristiana Towards An Interpretable Breast Cancer Detection And Diagnosis System",
    "chunk_index": 51,
    "total_chunks": 83,
    "text_content": "treatment directly, without further investigation. Unless a perfect system is created, there is always going to be a trade-off between sensitivity and precision, and we consider that the end-users should decide which one is more important. When comparing our method to neural networks, the comparison is not straightforward due to the different data used in the trainingComputers in Biology and Medicine 185 (2025) 109520 10 C. Moroz-Dubenco et al. Table 9 Comparison of the results obtained with dif",
    "full_text_length": 84216,
    "chunk_length": 1287
  },
  {
    "chunk_id": 498,
    "paper_filename": "cristiana_towards_an_interpretable_breast_cancer_detection_and_diagnosis_system.pdf",
    "paper_title": "Cristiana Towards An Interpretable Breast Cancer Detection And Diagnosis System",
    "chunk_index": 52,
    "total_chunks": 83,
    "text_content": "Removed step Accuracy Precision Sensitivity Specificity None 0.95 1.00 0.90 1.00 Feature selection 0.90 1.00 0.80 1.00 Feature extraction 0.40 0.42 0.50 0.30 Segmentation 0.65 0.71 0.50 0.80 Pre-processing 0.65 0.67 0.60 0.70 process. On the other hand, our approach is fully interpretable, while the mentioned neural networks are explainable. There is a fine line between interpretability and explainability as detailed in Section 1. We have chosen to prioritize interpretability, as we believe that",
    "full_text_length": 84216,
    "chunk_length": 1359
  },
  {
    "chunk_id": 499,
    "paper_filename": "cristiana_towards_an_interpretable_breast_cancer_detection_and_diagnosis_system.pdf",
    "paper_title": "Cristiana Towards An Interpretable Breast Cancer Detection And Diagnosis System",
    "chunk_index": 53,
    "total_chunks": 83,
    "text_content": "an ensemble of decision trees, resulting in what is knows as Random Forest classifier, drastically increases the prediction accuracy, while maintaining the transparency property of the individual decision trees. According to Dam et al. [45], ensemble methods provide the highest prediction accuracy after deep learning methods, and decision trees, the highest explainability. Thus, we consider our choice of using an ensemble of decision trees to be a good approach to balancing this trade-off, by in",
    "full_text_length": 84216,
    "chunk_length": 1321
  },
  {
    "chunk_id": 500,
    "paper_filename": "cristiana_towards_an_interpretable_breast_cancer_detection_and_diagnosis_system.pdf",
    "paper_title": "Cristiana Towards An Interpretable Breast Cancer Detection And Diagnosis System",
    "chunk_index": 54,
    "total_chunks": 83,
    "text_content": "prove the importance of each step, we perform an ablation study, keeping the configuration that yielded the best results, as reported in Section 3.4, and removing each transitional step at a time. Table 10presents the results of the ablation study. The first row contains the results obtained with the proposed system, using the following configuration, as explained in Section 2: 1.pre-processing: removal of noise and artifacts, pectoral muscle removal, and contrast-limited adaptive histogram equa",
    "full_text_length": 84216,
    "chunk_length": 1370
  },
  {
    "chunk_id": 501,
    "paper_filename": "cristiana_towards_an_interpretable_breast_cancer_detection_and_diagnosis_system.pdf",
    "paper_title": "Cristiana Towards An Interpretable Breast Cancer Detection And Diagnosis System",
    "chunk_index": 55,
    "total_chunks": 83,
    "text_content": "Thus, rows 2\u20135 ofTable 10hold the results obtained when removing all the other steps, as follows: feature selection, feature extraction, segmentation, and pre-processing, respectively. For the first case, when feature selection is no longer applied, the precision and specificity scores are not affected, while sensitivity de- creases by 10%, leading to a 5% decrease in accuracy. This proves that selecting the most relevant features leads to more malignant tumors being correctly identified. When f",
    "full_text_length": 84216,
    "chunk_length": 1314
  },
  {
    "chunk_id": 502,
    "paper_filename": "cristiana_towards_an_interpretable_breast_cancer_detection_and_diagnosis_system.pdf",
    "paper_title": "Cristiana Towards An Interpretable Breast Cancer Detection And Diagnosis System",
    "chunk_index": 56,
    "total_chunks": 83,
    "text_content": "significantly lower values not only compared to those obtained with the entire system, but also when the feature selection step is removed (first and second rows ofTable 10, respectively). Hence, we can state that, although not as much as feature selection, segmentation plays a very important role in the overall system. Finally, if we remove the pre-processing step, it means that segmen- tation is applied on the original mammogram. Comparing the results obtained in this manner to those obtained ",
    "full_text_length": 84216,
    "chunk_length": 1256
  },
  {
    "chunk_id": 503,
    "paper_filename": "cristiana_towards_an_interpretable_breast_cancer_detection_and_diagnosis_system.pdf",
    "paper_title": "Cristiana Towards An Interpretable Breast Cancer Detection And Diagnosis System",
    "chunk_index": 57,
    "total_chunks": 83,
    "text_content": "comparison between the segmentation obtained without pre-processing (Fig. 9(a)) and with pre-processing (Fig. 9(b)), with the ground truth depicted in blue inFig. 9(c). It is easy to notice, from this figure alone, the importance of the pre-processing: without this step, a label from the mammogram is segmented as the ROI, while the result of the segmentation after pre-processing is very close to the ground truth, considering that the ground truth provided with the dataset is an approximation. Ta",
    "full_text_length": 84216,
    "chunk_length": 1299
  },
  {
    "chunk_id": 504,
    "paper_filename": "cristiana_towards_an_interpretable_breast_cancer_detection_and_diagnosis_system.pdf",
    "paper_title": "Cristiana Towards An Interpretable Breast Cancer Detection And Diagnosis System",
    "chunk_index": 58,
    "total_chunks": 83,
    "text_content": "original file names and binary masks for the abnormalities. It consists of 1952 cases split into three categories: Normal ,Benign andCancer . Each case encompasses craniocaudal (CC) and mediolateral oblique (MLO) mammographies for a patient\u2019s left and right breasts and, if any abnormality is present, its binary mask. Fig. 10Computers in Biology and Medicine 185 (2025) 109520 11 C. Moroz-Dubenco et al. Fig. 9.Results of segmentation applied on the original and pre-processed mammogram. Fig. 10.Mam",
    "full_text_length": 84216,
    "chunk_length": 1291
  },
  {
    "chunk_id": 505,
    "paper_filename": "cristiana_towards_an_interpretable_breast_cancer_detection_and_diagnosis_system.pdf",
    "paper_title": "Cristiana Towards An Interpretable Breast Cancer Detection And Diagnosis System",
    "chunk_index": 59,
    "total_chunks": 83,
    "text_content": "for the methods achieving the best results on mini-MIAS: 1. noise, artifacts and pectoral mus- cle removal and CLAHE for pre-processing; 2. TbGC with automated seeds selection for segmentation; 3. LBP for feature extraction; 4. PCA for feature selection; and 5. RF (DT with ensemble-based pruning) for classification -, However, the image acquisition process differs across datasets, due to the type of machine used. In order to find the best-suited parameter values for every method, another grid se",
    "full_text_length": 84216,
    "chunk_length": 1281
  },
  {
    "chunk_id": 506,
    "paper_filename": "cristiana_towards_an_interpretable_breast_cancer_detection_and_diagnosis_system.pdf",
    "paper_title": "Cristiana Towards An Interpretable Breast Cancer Detection And Diagnosis System",
    "chunk_index": 60,
    "total_chunks": 83,
    "text_content": "more iterations (\ud835\udf03\ud835\udc56= 25) for the mini-DDSM images in order to achieve satisfactory results, as shown in [29]. For feature extraction, two out of the three LBP\u2019s parameters are modified, using nri_uniform variant instead of thedefault one, and without multiplication (the multiplier is equal to 1). For the next step, the same variant of PCA is employed - Kernel PCA -, using also a3\ud835\udc5f\ud835\udc51degree polynomial kernel; however, for mini-DDSM, a lower value is required for gamma (\ud835\udefe= 0.005). Moreover, the most",
    "full_text_length": 84216,
    "chunk_length": 1293
  },
  {
    "chunk_id": 507,
    "paper_filename": "cristiana_towards_an_interpretable_breast_cancer_detection_and_diagnosis_system.pdf",
    "paper_title": "Cristiana Towards An Interpretable Breast Cancer Detection And Diagnosis System",
    "chunk_index": 61,
    "total_chunks": 83,
    "text_content": "Fig. 11presents the results of the intermediate steps, with Fig. 11(a) holding the result of the pre-processing step, Fig. 11(b) holding the result of segmentation, andFig. 11(c) holding the extracted features. Using the parameters detailed inTable 11, the test results obtained on the images from mini-DDSM, after a 70%\u201330% train-test split, are as follows: \u2022Accuracy : 0.97, \u2022Precision : 0.95, \u2022Sensitivity : 1.00, \u2022Specificity : 0.95. These results prove the robustness of our proposed approach. T",
    "full_text_length": 84216,
    "chunk_length": 1315
  },
  {
    "chunk_id": 508,
    "paper_filename": "cristiana_towards_an_interpretable_breast_cancer_detection_and_diagnosis_system.pdf",
    "paper_title": "Cristiana Towards An Interpretable Breast Cancer Detection And Diagnosis System",
    "chunk_index": 62,
    "total_chunks": 83,
    "text_content": "was classified as benign \u2014 thus, the other way around. However, as previously highlighted, the trade-off between sensitivity and precision will always be present, and the decision on which of these metrics weighs heavier should belong to the end-users.Computers in Biology and Medicine 185 (2025) 109520 12 C. Moroz-Dubenco et al. Table 11 Parameters of the proposed system with their values for mini-MIAS and mini-DDSM. Step Method Parameter Mini-MIAS Mini-DDSM Pre-processing Thresholding Threshold",
    "full_text_length": 84216,
    "chunk_length": 1298
  },
  {
    "chunk_id": 509,
    "paper_filename": "cristiana_towards_an_interpretable_breast_cancer_detection_and_diagnosis_system.pdf",
    "paper_title": "Cristiana Towards An Interpretable Breast Cancer Detection And Diagnosis System",
    "chunk_index": 63,
    "total_chunks": 83,
    "text_content": "we experiment with a dataset almost triple in size than the one on which our approach was originally validated, it still misclassifies only one image. Therefore, we can conclude that the proposed system can be easily adapted for different datasets by changing only the values of some parameters, while maintaining high performance. 4.4. Clinical viewpoint While designing the proposed system for breast cancer detection and diagnosis from mammographies, we have collaborated with radiologists from th",
    "full_text_length": 84216,
    "chunk_length": 1293
  },
  {
    "chunk_id": 510,
    "paper_filename": "cristiana_towards_an_interpretable_breast_cancer_detection_and_diagnosis_system.pdf",
    "paper_title": "Cristiana Towards An Interpretable Breast Cancer Detection And Diagnosis System",
    "chunk_index": 64,
    "total_chunks": 83,
    "text_content": "similar approach for mammogram analysis would be useful. The \u2018\u2018perfect\u2019\u2019 system should have two components (as most CAD systems do): detection and diagnosis. For the first part, it should be able to examine the images without any input from the user, yielding a segmented region of interest in near real-time (NRT), such that the radiologists can inspect the already annotated mammograms. This way, they would just need to validate the output and manually segment the images only if the system missed",
    "full_text_length": 84216,
    "chunk_length": 1247
  },
  {
    "chunk_id": 511,
    "paper_filename": "cristiana_towards_an_interpretable_breast_cancer_detection_and_diagnosis_system.pdf",
    "paper_title": "Cristiana Towards An Interpretable Breast Cancer Detection And Diagnosis System",
    "chunk_index": 65,
    "total_chunks": 83,
    "text_content": "although usually the malignant ones have similar characteristics, they can resemble benign abnormalities enough to deceive even experts. This is where the CAD system comes in: being able to examine a lesion at pixel-level and to discover various patterns and connections that might not be visible to the naked eye. Although any system with the previously described capabilities would be helpful, due to the different machines being used for taking the mammograms, the need for a generalizable system ",
    "full_text_length": 84216,
    "chunk_length": 1252
  },
  {
    "chunk_id": 512,
    "paper_filename": "cristiana_towards_an_interpretable_breast_cancer_detection_and_diagnosis_system.pdf",
    "paper_title": "Cristiana Towards An Interpretable Breast Cancer Detection And Diagnosis System",
    "chunk_index": 66,
    "total_chunks": 83,
    "text_content": "require user input, but it can also be easily adapted to work on different datasets, as proven in Section 4.3. Due to the limitation of the employed TbGC algorithm to only 5 iterations, the region of interest is detected in NRT, while the fact that the method does not require any prior training reduces the amount of resources needed. The extraction (and selection) of textural features comes to meet the need for an in-depth analysis of a lesion\u2019s structure. In this manner, characteristics that ma",
    "full_text_length": 84216,
    "chunk_length": 1215
  },
  {
    "chunk_id": 513,
    "paper_filename": "cristiana_towards_an_interpretable_breast_cancer_detection_and_diagnosis_system.pdf",
    "paper_title": "Cristiana Towards An Interpretable Breast Cancer Detection And Diagnosis System",
    "chunk_index": 67,
    "total_chunks": 83,
    "text_content": "adapt real-life data to include the required information. To the best of our knowledge, no mammogram dataset with detailed labeling exists. Yet, as a means to ensure that the radiologists understand how our proposed approach works and, also, that they would feel confident to use it, we asked the coordinator of the Radiology and medical imag- ing laboratory from the \u2018\u2018Prof. Dr. Ion Chiricu\u0163\u0103\u2019\u2019 Oncology Institute to evaluate the system. As a means to this, we asked them to rate each step of our ap",
    "full_text_length": 84216,
    "chunk_length": 1255
  },
  {
    "chunk_id": 514,
    "paper_filename": "cristiana_towards_an_interpretable_breast_cancer_detection_and_diagnosis_system.pdf",
    "paper_title": "Cristiana Towards An Interpretable Breast Cancer Detection And Diagnosis System",
    "chunk_index": 68,
    "total_chunks": 83,
    "text_content": "mistake (if applicable). For each question, they were asked to rate their level of understanding on a 1 to 5 scale, with 5 meaning complete understanding. The results for each step are presented inFig. 12. As it can be easily noticed, the pre-processing methodology is abso- lutely clear, as the radiologist not only understands the inner workings but is also confident to manually modify the output if needed. For the segmentation step, some difficulties appear when it comes to the actual region gr",
    "full_text_length": 84216,
    "chunk_length": 1279
  },
  {
    "chunk_id": 515,
    "paper_filename": "cristiana_towards_an_interpretable_breast_cancer_detection_and_diagnosis_system.pdf",
    "paper_title": "Cristiana Towards An Interpretable Breast Cancer Detection And Diagnosis System",
    "chunk_index": 69,
    "total_chunks": 83,
    "text_content": "visual interpretation of the feature extraction and feature selection processes, we find this evaluation encouraging towards an interpretable CAD system. 5. Conclusion and future work In this paper, we advanced an automated interpretable breast cancer detection and diagnosis system, with the means of serving as a second opinion to doctors who analyze and interpret mammographic images. The system can be divided into five easy-to-understand, yet robust steps, which can be displayed in such a manne",
    "full_text_length": 84216,
    "chunk_length": 1349
  },
  {
    "chunk_id": 516,
    "paper_filename": "cristiana_towards_an_interpretable_breast_cancer_detection_and_diagnosis_system.pdf",
    "paper_title": "Cristiana Towards An Interpretable Breast Cancer Detection And Diagnosis System",
    "chunk_index": 70,
    "total_chunks": 83,
    "text_content": "Principal Component Analysis. These features serve as inputto a Random Forest classifier, which yields the final result: benign or malignant lesion. The proposed system is distinguished by its interpretability, achieved through the visual explanation of each processing step. Com- pared to Artificial Neural Networks, it is easier and quicker to train, which is crucial in healthcare, where datasets are often limited in size. Additionally, the Threshold-based GrowCut algorithm is integrated into th",
    "full_text_length": 84216,
    "chunk_length": 1350
  },
  {
    "chunk_id": 517,
    "paper_filename": "cristiana_towards_an_interpretable_breast_cancer_detection_and_diagnosis_system.pdf",
    "paper_title": "Cristiana Towards An Interpretable Breast Cancer Detection And Diagnosis System",
    "chunk_index": 71,
    "total_chunks": 83,
    "text_content": "dataset, namely mini-DDSM [10], obtaining 97% accuracy and 100% sensitivity, and have a radiologist evaluate it from an interpretability point of view, obtaining a satisfactory outcome. We also compared our proposed system with six other systems from the literature that use Decision Tree- based algorithms for classification, obtaining comparable or even better results. For future work, we aim to further validate our system using differ- ent datasets. Additionally, we plan to extend its capabilit",
    "full_text_length": 84216,
    "chunk_length": 1436
  },
  {
    "chunk_id": 518,
    "paper_filename": "cristiana_towards_an_interpretable_breast_cancer_detection_and_diagnosis_system.pdf",
    "paper_title": "Cristiana Towards An Interpretable Breast Cancer Detection And Diagnosis System",
    "chunk_index": 72,
    "total_chunks": 83,
    "text_content": "& editing, Validation, Supervision, Project administration. Camelia Chira: Writing \u2013 review & editing, Validation, Supervision. Declaration of competing interest The authors declare that they have no known competing finan- cial interests or personal relationships that could have appeared to influence the work reported in this paper. Acknowledgments This study was supported by the Ministry of Research, Innovation and Digitization, as Intermediary Body for the Operational Programme Competitiveness",
    "full_text_length": 84216,
    "chunk_length": 1610
  },
  {
    "chunk_id": 519,
    "paper_filename": "cristiana_towards_an_interpretable_breast_cancer_detection_and_diagnosis_system.pdf",
    "paper_title": "Cristiana Towards An Interpretable Breast Cancer Detection And Diagnosis System",
    "chunk_index": 73,
    "total_chunks": 83,
    "text_content": "https://joint-research-centre.ec.europa.eu/jrc- news-and-updates/cancer-cases-and-deaths-rise-eu-2023-10-02_en , Published on 02/10/2023. Accessed on 16/05/2024. [2]Alfredo Vellido, The importance of interpretability and visualization in machine learning for applications in medicine and health care, Neural Comput. Appl. 32 (24) (2020) 18069\u201318083, http://dx.doi.org/10.1007/s00521-019-04051-w .Computers in Biology and Medicine 185 (2025) 109520 14 C. Moroz-Dubenco et al. [3]Ramprasaath R. Selvara",
    "full_text_length": 84216,
    "chunk_length": 1786
  },
  {
    "chunk_id": 520,
    "paper_filename": "cristiana_towards_an_interpretable_breast_cancer_detection_and_diagnosis_system.pdf",
    "paper_title": "Cristiana Towards An Interpretable Breast Cancer Detection And Diagnosis System",
    "chunk_index": 74,
    "total_chunks": 83,
    "text_content": "classification from textural features, Stud. Univ. Babe\u015f-Bolyai Inform. 67 (2) (2023) 5\u201320, http://dx.doi.org/10.24193/subbi.2022.2.01 . [7]Cristiana Moroz-Dubenco, Laura Dio\u015fan, Anca Andreica, Mammography lesion detection using an improved GrowCut algorithm, Procedia Comput. Sci. 192 (2021) 308\u2013317, http://dx.doi.org/10.1016/j.procs.2021.08.032 , Knowledge- Based and Intelligent Information & Engineering Systems: Proceedings of the 25th International Conference KES2021. [8]Cristiana Moroz-Duben",
    "full_text_length": 84216,
    "chunk_length": 1660
  },
  {
    "chunk_id": 521,
    "paper_filename": "cristiana_towards_an_interpretable_breast_cancer_detection_and_diagnosis_system.pdf",
    "paper_title": "Cristiana Towards An Interpretable Breast Cancer Detection And Diagnosis System",
    "chunk_index": 75,
    "total_chunks": 83,
    "text_content": "Rajasekhara Babu, Classification of breast abnormality using decision tree based on GLCM features in mammograms, Int. J. Comput. Aided Eng. Technol. 10 (5) (2018) 504, http://dx.doi.org/10.1504/ijcaet.2018. 094328 . [12] Aleena Johny, Jincy J. Fernandez, Breast cancer detection in mammogram using fuzzy C-means and random forest classifier, Int. J. Sci. Res. Sci. Eng. Technol. 4 (8) (2018) 312\u2013321. [13] Sawsen Boudraa, Ahlem Melouah, Hayet Farida Merouani, Improving mass discrimination in mammogr",
    "full_text_length": 84216,
    "chunk_length": 1544
  },
  {
    "chunk_id": 522,
    "paper_filename": "cristiana_towards_an_interpretable_breast_cancer_detection_and_diagnosis_system.pdf",
    "paper_title": "Cristiana Towards An Interpretable Breast Cancer Detection And Diagnosis System",
    "chunk_index": 76,
    "total_chunks": 83,
    "text_content": "Januar, Classification of breast cancer tumors using a random forest on mammogram images, Appl. Med. Inform. 45 (1) (2023). [17] Ann-Kristin Balve, Peter Hendrix, Interpretable breast cancer classification using CNNs on mammographic images, 2024, http://dx.doi.org/10.48550/ARXIV.2408. 13154 . [18] Marco Tulio Ribeiro, Sameer Singh, Carlos Guestrin, \u2018\u2018Why should I trust you?\u2019\u2019: Explaining the predictions of any classifier, in: Proceedings of the 22nd ACM SIGKDD International Conference on Knowled",
    "full_text_length": 84216,
    "chunk_length": 1505
  },
  {
    "chunk_id": 523,
    "paper_filename": "cristiana_towards_an_interpretable_breast_cancer_detection_and_diagnosis_system.pdf",
    "paper_title": "Cristiana Towards An Interpretable Breast Cancer Detection And Diagnosis System",
    "chunk_index": 77,
    "total_chunks": 83,
    "text_content": "tiadis, Mohamed Seghier, Aamna Alshehhi, StethoNet: Robust breast cancer mammography classification framework, IEEE Access 12 (2024) 144890\u2013144904, http://dx.doi.org/10.1109/ACCESS.2024.3473010 . [21] Chunyan Cui, Li Li, Hongmin Cai, Zhihao Fan, Ling Zhang, Tingting Dan, Jiao Li, Jinghua Wang, The Chinese Mammography Database (CMMD): An Online Mammography Database with Biopsy Confirmed Types for Machine Diagnosis of Breast, The Cancer Imaging Archive, 2021, http://dx.doi.org/10.7937/TCIA. EQDE-4",
    "full_text_length": 84216,
    "chunk_length": 1648
  },
  {
    "chunk_id": 524,
    "paper_filename": "cristiana_towards_an_interpretable_breast_cancer_detection_and_diagnosis_system.pdf",
    "paper_title": "Cristiana Towards An Interpretable Breast Cancer Detection And Diagnosis System",
    "chunk_index": 78,
    "total_chunks": 83,
    "text_content": "images: Breast masses classification using ProtoPNet, in: Pattern Recognition, Computer Vision, and Image Processing. ICPR 2022 International Workshops and Chal- lenges, Springer Nature Switzerland, Cham, 2023, pp. 539\u2013557, http://dx.doi. org/10.1007/978-3-031-37660-3_38 . [25] Rebecca Sawyer-Lee, Francisco Gimenez, Assaf Hoogi, Daniel Rubin, Cu- rated breast imaging subset of digital database for screening mammography (CBIS-ddsm)[skup podataka], 2016, The Cancer Imaging Archive. [26] Ad\u00e9l Bajcs",
    "full_text_length": 84216,
    "chunk_length": 1681
  },
  {
    "chunk_id": 525,
    "paper_filename": "cristiana_towards_an_interpretable_breast_cancer_detection_and_diagnosis_system.pdf",
    "paper_title": "Cristiana Towards An Interpretable Breast Cancer Detection And Diagnosis System",
    "chunk_index": 79,
    "total_chunks": 83,
    "text_content": "Artificial Intelligent Systems, Springer Nature Switzerland, 2023, pp. 709\u2013720, http://dx.doi.org/10.1007/978-3-031-40725-3_60 . [30] Timo Ojala, Matti Pietik\u00e4inen, David Harwood, A comparative study of texture measures with classification based on featured distributions, Pattern Recognit. 29 (1) (1996) 51\u201359. [31] Timo Ojala, Matti Pietik\u00e4inen, Unsupervised texture segmentation using feature distributions, Pattern Recognit. 32 (3) (1999) 477\u2013486. [32] Timo Ojala, Matti Pietikainen, Topi Maenpaa",
    "full_text_length": 84216,
    "chunk_length": 1534
  },
  {
    "chunk_id": 526,
    "paper_filename": "cristiana_towards_an_interpretable_breast_cancer_detection_and_diagnosis_system.pdf",
    "paper_title": "Cristiana Towards An Interpretable Breast Cancer Detection And Diagnosis System",
    "chunk_index": 80,
    "total_chunks": 83,
    "text_content": "analysis using gray level run lengths, Comput. Graph. Image Process. 4 (2) (1975) 172\u2013179. [36] Haixia Li, Xianjing Meng, Tingwen Wang, Yuchun Tang, Yilong Yin, Breast masses in mammography classification with local contour features, BioMed. Eng. OnLine 16 (1) (2017) http://dx.doi.org/10.1186/s12938-017-0332-0 . [37] Ad\u00e9l Bajcsi, Anca Andreica, Camelia Chira, Significance of training images and feature extraction in lesion classification, in: Proceedings of the 16th International Conference on A",
    "full_text_length": 84216,
    "chunk_length": 1484
  },
  {
    "chunk_id": 527,
    "paper_filename": "cristiana_towards_an_interpretable_breast_cancer_detection_and_diagnosis_system.pdf",
    "paper_title": "Cristiana Towards An Interpretable Breast Cancer Detection And Diagnosis System",
    "chunk_index": 81,
    "total_chunks": 83,
    "text_content": "2 (11) (1901) 559\u2013572, http://dx.doi.org/10.1080/14786440109462720 . [40] Harold Hotelling, Analysis of a complex of statistical variables into principal components., J. Educ. Psychol. 24 (6) (1933) 417. [41] Keinosuke Fukunaga, Introduction to Statistical Pattern Recognition, Elsevier, 2013. [42] Ronald A. Fisher, The use of multiple measurements in taxonomic problems, Ann. Eugenics 7 (2) (1936) 179\u2013188. [43] Leo Breiman, Jerome Friedman, Richard Olshen, Charles Stone, Cart, in: Classi- ficatio",
    "full_text_length": 84216,
    "chunk_length": 1504
  },
  {
    "chunk_id": 528,
    "paper_filename": "cristiana_towards_an_interpretable_breast_cancer_detection_and_diagnosis_system.pdf",
    "paper_title": "Cristiana Towards An Interpretable Breast Cancer Detection And Diagnosis System",
    "chunk_index": 82,
    "total_chunks": 83,
    "text_content": "mammography, in: Digital Mammography: Nijmegen, 1998, Springer, 1998, pp. 457\u2013460. [47] Robin Hesse, Simone Schaub-Meyer, Stefan Roth, FunnyBirds: A synthetic vision dataset for a part-based analysis of explainable AI methods, in: ICCV, 2023, pp. 3981\u20133991, http://dx.doi.org/10.48550/arXiv.2308.06248 .Computers in Biology and Medicine 185 (2025) 109520 15",
    "full_text_length": 84216,
    "chunk_length": 357
  },
  {
    "chunk_id": 529,
    "paper_filename": "cynthia_2024_generating_colloquial_radiology_reports_with_large_language_models.pdf",
    "paper_title": "Cynthia 2024 Generating Colloquial Radiology Reports With Large Language Models",
    "chunk_index": 0,
    "total_chunks": 34,
    "text_content": "Research and Applications Generating colloquial radiology reports with large language models Cynthia Crystal Tang, BS\u2020,1, Supriya Nagesh, PhD\u2020,2, David A. Fussell , MD1,\ufffd, Justin Glavis-Bloom, MD1, Nina Mishra, PhD2, Charles Li, MD1, Gillean Cortes, DO1, Robert Hill, MD1, Jasmine Zhao, MD1, Angellica Gordon, MD1, Joshua Wright, MD1, Hayden Troutt, MPH1, Rod Tarrago, MD3, Daniel S. Chow, MD1 1Department of Radiological Sciences, University of California, Irvine, Irvine, CA 92868, United States, 2",
    "full_text_length": 35554,
    "chunk_length": 1453
  },
  {
    "chunk_id": 530,
    "paper_filename": "cynthia_2024_generating_colloquial_radiology_reports_with_large_language_models.pdf",
    "paper_title": "Cynthia 2024 Generating Colloquial Radiology Reports With Large Language Models",
    "chunk_index": 1,
    "total_chunks": 34,
    "text_content": "One solution is for radiologists to provide a \u201ccolloquial\u201d version that is accessible to the layperson. Because manually generating these colloquial translations would represent a significant burden for radiologists, a way to auto- matically produce accurate, accessible patient-facing reports is desired. We propose a novel method to produce colloquial translations of radiol - ogy reports by providing specialized prompts to a large language model (LLM). Materials and Methods: Our method automatic",
    "full_text_length": 35554,
    "chunk_length": 1421
  },
  {
    "chunk_id": 531,
    "paper_filename": "cynthia_2024_generating_colloquial_radiology_reports_with_large_language_models.pdf",
    "paper_title": "Cynthia 2024 Generating Colloquial Radiology Reports With Large Language Models",
    "chunk_index": 2,
    "total_chunks": 34,
    "text_content": "was 20% more accurate than the baseline method. Overall, translations were more readable than the original reports, as evaluated using standard readability indices. Conclusion: We find that our translations at the eighth-grade level strike an optimal balance between accuracy and readability. Notably, this cor- responds to nationally recognized recommendations for patient-facing health communication. We believe that using this approach to draft patient-accessible reports will benefit patients wit",
    "full_text_length": 35554,
    "chunk_length": 1452
  },
  {
    "chunk_id": 532,
    "paper_filename": "cynthia_2024_generating_colloquial_radiology_reports_with_large_language_models.pdf",
    "paper_title": "Cynthia 2024 Generating Colloquial Radiology Reports With Large Language Models",
    "chunk_index": 3,
    "total_chunks": 34,
    "text_content": "reports. These reports have historically been written to communicate with other health professionals using medical terminology that may not be readily understandable by patients and families.1 For example, 1 group found that standard radiology reports were written at greater than a 13th-grade reading level.2 Because many organizations release results rapidly to patients,3,4a patient may read a radiology report before the healthcare professional who ordered the study has had an opportunity to rev",
    "full_text_length": 35554,
    "chunk_length": 1382
  },
  {
    "chunk_id": 533,
    "paper_filename": "cynthia_2024_generating_colloquial_radiology_reports_with_large_language_models.pdf",
    "paper_title": "Cynthia 2024 Generating Colloquial Radiology Reports With Large Language Models",
    "chunk_index": 4,
    "total_chunks": 34,
    "text_content": "patient access to clinical content with the reality that patients may have difficulty comprehending that content. Some researchers have argued that the frequency of worry experienced by patients is low and that most patients prefer to receive results immediately.6Some have suggested that radiologists should generate a separate \u201cpatient-accessible\u201d summary, though existing high clinical volumes makes add- ing further tasks unappealing to radiologists.7Several organi - zations have experimented wi",
    "full_text_length": 35554,
    "chunk_length": 1539
  },
  {
    "chunk_id": 534,
    "paper_filename": "cynthia_2024_generating_colloquial_radiology_reports_with_large_language_models.pdf",
    "paper_title": "Cynthia 2024 Generating Colloquial Radiology Reports With Large Language Models",
    "chunk_index": 5,
    "total_chunks": 34,
    "text_content": "and Applications Downloaded from https://academic.oup.com/jamia/article/31/11/2660/7740004 by Georgia Institute of Technology user on 21 October 2024 definitions via hyperlinks, or to automatically replace informa - tion with standard concept names, though these approaches may lack specificity to individual patient contexts.8\u201310 More recently, there has been interest in leveraging large language models (LLMs) and generative Artificial Intelligence (Gen AI) to bridge gaps in physician-patient com",
    "full_text_length": 35554,
    "chunk_length": 1463
  },
  {
    "chunk_id": 535,
    "paper_filename": "cynthia_2024_generating_colloquial_radiology_reports_with_large_language_models.pdf",
    "paper_title": "Cynthia 2024 Generating Colloquial Radiology Reports With Large Language Models",
    "chunk_index": 6,
    "total_chunks": 34,
    "text_content": "inherent in a naive prompting strategy. We compare the accuracy, likability, and harm potential of translations generated by our method to those generated by a naive approach and measure readability of the translations using standardized scales. Finally, to verify quality and identify hallucinations, we describe a novel approach for mapping individual clauses from colloquial translations back to the portion of the original report from which they are derived. Methods Data description In this stud",
    "full_text_length": 35554,
    "chunk_length": 1360
  },
  {
    "chunk_id": 536,
    "paper_filename": "cynthia_2024_generating_colloquial_radiology_reports_with_large_language_models.pdf",
    "paper_title": "Cynthia 2024 Generating Colloquial Radiology Reports With Large Language Models",
    "chunk_index": 7,
    "total_chunks": 34,
    "text_content": "was to produce a collo - quial translation. We posited that a good colloquial transla - tion is concise, easy to understand, accurate, and comments primarily on the abnormal aspects in the report. Our solution was designed to meet the following require - ments: (1) Summarize the original report without clinical jar- gon; (2) Produce a translation that has readability at the given education level; and (3) Produce a granular attribution mapping from the translation to the original report. Responsi",
    "full_text_length": 35554,
    "chunk_length": 1255
  },
  {
    "chunk_id": 537,
    "paper_filename": "cynthia_2024_generating_colloquial_radiology_reports_with_large_language_models.pdf",
    "paper_title": "Cynthia 2024 Generating Colloquial Radiology Reports With Large Language Models",
    "chunk_index": 8,
    "total_chunks": 34,
    "text_content": "As a first attempt at a solution, we provided a prompt to an LLM instructing it to return a translation that meets the above requirements. To produce desirable translations, we provided few-shot examples, or demonstrations of reports and the corresponding translations, within the LLM prompt. See Figure S1 for an example of a prompt to produce a collo - quial translation given the original report, the desired educa - tion level, and few-shot examples. In this work, we used 5 original reports and ",
    "full_text_length": 35554,
    "chunk_length": 1273
  },
  {
    "chunk_id": 538,
    "paper_filename": "cynthia_2024_generating_colloquial_radiology_reports_with_large_language_models.pdf",
    "paper_title": "Cynthia 2024 Generating Colloquial Radiology Reports With Large Language Models",
    "chunk_index": 9,
    "total_chunks": 34,
    "text_content": "when compared to the long-form prompt used in the baseline method. Since LLMs such as Claude are designed to understand and generate natu- ral responses, pointed questions may reduce errors in long- form LLM responses.22 We implemented several additional enhancements to improve our results (Figure 1), including: Medical entity extraction The first step was to obtain a list of medical diagnoses from the reports using a medical entity extractor.23See Figure 2 for an example report and extracted en",
    "full_text_length": 35554,
    "chunk_length": 1203
  },
  {
    "chunk_id": 539,
    "paper_filename": "cynthia_2024_generating_colloquial_radiology_reports_with_large_language_models.pdf",
    "paper_title": "Cynthia 2024 Generating Colloquial Radiology Reports With Large Language Models",
    "chunk_index": 10,
    "total_chunks": 34,
    "text_content": "used to generate a defini - tion in simple terms for a given entity. Given a list of medical entities from the previous step, we prompted the LLM to gen- erate the definition for each of them. Figure 2 illustrates this step for entities extracted from the example report. Final translation The final step in generating the translation was providing the different entities and their definitions in addition to the origi- nal report in the prompt to the LLM. Figure S3 illustrates the prompt in the fin",
    "full_text_length": 35554,
    "chunk_length": 1281
  },
  {
    "chunk_id": 540,
    "paper_filename": "cynthia_2024_generating_colloquial_radiology_reports_with_large_language_models.pdf",
    "paper_title": "Cynthia 2024 Generating Colloquial Radiology Reports With Large Language Models",
    "chunk_index": 11,
    "total_chunks": 34,
    "text_content": "and harm potential. Assessments of accuracy, harm potential, and physician likability for Findings and Impression were per- formed by radiologists. Accuracy Each report\u2019s translation was labeled as 0 (inaccurate) or 1 (accurate) by a radiologist. We computed the percentage accuracy across the 100 reports. The accuracy of the transla - tions of the Findings and Impression sections is illustrated in Table 1 and in Figures 3A and B. We found that the Findings Journal of the American Medical Informa",
    "full_text_length": 35554,
    "chunk_length": 1318
  },
  {
    "chunk_id": 541,
    "paper_filename": "cynthia_2024_generating_colloquial_radiology_reports_with_large_language_models.pdf",
    "paper_title": "Cynthia 2024 Generating Colloquial Radiology Reports With Large Language Models",
    "chunk_index": 12,
    "total_chunks": 34,
    "text_content": "grade levels. We found a global notable improvement by using MK-based prompting along with study indications. For instance, the accuracy for the college grade level improved from 68.7% to 87.0%. We see the same pattern for Impression sections in terms of the accuracy improving with an increase in grade level, and the accuracy improving with MK-based prompting along with indications (Figure 3B). We observe that the accuracy of trans - lating the shorter Impression is substantially better than the",
    "full_text_length": 35554,
    "chunk_length": 1274
  },
  {
    "chunk_id": 542,
    "paper_filename": "cynthia_2024_generating_colloquial_radiology_reports_with_large_language_models.pdf",
    "paper_title": "Cynthia 2024 Generating Colloquial Radiology Reports With Large Language Models",
    "chunk_index": 13,
    "total_chunks": 34,
    "text_content": "in the case of both Figure 1. Our translation pipeline: medical knowledge (MK)-based prompting. Figure 2. Illustration of the steps involved for 1 example report. Table 1. Accuracy of MK-based prompting with indications and baseline methods. Grade Baseline MK \u0087Indications P-value Findings Fifth 60.0% 83.0% .0006 Eighth 65.0% 88.0% .0002 12th 68.0% 87.0% .0023 College 68.7% 87.0% .0033 Impression Fifth 82.0% 90.0% .32 Eighth 83.0% 93.0% .05 12th 84.0% 94.0% .04 College 91.0% 98.0% 1 Bold indicate",
    "full_text_length": 35554,
    "chunk_length": 1384
  },
  {
    "chunk_id": 543,
    "paper_filename": "cynthia_2024_generating_colloquial_radiology_reports_with_large_language_models.pdf",
    "paper_title": "Cynthia 2024 Generating Colloquial Radiology Reports With Large Language Models",
    "chunk_index": 14,
    "total_chunks": 34,
    "text_content": "mean of 5 standard readability scores: Flesch-Kincaid,21Gunning- Fog,24SMOG,25Coleman-Liau,26and Automated Readability Index.27The RI is an estimate of the grade level required for comprehension, such that a lower RI represents more readable text. Mean RI of Findings sections from the original reports was 14.06 and that of Impression sections was 14.31. RI of translations produced by the baseline and MK methods ranged from 6.64 to 10.00 depending on target grade level (Figures 3E and 3F); all tr",
    "full_text_length": 35554,
    "chunk_length": 1348
  },
  {
    "chunk_id": 544,
    "paper_filename": "cynthia_2024_generating_colloquial_radiology_reports_with_large_language_models.pdf",
    "paper_title": "Cynthia 2024 Generating Colloquial Radiology Reports With Large Language Models",
    "chunk_index": 15,
    "total_chunks": 34,
    "text_content": "education levels. Lower readability index corresponds to simpler language. (G and H): Harm potential of translations generated by each prompting method at 4 different education levels. Lower is better. In all panels, asterisk indicates statistical significance for MK \u0087Indications model vs baseline (PD.05).Journal of the American Medical Informatics Association, 2024, Vol. 31, No. 11 2663 Downloaded from https://academic.oup.com/jamia/article/31/11/2660/7740004 by Georgia Institute of Technology ",
    "full_text_length": 35554,
    "chunk_length": 1368
  },
  {
    "chunk_id": 545,
    "paper_filename": "cynthia_2024_generating_colloquial_radiology_reports_with_large_language_models.pdf",
    "paper_title": "Cynthia 2024 Generating Colloquial Radiology Reports With Large Language Models",
    "chunk_index": 16,
    "total_chunks": 34,
    "text_content": "harm potential than those of Findings sections, consistent with the results for accuracy. Among the different methods, we found lower harm potential when we included MK prompting. Importantly, by adding medical knowledge to the prompt, we saw moderate decreases in harm potential; for example, at the eighth-grade level, harm potential of Findings section translations decreased from 36% to 10%. Neverthe - less, some harm potential is present in translations across all the methods. This is signific",
    "full_text_length": 35554,
    "chunk_length": 1349
  },
  {
    "chunk_id": 546,
    "paper_filename": "cynthia_2024_generating_colloquial_radiology_reports_with_large_language_models.pdf",
    "paper_title": "Cynthia 2024 Generating Colloquial Radiology Reports With Large Language Models",
    "chunk_index": 17,
    "total_chunks": 34,
    "text_content": "to baseline.28,29A downside is that there exists a potential for oversimplification of radiology reports. This, in turn, creates increased risk of inaccuracy through hallucina - tions or information omission.30 Readability vs accuracy We observe that readability, or ease of reading, and accuracy ratings were inversely related; that is, as higher grade levels were targeted, translations became more accurate at the cost of being less readable. However, gains in accuracy above the eighth-grade leve",
    "full_text_length": 35554,
    "chunk_length": 1435
  },
  {
    "chunk_id": 547,
    "paper_filename": "cynthia_2024_generating_colloquial_radiology_reports_with_large_language_models.pdf",
    "paper_title": "Cynthia 2024 Generating Colloquial Radiology Reports With Large Language Models",
    "chunk_index": 18,
    "total_chunks": 34,
    "text_content": "prompt helps focus the LLM and limit distractions from irrelevant information.32This corroborates findings of LLMs producing more relevant out- put when provided with contextual information, as explored across other domains.33 Translation accuracy: Findings vs impression The Impression section of a radiology report is essentially a short summary of the more detailed Findings section. In our study, translations of the more concise Impression sections had greater average accuracy; this association",
    "full_text_length": 35554,
    "chunk_length": 1390
  },
  {
    "chunk_id": 548,
    "paper_filename": "cynthia_2024_generating_colloquial_radiology_reports_with_large_language_models.pdf",
    "paper_title": "Cynthia 2024 Generating Colloquial Radiology Reports With Large Language Models",
    "chunk_index": 19,
    "total_chunks": 34,
    "text_content": "harm in a translation could be caused by incorrect translation of sentences in the report, omission of important aspects of the original report, or addi- tion of new concepts to the translation (hallucination). In this study, we undertook preliminary work to detect sour- ces of harm and thus reduce the risk of patients receiving inac- curate translations. The objective is to automatically attribute each sentence in the translation to one in the original report (Figures 4 and 5). This is done by ",
    "full_text_length": 35554,
    "chunk_length": 1219
  },
  {
    "chunk_id": 549,
    "paper_filename": "cynthia_2024_generating_colloquial_radiology_reports_with_large_language_models.pdf",
    "paper_title": "Cynthia 2024 Generating Colloquial Radiology Reports With Large Language Models",
    "chunk_index": 20,
    "total_chunks": 34,
    "text_content": "10 millimeter collection of blood along the right outer brain\u201d is attributed to \u201cThere is a 10 mm subdural hematoma along the convexity of the right cerebral hemisphere\u201d in the original report. This is a way to check for the first source of harm: incorrect transla - tions. Second, attributions help with detecting the omission of parts of the original report. In Figure 4, we see sentences in the original report such as \u201cNo evidence of additional intra- cranial hemorrhage, acute infarct, or mass l",
    "full_text_length": 35554,
    "chunk_length": 1247
  },
  {
    "chunk_id": 550,
    "paper_filename": "cynthia_2024_generating_colloquial_radiology_reports_with_large_language_models.pdf",
    "paper_title": "Cynthia 2024 Generating Colloquial Radiology Reports With Large Language Models",
    "chunk_index": 21,
    "total_chunks": 34,
    "text_content": "new concepts (hallucinations) to the translation. Figure 5 shows an example where the original Findings reads \u201cPlease see impression.\u201d The LLM is only given the Findings to produce the translation. However, the generated translation here includes multiple new concepts or hallucinations. From the attribution output, we find that none of the sentences in the translation can be attributed back to the Findings. This is one way to detect and screen out potential hallucinations. We believe that attrib",
    "full_text_length": 35554,
    "chunk_length": 1301
  },
  {
    "chunk_id": 551,
    "paper_filename": "cynthia_2024_generating_colloquial_radiology_reports_with_large_language_models.pdf",
    "paper_title": "Cynthia 2024 Generating Colloquial Radiology Reports With Large Language Models",
    "chunk_index": 22,
    "total_chunks": 34,
    "text_content": "levels and translations before and after prompt-engineering and context information, each report required multiple evaluations. We considered pos- sible effects, including reader fatigue, to reduce confounding variables. Future studies may include a greater number of reports and increase the number of readers. Second, this study evaluated just 1 LLM, Claude v1.3. Ideally, multiple LLMs would have been compared. However, there are practical 2664 Journal of the American Medical Informatics Associa",
    "full_text_length": 35554,
    "chunk_length": 1355
  },
  {
    "chunk_id": 552,
    "paper_filename": "cynthia_2024_generating_colloquial_radiology_reports_with_large_language_models.pdf",
    "paper_title": "Cynthia 2024 Generating Colloquial Radiology Reports With Large Language Models",
    "chunk_index": 23,
    "total_chunks": 34,
    "text_content": "second or third when com- pared to numerous other LLMs. In another work,40the authors propose a method to reduce inference time in which Claude v1.3 performs better than vanilla-GPT 4. Future work is needed to compare other open source and GPT models. Next, the included radiology reports were limited to CT scans of the head. Future studies may include greater variety of imaging types across other clinical domains. Finally, our study specifies only textual data in the form of clinical indications",
    "full_text_length": 35554,
    "chunk_length": 1372
  },
  {
    "chunk_id": 553,
    "paper_filename": "cynthia_2024_generating_colloquial_radiology_reports_with_large_language_models.pdf",
    "paper_title": "Cynthia 2024 Generating Colloquial Radiology Reports With Large Language Models",
    "chunk_index": 24,
    "total_chunks": 34,
    "text_content": "writing style of reports to favor readability. However, this conflicts with widely recognized radiology guidelines emphasizing the importance of precise anatomic and radiologic terminology for medical clarity. The results of this study indicate great potential for LLMs to generate the colloquial reports pre- ferred by patients without disrupting workflows for clinical stakeholders, including radiologists, referring physicians, and insurers.36More work must be done to investigate the use of LLMs ",
    "full_text_length": 35554,
    "chunk_length": 1443
  },
  {
    "chunk_id": 554,
    "paper_filename": "cynthia_2024_generating_colloquial_radiology_reports_with_large_language_models.pdf",
    "paper_title": "Cynthia 2024 Generating Colloquial Radiology Reports With Large Language Models",
    "chunk_index": 25,
    "total_chunks": 34,
    "text_content": "statistical analysis. Gillean Cortes, Robert Hill, Jasmine Zhao, Angellica Gordon, and Joshua Wright contributed to report analysis. All authors reviewed the manuscript. Supplementary material Supplementary material is available at Journal of the Ameri - can Medical Informatics Association online. Figure 4. Example of attribution when the translation is accurate. The attribution is shown by color coding the sentences, for example, the text in the translation is attributed to the text in the orig",
    "full_text_length": 35554,
    "chunk_length": 1369
  },
  {
    "chunk_id": 555,
    "paper_filename": "cynthia_2024_generating_colloquial_radiology_reports_with_large_language_models.pdf",
    "paper_title": "Cynthia 2024 Generating Colloquial Radiology Reports With Large Language Models",
    "chunk_index": 26,
    "total_chunks": 34,
    "text_content": "The study received no outside funding. Conflicts of interest Authors S.N., N.M., and R.T. are employed by Amazon Web Services. Data availability Patient data analyzed as part of this study are not publicly available to protect participant privacy. Data generated by physician raters areavailable by request. Ethics approval The study was approved by the UC Irvine institutional review board. References 01. Trofimova A, Vey BL, Safdar NM, et al. Radiology report read- ability: an opportunity to impr",
    "full_text_length": 35554,
    "chunk_length": 1343
  },
  {
    "chunk_id": 556,
    "paper_filename": "cynthia_2024_generating_colloquial_radiology_reports_with_large_language_models.pdf",
    "paper_title": "Cynthia 2024 Generating Colloquial Radiology Reports With Large Language Models",
    "chunk_index": 27,
    "total_chunks": 34,
    "text_content": "simulations to investigate patient preferences. J Am Coll Radiol. 2012;9(4):256-263. 05. Alarifi M, Patrick T, Jabour A, et al. Understanding patient needs and gaps in radiology reports through online discussion forum analysis. Insights Imaging. 2021;12(1):50-59. 06. Steitz BD, Turer RW, Lin CT, et al. Perspectives of patients about immediate access to test results through an online patient portal. JAMA Netw Open. 2023;6(3):e233572. 07. Amin K, Khosla P, Doshi R, et al. Focus: big data: artifici",
    "full_text_length": 35554,
    "chunk_length": 1357
  },
  {
    "chunk_id": 557,
    "paper_filename": "cynthia_2024_generating_colloquial_radiology_reports_with_large_language_models.pdf",
    "paper_title": "Cynthia 2024 Generating Colloquial Radiology Reports With Large Language Models",
    "chunk_index": 28,
    "total_chunks": 34,
    "text_content": "health vocabulary to generate patient-centered radiol - ogy reporting: translation and evaluation. J Med Internet Res. 2017;19(12):e8536. 11. Thirunavukarasu AJ, Ting DSJ, Elangovan K, et al. Large language models in medicine. Nature Medicine. 2023;29(8):1930-1940. 12. Mesk \ufffdo B, Topol EJ. The imperative for regulatory oversight of large language models (or generative AI) in healthcare. NPJ Digital Medicine. 2023;6(1):120-126. 13. Tippareddy C, Jiang S, Bera K, et al. Radiology reading room for ",
    "full_text_length": 35554,
    "chunk_length": 1367
  },
  {
    "chunk_id": 558,
    "paper_filename": "cynthia_2024_generating_colloquial_radiology_reports_with_large_language_models.pdf",
    "paper_title": "Cynthia 2024 Generating Colloquial Radiology Reports With Large Language Models",
    "chunk_index": 29,
    "total_chunks": 34,
    "text_content": "prompt learn - ing: results, limitations, and potential. Vis Comput Ind Biomed Art. 2023;6(1):9. 17. Sarangi PK, Lumbani A, Swarup MS, et al. Assessing ChatGPT\u2019s proficiency in simplifying radiological reports for healthcare pro- fessionals and patients. Cureus. 2023;15(12):e50881. 18. Jeblick K, Schachtner B, Dexl J, et al. ChatGPT makes medicine easy to swallow: an exploratory case study on simplified radiology reports. Eur Radiol. 2023;34(5):2817-2825. 19. Gonzales A, Guruswamy G, Smith SR. S",
    "full_text_length": 35554,
    "chunk_length": 1441
  },
  {
    "chunk_id": 559,
    "paper_filename": "cynthia_2024_generating_colloquial_radiology_reports_with_large_language_models.pdf",
    "paper_title": "Cynthia 2024 Generating Colloquial Radiology Reports With Large Language Models",
    "chunk_index": 30,
    "total_chunks": 34,
    "text_content": "https://apps.dtic.mil/sti/citations/ADA006655. 22. Dhuliawala S, Komeili M, Xu J, et al. 2023. Chain-of-verification reduces hallucination in large language models. arXiv, arXiv:230911495, preprint: not peer reviewed. 23. Bhatia P, Celikkaya B, Khalilia M, et al. Comprehend medical: A named entity recognition and relationship extraction web service. In: Proceedings \u2013 18th IEEE International Conference on Machine Learning and Applications, IEEE ICMLA 2019, Boca Raton, FL, USA. 2019:1844-1851. 24.",
    "full_text_length": 35554,
    "chunk_length": 1393
  },
  {
    "chunk_id": 560,
    "paper_filename": "cynthia_2024_generating_colloquial_radiology_reports_with_large_language_models.pdf",
    "paper_title": "Cynthia 2024 Generating Colloquial Radiology Reports With Large Language Models",
    "chunk_index": 31,
    "total_chunks": 34,
    "text_content": "prompt, and predict: a sys- tematic survey of prompting methods in natural language process - ing. ACM Comput Surv. 2023;55(9):1-35. 30. Olthof AW, van Ooijen PMA, Cornelissen LJ. Deep learning- based natural language processing in radiology: the impact of report complexity, disease prevalence, dataset size, and algorithm type on model performance. J Med Syst. 2021;45(10):91. 31. Rooney MK, Santiago G, Perni S, et al. Readability of patient edu- cation materials from high-impact medical journals",
    "full_text_length": 35554,
    "chunk_length": 1375
  },
  {
    "chunk_id": 561,
    "paper_filename": "cynthia_2024_generating_colloquial_radiology_reports_with_large_language_models.pdf",
    "paper_title": "Cynthia 2024 Generating Colloquial Radiology Reports With Large Language Models",
    "chunk_index": 32,
    "total_chunks": 34,
    "text_content": "Martin-Carreras T, Cook TS, Kahn CE. Readability of radiology reports: implications for patient-centered care. Clin Imaging. 2019;54:116-120. 35. Cabarrus M, Naeger DM, Rybkin A, et al. Patients prefer results from the ordering provider and access to their radiology reports. J Am Coll Radiol. 2015;12(6):556-562. 36. Hall FM. The radiology report of the future. Radiology. 2009;251 (2):313-316. 37. Youdelman M, Turner W, Coursolle A, et al. Questions and Answers on the 2022 Proposed Rule Addressin",
    "full_text_length": 35554,
    "chunk_length": 1374
  },
  {
    "chunk_id": 562,
    "paper_filename": "cynthia_2024_generating_colloquial_radiology_reports_with_large_language_models.pdf",
    "paper_title": "Cynthia 2024 Generating Colloquial Radiology Reports With Large Language Models",
    "chunk_index": 33,
    "total_chunks": 34,
    "text_content": "Han Y, Zhao Z, et al. Scieval: a multi-level large language model evaluation benchmark for scientific research. Proceedings of the AAAI Conference on Artificial Intelligence. 2024;38 (17):19053-19061. 40. Jiang H, Wu Q, Lin CY, Yang Y, Qiu L. 2023. Llmlingua: Compressing prompts for accelerated inference of large language models. arXiv, arXiv:2310.05736, preprint: not peer reviewed. \u00a9 The Author(s) 2024. Published by Oxford University Press on behalf of the American Medical Informatics Associati",
    "full_text_length": 35554,
    "chunk_length": 941
  },
  {
    "chunk_id": 563,
    "paper_filename": "daksh_2024_diagnostic_test_accuracy_of_AI_assisted_in_mammography_for_breast_imaging.pdf",
    "paper_title": "Daksh 2024 Diagnostic Test Accuracy Of Ai Assisted In Mammography For Breast Imaging",
    "chunk_index": 0,
    "total_chunks": 90,
    "text_content": "Submitted 22 July 2024 Accepted 14 October 2024 Published 19 February 2025 Corresponding author Adnan Akhunzada, Adnan.Adnan@udst.edu.qa Academic editor Bilal Alatas Additional Information and Declarations can be found on page 24 DOI 10.7717/peerj-cs.2476 Copyright 2025 Dave et al. Distributed under Creative Commons CC-BY 4.0 OPEN ACCESSDiagnostic test accuracy of AI-assisted mammography for breast imaging: a narrative review Daksh Dave1, Adnan Akhunzada2, Nikola Ivkovi\u00a23, Sujan Gyawali4, Korhan",
    "full_text_length": 97532,
    "chunk_length": 1574
  },
  {
    "chunk_id": 564,
    "paper_filename": "daksh_2024_diagnostic_test_accuracy_of_AI_assisted_in_mammography_for_breast_imaging.pdf",
    "paper_title": "Daksh 2024 Diagnostic Test Accuracy Of Ai Assisted In Mammography For Breast Imaging",
    "chunk_index": 1,
    "total_chunks": 90,
    "text_content": "Cybersecurity, Al-Ahliyya Amman University, Amman, Jordan ABSTRACT The integration of artificial intelligence into healthcare, particularly in mammography, holds immense potential for improving breast cancer diagnosis. Artificial intelligence (AI), with its ability to process vast amounts of data and detect intricate patterns, offers a solution to the limitations of traditional mammography, including missed diagnoses and false positives. This review focuses on the diagnostic accuracy of AI-assis",
    "full_text_length": 97532,
    "chunk_length": 1583
  },
  {
    "chunk_id": 565,
    "paper_filename": "daksh_2024_diagnostic_test_accuracy_of_AI_assisted_in_mammography_for_breast_imaging.pdf",
    "paper_title": "Daksh 2024 Diagnostic Test Accuracy Of Ai Assisted In Mammography For Breast Imaging",
    "chunk_index": 2,
    "total_chunks": 90,
    "text_content": "crucial for ensuring ethical, reliable, and inclusive integration into clinical practice. Subjects Bioinformatics, Computational Biology, Artificial Intelligence, Computer Vision Keywords Breast cancer, Mammography, Artificial intelligence, Medical imaging, Health care INTRODUCTION Breast cancer, a pervasive global health concern, remains a formidable adversary in terms of both morbidity and mortality rates ( King, 2004 ). The timely and accurate detection of breast cancer is a paramount goal th",
    "full_text_length": 97532,
    "chunk_length": 1421
  },
  {
    "chunk_id": 566,
    "paper_filename": "daksh_2024_diagnostic_test_accuracy_of_AI_assisted_in_mammography_for_breast_imaging.pdf",
    "paper_title": "Daksh 2024 Diagnostic Test Accuracy Of Ai Assisted In Mammography For Breast Imaging",
    "chunk_index": 3,
    "total_chunks": 90,
    "text_content": "test accu- racy of AI-assisted mammography for breast imaging: a narrative review. PeerJ Comput. Sci. 11:e2476 http://doi.org/10.7717/peerj-cs.2476 persists as a dynamic and evolving frontier. In recent years, medical imaging with artificial intelligence (AI) has ushered in an era of profound innovation. The advent of AI, particularly underpinned by sophisticated deep learning and machine learning algorithms, has demonstrated remarkable proficiency in deciphering complex medical images, recogniz",
    "full_text_length": 97532,
    "chunk_length": 1441
  },
  {
    "chunk_id": 567,
    "paper_filename": "daksh_2024_diagnostic_test_accuracy_of_AI_assisted_in_mammography_for_breast_imaging.pdf",
    "paper_title": "Daksh 2024 Diagnostic Test Accuracy Of Ai Assisted In Mammography For Breast Imaging",
    "chunk_index": 4,
    "total_chunks": 90,
    "text_content": "2020 ;Mayo & Leung, 2018 ). This narrative review embarks on a comprehensive and critical synthesis of the existing body of literature dedicated to this multifaceted nexus. Our objective is to meticulously analyze the collective findings of relevant studies, thereby elucidating the current landscape surrounding the diagnostic efficacy of AI-assisted mammography within the realm of breast imaging. Breast cancer remains one of the leading causes of death among women worldwide, making early detecti",
    "full_text_length": 97532,
    "chunk_length": 1445
  },
  {
    "chunk_id": 568,
    "paper_filename": "daksh_2024_diagnostic_test_accuracy_of_AI_assisted_in_mammography_for_breast_imaging.pdf",
    "paper_title": "Daksh 2024 Diagnostic Test Accuracy Of Ai Assisted In Mammography For Breast Imaging",
    "chunk_index": 5,
    "total_chunks": 90,
    "text_content": "has introduced a new frontier in medical imaging, particularly in mammography. AI models, especially those based on deep learning algorithms like convolutional neural networks (CNNs), have demonstrated remarkable potential in augmenting the diagnostic capabilities of radiologists. These models are capable of analyzing mammographic images with unprecedented accuracy, identifying patterns that may not be visible to the human eye, and providing risk assessments based on vast amounts of data. The au",
    "full_text_length": 97532,
    "chunk_length": 1452
  },
  {
    "chunk_id": 569,
    "paper_filename": "daksh_2024_diagnostic_test_accuracy_of_AI_assisted_in_mammography_for_breast_imaging.pdf",
    "paper_title": "Daksh 2024 Diagnostic Test Accuracy Of Ai Assisted In Mammography For Breast Imaging",
    "chunk_index": 6,
    "total_chunks": 90,
    "text_content": "provide a comprehensive understanding of the current state of AI-assisted mammography, its benefits, and the challenges that need to be addressed for its successful integration into healthcare systems. Portions of this text were previously published as part of a preprint (https:/ / doi.org/ 10. 36227/ techrxiv.24601923.v1). Dave et al. (2025), PeerJ Comput. Sci. , DOI 10.7717/peerj-cs.2476 2/32 Challenges in AI-assisted mammography As the integration of AI into medical imaging gains momentum, th",
    "full_text_length": 97532,
    "chunk_length": 1455
  },
  {
    "chunk_id": 570,
    "paper_filename": "daksh_2024_diagnostic_test_accuracy_of_AI_assisted_in_mammography_for_breast_imaging.pdf",
    "paper_title": "Daksh 2024 Diagnostic Test Accuracy Of Ai Assisted In Mammography For Breast Imaging",
    "chunk_index": 7,
    "total_chunks": 90,
    "text_content": "upon exposure to a comprehensive range of breast conditions, encompassing diverse anatomical variations, breast compositions, and demographic factors ( Jairam & Ha, 2022 ). However, assembling a dataset that genuinely mirrors the complex spectrum of breast characteristics presents a formidable challenge. Historically, medical imaging datasets have often skewed towards certain demographic groups, potentially introducing bias and limiting the generalizability of AI algorithms ( Sheth & Giger, 2020",
    "full_text_length": 97532,
    "chunk_length": 1497
  },
  {
    "chunk_id": 571,
    "paper_filename": "daksh_2024_diagnostic_test_accuracy_of_AI_assisted_in_mammography_for_breast_imaging.pdf",
    "paper_title": "Daksh 2024 Diagnostic Test Accuracy Of Ai Assisted In Mammography For Breast Imaging",
    "chunk_index": 8,
    "total_chunks": 90,
    "text_content": "base and partnering with radiologists to annotate images accurately. Leveraging data augmentation techniques can help expand the dataset's diversity by simulating various imaging scenarios and breast compositions. Moreover, data quality is paramount for robust AI model training. The dataset must be meticulously quality- controlled to eliminate artifacts, ensure proper image alignment, and standardize image acquisition protocols. Inconsistencies in data quality can compromise the AI algorithm's a",
    "full_text_length": 97532,
    "chunk_length": 1490
  },
  {
    "chunk_id": 572,
    "paper_filename": "daksh_2024_diagnostic_test_accuracy_of_AI_assisted_in_mammography_for_breast_imaging.pdf",
    "paper_title": "Daksh 2024 Diagnostic Test Accuracy Of Ai Assisted In Mammography For Breast Imaging",
    "chunk_index": 9,
    "total_chunks": 90,
    "text_content": "across diverse patient populations. Data diversity and skewed datasets are critical concerns in artificial intelligence (AI), especially regarding model performance and generalizability. Skewed datasets refer to datasets where one class or group is disproportionately represented compared to others. This imbalance can lead AI models to overfit on the majority class while underperforming on minority classes. Balashankar et al. (2019) Dave et al. (2025), PeerJ Comput. Sci. , DOI 10.7717/peerj-cs.24",
    "full_text_length": 97532,
    "chunk_length": 1470
  },
  {
    "chunk_id": 573,
    "paper_filename": "daksh_2024_diagnostic_test_accuracy_of_AI_assisted_in_mammography_for_breast_imaging.pdf",
    "paper_title": "Daksh 2024 Diagnostic Test Accuracy Of Ai Assisted In Mammography For Breast Imaging",
    "chunk_index": 10,
    "total_chunks": 90,
    "text_content": "datasets. Their work shows that enriching datasets with strategic diversification improves model generalizability. To improve the performance of AI in mammography and ensure equitable healthcare outcomes, it is crucial to include geographically diverse datasets in the training process. Incorporating mammograms from various regions can help AI models generalize better across different populations and healthcare systems. A more geographically diverse dataset accounts for variations in imaging devi",
    "full_text_length": 97532,
    "chunk_length": 1485
  },
  {
    "chunk_id": 574,
    "paper_filename": "daksh_2024_diagnostic_test_accuracy_of_AI_assisted_in_mammography_for_breast_imaging.pdf",
    "paper_title": "Daksh 2024 Diagnostic Test Accuracy Of Ai Assisted In Mammography For Breast Imaging",
    "chunk_index": 11,
    "total_chunks": 90,
    "text_content": "including AI-assisted mammography, hinges on their capacity to generalize findings from controlled datasets to real-world clinical scenarios. While AI models might excel in specific datasets, ensuring their consistent performance across diverse clinical conditions is a critical challenge ( Barnett et al., 2021 ). Real-world mammography encounters a myriad of variables that can impact image quality and interpretation. Imaging devices vary in terms of resolution, noise levels, and imaging protocol",
    "full_text_length": 97532,
    "chunk_length": 1540
  },
  {
    "chunk_id": 575,
    "paper_filename": "daksh_2024_diagnostic_test_accuracy_of_AI_assisted_in_mammography_for_breast_imaging.pdf",
    "paper_title": "Daksh 2024 Diagnostic Test Accuracy Of Ai Assisted In Mammography For Breast Imaging",
    "chunk_index": 12,
    "total_chunks": 90,
    "text_content": "imaging devices and protocols, is pivotal. Augmenting the training dataset with synthetic variations can further enhance the AI model's ability to adapt to real-world challenges. The challenge of generalization also necessitates continuous monitoring and validation post-deployment. AI algorithms should be rigorously tested across multiple clinical settings to ensure consistent performance. Feedback loops, where AI-generated interpretations are compared against ground truth assessments by radiolo",
    "full_text_length": 97532,
    "chunk_length": 1540
  },
  {
    "chunk_id": 576,
    "paper_filename": "daksh_2024_diagnostic_test_accuracy_of_AI_assisted_in_mammography_for_breast_imaging.pdf",
    "paper_title": "Daksh 2024 Diagnostic Test Accuracy Of Ai Assisted In Mammography For Breast Imaging",
    "chunk_index": 13,
    "total_chunks": 90,
    "text_content": "of effective generalization, collaboration between AI researchers, radiologists, and healthcare institutions is paramount. A diverse training dataset, continuous validation, and ongoing refinement of AI models contribute to their ability to navigate the intricacies of real-world mammography and provide reliable support to clinicians. Interpretability The integration of AI into medical imaging, such as AI-assisted mammography, brings to the forefront the challenge of interpretability - the abilit",
    "full_text_length": 97532,
    "chunk_length": 1450
  },
  {
    "chunk_id": 577,
    "paper_filename": "daksh_2024_diagnostic_test_accuracy_of_AI_assisted_in_mammography_for_breast_imaging.pdf",
    "paper_title": "Daksh 2024 Diagnostic Test Accuracy Of Ai Assisted In Mammography For Breast Imaging",
    "chunk_index": 14,
    "total_chunks": 90,
    "text_content": "have confidence not only in the accuracy of AI predictions but also in the insights that underpin those predictions. Addressing the challenge of interpretability involves developing methods that shed light on the inner workings of AI models. Researchers are exploring techniques to visualize the features and patterns that AI algorithms focus on when making decisions ( Thrall, Fessell & Pandharipande, 2021 ). These visualizations can help radiologists understand the reasoning behind AI-generated f",
    "full_text_length": 97532,
    "chunk_length": 1416
  },
  {
    "chunk_id": 578,
    "paper_filename": "daksh_2024_diagnostic_test_accuracy_of_AI_assisted_in_mammography_for_breast_imaging.pdf",
    "paper_title": "Daksh 2024 Diagnostic Test Accuracy Of Ai Assisted In Mammography For Breast Imaging",
    "chunk_index": 15,
    "total_chunks": 90,
    "text_content": "involves developing ``explainable AI'' methods. These methods aim to generate explanations that are comprehensible to humans, providing insight into the factors that influenced an AI decision. Techniques like feature importance scores, attention maps, and saliency maps offer avenues for presenting AI- generated results in a manner that resonates with the expertise of radiologists. Furthermore, efforts to address interpretability intersect with the broader theme of trust-building in AI- assisted ",
    "full_text_length": 97532,
    "chunk_length": 1465
  },
  {
    "chunk_id": 579,
    "paper_filename": "daksh_2024_diagnostic_test_accuracy_of_AI_assisted_in_mammography_for_breast_imaging.pdf",
    "paper_title": "Daksh 2024 Diagnostic Test Accuracy Of Ai Assisted In Mammography For Breast Imaging",
    "chunk_index": 16,
    "total_chunks": 90,
    "text_content": "explainability requires innovative research into model architectures and interpretability techniques. In the pursuit of enhancing interpretability, collaboration between AI researchers, radiologists, and ethicists is essential. Transparent and interpretable AI algorithms hold the potential to not only improve diagnostic accuracy but also foster a cooperative synergy between AI systems and human experts. Regulatory and ethical concerns The integration of AI into medical imaging, particularly AI-a",
    "full_text_length": 97532,
    "chunk_length": 1491
  },
  {
    "chunk_id": 580,
    "paper_filename": "daksh_2024_diagnostic_test_accuracy_of_AI_assisted_in_mammography_for_breast_imaging.pdf",
    "paper_title": "Daksh 2024 Diagnostic Test Accuracy Of Ai Assisted In Mammography For Breast Imaging",
    "chunk_index": 17,
    "total_chunks": 90,
    "text_content": "to their safe and effective deployment. The validation process involves demonstrating the algorithm's performance on diverse datasets and clinical scenarios, as well as addressing issues related to generalizability and robustness. Data privacy and security. The use of AI in medical imaging involves handling sensitive patient data. Protecting patient privacy and data security are paramount ( Sabottke & Spieler, 2020;Bitencourt et al., 2021 ). AI models often require access to large datasets for t",
    "full_text_length": 97532,
    "chunk_length": 1438
  },
  {
    "chunk_id": 581,
    "paper_filename": "daksh_2024_diagnostic_test_accuracy_of_AI_assisted_in_mammography_for_breast_imaging.pdf",
    "paper_title": "Daksh 2024 Diagnostic Test Accuracy Of Ai Assisted In Mammography For Breast Imaging",
    "chunk_index": 18,
    "total_chunks": 90,
    "text_content": "Transparent communication is essential to uphold patient autonomy and foster trust in the healthcare provider-patient relationship. Liability and accountability. The emergence of AI-assisted diagnoses raises questions about liability in cases of errors or misdiagnoses ( Wu et al., 2019 ;Zhen & Chan, 2001 ). Determining who is accountable for AI-generated results can be challenging, as it involves a blend of human expertise and algorithmic decision-making. Clear guidelines and legal frameworks ne",
    "full_text_length": 97532,
    "chunk_length": 1425
  },
  {
    "chunk_id": 582,
    "paper_filename": "daksh_2024_diagnostic_test_accuracy_of_AI_assisted_in_mammography_for_breast_imaging.pdf",
    "paper_title": "Daksh 2024 Diagnostic Test Accuracy Of Ai Assisted In Mammography For Breast Imaging",
    "chunk_index": 19,
    "total_chunks": 90,
    "text_content": "explainability. Ethical considerations extend to the transparency and explainability of AI algorithms ( Houssami et al., 2017 ). Patients and clinicians have the right to understand how AI arrives at its conclusions. Ensuring that AI-generated results can be explained and understood contributes to fostering trust and acceptance. Continual monitoring and improvement. Ethical considerations encompass the ongoing monitoring and improvement of AI algorithms post-deployment ( Zhen & Chan, 2001 ; Dale",
    "full_text_length": 97532,
    "chunk_length": 1492
  },
  {
    "chunk_id": 583,
    "paper_filename": "daksh_2024_diagnostic_test_accuracy_of_AI_assisted_in_mammography_for_breast_imaging.pdf",
    "paper_title": "Daksh 2024 Diagnostic Test Accuracy Of Ai Assisted In Mammography For Breast Imaging",
    "chunk_index": 20,
    "total_chunks": 90,
    "text_content": "and benchmarking The integration of AI into medical imaging, including AI-assisted mammography, necessitates a rigorous process of validation and benchmarking to ensure the reliability and reproducibility of AI-generated results ( Yala et al., 2022 ). The validation framework serves as a critical bridge between algorithm development and clinical implementation, guiding the assessment of AI performance and enhancing its clinical utility. Establishing ground truth. Validating AI algorithms demands",
    "full_text_length": 97532,
    "chunk_length": 1487
  },
  {
    "chunk_id": 584,
    "paper_filename": "daksh_2024_diagnostic_test_accuracy_of_AI_assisted_in_mammography_for_breast_imaging.pdf",
    "paper_title": "Daksh 2024 Diagnostic Test Accuracy Of Ai Assisted In Mammography For Breast Imaging",
    "chunk_index": 21,
    "total_chunks": 90,
    "text_content": "and demographic groups helps ensure that AI models perform consistently across varied settings. Cross validation and external validation. Cross-validation involves partitioning the dataset into subsets for training and testing, enabling robust evaluation of AI performance ( Katzen & Dodelzon, 2018 ). External validation, where AI models are tested on datasets from different institutions, reinforces the model's ability to generalize across diverse clinical contexts. Clinical relevance. Validation",
    "full_text_length": 97532,
    "chunk_length": 1574
  },
  {
    "chunk_id": 585,
    "paper_filename": "daksh_2024_diagnostic_test_accuracy_of_AI_assisted_in_mammography_for_breast_imaging.pdf",
    "paper_title": "Daksh 2024 Diagnostic Test Accuracy Of Ai Assisted In Mammography For Breast Imaging",
    "chunk_index": 22,
    "total_chunks": 90,
    "text_content": "the clinical landscape evolves, potentially necessitating updates to accommodate emerging pathologies and imaging technologies. Benchmarks and external comparisons. Benchmarking AI performance against established standards and external comparative datasets fosters a robust understanding of its strengths and limitations ( Katzen & Dodelzon, 2018 ). Collaborative efforts to develop standardized benchmarks facilitate cross-study comparisons and provide a basis for evaluating algorithmic progress. I",
    "full_text_length": 97532,
    "chunk_length": 1480
  },
  {
    "chunk_id": 586,
    "paper_filename": "daksh_2024_diagnostic_test_accuracy_of_AI_assisted_in_mammography_for_breast_imaging.pdf",
    "paper_title": "Daksh 2024 Diagnostic Test Accuracy Of Ai Assisted In Mammography For Breast Imaging",
    "chunk_index": 23,
    "total_chunks": 90,
    "text_content": "from a systematic review of significant sources such as PubMed, IEEE Xplore, and Scopus databases in capturing the majority of studies relative to the integration of AI in mammography. All searches before August 10, 2023, utilize a search strategy with the following search terms, Medical Subject Headings (MeSH), and relevant keywords by employing Boolean operators to refine results. These criteria are applied very rigorously, focusing on studies with reported quantitative diagnostic accuracy met",
    "full_text_length": 97532,
    "chunk_length": 1446
  },
  {
    "chunk_id": 587,
    "paper_filename": "daksh_2024_diagnostic_test_accuracy_of_AI_assisted_in_mammography_for_breast_imaging.pdf",
    "paper_title": "Daksh 2024 Diagnostic Test Accuracy Of Ai Assisted In Mammography For Breast Imaging",
    "chunk_index": 24,
    "total_chunks": 90,
    "text_content": "follows guidelines for systematic reviews, and only uses publicly available data where individual patient information is de-identified. This methodology attempts a complete, accurate, and ethical appraisal of diagnostic test accuracy for AI-assisted mammography. Audience The audience for this narrative review on the diagnostic test accuracy of AI-assisted mammography for breast imaging includes medical researchers, clinicians, healthcare administrators and decision-makers. Dave et al. (2025), Pe",
    "full_text_length": 97532,
    "chunk_length": 1532
  },
  {
    "chunk_id": 588,
    "paper_filename": "daksh_2024_diagnostic_test_accuracy_of_AI_assisted_in_mammography_for_breast_imaging.pdf",
    "paper_title": "Daksh 2024 Diagnostic Test Accuracy Of Ai Assisted In Mammography For Breast Imaging",
    "chunk_index": 25,
    "total_chunks": 90,
    "text_content": "meticulously crafted, incorporating a combination of Medical Subject Headings (MeSH) terms and pertinent keywords related to ``AI'', ``mammography'', ``breast imaging'', and ``diagnostic accuracy''. Boolean operators, such as ``AND'' and ``OR'', were strategically employed to refine the search results. The temporal scope of the search encompassed studies published up to 10th August 2023. Additionally, a snowballing approach was adopted, whereby the references of relevant articles were manually e",
    "full_text_length": 97532,
    "chunk_length": 1517
  },
  {
    "chunk_id": 589,
    "paper_filename": "daksh_2024_diagnostic_test_accuracy_of_AI_assisted_in_mammography_for_breast_imaging.pdf",
    "paper_title": "Daksh 2024 Diagnostic Test Accuracy Of Ai Assisted In Mammography For Breast Imaging",
    "chunk_index": 26,
    "total_chunks": 90,
    "text_content": "inclusion. Human Subjects: To capture the real-world clinical application of AI- assisted mammography, only studies involving human subjects were included. Diagnostic Accuracy Metrics: Studies that reported quantitative diagnostic accuracy metrics, including sensitivity, specificity, area under the receiver operating characteristic curve (AUC-ROC), or other relevant metrics, were included. Publication Language: To ensure accessibility and feasibility for the review authors and readers, only stud",
    "full_text_length": 97532,
    "chunk_length": 1510
  },
  {
    "chunk_id": 590,
    "paper_filename": "daksh_2024_diagnostic_test_accuracy_of_AI_assisted_in_mammography_for_breast_imaging.pdf",
    "paper_title": "Daksh 2024 Diagnostic Test Accuracy Of Ai Assisted In Mammography For Breast Imaging",
    "chunk_index": 27,
    "total_chunks": 90,
    "text_content": "were excluded. Non-English Publications: Studies published in languages Dave et al. (2025), PeerJ Comput. Sci. , DOI 10.7717/peerj-cs.2476 9/32 other than English were excluded due to potential language barriers and the available resources of the review. Data extraction A structured data extraction process was employed to systematically gather pertinent information from the selected studies. A standardized data extraction form was used to capture essential study characteristics ( e.g., authors, ",
    "full_text_length": 97532,
    "chunk_length": 1538
  },
  {
    "chunk_id": 591,
    "paper_filename": "daksh_2024_diagnostic_test_accuracy_of_AI_assisted_in_mammography_for_breast_imaging.pdf",
    "paper_title": "Daksh 2024 Diagnostic Test Accuracy Of Ai Assisted In Mammography For Breast Imaging",
    "chunk_index": 28,
    "total_chunks": 90,
    "text_content": "patterns, and variations in the reported diagnostic accuracy outcomes were identified across the studies. Due to the substantial methodological diversity, variances in study design, and variations in AI algorithms used, a quantitative meta-analysis was deemed impractical. As an alternative, a qualitative synthesis approach was adopted, allowing for a comprehensive narrative overview of the diagnostic accuracy landscape within the context of AI-assisted mammography. In this synthesis, the diversi",
    "full_text_length": 97532,
    "chunk_length": 1506
  },
  {
    "chunk_id": 592,
    "paper_filename": "daksh_2024_diagnostic_test_accuracy_of_AI_assisted_in_mammography_for_breast_imaging.pdf",
    "paper_title": "Daksh 2024 Diagnostic Test Accuracy Of Ai Assisted In Mammography For Breast Imaging",
    "chunk_index": 29,
    "total_chunks": 90,
    "text_content": "individual patient data or personally identifiable information were utilized. The review was conducted in adherence to established ethical guidelines for systematic reviews and meta-analyses. Given the nature of the review, no ethical approval was required or sought. AI-assisted mammography The intersection of AI and mammography is a monumental epoch that converges cutting- edge technology with critical healthcare needs. This section embarks on an illuminative expedition through the multidimensi",
    "full_text_length": 97532,
    "chunk_length": 1500
  },
  {
    "chunk_id": 593,
    "paper_filename": "daksh_2024_diagnostic_test_accuracy_of_AI_assisted_in_mammography_for_breast_imaging.pdf",
    "paper_title": "Daksh 2024 Diagnostic Test Accuracy Of Ai Assisted In Mammography For Breast Imaging",
    "chunk_index": 30,
    "total_chunks": 90,
    "text_content": "have the capacity to recognize intricate patterns and minute features that often elude human perception. By being trained on extensive datasets of annotated mammograms, AI algorithms learn to discern subtle hallmarks of potential malignancies, providing a foundation to augment diagnostic accuracy and mitigate the risks of misdiagnosis. Diverse AI approaches: navigating a multiverse of techniques Within the spectrum of AI-assisted mammography, diverse strategies converge to address the multifacet",
    "full_text_length": 97532,
    "chunk_length": 1463
  },
  {
    "chunk_id": 594,
    "paper_filename": "daksh_2024_diagnostic_test_accuracy_of_AI_assisted_in_mammography_for_breast_imaging.pdf",
    "paper_title": "Daksh 2024 Diagnostic Test Accuracy Of Ai Assisted In Mammography For Breast Imaging",
    "chunk_index": 31,
    "total_chunks": 90,
    "text_content": "of AI's metamorphosis lies in its capacity to learn from data at a scale and complexity unattainable by human endeavor. Training AI models for mammography entails immersing them in meticulously annotated datasets ( Mayo et al., 2019 ). Each image serves as a repository of diagnostic outcomes, enabling the algorithms to decipher the intricate tapestry of patterns and pathologies. Through iterative refinement, these models internalize mammographic nuances, unraveling even the most nuanced signs of",
    "full_text_length": 97532,
    "chunk_length": 1425
  },
  {
    "chunk_id": 595,
    "paper_filename": "daksh_2024_diagnostic_test_accuracy_of_AI_assisted_in_mammography_for_breast_imaging.pdf",
    "paper_title": "Daksh 2024 Diagnostic Test Accuracy Of Ai Assisted In Mammography For Breast Imaging",
    "chunk_index": 32,
    "total_chunks": 90,
    "text_content": "(2023) explore the clinical characteristics, diagnosis, and management of Sweet syndrome induced by azathioprine, providing crucial insights into autoimmune responses and drug interactions. Zhu (2024) focuses on a computational intelligence-based classification system for diagnosing memory impairment in psychoactive substance users, offering novel approaches to mental health diagnostics. He et al. (2020) presents a new method for recognizing circulating tumor cells (CTC) using machine learning, ",
    "full_text_length": 97532,
    "chunk_length": 1515
  },
  {
    "chunk_id": 596,
    "paper_filename": "daksh_2024_diagnostic_test_accuracy_of_AI_assisted_in_mammography_for_breast_imaging.pdf",
    "paper_title": "Daksh 2024 Diagnostic Test Accuracy Of Ai Assisted In Mammography For Breast Imaging",
    "chunk_index": 33,
    "total_chunks": 90,
    "text_content": "(2024) detail the use of a convolution-transformer for image feature extraction, highlighting innovative approaches in engineering and AI integration in medical imaging. Diagnostic enhancement potential: envisioning uncharted diagnostic horizons The promise of AI in mammography resonates with its potential to elevate diagnostic accuracy. Collaborating with radiologists' clinical expertise, AI algorithms forge a symbiotic alliance. This partnership amplifies diagnostic precision, mitigates the ri",
    "full_text_length": 97532,
    "chunk_length": 1466
  },
  {
    "chunk_id": 597,
    "paper_filename": "daksh_2024_diagnostic_test_accuracy_of_AI_assisted_in_mammography_for_breast_imaging.pdf",
    "paper_title": "Daksh 2024 Diagnostic Test Accuracy Of Ai Assisted In Mammography For Breast Imaging",
    "chunk_index": 34,
    "total_chunks": 90,
    "text_content": "The intricate diversity of breast tissue composition, the nuances of image quality across diverse modalities, and the aspiration for algorithms to generalize across diverse populations present formidable hurdles ( Bahl, 2019 ;L\u00e5ng et al., 2021a ). Ethical considerations loom large, encompassing data privacy, algorithmic bias, and the imperative to address potential underrepresentation. Harmonizing AI into clinical workflows necessitates a seismic shift, incorporating interpretability, accountabi",
    "full_text_length": 97532,
    "chunk_length": 1490
  },
  {
    "chunk_id": 598,
    "paper_filename": "daksh_2024_diagnostic_test_accuracy_of_AI_assisted_in_mammography_for_breast_imaging.pdf",
    "paper_title": "Daksh 2024 Diagnostic Test Accuracy Of Ai Assisted In Mammography For Breast Imaging",
    "chunk_index": 35,
    "total_chunks": 90,
    "text_content": "literature review journey traverses the breadth of 13 distinct studies, extracting their quintessence to weave a comprehensive narrative that highlights the multifaceted dimensions of diagnostic test accuracy in AI- assisted mammography as shown in Fig. 1. Yala et al. (2019) illuminate the path with their deployment of ResNet-18, a neural network renowned for its convolutional architecture. Their exploration, nested within a monumental dataset of 212,276 cases in the United States, yielded a lum",
    "full_text_length": 97532,
    "chunk_length": 1368
  },
  {
    "chunk_id": 599,
    "paper_filename": "daksh_2024_diagnostic_test_accuracy_of_AI_assisted_in_mammography_for_breast_imaging.pdf",
    "paper_title": "Daksh 2024 Diagnostic Test Accuracy Of Ai Assisted In Mammography For Breast Imaging",
    "chunk_index": 36,
    "total_chunks": 90,
    "text_content": "raphy. It outlines key steps from initial identification of studies, removal of duplicates, and screening to the final inclusion based on diagnostic accuracy metrics. Full-size DOI: 10.7717/peerjcs.2476/fig-1 transcended boundaries, encompassing the United Kingdom and the United States. Amidst a canvas of 26,142 cases, their results unfolded with a sensitivity of 65.42% and a specificity Dave et al. (2025), PeerJ Comput. Sci. , DOI 10.7717/peerj-cs.2476 13/32 of 94.12%, underscoring AI's adaptab",
    "full_text_length": 97532,
    "chunk_length": 1308
  },
  {
    "chunk_id": 600,
    "paper_filename": "daksh_2024_diagnostic_test_accuracy_of_AI_assisted_in_mammography_for_breast_imaging.pdf",
    "paper_title": "Daksh 2024 Diagnostic Test Accuracy Of Ai Assisted In Mammography For Breast Imaging",
    "chunk_index": 37,
    "total_chunks": 90,
    "text_content": "2018 ( Kyono, Gilbert & Van der Schaar, 2018 ) and 2020 ( Kyono, Gilbert & Van der Schaar, 2020 ), intertwine diagnostic metrics with the fabric of machine learning. The UK-based study of 2018 harnessed InceptionResNetV2, revealing metrics such as Cohen's k of 0.716 and an F1 statistical test score of 0.757. The subsequent chapter, cast in 2020, is marked by an exploration of NPV beyond 99.0%. Together, they reflect AI's dual facets as an analytical instrument and a safeguard of diagnostic preci",
    "full_text_length": 97532,
    "chunk_length": 1323
  },
  {
    "chunk_id": 601,
    "paper_filename": "daksh_2024_diagnostic_test_accuracy_of_AI_assisted_in_mammography_for_breast_imaging.pdf",
    "paper_title": "Daksh 2024 Diagnostic Test Accuracy Of Ai Assisted In Mammography For Breast Imaging",
    "chunk_index": 38,
    "total_chunks": 90,
    "text_content": "and RetinaNet. Against the backdrop of 97,769 cases in the United States, their journey resonates with sensitivities of 96.2% and specificities of 90.9%, underlining AI's promise in enriching diagnostic accuracy. Schaffter et al. (2020) conduct a symposium of networks, harmonizing CEM and VGG ensembles. Their exploration unfurls across the United States and Sweden, celebrating sensitivities of 85.9% and specificities of 88.0%. This narrative within networks captures the essence of balanced diagn",
    "full_text_length": 97532,
    "chunk_length": 1396
  },
  {
    "chunk_id": 602,
    "paper_filename": "daksh_2024_diagnostic_test_accuracy_of_AI_assisted_in_mammography_for_breast_imaging.pdf",
    "paper_title": "Daksh 2024 Diagnostic Test Accuracy Of Ai Assisted In Mammography For Breast Imaging",
    "chunk_index": 39,
    "total_chunks": 90,
    "text_content": "human-AI symbiosis, illuminating the United Kingdom with computer-aided detection prompts. This narrative oscillates with a sensitivity of 76.0% and specificity of 83.0%, reflecting the fine dance of human expertise and AI's analytical precision. Fenton et al. (2011) undertake an opus across 1.6 million images in the United States, revealing sensitivities of 85.3% and specificities of 91.4%. Their contribution punctuates the landscape with benchmarks for AI's diagnostic aspirations. Dave et al. ",
    "full_text_length": 97532,
    "chunk_length": 1325
  },
  {
    "chunk_id": 603,
    "paper_filename": "daksh_2024_diagnostic_test_accuracy_of_AI_assisted_in_mammography_for_breast_imaging.pdf",
    "paper_title": "Daksh 2024 Diagnostic Test Accuracy Of Ai Assisted In Mammography For Breast Imaging",
    "chunk_index": 40,
    "total_chunks": 90,
    "text_content": "(2019b) 83.0 77.0 Transpara 189,000 Multiple Lotter et al. (2021) 96.2 90.9 ResNet-50, RetinaNet 97,769 US Schaffter et al. (2020) 85.9 88.0 Ensemble, VGG 160,897 US and Sweden Salim et al. (2020) 81.9 96.6 ResNet-34, MobileNet 752,000 Sweden Kim et al. (2020) 88.8 81.9 ResNet-34 152,693 South Korea Taylor et al. (2005) 76.0 83.0 e R2 ImageChecker 300 UK Fenton et al. (2011) 85.3 91.4 NA 1,600,000 US Becker et al. (2017) 73.7 72.0 ViDi Suite 286 Switzerland Becker et al. (2017) add a Swiss touch",
    "full_text_length": 97532,
    "chunk_length": 1280
  },
  {
    "chunk_id": 604,
    "paper_filename": "daksh_2024_diagnostic_test_accuracy_of_AI_assisted_in_mammography_for_breast_imaging.pdf",
    "paper_title": "Daksh 2024 Diagnostic Test Accuracy Of Ai Assisted In Mammography For Breast Imaging",
    "chunk_index": 41,
    "total_chunks": 90,
    "text_content": "in Table 1. Approaches used: unleashing architectural marvels Across the spectrum of studies, algorithmic techniques emerge as a critical theme, showcasing the power of deep learning architectures. ResNet-18, a convolutional neural network, stands tall in the work of Yala et al. (2019) , yielding a remarkable sensitivity of 90.1% and specificity of 94.2% in the United States. InceptionResNetV2, embraced byKyono, Gilbert & Van der Schaar (2018) andKyono, Gilbert & Van der Schaar (2020) , unravels",
    "full_text_length": 97532,
    "chunk_length": 1394
  },
  {
    "chunk_id": 605,
    "paper_filename": "daksh_2024_diagnostic_test_accuracy_of_AI_assisted_in_mammography_for_breast_imaging.pdf",
    "paper_title": "Daksh 2024 Diagnostic Test Accuracy Of Ai Assisted In Mammography For Breast Imaging",
    "chunk_index": 42,
    "total_chunks": 90,
    "text_content": "and RetinaNet to achieve a sensitivity of 65.42% and specificity of 94.12% across the United Kingdom and the United States. Schaffter et al. (2020) harmonize CEM and VGG networks, juxtaposing sensitivities of 85.9% and specificities of 88.0% between the United States and Sweden. Such cross-continental explorations underscore AI's potential to bridge diagnostic challenges inherent to diverse healthcare ecosystems. Dave et al. (2025), PeerJ Comput. Sci. , DOI 10.7717/peerj-cs.2476 15/32 Table 2 Ad",
    "full_text_length": 97532,
    "chunk_length": 1581
  },
  {
    "chunk_id": 606,
    "paper_filename": "daksh_2024_diagnostic_test_accuracy_of_AI_assisted_in_mammography_for_breast_imaging.pdf",
    "paper_title": "Daksh 2024 Diagnostic Test Accuracy Of Ai Assisted In Mammography For Breast Imaging",
    "chunk_index": 43,
    "total_chunks": 90,
    "text_content": "an additional layer of review, minimizing oversight errors. Continuous monitoring.Challenges in integrating AI with existing workflow systems. Possibility of algorithmic bias and errors. Dependence on regular updates and maintenance. Personalized Treatment Aids in tailoring treatment plans based on individual patient characteristics. Precision medicine potential.Lack of longitudinal patient data in certain cases. Ethical concerns related to privacy and patient consent. Need for robust validation",
    "full_text_length": 97532,
    "chunk_length": 1494
  },
  {
    "chunk_id": 607,
    "paper_filename": "daksh_2024_diagnostic_test_accuracy_of_AI_assisted_in_mammography_for_breast_imaging.pdf",
    "paper_title": "Daksh 2024 Diagnostic Test Accuracy Of Ai Assisted In Mammography For Breast Imaging",
    "chunk_index": 44,
    "total_chunks": 90,
    "text_content": "attaining a sensitivity of 81.9% and specificity of 96.6%. These diagnostic metrics not only quantify AI's performance but also articulate its potential in recalibrating breast cancer diagnosis. Arti\ufb01cial intelligence based models in mammography Deep learning, a subset of AI, has gained significant traction in mammography due to its ability to analyze large datasets and identify patterns that may go unnoticed by human radiologists. Convolutional neural networks (CNNs), in particular, are widely ",
    "full_text_length": 97532,
    "chunk_length": 1457
  },
  {
    "chunk_id": 608,
    "paper_filename": "daksh_2024_diagnostic_test_accuracy_of_AI_assisted_in_mammography_for_breast_imaging.pdf",
    "paper_title": "Daksh 2024 Diagnostic Test Accuracy Of Ai Assisted In Mammography For Breast Imaging",
    "chunk_index": 45,
    "total_chunks": 90,
    "text_content": "AI systems into radiology workflows helped radiologists identify breast cancer in earlier stages, reducing false negatives and potentially improving patient outcomes. Machine learning techniques are also being employed to enhance diagnostic accuracy in mammography. These models learn from large datasets of mammograms and can assist radiologists by suggesting areas of concern, which helps in reducing human error. Zhu (2024) explored how AI-based classification systems could improve the diagnosis ",
    "full_text_length": 97532,
    "chunk_length": 1410
  },
  {
    "chunk_id": 609,
    "paper_filename": "daksh_2024_diagnostic_test_accuracy_of_AI_assisted_in_mammography_for_breast_imaging.pdf",
    "paper_title": "Daksh 2024 Diagnostic Test Accuracy Of Ai Assisted In Mammography For Breast Imaging",
    "chunk_index": 46,
    "total_chunks": 90,
    "text_content": "that AI tools significantly improved the diagnostic performance of radiologists when used as decision support tools. This research highlights the growing role of AI in clinical settings where it enhances diagnostic capabilities and reduces the time required to interpret images. AI models have been particularly effective in addressing the issue of false positives and false negatives in mammography, which can lead to unnecessary biopsies or missed cancer diagnoses. Salim et al. (2020) conducted an",
    "full_text_length": 97532,
    "chunk_length": 1424
  },
  {
    "chunk_id": 610,
    "paper_filename": "daksh_2024_diagnostic_test_accuracy_of_AI_assisted_in_mammography_for_breast_imaging.pdf",
    "paper_title": "Daksh 2024 Diagnostic Test Accuracy Of Ai Assisted In Mammography For Breast Imaging",
    "chunk_index": 47,
    "total_chunks": 90,
    "text_content": "shown to provide greater accuracy in these difficult cases. Hu et al. (2024) highlighted the need for trustworthy AI systems in mammography, advocating for evidence-based approaches to AI model development that prioritize transparency and interpretability. They emphasized the importance of designing AI systems that radiologists and healthcare providers can trust, especially when patients' lives depend on accurate diagnoses. In addition, Zhu (2024) pointed out the risk of bias in AI models that a",
    "full_text_length": 97532,
    "chunk_length": 1428
  },
  {
    "chunk_id": 611,
    "paper_filename": "daksh_2024_diagnostic_test_accuracy_of_AI_assisted_in_mammography_for_breast_imaging.pdf",
    "paper_title": "Daksh 2024 Diagnostic Test Accuracy Of Ai Assisted In Mammography For Breast Imaging",
    "chunk_index": 48,
    "total_chunks": 90,
    "text_content": "view of a patient's breast cancer risk, thereby enabling personalized treatment plans. Comparison between approaches used for AI-assisted mammography of breast imaging Table 3 has reported the comparison between different approaches used for AI-assisted mammography for breast imaging. CONCLUSIONS AND DISCUSSION Comparative performance of AI techniques The diverse array of AI techniques employed in the reviewed studies highlights the versatility of machine learning in enhancing mammographic inter",
    "full_text_length": 97532,
    "chunk_length": 1504
  },
  {
    "chunk_id": 612,
    "paper_filename": "daksh_2024_diagnostic_test_accuracy_of_AI_assisted_in_mammography_for_breast_imaging.pdf",
    "paper_title": "Daksh 2024 Diagnostic Test Accuracy Of Ai Assisted In Mammography For Breast Imaging",
    "chunk_index": 49,
    "total_chunks": 90,
    "text_content": "2024)AI integrated with IoMT Potential privacy risks Multi-platform diagnosis Detection of Contralateral Breast Cancer ( Jung et al., 2024b )Lunit INSIGHT system Requires validation Accurate in detection AI-Driven Innovations in Healthcare ( Khang, 2024 )General overview of AI Generalized data Early disease detection AI-Assisted Deep Learn- ing (Islam, Yasmin & Chowdhury, 2023 )Deep learning model High computational cost Accurate classification AI as Additional Reader in Screening ( Seker et al.",
    "full_text_length": 97532,
    "chunk_length": 1465
  },
  {
    "chunk_id": 613,
    "paper_filename": "daksh_2024_diagnostic_test_accuracy_of_AI_assisted_in_mammography_for_breast_imaging.pdf",
    "paper_title": "Daksh 2024 Diagnostic Test Accuracy Of Ai Assisted In Mammography For Breast Imaging",
    "chunk_index": 50,
    "total_chunks": 90,
    "text_content": "only enable fair comparisons between different AI models but also establish a foundation for selecting the most suitable algorithm for specific clinical contexts. Furthermore, the need for interpretable AI outcomes cannot be understated. Radiologists' confidence in AI-generated results relies on the transparency of the decision-making process, thus necessitating the development of methods that provide insights into how AI arrives at its conclusions. Diversity in data and generalization While som",
    "full_text_length": 97532,
    "chunk_length": 1475
  },
  {
    "chunk_id": 614,
    "paper_filename": "daksh_2024_diagnostic_test_accuracy_of_AI_assisted_in_mammography_for_breast_imaging.pdf",
    "paper_title": "Daksh 2024 Diagnostic Test Accuracy Of Ai Assisted In Mammography For Breast Imaging",
    "chunk_index": 51,
    "total_chunks": 90,
    "text_content": "populations. Achieving this goal will contribute to a more equitable distribution of healthcare outcomes. Dave et al. (2025), PeerJ Comput. Sci. , DOI 10.7717/peerj-cs.2476 18/32 Interpretability and trustworthiness One of the persistent challenges in AI-assisted mammography lies in the interpretability of AI-generated results. Radiologists, as the primary decision-makers, require not only accurate predictions but also an understanding of the reasoning behind those predictions ( Ongena et al., 2",
    "full_text_length": 97532,
    "chunk_length": 1438
  },
  {
    "chunk_id": 615,
    "paper_filename": "daksh_2024_diagnostic_test_accuracy_of_AI_assisted_in_mammography_for_breast_imaging.pdf",
    "paper_title": "Daksh 2024 Diagnostic Test Accuracy Of Ai Assisted In Mammography For Breast Imaging",
    "chunk_index": 52,
    "total_chunks": 90,
    "text_content": "regulatory considerations As AI-assisted mammography inches closer to clinical integration, ethical and regulatory considerations loom large. Ensuring patient privacy, obtaining informed consent for AI- generated diagnoses, and addressing potential algorithmic biases are integral components of responsible AI deployment. The dynamic nature of AI algorithms, which can evolve and adapt over time, raises questions about the stability and reliability of these systems. Striking the right balance betwe",
    "full_text_length": 97532,
    "chunk_length": 1520
  },
  {
    "chunk_id": 616,
    "paper_filename": "daksh_2024_diagnostic_test_accuracy_of_AI_assisted_in_mammography_for_breast_imaging.pdf",
    "paper_title": "Daksh 2024 Diagnostic Test Accuracy Of Ai Assisted In Mammography For Breast Imaging",
    "chunk_index": 53,
    "total_chunks": 90,
    "text_content": "However, achieving this collaboration requires extensive training for radiologists to understand AI-generated results, enabling them to effectively leverage AI's capabilities in their clinical practice. Striking the right balance between human expertise and AI support is essential to ensure optimal patient outcomes. Continued research and global impact While the current studies showcase the potential of AI-assisted mammography, ongoing research is indispensable. Addressing algorithmic biases, ev",
    "full_text_length": 97532,
    "chunk_length": 1484
  },
  {
    "chunk_id": 617,
    "paper_filename": "daksh_2024_diagnostic_test_accuracy_of_AI_assisted_in_mammography_for_breast_imaging.pdf",
    "paper_title": "Daksh 2024 Diagnostic Test Accuracy Of Ai Assisted In Mammography For Breast Imaging",
    "chunk_index": 54,
    "total_chunks": 90,
    "text_content": "et al. (2025), PeerJ Comput. Sci. , DOI 10.7717/peerj-cs.2476 19/32 Practical clinical implementation of AI-assisted mammography AI-assisted mammography holds the potential to significantly enhance the accuracy and efficiency of breast cancer detection. Practical clinical implementation involves several critical steps: \u000fData Quality and Diversity : To integrate AI effectively into clinical practice, robust and diverse datasets are essential. AI models must be trained on mammograms representing v",
    "full_text_length": 97532,
    "chunk_length": 1498
  },
  {
    "chunk_id": 618,
    "paper_filename": "daksh_2024_diagnostic_test_accuracy_of_AI_assisted_in_mammography_for_breast_imaging.pdf",
    "paper_title": "Daksh 2024 Diagnostic Test Accuracy Of Ai Assisted In Mammography For Breast Imaging",
    "chunk_index": 55,
    "total_chunks": 90,
    "text_content": "cases while relying on AI to handle routine analyses, improving overall diagnostic accuracy and reducing false negatives. \u000fInterpretability and Trust : Implementing AI systems requires ensuring that these models are interpretable to radiologists and clinicians. Techniques like saliency maps and feature importance scores can be incorporated into the workflow, allowing clinicians to understand why AI reached certain conclusions. This improves trust in AI-generated diagnoses and enables better deci",
    "full_text_length": 97532,
    "chunk_length": 1483
  },
  {
    "chunk_id": 619,
    "paper_filename": "daksh_2024_diagnostic_test_accuracy_of_AI_assisted_in_mammography_for_breast_imaging.pdf",
    "paper_title": "Daksh 2024 Diagnostic Test Accuracy Of Ai Assisted In Mammography For Breast Imaging",
    "chunk_index": 56,
    "total_chunks": 90,
    "text_content": "operations. This involves providing intuitive user interfaces that radiologists can easily navigate and ensuring that the AI-generated outputs align with radiologists' existing practices. Ethical concerns The ethical implications of AI integration in mammography must be carefully considered to ensure that the technology is used responsibly and equitably. \u000fBias and Fairness : One of the main ethical concerns is bias in AI algorithms, which can arise if the training data is not representative of t",
    "full_text_length": 97532,
    "chunk_length": 1352
  },
  {
    "chunk_id": 620,
    "paper_filename": "daksh_2024_diagnostic_test_accuracy_of_AI_assisted_in_mammography_for_breast_imaging.pdf",
    "paper_title": "Daksh 2024 Diagnostic Test Accuracy Of Ai Assisted In Mammography For Breast Imaging",
    "chunk_index": 57,
    "total_chunks": 90,
    "text_content": "secure patient information. Techniques like differential privacy and secure multi-party computation can ensure that patient data is protected during model training and inference. \u000fInformed Consent : Patients should be informed when AI systems are being used in their diagnosis. This includes explaining the role of AI in the decision-making process, its benefits, and its limitations. Informed consent processes must be adapted to include AI's role in clinical care, ensuring transparency and maintai",
    "full_text_length": 97532,
    "chunk_length": 1444
  },
  {
    "chunk_id": 621,
    "paper_filename": "daksh_2024_diagnostic_test_accuracy_of_AI_assisted_in_mammography_for_breast_imaging.pdf",
    "paper_title": "Daksh 2024 Diagnostic Test Accuracy Of Ai Assisted In Mammography For Breast Imaging",
    "chunk_index": 58,
    "total_chunks": 90,
    "text_content": "how AI reached a particular diagnosis. Developing interpretable AI models that radiologists can understand and explain to patients is crucial for fostering trust in the technology. \u000fRegulatory Approval : AI systems must undergo thorough regulatory scrutiny before being deployed in clinical settings. Regulatory agencies, such as the U.S. Food and Drug Administration (FDA) or European Medicines Agency (EMA), are responsible for ensuring that AI models meet strict standards of safety and effectiven",
    "full_text_length": 97532,
    "chunk_length": 1415
  },
  {
    "chunk_id": 622,
    "paper_filename": "daksh_2024_diagnostic_test_accuracy_of_AI_assisted_in_mammography_for_breast_imaging.pdf",
    "paper_title": "Daksh 2024 Diagnostic Test Accuracy Of Ai Assisted In Mammography For Breast Imaging",
    "chunk_index": 59,
    "total_chunks": 90,
    "text_content": "the capacity to expedite image analysis and reduce radiologist workload ( Fazal et al., 2018 ). This can lead to faster turnaround times, enabling timely diagnoses and treatment decisions. Moreover, the collaborative approach between radiologists and AI systems, where AI acts as a complementary assistant rather than a replacement, aligns with the concept of augmented intelligence. This collaborative framework promises to augment radiologists' capabilities, enhance diagnostic accuracy, and ultima",
    "full_text_length": 97532,
    "chunk_length": 1436
  },
  {
    "chunk_id": 623,
    "paper_filename": "daksh_2024_diagnostic_test_accuracy_of_AI_assisted_in_mammography_for_breast_imaging.pdf",
    "paper_title": "Daksh 2024 Diagnostic Test Accuracy Of Ai Assisted In Mammography For Breast Imaging",
    "chunk_index": 60,
    "total_chunks": 90,
    "text_content": "(2025), PeerJ Comput. Sci. , DOI 10.7717/peerj-cs.2476 21/32 This shift towards personalized treatment not only enhances patient outcomes but also contributes to the realization of precision medicine in breast cancer care. While the current studies provide a glimpse into AI's potential, future implications hinge on rigorous research and validation efforts. Longitudinal studies assessing the sustained impact of AI-assisted mammography on patient outcomes are essential to establish its long-term e",
    "full_text_length": 97532,
    "chunk_length": 1503
  },
  {
    "chunk_id": 624,
    "paper_filename": "daksh_2024_diagnostic_test_accuracy_of_AI_assisted_in_mammography_for_breast_imaging.pdf",
    "paper_title": "Daksh 2024 Diagnostic Test Accuracy Of Ai Assisted In Mammography For Breast Imaging",
    "chunk_index": 61,
    "total_chunks": 90,
    "text_content": "Allen et al., 2021 ). Furthermore, ethical frameworks should encompass guidelines for obtaining informed patient consent, ensuring data privacy, and mitigating algorithmic biases. Regulatory bodies and healthcare institutions play a pivotal role in shaping the responsible integration of AI into clinical workflows, safeguarding patient rights, and maintaining the highest standards of patient care. The future implications of AI-assisted mammography extend beyond technological advancements. The glo",
    "full_text_length": 97532,
    "chunk_length": 1461
  },
  {
    "chunk_id": 625,
    "paper_filename": "daksh_2024_diagnostic_test_accuracy_of_AI_assisted_in_mammography_for_breast_imaging.pdf",
    "paper_title": "Daksh 2024 Diagnostic Test Accuracy Of Ai Assisted In Mammography For Breast Imaging",
    "chunk_index": 62,
    "total_chunks": 90,
    "text_content": "with the integration of AI-assisted mammography. The convergence of advanced machine learning techniques and medical imaging is poised to reshape clinical practice, enhance patient care, and advance our understanding of this complex disease ( Larsen et al., 2022 ; Fusco et al., 2021 ). This narrative review has illuminated the key facets of AI's potential, challenges, and future implications in breast cancer diagnosis. The amalgamation of AI algorithms with mammographic images has demonstrated t",
    "full_text_length": 97532,
    "chunk_length": 1466
  },
  {
    "chunk_id": 626,
    "paper_filename": "daksh_2024_diagnostic_test_accuracy_of_AI_assisted_in_mammography_for_breast_imaging.pdf",
    "paper_title": "Daksh 2024 Diagnostic Test Accuracy Of Ai Assisted In Mammography For Breast Imaging",
    "chunk_index": 63,
    "total_chunks": 90,
    "text_content": "the complexity of AI-assisted mammography ( Ndikum- Moffor et al., 2013 ;Le et al., 2019 ). Addressing these challenges requires multidisciplinary collaboration, incorporating inputs from radiologists, data scientists, ethicists, and regulatory bodies. A comprehensive approach ensures that AI's benefits are maximized while minimizing potential pitfalls. The future of breast cancer diagnosis is inherently Dave et al. (2025), PeerJ Comput. Sci. , DOI 10.7717/peerj-cs.2476 22/32 collaborative, wher",
    "full_text_length": 97532,
    "chunk_length": 1463
  },
  {
    "chunk_id": 627,
    "paper_filename": "daksh_2024_diagnostic_test_accuracy_of_AI_assisted_in_mammography_for_breast_imaging.pdf",
    "paper_title": "Daksh 2024 Diagnostic Test Accuracy Of Ai Assisted In Mammography For Breast Imaging",
    "chunk_index": 64,
    "total_chunks": 90,
    "text_content": "forefront. Striking a balance between innovation and patient privacy, ensuring informed consent, and addressing algorithmic biases are vital for responsible AI deployment. The patient-centered approach remains paramount, ensuring that AI tools enhance individualized treatment plans and contribute to improved patient outcomes. The journey of AI-assisted mammography is one of ongoing exploration. Continued research, validation studies, and long-term outcome assessments are pivotal in establishing ",
    "full_text_length": 97532,
    "chunk_length": 1546
  },
  {
    "chunk_id": 628,
    "paper_filename": "daksh_2024_diagnostic_test_accuracy_of_AI_assisted_in_mammography_for_breast_imaging.pdf",
    "paper_title": "Daksh 2024 Diagnostic Test Accuracy Of Ai Assisted In Mammography For Breast Imaging",
    "chunk_index": 65,
    "total_chunks": 90,
    "text_content": "Additionally, implementing data augmentation techniques, such as simulating different imaging scenarios, will artificially expand dataset diversity, ensuring that AI models can generalize across a broader range of clinical conditions. Furthermore, collaboration across institutions to standardize image acquisition protocols is crucial for ensuring consistency in data collection, which will reduce noise in AI training data and improve model performance. Generalization of AI models To ensure AI mod",
    "full_text_length": 97532,
    "chunk_length": 1493
  },
  {
    "chunk_id": 629,
    "paper_filename": "daksh_2024_diagnostic_test_accuracy_of_AI_assisted_in_mammography_for_breast_imaging.pdf",
    "paper_title": "Daksh 2024 Diagnostic Test Accuracy Of Ai Assisted In Mammography For Breast Imaging",
    "chunk_index": 66,
    "total_chunks": 90,
    "text_content": "Interpretability To facilitate the integration of AI into clinical workflows, it is essential to develop explainable AI methods, such as attention maps, saliency maps, and feature importance scores, which help radiologists understand why AI algorithms make specific predictions. These tools will enhance transparency and increase confidence in AI-driven decisions ( Batchu et al., 2021 ). Additionally, simplifying model architectures can strike a balance between complexity Dave et al. (2025), PeerJ",
    "full_text_length": 97532,
    "chunk_length": 1417
  },
  {
    "chunk_id": 630,
    "paper_filename": "daksh_2024_diagnostic_test_accuracy_of_AI_assisted_in_mammography_for_breast_imaging.pdf",
    "paper_title": "Daksh 2024 Diagnostic Test Accuracy Of Ai Assisted In Mammography For Breast Imaging",
    "chunk_index": 67,
    "total_chunks": 90,
    "text_content": "by the authors: Qatar National Library. Competing Interests The authors declare there are no competing interests. Author Contributions \u000fDaksh Dave conceived and designed the experiments, prepared figures and/or tables, and approved the final draft. \u000fAdnan Akhunzada performed the experiments, authored or reviewed drafts of the article, and approved the final draft. \u000fNikola Ivkovi\u00a2 analyzed the data, performed the computation work, prepared figures and/or tables, and approved the final draft. \u000fSuj",
    "full_text_length": 97532,
    "chunk_length": 1406
  },
  {
    "chunk_id": 631,
    "paper_filename": "daksh_2024_diagnostic_test_accuracy_of_AI_assisted_in_mammography_for_breast_imaging.pdf",
    "paper_title": "Daksh 2024 Diagnostic Test Accuracy Of Ai Assisted In Mammography For Breast Imaging",
    "chunk_index": 68,
    "total_chunks": 90,
    "text_content": "following information was supplied regarding data availability: This is a literature review and did not utilize raw data. REFERENCES Aggarwal R, Sounderajah V, Martin G, Ting DS, Karthikesalingam A, King D, Ashrafian H, Darzi A. 2021. Diagnostic accuracy of deep learning in medical imaging: a systematic review and meta-analysis. NPJ Digital Medicine 4:65 DOI 10.1038/s41746-021-00438-z. Dave et al. (2025), PeerJ Comput. Sci. , DOI 10.7717/peerj-cs.2476 24/32 Allen B, Agarwal S, Coombs L, Wald C, ",
    "full_text_length": 97532,
    "chunk_length": 1384
  },
  {
    "chunk_id": 632,
    "paper_filename": "daksh_2024_diagnostic_test_accuracy_of_AI_assisted_in_mammography_for_breast_imaging.pdf",
    "paper_title": "Daksh 2024 Diagnostic Test Accuracy Of Ai Assisted In Mammography For Breast Imaging",
    "chunk_index": 69,
    "total_chunks": 90,
    "text_content": "C, Chen C, Ren Y, Lo JY, Rudin C. 2021. A case- based interpretable deep learning model for classification of mass lesions in digital mammography. Nature Machine Intelligence 3(12) :1061\u00151070 DOI 10.1038/s42256-021-00423-x. Batchu S, Liu F, Amireh A, Waller J, Umair M. 2021. A review of applications of machine learning in mammography and future challenges. Oncology 99(8) :483\u0015490 DOI 10.1159/000515698. Becker AS, Marcon M, Ghafoor S, Wurnig MC, Frauenfelder T, Boss A. 2017. Deep learning in mamm",
    "full_text_length": 97532,
    "chunk_length": 1404
  },
  {
    "chunk_id": 633,
    "paper_filename": "daksh_2024_diagnostic_test_accuracy_of_AI_assisted_in_mammography_for_breast_imaging.pdf",
    "paper_title": "Daksh 2024 Diagnostic Test Accuracy Of Ai Assisted In Mammography For Breast Imaging",
    "chunk_index": 70,
    "total_chunks": 90,
    "text_content": "PY, Cheng YC, Zhong ZH, Zhang FZ, Pai N-S, Li C-M, Lin C-H. 2024. Informa- tion security and artificial intelligence\u0015assisted diagnosis in an Internet of Medical Thing System (IoMTS). IEEE Access 12:9757\u00159775 DOI 10.1109/ACCESS.2024.3351373. Daley CM, Filippi M, James AS, Weir M, Braiuca S, Kaur B, Choi WS, Greiner KA. 2012. American Indian community leader and provider views of needs and barriers to mammography. Journal of Community Health 37:307\u0015315 DOI 10.1007/s10900-011-9446-7. Dave D. 2023.",
    "full_text_length": 97532,
    "chunk_length": 1444
  },
  {
    "chunk_id": 634,
    "paper_filename": "daksh_2024_diagnostic_test_accuracy_of_AI_assisted_in_mammography_for_breast_imaging.pdf",
    "paper_title": "Daksh 2024 Diagnostic Test Accuracy Of Ai Assisted In Mammography For Breast Imaging",
    "chunk_index": 71,
    "total_chunks": 90,
    "text_content": "precision oncology. Computational and Structural Biotechnology Journal 18:2300\u00152311 DOI 10.1016/j.csbj.2020.08.019. Dave et al. (2025), PeerJ Comput. Sci. , DOI 10.7717/peerj-cs.2476 25/32 Engelman KK, Daley CM, Gajewski BJ, Ndikum-Moffor F, Faseru B, Braiuca S, Joseph S, Ellerbeck EF, Greiner KA. 2010. An assessment of American Indian women's mammography experiences. BMC Women's Health 10:34 DOI 10.1186/1472-6874-10-34. Fan Z, He Y, Sun W, Li Z, Ye C, Wang C. 2023. Clinical characteristics, dia",
    "full_text_length": 97532,
    "chunk_length": 1453
  },
  {
    "chunk_id": 635,
    "paper_filename": "daksh_2024_diagnostic_test_accuracy_of_AI_assisted_in_mammography_for_breast_imaging.pdf",
    "paper_title": "Daksh 2024 Diagnostic Test Accuracy Of Ai Assisted In Mammography For Breast Imaging",
    "chunk_index": 72,
    "total_chunks": 90,
    "text_content": "DOI 10.1093/jnci/djr206. Freeman K, Geppert J, Stinton C, Todkill D, Johnson S, Clarke A, Taylor-Phillips S. 2021. Use of artificial intelligence for image analysis in breast cancer screening pro- grammes: systematic review of test accuracy. BMJ 374:n1872 DOI 10.1136/bmj.n1872. Fusco R, Piccirillo A, Sansone M, Granata V, Rubulotta MR, Petrosino T, Barretta ML, Vallone P, Di Giacomo R, Esposito E, Di Bonito M, Petrillo A. 2021. Radiomics and artificial intelligence analysis with textural metrics",
    "full_text_length": 97532,
    "chunk_length": 1389
  },
  {
    "chunk_id": 636,
    "paper_filename": "daksh_2024_diagnostic_test_accuracy_of_AI_assisted_in_mammography_for_breast_imaging.pdf",
    "paper_title": "Daksh 2024 Diagnostic Test Accuracy Of Ai Assisted In Mammography For Breast Imaging",
    "chunk_index": 73,
    "total_chunks": 90,
    "text_content": "A, Caleffi M, Dunstan Yataco JA, Gyawali B, McCormack V, McLaughlin de Anderson M, Mehrotra R, Mohar A, Murillo R, Pace LE, Paskett ED, Romanoff A, Rositch AF, Sheel JR, Schneidman M, Unger- Salda\u00f1a K, Vanderpuye V, Wu T-Y, Yuma S, Dvaladze A, Duggan C, Anderson BO. 2020. Breast cancer early detection: a phased approach to implementation. Cancer 126:2379\u00152393 DOI 10.1002/cncr.32887. Halling-Brown MD, Warren LM, Ward D, Lewis E, Mackenzie A, Wallis MG, Wilkin- son LS, Given-Wilson RM, MvAvinchey ",
    "full_text_length": 97532,
    "chunk_length": 1273
  },
  {
    "chunk_id": 637,
    "paper_filename": "daksh_2024_diagnostic_test_accuracy_of_AI_assisted_in_mammography_for_breast_imaging.pdf",
    "paper_title": "Daksh 2024 Diagnostic Test Accuracy Of Ai Assisted In Mammography For Breast Imaging",
    "chunk_index": 74,
    "total_chunks": 90,
    "text_content": "A novel method for tumor detection using deep learning techniques. Journal of Medical Imaging and Analysis 45(3) :123\u0015134. Dave et al. (2025), PeerJ Comput. Sci. , DOI 10.7717/peerj-cs.2476 26/32 Hickman SE, Woitek R, Le EPV, Im YR, Mouritsen Luxh\u00f8j M, Aviles-Rivero AI, Baxter GC, MacKay JW, Gilbert FJ. 2022. Machine learning for workflow applications in screening mammography: systematic review and meta-analysis. Radiology 302(1) :88\u0015104 DOI 10.1148/radiol.2021210391. Houssami N, Lee CI, Buist D",
    "full_text_length": 97532,
    "chunk_length": 1344
  },
  {
    "chunk_id": 638,
    "paper_filename": "daksh_2024_diagnostic_test_accuracy_of_AI_assisted_in_mammography_for_breast_imaging.pdf",
    "paper_title": "Daksh 2024 Diagnostic Test Accuracy Of Ai Assisted In Mammography For Breast Imaging",
    "chunk_index": 75,
    "total_chunks": 90,
    "text_content": "He W, Sun Y, Cai J, Li Q, Chen Y, Yang S, Chongqing Primary Aldosteronism Study (CONPASS) Group. 2023. Accuracy of gallium- 68 pentixafor positron emission tomography\u0016computed tomography for sub- typing diagnosis of primary aldosteronism. JAMA Network Open 6(2):e2255609 DOI 10.1001/jamanetworkopen.2022.55609. Islam MS, Yasmin S, Chowdhury TA, Sayem A, Mim K. 2023. AI-assisted breast cancer diagnosis: Deep learning on RSNA mammogram images for improved classification. In:26th International confer",
    "full_text_length": 97532,
    "chunk_length": 1480
  },
  {
    "chunk_id": 639,
    "paper_filename": "daksh_2024_diagnostic_test_accuracy_of_AI_assisted_in_mammography_for_breast_imaging.pdf",
    "paper_title": "Daksh 2024 Diagnostic Test Accuracy Of Ai Assisted In Mammography For Breast Imaging",
    "chunk_index": 76,
    "total_chunks": 90,
    "text_content": "2(1):27\u001536 DOI 10.22376/ijtos.2024.2.1.27-36. Jia X, Ren L, Cai J. 2020. Clinical implementation of AI technologies will require interpretable AI models. Medical Physics 47(1) :1\u00154 DOI 10.1002/mp.13891. Jung J, Dai J, Liu B, Wu Q. 2024a. Artificial intelligence in fracture detection with different image modalities and data types: a systematic review and meta-analysis. PLOS Digital Health 3(1):e0000438 DOI 10.1371/journal.pdig.0000438. Jung JJ, Kang E, Byeon J, Kim HK, Lee HB, Moon HG, Han W. 202",
    "full_text_length": 97532,
    "chunk_length": 1501
  },
  {
    "chunk_id": 640,
    "paper_filename": "daksh_2024_diagnostic_test_accuracy_of_AI_assisted_in_mammography_for_breast_imaging.pdf",
    "paper_title": "Daksh 2024 Diagnostic Test Accuracy Of Ai Assisted In Mammography For Breast Imaging",
    "chunk_index": 77,
    "total_chunks": 90,
    "text_content": "al. (2025), PeerJ Comput. Sci. , DOI 10.7717/peerj-cs.2476 27/32 Kim HE, Kim HH, Han BK, Kim KH, Han K, Nam H, Lee EH, Kim EK. 2020. Changes in cancer detection and false-positive recall in mammography using artificial intelli- gence: a retrospective, multireader study. The Lancet. Digital Health 2(3):e138\u0015e148 DOI 10.1016/S2589-7500(20)30003-0. King S. 2004. Pink Ribbons Inc: breast cancer activism and the politics of philan- thropy. International Journal of Qualitative Studies in Education 17(",
    "full_text_length": 97532,
    "chunk_length": 1415
  },
  {
    "chunk_id": 641,
    "paper_filename": "daksh_2024_diagnostic_test_accuracy_of_AI_assisted_in_mammography_for_breast_imaging.pdf",
    "paper_title": "Daksh 2024 Diagnostic Test Accuracy Of Ai Assisted In Mammography For Breast Imaging",
    "chunk_index": 78,
    "total_chunks": 90,
    "text_content": "T, Gilbert FJ, Van der Schaar M. 2020. Improving workflow efficiency for mammography using machine learning. Journal of the American College of Radiology 17(1 Pt A) :56\u001563 DOI 10.1016/j.jacr.2019.05.012. L\u00e5ng K, Dustler M, Dahlblom V, \u00c5kesson A, Andersson I, Zackrisson S. 2021a. Identifying normal mammograms in a large screening population using artificial intelligence. European Radiology 31:1687\u00151692 DOI 10.1007/s00330-020-07165-1. L\u00e5ng K, Hofvind S, Rodr\u00edguez-Ruiz A, Andersson I. 2021b. Can ar",
    "full_text_length": 97532,
    "chunk_length": 1462
  },
  {
    "chunk_id": 642,
    "paper_filename": "daksh_2024_diagnostic_test_accuracy_of_AI_assisted_in_mammography_for_breast_imaging.pdf",
    "paper_title": "Daksh 2024 Diagnostic Test Accuracy Of Ai Assisted In Mammography For Breast Imaging",
    "chunk_index": 79,
    "total_chunks": 90,
    "text_content": "DOI 10.1148/radiol.232479. Lauritzen AD, Rodr\u00edguez-Ruiz A, Von Euler-Chelpin MC, Lynge E, Vejborg I, Nielsen M, Karssemeijer N, Lillholm M. 2022. An artificial intelligence\u0015based mammog- raphy screening protocol for breast cancer: outcome and radiologist workload. Radiology 304(1) :41\u001549 DOI 10.1148/radiol.210948. Le EPV, Wang Y, Huang Y, Hickman S, Gilbert FJ. 2019. Artificial intelligence in breast imaging. Clinical Radiology 74(5) :357\u0015366 DOI 10.1016/j.crad.2019.02.006. Le Boulc'h M, Bekhouc",
    "full_text_length": 97532,
    "chunk_length": 1437
  },
  {
    "chunk_id": 643,
    "paper_filename": "daksh_2024_diagnostic_test_accuracy_of_AI_assisted_in_mammography_for_breast_imaging.pdf",
    "paper_title": "Daksh 2024 Diagnostic Test Accuracy Of Ai Assisted In Mammography For Breast Imaging",
    "chunk_index": 80,
    "total_chunks": 90,
    "text_content": "4(7):e507\u0015e519 DOI 10.1016/S2589-7500(22)00070-X. Lotter W, Diab AR, Haslam B, Kim JG, Grisot G, Wu E, Wu K, Onieva JO, Boyer Y, Boxerman JL, Wang M, Bandler M, Vijayaraghavan GR, Sorensen AG. 2021. Robust breast cancer detection in mammography and digital breast tomosyn- thesis using an annotation-efficient deep learning approach. Nature Medicine 27(2) :244\u0015249 DOI 10.1038/s41591-020-01174-9. Mayo RC, Kent D, Sen LC, Kapoor M, Leung JW, Watanabe AT. 2019. Reduction of false-positive markings on",
    "full_text_length": 97532,
    "chunk_length": 1387
  },
  {
    "chunk_id": 644,
    "paper_filename": "daksh_2024_diagnostic_test_accuracy_of_AI_assisted_in_mammography_for_breast_imaging.pdf",
    "paper_title": "Daksh 2024 Diagnostic Test Accuracy Of Ai Assisted In Mammography For Breast Imaging",
    "chunk_index": 81,
    "total_chunks": 90,
    "text_content": "Mostofi H, Peng L, Reicher JJ, Romera-Paredes B, Sidebottom R, Suleyman M, Tse D, Young KC, De Fauw J, Shetty S. 2020. International evaluation of an AI system for breast cancer screening. Nature 577(7788) :89\u001594 DOI 10.1038/s41586-019-1799-6. Mehrotra R, Ansari MA, Agrawal R, Anand RS. 2020. A transfer learning approach for AI-based classification of brain tumors. Machine Learning with Applications 2:100003 DOI 10.1016/j.mlwa.2020.100003. Ndikum-Moffor FM, Braiuca S, Daley CM, Gajewski BJ, Enge",
    "full_text_length": 97532,
    "chunk_length": 1581
  },
  {
    "chunk_id": 645,
    "paper_filename": "daksh_2024_diagnostic_test_accuracy_of_AI_assisted_in_mammography_for_breast_imaging.pdf",
    "paper_title": "Daksh 2024 Diagnostic Test Accuracy Of Ai Assisted In Mammography For Breast Imaging",
    "chunk_index": 82,
    "total_chunks": 90,
    "text_content": "Fillard P. 2020. Improving breast cancer detection accuracy of mammography with the concurrent use of an artificial intelligence tool. Radiology: Artificial Intelligence 2(6):e190208 DOI 10.1148/ryai.2020190208. Rodr\u00edguez-Ruiz A, Krupinski E, Mordang JJ, Schilling K, Heywang-K\u00f6brunner SH, Sechopoulos I, Mann RM. 2019. Detection of breast cancer with mammography: effect of an artificial intelligence support system. Radiology 290(2) :305\u0015314 DOI 10.1148/radiol.2018181371. Dave et al. (2025), PeerJ",
    "full_text_length": 97532,
    "chunk_length": 1505
  },
  {
    "chunk_id": 646,
    "paper_filename": "daksh_2024_diagnostic_test_accuracy_of_AI_assisted_in_mammography_for_breast_imaging.pdf",
    "paper_title": "Daksh 2024 Diagnostic Test Accuracy Of Ai Assisted In Mammography For Breast Imaging",
    "chunk_index": 83,
    "total_chunks": 90,
    "text_content": "I, Mann RM. 2019b. Can we reduce the workload of mammographic screening by automatic identification of normal exams with artificial intelligence? A feasibility study. European Radiology 29(9) :4825\u00154832 DOI 10.1007/s00330-019-06186-9. Romero-Mart\u00edn S, El\u00edas-Cabot E, Raya-Povedano JL, Gubern-M\u00e9rida A, Rodr\u00edguez- Ruiz A, \u00c1lvarez-Benito M. 2022. Stand-alone use of artificial intelligence for digital mammography and digital breast tomosynthesis screening: a retrospective evaluation. Radiology 302(3)",
    "full_text_length": 97532,
    "chunk_length": 1365
  },
  {
    "chunk_id": 647,
    "paper_filename": "daksh_2024_diagnostic_test_accuracy_of_AI_assisted_in_mammography_for_breast_imaging.pdf",
    "paper_title": "Daksh 2024 Diagnostic Test Accuracy Of Ai Assisted In Mammography For Breast Imaging",
    "chunk_index": 84,
    "total_chunks": 90,
    "text_content": "Kim HE, Albiol F, Albiol A, Morrell S, Wojna Z, Ahsen ME, Asif U, Jimeno Yepes A, Yohanandan S, Rabinovici-Cohen S, Yi D, Hoff B, Yu T, Neto EC, Rubin DL, Lindholm P, Margolies LR, McBride RB, Rothstein JH, Sieh W, Ben-Ari R, Harrer S, Trister A, Friend S, Norman T, Sahiner B, Strand F, Guinney J, Stolovitzky G, The DM Dream Consor- tium. 2020. Evaluation of combined artificial intelligence and radiologist assess- ment to interpret screening mammograms. JAMA Network Open 3(3):e200265 DOI 10.1001",
    "full_text_length": 97532,
    "chunk_length": 1429
  },
  {
    "chunk_id": 648,
    "paper_filename": "daksh_2024_diagnostic_test_accuracy_of_AI_assisted_in_mammography_for_breast_imaging.pdf",
    "paper_title": "Daksh 2024 Diagnostic Test Accuracy Of Ai Assisted In Mammography For Breast Imaging",
    "chunk_index": 85,
    "total_chunks": 90,
    "text_content": "Un- derdiagnosis bias of artificial intelligence algorithms applied to chest radio- graphs in under-served patient populations. Nature Medicine 27(12) :2176\u00152182 DOI 10.1038/s41591-021-01595-0. Shen L, Margolies LR, Rothstein JH, Fluder E, McBride R, Sieh W. 2019. Deep learning to improve breast cancer detection on screening mammography. Scientific Reports 9:12495 DOI 10.1038/s41598-019-48995-4. Dave et al. (2025), PeerJ Comput. Sci. , DOI 10.7717/peerj-cs.2476 30/32 Sheth D, Giger ML. 2020. Art",
    "full_text_length": 97532,
    "chunk_length": 1514
  },
  {
    "chunk_id": 649,
    "paper_filename": "daksh_2024_diagnostic_test_accuracy_of_AI_assisted_in_mammography_for_breast_imaging.pdf",
    "paper_title": "Daksh 2024 Diagnostic Test Accuracy Of Ai Assisted In Mammography For Breast Imaging",
    "chunk_index": 86,
    "total_chunks": 90,
    "text_content": "for precision diagnosis. Journal of the American College of Radiology 18(1) :174\u0015179 DOI 10.1016/j.jacr.2020.07.010. T\u00f6rnquist E, Caulk RA. 2024. Curating grounded synthetic data with global perspectives for equitable AI. ArXiv arXiv:2406.10258. Vyborny CJ, Giger ML. 1994. Computer vision and artificial intelligence in mammogra- phy. American Journal of Roentgenology 162(3) :699\u0015708 DOI 10.2214/ajr.162.3.8109525. Wanders AJ, Mees W, Bun PA, Janssen N, Rodr\u00edguez-Ruiz A, Dalm\u0019 s MU, Karssemeijer N",
    "full_text_length": 97532,
    "chunk_length": 1268
  },
  {
    "chunk_id": 650,
    "paper_filename": "daksh_2024_diagnostic_test_accuracy_of_AI_assisted_in_mammography_for_breast_imaging.pdf",
    "paper_title": "Daksh 2024 Diagnostic Test Accuracy Of Ai Assisted In Mammography For Breast Imaging",
    "chunk_index": 87,
    "total_chunks": 90,
    "text_content": "T, Katsnelson J, Kim E, Wolfson S, Parikh U, Gaddam S, Lin LLY, Ho K, Weinstein JD, Reig B, Gao Y, Toth H, Pysarenko K, Lewin A, Lee J, Airola K, Mema E, Chung S, Hwang E, Samreen N, Kim SG, Heacock L, Moy L, Cho K, Geras KJ. 2019. Deep neural networks improve radiologists' performance in breast cancer screening. IEEE Transactions on Medical Imaging 39(4) :1184\u00151194 DOI 10.1109/TMI.2019.2945514. Xia J, Cai Z, Heidari AA, Ye Y, Chen H, Pan Z. 2023. Enhanced moth-flame opti- mizer with quasi-refle",
    "full_text_length": 97532,
    "chunk_length": 1293
  },
  {
    "chunk_id": 651,
    "paper_filename": "daksh_2024_diagnostic_test_accuracy_of_AI_assisted_in_mammography_for_breast_imaging.pdf",
    "paper_title": "Daksh 2024 Diagnostic Test Accuracy Of Ai Assisted In Mammography For Breast Imaging",
    "chunk_index": 88,
    "total_chunks": 90,
    "text_content": "S, Guindy M, Barzilay R. 2022. Multi-institutional validation of a mammography-based breast cancer risk model. Journal of Clinical Oncology 40(16) :1732\u00151740 DOI 10.1200/JCO.21.01337. Yala A, Schuster T, Miles R, Barzilay R, Lehman C. 2019. A deep learning model to triage screening mammograms: a simulation study. Radiology 293(1) :38\u001546 DOI 10.1148/radiol.2019182908. Dave et al. (2025), PeerJ Comput. Sci. , DOI 10.7717/peerj-cs.2476 31/32 Yang C, Sheng D, Yang B, Zheng W, Liu C. 2024. A dual-dom",
    "full_text_length": 97532,
    "chunk_length": 1441
  },
  {
    "chunk_id": 652,
    "paper_filename": "daksh_2024_diagnostic_test_accuracy_of_AI_assisted_in_mammography_for_breast_imaging.pdf",
    "paper_title": "Daksh 2024 Diagnostic Test Accuracy Of Ai Assisted In Mammography For Breast Imaging",
    "chunk_index": 89,
    "total_chunks": 90,
    "text_content": "AK. 2001. An artificial intelligent algorithm for tumor detection in screening mammogram. IEEE Transactions on Medical Imaging 20(7) :559\u0015567 DOI 10.1109/42.932741. Zhu C. 2024. Computational intelligence-based classification system for the diagnosis of memory impairment in psychoactive substance users. Journal of Cloud Computing 13:119 DOI 10.1186/s13677-024-00675-z. Dave et al. (2025), PeerJ Comput. Sci. , DOI 10.7717/peerj-cs.2476 32/32",
    "full_text_length": 97532,
    "chunk_length": 443
  },
  {
    "chunk_id": 653,
    "paper_filename": "den_2024_diagnostic_accuracy_of_supplemetal_three_dimetnional_breast_cancer.pdf",
    "paper_title": "Den 2024 Diagnostic Accuracy Of Supplemetal Three Dimetnional Breast Cancer",
    "chunk_index": 0,
    "total_chunks": 43,
    "text_content": "den Dekker et al .Insights into Imaging (2024) 15:131 https://doi.org/10.1186/s13244-024-01714-8 ORIGINAL ARTICLE Open Access Diagnostic accuracy of supplemental three-dimensional breast ultrasound in thework-up of BI-RADS 0 screening recalls Bianca M. den Dekker1*, Mireille J. M. Broeders2,3, Carla Meeuwis4, Wikke Setz-Pels5, Alexander Venmans6, Carla H. van Gils7and Ruud M. Pijnappel1,3 Abstract Objective To evaluate the diagnostic accuracy of supplemental 3D automated breast ultrasound (ABUS)",
    "full_text_length": 41745,
    "chunk_length": 1388
  },
  {
    "chunk_id": 654,
    "paper_filename": "den_2024_diagnostic_accuracy_of_supplemetal_three_dimetnional_breast_cancer.pdf",
    "paper_title": "Den 2024 Diagnostic Accuracy Of Supplemetal Three Dimetnional Breast Cancer",
    "chunk_index": 1,
    "total_chunks": 43,
    "text_content": "total of 501 women (median age 55 years, IQR [51 \u201364]) with 525 BI-RADS 0 lesions were included between April 2018 and March 2020. Cancer was diagnosed in 45 patients. 3D ABUS sensitivity was 72.1% (95% CI [57.2 \u201383.4%]), speci \ufb01city 84.4% (95% CI [80.8 \u201387.4%]), PPV 29.2% (95% CI [21.4 \u201338.5%]), and NPV 97.1% 95.0 \u201398.4%). Sensitivity of DBT plus HHUS was 100% (95% CI [90.2 \u2013100%]), speci \ufb01city 71.4% (95% CI [67.2 \u201375.2%]), PPV 23.8% (95% CI [18.1 \u201330.5%]) and NPV 100% (95% CI [98.7 \u2013100%]). Tw",
    "full_text_length": 41745,
    "chunk_length": 1230
  },
  {
    "chunk_id": 655,
    "paper_filename": "den_2024_diagnostic_accuracy_of_supplemetal_three_dimetnional_breast_cancer.pdf",
    "paper_title": "Den 2024 Diagnostic Accuracy Of Supplemetal Three Dimetnional Breast Cancer",
    "chunk_index": 2,
    "total_chunks": 43,
    "text_content": "DBT and should not be used to omit biopsy. Supplemental 3D ABUS increases the benign biopsy rate. Trial registration Dutch Trial Register, available via https://www.onderzoekmetmensen.nl/en/trial/29659 Critical relevance statement Supplemental 3D automated breast ultrasound in the work-up of BI-RADS 0 recalls may miss over a quarter of cancers detected with other methods and should not be used to omit biopsy; ABUS \ufb01ndings did increase benign biopsy rate. Key Points \u25cfAutomated breast ultrasound (",
    "full_text_length": 41745,
    "chunk_length": 1315
  },
  {
    "chunk_id": 656,
    "paper_filename": "den_2024_diagnostic_accuracy_of_supplemetal_three_dimetnional_breast_cancer.pdf",
    "paper_title": "Den 2024 Diagnostic Accuracy Of Supplemetal Three Dimetnional Breast Cancer",
    "chunk_index": 3,
    "total_chunks": 43,
    "text_content": "credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article \u2019s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article \u2019s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permi",
    "full_text_length": 41745,
    "chunk_length": 1363
  },
  {
    "chunk_id": 657,
    "paper_filename": "den_2024_diagnostic_accuracy_of_supplemetal_three_dimetnional_breast_cancer.pdf",
    "paper_title": "Den 2024 Diagnostic Accuracy Of Supplemetal Three Dimetnional Breast Cancer",
    "chunk_index": 4,
    "total_chunks": 43,
    "text_content": "New 3D ABUS findings increase the benign biopsy rate.Diagnostic accuracy of supplemental three- dimensional breast ultrasound in the work- up of BI-RADS 0 screening recalls Insights Imaging (2024) den Dekker BM, BroedersMJM, MeeuwisC, et al. DOI:10.1186/s13244-024-01714-8 Introduction In the Netherlands, women between the ages of 50 and 75 years are invited for biennial breast cancer screeningusing full- \ufb01eld digital mammography. Each year approximately one million women participate and ~23per 1",
    "full_text_length": 41745,
    "chunk_length": 1407
  },
  {
    "chunk_id": 658,
    "paper_filename": "den_2024_diagnostic_accuracy_of_supplemetal_three_dimetnional_breast_cancer.pdf",
    "paper_title": "Den 2024 Diagnostic Accuracy Of Supplemetal Three Dimetnional Breast Cancer",
    "chunk_index": 5,
    "total_chunks": 43,
    "text_content": "in the diagnostic work-up of BI-RADS 0 recalls may reduce the benign biopsy rate. 3D ABUS was originally designed as an adjunct screening tool for women with dense breasts to overcomethe limitations of hand-held ultrasound (HHUS), which isoperator-dependent, lacks standardization and reprodu-cibility, and imposes a considerable workload on radi-ologists [ 2]. 3D ABUS enables standardized acquisition of volumetric images of the whole breast, facilitating double reading and objective comparison wi",
    "full_text_length": 41745,
    "chunk_length": 1362
  },
  {
    "chunk_id": 659,
    "paper_filename": "den_2024_diagnostic_accuracy_of_supplemetal_three_dimetnional_breast_cancer.pdf",
    "paper_title": "Den 2024 Diagnostic Accuracy Of Supplemetal Three Dimetnional Breast Cancer",
    "chunk_index": 6,
    "total_chunks": 43,
    "text_content": "[ 10]. Spiculation of malignant lesions and the retraction phenomenon caused by invasivegrowth and surrounding desmoplastic reaction lead toarchitectural distortions, which may be appreciated bestin the coronal plane [ 7,11,12]. Using the coronal view, benign lesions seen in the transverse plane may be downgraded in up to 18% of benign cases, potentially avoiding benign biopsy [ 10]. False positive \ufb01ndings necessitate further imaging, biopsy, or additional follow-up examinations which often caus",
    "full_text_length": 41745,
    "chunk_length": 1430
  },
  {
    "chunk_id": 660,
    "paper_filename": "den_2024_diagnostic_accuracy_of_supplemetal_three_dimetnional_breast_cancer.pdf",
    "paper_title": "Den 2024 Diagnostic Accuracy Of Supplemetal Three Dimetnional Breast Cancer",
    "chunk_index": 7,
    "total_chunks": 43,
    "text_content": "wasapproved by the Medical Research Ethics Committee ofthe University Medical Center Utrecht. Written informedconsent was obtained from all participants prior to the 3DABUS examination. The study protocol is available viaDutch Trial Register ( https://www.onderzoekmetmensen. nl/en/trial/29659 ). Between April 2018 and March 2020, all Dutch breast cancer screening participants (aged 50 \u201375 years), recalled for diagnostic work-up after a BI-RADS 0 screeningmammography result and referred to one of",
    "full_text_length": 41745,
    "chunk_length": 1457
  },
  {
    "chunk_id": 661,
    "paper_filename": "den_2024_diagnostic_accuracy_of_supplemetal_three_dimetnional_breast_cancer.pdf",
    "paper_title": "Den 2024 Diagnostic Accuracy Of Supplemetal Three Dimetnional Breast Cancer",
    "chunk_index": 8,
    "total_chunks": 43,
    "text_content": "practice, DBT and HHUS were evaluated together leading to a \ufb01nal BI-RADS clas- si\ufb01cation. In addition to the usual care work-up, all study participants received supplemental bilateral 3D ABUS. 3DABUS imaging was evaluated directly after the evaluationof DBT and HHUS by the same radiologist, who was notblinded for the results of previous imaging. All breast imaging was interpreted according to the ACR BI-RADS Atlas (Fifth Edition) by one of 15 radi- ologists (including C.M., W.S.P., and A.V.), al",
    "full_text_length": 41745,
    "chunk_length": 1341
  },
  {
    "chunk_id": 662,
    "paper_filename": "den_2024_diagnostic_accuracy_of_supplemetal_three_dimetnional_breast_cancer.pdf",
    "paper_title": "Den 2024 Diagnostic Accuracy Of Supplemetal Three Dimetnional Breast Cancer",
    "chunk_index": 9,
    "total_chunks": 43,
    "text_content": "(B), heterogeneously dense breasts (C), and extremely densebreasts (D) [ 13]. Malignant lesion size, measured as the largest diameter on any imaging modality, was recorded. 3D ABUS imaging 3D ABUS imaging was obtained by trained technicians using an ABUS system (InveniaTMABUS, Automated Breast Ultrasound System, GE Healthcare, Sunnyvale, CA,USA). This ABUS system consists of a scan station, equipped with a 6 \u201315 MHz wide transducer attached to a \ufb02exible arm, a touch-screen monitor, and a dedicat",
    "full_text_length": 41745,
    "chunk_length": 1333
  },
  {
    "chunk_id": 663,
    "paper_filename": "den_2024_diagnostic_accuracy_of_supplemetal_three_dimetnional_breast_cancer.pdf",
    "paper_title": "Den 2024 Diagnostic Accuracy Of Supplemetal Three Dimetnional Breast Cancer",
    "chunk_index": 10,
    "total_chunks": 43,
    "text_content": "2 mm. Of each breast, anterior-posterior, lat- eral, and medial views were obtained. Additional superiorand inferior anterior-posterior views were obtained inlarge breasts (Fig. 2). 3D ABUS imaging was sent to the dedicated work- station for multiplanar reconstruction and review in thetransverse, sagittal, and coronal plane. All radiologistsreceived \ufb01ve hours of peer-to-peer training in 3D ABUS interpretation from an expert radiologist and had access to a digital learning environment to practice",
    "full_text_length": 41745,
    "chunk_length": 1323
  },
  {
    "chunk_id": 664,
    "paper_filename": "den_2024_diagnostic_accuracy_of_supplemetal_three_dimetnional_breast_cancer.pdf",
    "paper_title": "Den 2024 Diagnostic Accuracy Of Supplemetal Three Dimetnional Breast Cancer",
    "chunk_index": 11,
    "total_chunks": 43,
    "text_content": "were referred back to the breast cancer screening programme to get reinvitedfor screening mammography in the next round after twoyears. In case of a BI-RADS 3 result after diagnostic work-up, the Dutch guideline recommends tissue diagnosis or short-interval (i.e., six months) follow-up. In all partici- pants with a BI-RADS 3 result without histopathologicaldiagnosis, the results of follow-up imaging (and histo-pathology if available) of at least six months after inclu-sion were recorded. Statist",
    "full_text_length": 41745,
    "chunk_length": 1317
  },
  {
    "chunk_id": 665,
    "paper_filename": "den_2024_diagnostic_accuracy_of_supplemetal_three_dimetnional_breast_cancer.pdf",
    "paper_title": "Den 2024 Diagnostic Accuracy Of Supplemetal Three Dimetnional Breast Cancer",
    "chunk_index": 12,
    "total_chunks": 43,
    "text_content": "true-negative examinations by the total num- ber of benign cases. Positive predictive value wasden Dekker et al .Insights into Imaging (2024) 15:131 Page 3 of 12 Fig. 1 Study \ufb02owchart. A total of 1059 women, recalled for diagnostic work-up after a BI-RADS 0 screening mammography result and referred to one of three participating hospitals between April 2018 and March 2020 were considered eligible. A total of 501 women signed informed consent and received supplemental 3D ABUS, in addition to usual",
    "full_text_length": 41745,
    "chunk_length": 1246
  },
  {
    "chunk_id": 666,
    "paper_filename": "den_2024_diagnostic_accuracy_of_supplemetal_three_dimetnional_breast_cancer.pdf",
    "paper_title": "Den 2024 Diagnostic Accuracy Of Supplemetal Three Dimetnional Breast Cancer",
    "chunk_index": 13,
    "total_chunks": 43,
    "text_content": "to follow up. After a minimum of six months, follow-up imaging was performed in 27 participants, after which 23 wer e referred back to the screening programme and four underwent breast biopsy. 84 patients were classi \ufb01ed BIRADS 4/5; all underwent breast biopsy. In total, 45/501 participants were diagnosed with cancer; 44 had breast cancer and one participant was diagnosed with non-Hodgkin lymphoma afterdetection of an intramammary lymph node on breast imagingden Dekker et al .Insights into Imagi",
    "full_text_length": 41745,
    "chunk_length": 1347
  },
  {
    "chunk_id": 667,
    "paper_filename": "den_2024_diagnostic_accuracy_of_supplemetal_three_dimetnional_breast_cancer.pdf",
    "paper_title": "Den 2024 Diagnostic Accuracy Of Supplemetal Three Dimetnional Breast Cancer",
    "chunk_index": 14,
    "total_chunks": 43,
    "text_content": "of three participating hospitals betweenApril 2018 and March 2020 were considered eligible. Atotal of 501 women (median age 55 years, IQR [51 \u201364]) signed informed consent (Fig. 1). Recorded reasons for patient non-participation were: no interest in research participation ( n=33), breast cancer anxiety related to the diagnostic work-up ( n=20), no time for the ABUS examination ( n=11) and three women could not parti- cipate because of physical impairments that hamperedcorrect positioning for 3D ",
    "full_text_length": 41745,
    "chunk_length": 1354
  },
  {
    "chunk_id": 668,
    "paper_filename": "den_2024_diagnostic_accuracy_of_supplemetal_three_dimetnional_breast_cancer.pdf",
    "paper_title": "Den 2024 Diagnostic Accuracy Of Supplemetal Three Dimetnional Breast Cancer",
    "chunk_index": 15,
    "total_chunks": 43,
    "text_content": "were descri-bed as masses (327/525) on screening mammography, followed by asymmetries (159/525) and architectural distortions in one direction (24/525). There were15 lesions categorized as BI-RADS 0 that, in hindsight, should have been categorized differently based on thelesion description in the recall letter: eleven architecturaldistortions in two directions, two masses with archi-tectural distortion, one calci \ufb01cations-only lesion and one mass with calci \ufb01cations. According to the Dutch scree",
    "full_text_length": 41745,
    "chunk_length": 1348
  },
  {
    "chunk_id": 669,
    "paper_filename": "den_2024_diagnostic_accuracy_of_supplemetal_three_dimetnional_breast_cancer.pdf",
    "paper_title": "Den 2024 Diagnostic Accuracy Of Supplemetal Three Dimetnional Breast Cancer",
    "chunk_index": 16,
    "total_chunks": 43,
    "text_content": "and 3DABUS, the \ufb01nal clinical BI-RADS classi \ufb01cation was BI- RADS 1 or 2 in 341 (68.1%) parti cipants; these partici- pants were referred back to the breast cancer screeningprogramme. 12 (2.4%) participants with a \ufb01nal BI-RADS 5 and 72 (14.4%) with a BI-RADS 4 result underwent breast biopsy for histopatho logical diagnosis, yielding breast cancer in 12/12 (100%) and in 29/72 (40.3%)participants respectively. Out of 76 (15.2%) participantswith a BI-RADS 3 result, 43 women underwent breastbiopsy a",
    "full_text_length": 41745,
    "chunk_length": 1266
  },
  {
    "chunk_id": 670,
    "paper_filename": "den_2024_diagnostic_accuracy_of_supplemetal_three_dimetnional_breast_cancer.pdf",
    "paper_title": "Den 2024 Diagnostic Accuracy Of Supplemetal Three Dimetnional Breast Cancer",
    "chunk_index": 17,
    "total_chunks": 43,
    "text_content": "BThe \ufb01ve views for full coverage of large breasts with additional superior (SUP) and inferior (INF) viewsden Dekker et al .Insights into Imaging (2024) 15:131 Page 5 of 12 back to the screening programme and four underwent breast biopsy. Out of 76 participants with a \ufb01nal BI- RADS 3 result, three (3.9%) were diagnosed with breastcancer. In total, 44/501 (8.8%) participants were diag- nosed with breast cancer (median lesion size 9.5 mm, range 4 \u201369 mm). In addition, one participant was diag- nose",
    "full_text_length": 41745,
    "chunk_length": 1249
  },
  {
    "chunk_id": 671,
    "paper_filename": "den_2024_diagnostic_accuracy_of_supplemetal_three_dimetnional_breast_cancer.pdf",
    "paper_title": "Den 2024 Diagnostic Accuracy Of Supplemetal Three Dimetnional Breast Cancer",
    "chunk_index": 18,
    "total_chunks": 43,
    "text_content": "of non-Hodgkin lym-phoma. Complete 3D ABUS examination was available for523/525 BI-RADS 0 lesions (Table 2), two examinations were terminated before completion due to adverse events.3D ABUS sensitivity was 72.1% (95% CI [57.2 \u201383.4%]), speci \ufb01city was 84.4% (95% CI [80.8 \u201387.4%]), FPR was 15.6% (95% CI [12.6 \u201319.2%]), FNR was 27.9% (16.6 \u201342.8%),PPV was 29.2% (95% CI [21.4 \u201338.5%]) and NPV was 97.1% (95.0 \u201398.4%) (Table 2). Twelve lesions recalled from screening with a BI-RADS 0 result thatconta",
    "full_text_length": 41745,
    "chunk_length": 1186
  },
  {
    "chunk_id": 672,
    "paper_filename": "den_2024_diagnostic_accuracy_of_supplemetal_three_dimetnional_breast_cancer.pdf",
    "paper_title": "Den 2024 Diagnostic Accuracy Of Supplemetal Three Dimetnional Breast Cancer",
    "chunk_index": 19,
    "total_chunks": 43,
    "text_content": "mm), one grade II invasive lobular carcinoma (9 mm), one grade II invasive micropapillary carcinoma (5 mm) and one grade I DCIS (5 mm). An imaging example of a breast cancermissed on 3D ABUS is provided in Fig. 3. In addition, there were two new cancers detected on both DBT and HHUS during diagnostic work-up that wasmissed on 3D ABUS. Cancers that were missed on 3DABUS had a mean lesion size of 9.0 mm, versus 14.6 mmin cancers detected on 3D ABUS ( p-value 0.06). A detailed overview of all 3D AB",
    "full_text_length": 41745,
    "chunk_length": 1246
  },
  {
    "chunk_id": 673,
    "paper_filename": "den_2024_diagnostic_accuracy_of_supplemetal_three_dimetnional_breast_cancer.pdf",
    "paper_title": "Den 2024 Diagnostic Accuracy Of Supplemetal Three Dimetnional Breast Cancer",
    "chunk_index": 20,
    "total_chunks": 43,
    "text_content": "NA Intraductal papilloma/papillary lesion 8 Complex sclerosing lesion/radial scar 3FEA/CCL 3Benign n=64 n=2 n=7 Fibrosis/ \ufb01brocystic changes 22 1 3 Fibroadenoma 14 1 1Reactive changes 6UDH 5 1 Lymph node 3 Hemangioma 2PASH 2Apocrine cyst/metaplasia 4Adenosis 1Benign skin lesion 1 1No abnormalities 4 CCL columnar cell lesion, DCIS ductal carcinoma in situ, FEA\ufb02at epithelial atypia, NAnot applicable, NST no special type, PASH pseudo angiomatous stromal hyperplasia, UDH usual-type ductal hyperplasi",
    "full_text_length": 41745,
    "chunk_length": 1239
  },
  {
    "chunk_id": 674,
    "paper_filename": "den_2024_diagnostic_accuracy_of_supplemetal_three_dimetnional_breast_cancer.pdf",
    "paper_title": "Den 2024 Diagnostic Accuracy Of Supplemetal Three Dimetnional Breast Cancer",
    "chunk_index": 21,
    "total_chunks": 43,
    "text_content": "performed, yielding benign histopathologicaldiagnoses in all. In the remaining BI-RADS 3 lesion a follow-up HHUS was performed and the lesion wasclassi \ufb01ed as BI-RADS 2. Furthermore, one BI-RADS 0 lesion was classi \ufb01ed BI- RADS 2 on DBT and HHUS, but BI-RADS 3 on 3DABUS, leading to an additional target HHUS and biopsy procedure. In total, 3D ABUS resulted in six extra benign biopsy procedures.Table 2 Measures of diagnostic accuracy PA or \ufb01nal imaging result Breast cancer Benign Total Measure % (",
    "full_text_length": 41745,
    "chunk_length": 1224
  },
  {
    "chunk_id": 675,
    "paper_filename": "den_2024_diagnostic_accuracy_of_supplemetal_three_dimetnional_breast_cancer.pdf",
    "paper_title": "Den 2024 Diagnostic Accuracy Of Supplemetal Three Dimetnional Breast Cancer",
    "chunk_index": 22,
    "total_chunks": 43,
    "text_content": "positive 43 138 181 PPV 43/181, 23.8%21.4 \u201338.5% HHUS+DBT negative 0 344 344 NPV 344/344, 100% 95.0 \u201398.4% Total 43 482 525Measure %, 95% CI Sensitivity 43/43, 100%90.2 \u2013100%Speci \ufb01city 344/482, 71.4%67.2 \u201375.2% Measure %, 95% CI FNR 0/43, 0%0\u20139.8%FPR 138/482, 28.6%24.8 \u201332.8% Breast cancer diagnosis was de \ufb01ned as a histopathological diagnosis of DCIS or invasive malignancy, all other histopathological \ufb01ndings were considered benign. Histopathology was indicated for all lesions classi \ufb01ed BI-RA",
    "full_text_length": 41745,
    "chunk_length": 1362
  },
  {
    "chunk_id": 676,
    "paper_filename": "den_2024_diagnostic_accuracy_of_supplemetal_three_dimetnional_breast_cancer.pdf",
    "paper_title": "Den 2024 Diagnostic Accuracy Of Supplemetal Three Dimetnional Breast Cancer",
    "chunk_index": 23,
    "total_chunks": 43,
    "text_content": "three-dimensional automated breast ultrasound system, DBT digital breast tomosynthesis, HHUS hand-held ultrasound, FPR False-positive rate, FNR false- negative rate, NPV negative predictive value, PPV positive predictive valueden Dekker et al .Insights into Imaging (2024) 15:131 Page 7 of 12 Adverse events In two participants an erythematous skin reaction occurred after application o f the lotion. In one partici- pant the 3D ABUS examination was not completed forthis reason. In both patients, th",
    "full_text_length": 41745,
    "chunk_length": 1320
  },
  {
    "chunk_id": 677,
    "paper_filename": "den_2024_diagnostic_accuracy_of_supplemetal_three_dimetnional_breast_cancer.pdf",
    "paper_title": "Den 2024 Diagnostic Accuracy Of Supplemetal Three Dimetnional Breast Cancer",
    "chunk_index": 24,
    "total_chunks": 43,
    "text_content": "missed, despite being detected onDBT or HHUS. In contrast to our hypothesis that 3DABUS may reduce the benign biopsy rate, these \ufb01ndings indicate that 3D ABUS should not be used to omit biopsy. Supplemental 3D ABUS in addition to HHUS and DBTresulted in the detection of 57 new lesions and six extrabiopsy procedures, all of which were benign. As such,supplemental 3D ABUS increased the benign biopsy rate. Previous studies yielded a better diagnostic performance of 3D ABUS compared to our study, al",
    "full_text_length": 41745,
    "chunk_length": 1149
  },
  {
    "chunk_id": 678,
    "paper_filename": "den_2024_diagnostic_accuracy_of_supplemetal_three_dimetnional_breast_cancer.pdf",
    "paper_title": "Den 2024 Diagnostic Accuracy Of Supplemetal Three Dimetnional Breast Cancer",
    "chunk_index": 25,
    "total_chunks": 43,
    "text_content": "mammography ( A: CC view, B: MLO view). The lesion was classi \ufb01ed as BI-RADS 4 on digital breast tomosynthesis ( C: CC view, D: MLO view). On hand-held ultrasound the lesion was dif \ufb01cult to visualize and described as a mass of 4 mm in diameter, classi \ufb01ed as BI-RADS 3 ( E: ultrasound). The lesion was missed on 3D ABUS imaging. In hindsight, the exact location of the lesion was reevaluated on 3D ABUS and a 3 mm lesion was noted on the anterior-posterior view ( F: AP view, G: lateral view). The p",
    "full_text_length": 41745,
    "chunk_length": 1223
  },
  {
    "chunk_id": 679,
    "paper_filename": "den_2024_diagnostic_accuracy_of_supplemetal_three_dimetnional_breast_cancer.pdf",
    "paper_title": "Den 2024 Diagnostic Accuracy Of Supplemetal Three Dimetnional Breast Cancer",
    "chunk_index": 26,
    "total_chunks": 43,
    "text_content": "Age Breast densityScreening mammographyDigital breast tomosynthesis Hand-held ultrasound 3D ABUS Histopathology 1 68 B BI-RADS 0 right breast, asymmetry inferior breast on MLO viewBI-RADS 4 right breast, spiculated irregular mass 6 mmBI-RADS 4 right breast, 1. spiculated mass 9 mm 2. subareolar architecturaldistortion with calci \ufb01cationsBI-RADS 4 right breast, 1. lesion is not visualized 2. subareolar architecturaldistortion1. Invasive carcinoma NST, grade I ER+/PR+/HER2\u2212,2 m m 2. DCIS, grade II",
    "full_text_length": 41745,
    "chunk_length": 1377
  },
  {
    "chunk_id": 680,
    "paper_filename": "den_2024_diagnostic_accuracy_of_supplemetal_three_dimetnional_breast_cancer.pdf",
    "paper_title": "Den 2024 Diagnostic Accuracy Of Supplemetal Three Dimetnional Breast Cancer",
    "chunk_index": 27,
    "total_chunks": 43,
    "text_content": "right breast, mass superior lateral quadrantBI-RADS 4 right breast bilobar mass indistinct margin, 10 mmBI-RADS 1 right breast lesion is not visualizedBI-RADS 1 right breast, lesion is not visualizedDCIS, grade I 5 mm 5 55 C BI-RADS 0 left breast, asymmetry superior breaston MLO viewBI-RADS 3 left breast, asymmetry 7m mBI-RADS 3 left breast, dubious massBI-RADS 1 left breast, lesion is not visualizedInvasive carcinoma NST, grade I ER+/PR\u2212/HER2\u22128m m 6 52 B BI-RADS 0 left breast, asymmetry superio",
    "full_text_length": 41745,
    "chunk_length": 1291
  },
  {
    "chunk_id": 681,
    "paper_filename": "den_2024_diagnostic_accuracy_of_supplemetal_three_dimetnional_breast_cancer.pdf",
    "paper_title": "Den 2024 Diagnostic Accuracy Of Supplemetal Three Dimetnional Breast Cancer",
    "chunk_index": 28,
    "total_chunks": 43,
    "text_content": "ER+/PR+/HER2\u22129m m 8 52 B BI-RADS 0 left breast, mass central superior breastBI-RADS 3 left breast, irregular mas 4 mmBI-RADS 4 left breast, 1. spiculated mass 5 mm2. lobulated mass 13 mm(BI-RADS 3)BI-RADS 3 left breast, 1. Spiculated mass is notvisualized2. Lobulated mass 13 mm1. Invasive carcinoma NST, grade I ER+/PR\u2212/HER2\u22126m m 2. Fibroadenoma 9 60 B BI-RADS 0 left breast, mass superior lateral quadrantBI-RADS 4 left breast, mass indistinct margin 8 mmBI-RADS 3 left breast, dubious mass 4 mmBI-",
    "full_text_length": 41745,
    "chunk_length": 1244
  },
  {
    "chunk_id": 682,
    "paper_filename": "den_2024_diagnostic_accuracy_of_supplemetal_three_dimetnional_breast_cancer.pdf",
    "paper_title": "Den 2024 Diagnostic Accuracy Of Supplemetal Three Dimetnional Breast Cancer",
    "chunk_index": 29,
    "total_chunks": 43,
    "text_content": "breast, 7 mm BI-RADS 1 right breast, lesion is not visualizedInvasive carcinoma NST grade I ER+/PR\u2212/HER2\u221210 mm and DCIS grade II 10 mm 12 69 B BI-RADS 0 left breast, mass superior central breastBI-RADS 4 left breast, irregular mass 21 mmBI-RADS 4 left breast, irregular mass 19 mmBI-RADS 2 left breast, lesion is not visualizedInvasive carcinoma NST, grade I, ER+/PR+/HER2\u221218 mmden Dekker et al .Insights into Imaging (2024) 15:131 Page 9 of 12 mammographic \ufb01nding in screening [ 15]. However, this p",
    "full_text_length": 41745,
    "chunk_length": 1257
  },
  {
    "chunk_id": 683,
    "paper_filename": "den_2024_diagnostic_accuracy_of_supplemetal_three_dimetnional_breast_cancer.pdf",
    "paper_title": "Den 2024 Diagnostic Accuracy Of Supplemetal Three Dimetnional Breast Cancer",
    "chunk_index": 30,
    "total_chunks": 43,
    "text_content": "lesion size of 9.0 mm versus 14.6 mm in cancersthat were detected on 3D ABUS. Radiologists commented that the resolution of 3D ABUS was lower compared to HHUS, which complicated lesion detection and char-acterization, speci \ufb01cally of smaller lesions. In line with this, Shin et al described that lesion detection was reliableonly when the mean lesion diameter was > 1.2 cm and Jehet al mentioned that smaller lesions were statisticallysigni\ufb01cantly less frequently detected on 3D ABUS [ 18,19]. In our",
    "full_text_length": 41745,
    "chunk_length": 1289
  },
  {
    "chunk_id": 684,
    "paper_filename": "den_2024_diagnostic_accuracy_of_supplemetal_three_dimetnional_breast_cancer.pdf",
    "paper_title": "Den 2024 Diagnostic Accuracy Of Supplemetal Three Dimetnional Breast Cancer",
    "chunk_index": 31,
    "total_chunks": 43,
    "text_content": "right skilllevel. As with other imaging modalities, experience improves the performance of radiologists. In our study 15 study radiologists participated, resulting in an averageof ~33 ABUS examinations per radiologist. Although allradiologists received training and were encouraged topractice 3D ABUS readings in a digital learning environ-ment, the limited experience with 3D ABUS and lack ofcontinuity, the number of examinations may have beeninsuf \ufb01cient to reach the optimal skill level. In addit",
    "full_text_length": 41745,
    "chunk_length": 1224
  },
  {
    "chunk_id": 685,
    "paper_filename": "den_2024_diagnostic_accuracy_of_supplemetal_three_dimetnional_breast_cancer.pdf",
    "paper_title": "Den 2024 Diagnostic Accuracy Of Supplemetal Three Dimetnional Breast Cancer",
    "chunk_index": 32,
    "total_chunks": 43,
    "text_content": "h er a d i o l o g i s tm a yb em o r ei n c l i n e dt os c o r et h e3 DA B U Si nline with the results of previous imaging. However, despite thedetection of a suspicious lesion on DBT and/or HHUS, inover a quarter of cancers in BI-RADS 0 recalls the lesion wasmissed on 3D ABUS. This further underlines that 3D ABUSs h o u l dn o tb eu s e dt oo m i tb i o p s y . Follow-up imaging was lacking in 6 out of 33 patients scheduled for follow-up after a BI-RADS 3 result. The sixTable 3 continued Age",
    "full_text_length": 41745,
    "chunk_length": 1030
  },
  {
    "chunk_id": 686,
    "paper_filename": "den_2024_diagnostic_accuracy_of_supplemetal_three_dimetnional_breast_cancer.pdf",
    "paper_title": "Den 2024 Diagnostic Accuracy Of Supplemetal Three Dimetnional Breast Cancer",
    "chunk_index": 33,
    "total_chunks": 43,
    "text_content": "linear \ufb01ne pleomorphic calci\ufb01cations 25 mmBI-RADS 4 right breast, 1. mass 4 mm (BI-RADS 3)2. linear \ufb01ne pleomorphic calci\ufb01cations 20 mmBI-RADS 3 right breast, 1. mass2. lesion is not visualized1. Invasive carcinoma NST, grade II ER+/PR+/HER2\u22126m m 2. DCIS grade III, 40 mm 14* 54 A BI-RADS 0 right breast, mass inferior medial quadrantBI-RADS 4 right breast, 1. circumscript mass 12 mm2. microlobulated mass 10 mmBI-RADS 4 right breast, 1. circumscript mass 13 mm2. microlobulated mass 11 mmBI-RADS 4 ",
    "full_text_length": 41745,
    "chunk_length": 1249
  },
  {
    "chunk_id": 687,
    "paper_filename": "den_2024_diagnostic_accuracy_of_supplemetal_three_dimetnional_breast_cancer.pdf",
    "paper_title": "Den 2024 Diagnostic Accuracy Of Supplemetal Three Dimetnional Breast Cancer",
    "chunk_index": 34,
    "total_chunks": 43,
    "text_content": "based on all available imaging and included in our analysis. Exclu-sion of these six lesions does not affect the reported 3DABUS sensitivity (31/43, 72.1%) nor the increase in benignbiopsies. Looking to the future of 3D ABUS, development should focus on further improvement of 3D ABUSresolution. Future resear ch may look into strati \ufb01ed analysis for breast density and different applications of3D ABUS, such as operative planning and follow-up of benign lesions [ 20,21]. In addition, prototypes are",
    "full_text_length": 41745,
    "chunk_length": 1393
  },
  {
    "chunk_id": 688,
    "paper_filename": "den_2024_diagnostic_accuracy_of_supplemetal_three_dimetnional_breast_cancer.pdf",
    "paper_title": "Den 2024 Diagnostic Accuracy Of Supplemetal Three Dimetnional Breast Cancer",
    "chunk_index": 35,
    "total_chunks": 43,
    "text_content": "used to omit biopsy. Supplemental 3D ABUS after DBT and HHUS increases the benign biopsy rate. Abbreviations CC CraniocaudalDBT Digital breast tomosynthesisHHUS Handheld ultrasoundMLO Mediolateral oblique 3D ABUS Three-dimensional automated breast ultrasound Acknowledgements The authors acknowledge the study participants for their contributions andthank prof. dr. H.M. Verkooijen, the study radiologists, technicians and projectsupport personnel for their valuable contribution to this work. Author",
    "full_text_length": 41745,
    "chunk_length": 1505
  },
  {
    "chunk_id": 689,
    "paper_filename": "den_2024_diagnostic_accuracy_of_supplemetal_three_dimetnional_breast_cancer.pdf",
    "paper_title": "Den 2024 Diagnostic Accuracy Of Supplemetal Three Dimetnional Breast Cancer",
    "chunk_index": 36,
    "total_chunks": 43,
    "text_content": "this manuscript available upon reasonable request. Declarations Ethics approval and consent to participateThis prospective multicenter diagnostic study was approved by the MedicalResearch Ethics Committee of the University Medical Center Utrecht. Writteninformed consent was obtained from all participants. Consent for publication Written informed consent was obtained. Competing interests The authors of this manuscript report \ufb01nancial support from General Electric (GE). GE had no role in the desig",
    "full_text_length": 41745,
    "chunk_length": 1659
  },
  {
    "chunk_id": 690,
    "paper_filename": "den_2024_diagnostic_accuracy_of_supplemetal_three_dimetnional_breast_cancer.pdf",
    "paper_title": "Den 2024 Diagnostic Accuracy Of Supplemetal Three Dimetnional Breast Cancer",
    "chunk_index": 37,
    "total_chunks": 43,
    "text_content": "The Netherlands.5Department of Radiology, Catharina Hospital, Eindhoven, The Netherlands.6Department of Radiology, Elisabeth-TweeSteden Hospital, Tilburg, The Netherlands.7Julius Center for Health Sciences and Primary Care, University Medical Center Utrecht, UtrechtUniversity, Utrecht, The Netherlands. Received: 21 December 2023 Accepted: 4 May 2024 References 1. Monitor bevolkingsonderzoek borstkanker 2016. Integraal Kankercentrum Nederland 2018. http://www.rivm.nl/documenten/monitor- bevolking",
    "full_text_length": 41745,
    "chunk_length": 1477
  },
  {
    "chunk_id": 691,
    "paper_filename": "den_2024_diagnostic_accuracy_of_supplemetal_three_dimetnional_breast_cancer.pdf",
    "paper_title": "Den 2024 Diagnostic Accuracy Of Supplemetal Three Dimetnional Breast Cancer",
    "chunk_index": 38,
    "total_chunks": 43,
    "text_content": "(3.6%)Fibroadenoma 1 (1.8%) HHUS hand-held ultrasound, NAnot applicableden Dekker et al .Insights into Imaging (2024) 15:131 Page 11 of 12 3. Lin X, Jia M, Zhou X et al (2021) The diagnostic performance of auto- mated versus handheld breast ultrasound and mammography in symp-tomatic outpatient women: a multicenter, cross-sectional study in China.Eur Radiol 31:947 \u2013957 4. Wang HY, Jiang YX, Zhu QL et al (2012) Differentiation of benign and malignant breast lesions: a comparison between automatica",
    "full_text_length": 41745,
    "chunk_length": 1293
  },
  {
    "chunk_id": 692,
    "paper_filename": "den_2024_diagnostic_accuracy_of_supplemetal_three_dimetnional_breast_cancer.pdf",
    "paper_title": "Den 2024 Diagnostic Accuracy Of Supplemetal Three Dimetnional Breast Cancer",
    "chunk_index": 39,
    "total_chunks": 43,
    "text_content": "Han F, Fu J, Li A (2012) Analysis of eighty-one cases with breast lesions using automated breast volume scanner and comparisonwith handheld ultrasound. Eur J Radiol 81:873 \u2013878 8. Kim SH, Kang BJ, Choi BG et al (2013) Radiologists \u2019performance for detecting lesions and the interobserver variability of automated whole breast ultrasound. Korean J Radiol 14:154 \u2013163 9. Zhang Q, Hu B, Li WB (2012) Detection of breast lesions using an auto- mated breast volume scanner system. J Int Med Res 40:300 \u201330",
    "full_text_length": 41745,
    "chunk_length": 1293
  },
  {
    "chunk_id": 693,
    "paper_filename": "den_2024_diagnostic_accuracy_of_supplemetal_three_dimetnional_breast_cancer.pdf",
    "paper_title": "Den 2024 Diagnostic Accuracy Of Supplemetal Three Dimetnional Breast Cancer",
    "chunk_index": 40,
    "total_chunks": 43,
    "text_content": "XH et al (2013) Comparative study of automated breast 3-D ultrasound and handheld B-mode ultrasound for differentia- tion of benign and malignant breast masses. Ultrasound Med Biol 39:1735 \u2013174213. American College of Radiology (ACR) (2013) Breast Imaging Reporting and Data System (BI-RADS) Atlas: breast imaging reporting and datasystem. 5th ed. Virginia, Reston 14. Agresti A, Coull BA (1998) Approximate is better than \u201cexact \u201dfor interval estimation of binomial proportions. Am Stat 52:119 \u2013126 ",
    "full_text_length": 41745,
    "chunk_length": 1358
  },
  {
    "chunk_id": 694,
    "paper_filename": "den_2024_diagnostic_accuracy_of_supplemetal_three_dimetnional_breast_cancer.pdf",
    "paper_title": "Den 2024 Diagnostic Accuracy Of Supplemetal Three Dimetnional Breast Cancer",
    "chunk_index": 41,
    "total_chunks": 43,
    "text_content": "volume scanner (ABVS) interpretation and agreementof ABVS \ufb01ndings with hand held breast ultrasound (HHUS), mammo- graphy and pathology results. Eur J Radiol 82:e332 \u2013e336 18. Jeh SK, Kim SH, Choi JJ et al (2016) Comparison of automated breast ultrasonography to handheld ultrasonography in detecting and diag-nosing breast lesions. Acta Radio 57:162 \u2013169 19. Shin HJ, Kim HH, Cha JH, Park JH, Lee KE, Kim JH (2011) Automated ultrasound of the breast for diagnosis: interobserver agreement on lesionde",
    "full_text_length": 41745,
    "chunk_length": 1247
  },
  {
    "chunk_id": 695,
    "paper_filename": "den_2024_diagnostic_accuracy_of_supplemetal_three_dimetnional_breast_cancer.pdf",
    "paper_title": "Den 2024 Diagnostic Accuracy Of Supplemetal Three Dimetnional Breast Cancer",
    "chunk_index": 42,
    "total_chunks": 43,
    "text_content": "prototype to combine automated breast ultrasound and tomo-synthesis. Eur Radiol 31:3712 \u20133720 Publisher \u2019s Note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional af \ufb01liations.den Dekker et al .Insights into Imaging (2024) 15:131 Page 12 of 12",
    "full_text_length": 41745,
    "chunk_length": 297
  },
  {
    "chunk_id": 696,
    "paper_filename": "dilukshi_2023_distribution_of_breast_density_in_woman_aged.pdf",
    "paper_title": "Dilukshi 2023 Distribution Of Breast Density In Woman Aged",
    "chunk_index": 0,
    "total_chunks": 46,
    "text_content": "Vol.:(0123456789)Breast Cancer Research and Treatment (2024) 205:521\u2013531 https://doi.org/10.1007/s10549-024-07269-y EPIDEMIOLOGY The distribution of breast density in women aged 18 years and older Dilukshi Perera1 \u00b7 Sarah Pirikahu1 \u00b7 Jane Walter2 \u00b7 Gemma Cadby1 \u00b7 Ellie Darcey1 \u00b7 Rachel Lloyd1 \u00b7 Martha Hickey3 \u00b7 Christobel Saunders4 \u00b7 Michael Hackmann5,6 \u00b7 David D. Sampson7 \u00b7 John Shepherd8 \u00b7 Lothar Lilge2,9 \u00b7 Jennifer Stone1 Received: 21 September 2023 / Accepted: 24 January 2024 / Published onl",
    "full_text_length": 43773,
    "chunk_length": 1378
  },
  {
    "chunk_id": 697,
    "paper_filename": "dilukshi_2023_distribution_of_breast_density_in_woman_aged.pdf",
    "paper_title": "Dilukshi 2023 Distribution Of Breast Density In Woman Aged",
    "chunk_index": 1,
    "total_chunks": 46,
    "text_content": "describe the distributions of breast density across categories of age and BMI. Methods Breast density measures were estimated for 1,961 Australian women aged 18\u201397 years using OBS (%water and %water + %collagen). Of these, 935 women had DXA measures (percent and absolute fibroglandular dense volume, %FGV and FGV, respectively) and 354 had conventional mammographic measures (percent and absolute dense area). The distribu- tions for each breast density measure were described across categories of a",
    "full_text_length": 43773,
    "chunk_length": 1317
  },
  {
    "chunk_id": 698,
    "paper_filename": "dilukshi_2023_distribution_of_breast_density_in_woman_aged.pdf",
    "paper_title": "Dilukshi 2023 Distribution Of Breast Density In Woman Aged",
    "chunk_index": 2,
    "total_chunks": 46,
    "text_content": "of measurement. While this study is the largest of its kind, larger sample sizes are needed to provide clinically useful age-standardized measures to identify women with high breast density for their age or BMI. Keywords Breast density \u00b7 Breast cancer \u00b7 Risk factor \u00b7 Measurement Abbreviations DXA Dual X-ray absorptiometry OBS Optical breast spectroscopy BMI Body mass index%FGV Percent fibroglandular dense volume FGV Absolute fibroglandular dense volume * Jennifer Stone Jennifer.stone@uwa.edu.au ",
    "full_text_length": 43773,
    "chunk_length": 1442
  },
  {
    "chunk_id": 699,
    "paper_filename": "dilukshi_2023_distribution_of_breast_density_in_woman_aged.pdf",
    "paper_title": "Dilukshi 2023 Distribution Of Breast Density In Woman Aged",
    "chunk_index": 3,
    "total_chunks": 46,
    "text_content": "School of Electrical, Electronic and Computer Engineering, The University of Western Australia, Perth, WA, Australia 7 Surry Biophotonics, Advanced Technology Institute and School of Biosciences and Medicine, The University of Surrey, Guildford, Surrey, UK 8 Epidemiology and Population Sciences in the Pacific Program, University of Hawaii Cancer Center, Honolulu, HI, USA 9 Medical Biophysics, University of Toronto, Toronto, ON, Canada 522 Breast Cancer Research and Treatment (2024) 205:521\u2013531 N",
    "full_text_length": 43773,
    "chunk_length": 1379
  },
  {
    "chunk_id": 700,
    "paper_filename": "dilukshi_2023_distribution_of_breast_density_in_woman_aged.pdf",
    "paper_title": "Dilukshi 2023 Distribution Of Breast Density In Woman Aged",
    "chunk_index": 4,
    "total_chunks": 46,
    "text_content": "sensitivity of mam- mography, as cancers also appear white on a mammogram, resulting in a \"masking\" effect [4 \u20136]. The risk of cancer going undetected at screening (i.e., interval cancer) is pri- marily attributed to increased breast density in asymptomatic women [7 , 8]. Other than being female, age is the strongest predictor of breast cancer risk and is mainly responsible for the large variation of breast density across women [1 ]. Breast density decreases with age but breast cancer risk incre",
    "full_text_length": 43773,
    "chunk_length": 1248
  },
  {
    "chunk_id": 701,
    "paper_filename": "dilukshi_2023_distribution_of_breast_density_in_woman_aged.pdf",
    "paper_title": "Dilukshi 2023 Distribution Of Breast Density In Woman Aged",
    "chunk_index": 5,
    "total_chunks": 46,
    "text_content": "years of age) who may benefit from early breast screening. BMI also negatively confounds the association between breast density and breast cancer risk. Higher BMI increases the risk of breast cancer in postmenopausal women but is negatively associated with percent breast density [9 ]. Increasing BMI is associated with increasing total breast size [9 ]; therefore, measures of percent breast density are often negatively associated with BMI, given how it is calcu- lated. The association between BMI",
    "full_text_length": 43773,
    "chunk_length": 1294
  },
  {
    "chunk_id": 702,
    "paper_filename": "dilukshi_2023_distribution_of_breast_density_in_woman_aged.pdf",
    "paper_title": "Dilukshi 2023 Distribution Of Breast Density In Woman Aged",
    "chunk_index": 6,
    "total_chunks": 46,
    "text_content": "nor do they extend to women aged < 40 years who are not recommended for routine mam- mographic screening. As it has been shown that reducing breast density through endocrine therapy can reduce breast cancer risk [13], risk-reducing strategies targeting younger women could provide the largest benefits [14]. A continuous measure of breast density, instead of categorical measures, could help monitor changes in breast density and be used as a biomarker of the effectiveness of risk-reducing inter - v",
    "full_text_length": 43773,
    "chunk_length": 1303
  },
  {
    "chunk_id": 703,
    "paper_filename": "dilukshi_2023_distribution_of_breast_density_in_woman_aged.pdf",
    "paper_title": "Dilukshi 2023 Distribution Of Breast Density In Woman Aged",
    "chunk_index": 7,
    "total_chunks": 46,
    "text_content": "(MRI) [15, 16], dual X-ray absorpti- ometry (DXA) [ 17\u201321] and optical breast spectroscopy (OBS) [ 22\u201326]. MRI measures breast tissue water content to estimate breast density but unfortunately is not an effi- cient modality for routine breast density assessment due to accessibility issues, cost and invasiveness. DXA measures fibroglandular breast tissue volume and is relatively more accessible and less costly/invasive but still requires small doses of ionizing radiation and therefore is not suit",
    "full_text_length": 43773,
    "chunk_length": 1411
  },
  {
    "chunk_id": 704,
    "paper_filename": "dilukshi_2023_distribution_of_breast_density_in_woman_aged.pdf",
    "paper_title": "Dilukshi 2023 Distribution Of Breast Density In Woman Aged",
    "chunk_index": 8,
    "total_chunks": 46,
    "text_content": "breast collagen and water content measurements could be more meaningful when directly estimating breast density [29]. This study illustrates the distribution of breast density in Australian women aged 18\u201397 across categories of age and BMI using three different modalities: OBS, DXA and mam - mography. Age- and BMI-categorized distributions enable individual comparison of breast density to other women of similar age or BMI which in future could be used to inform individual breast cancer risk asse",
    "full_text_length": 43773,
    "chunk_length": 1268
  },
  {
    "chunk_id": 705,
    "paper_filename": "dilukshi_2023_distribution_of_breast_density_in_woman_aged.pdf",
    "paper_title": "Dilukshi 2023 Distribution Of Breast Density In Woman Aged",
    "chunk_index": 9,
    "total_chunks": 46,
    "text_content": "June 2018. These women were recalled 2+ years after their first OBS scan (Pilot Study 2, n = 283). The Raine Study is an ongoing prospective pregnancy cohort study established in 1989 with various prospective follow-ups [33, 34]. Female participants from the Raine Study were recruited at age 27 (Gen2-27, n = 452) and recalled again at age 28 years (Gen2-28, n = 356). The overlap between Gen2-27 and Gen2-28 is 217. The biological mothers (Gen1, n = 460) and grandmothers (Gen0, n = 104) of Raine S",
    "full_text_length": 43773,
    "chunk_length": 1277
  },
  {
    "chunk_id": 706,
    "paper_filename": "dilukshi_2023_distribution_of_breast_density_in_woman_aged.pdf",
    "paper_title": "Dilukshi 2023 Distribution Of Breast Density In Woman Aged",
    "chunk_index": 10,
    "total_chunks": 46,
    "text_content": "whole-body and breast- DXA scan. Gen2-27 participants were not invited to have a DXA scan. Gen0 and Gen1 participants consented for a copy of existing mammograms to be released from the population-based screening program, BreastScreen Western Australia. Exclusion criteria Women who were treated for breast cancer (endocrine-, radio- or chemo-therapy) or had breast surgery affecting both breasts (e.g., mastectomy, lumpectomy, augmentation, and/ or reduction) were not eligible to participate. Women",
    "full_text_length": 43773,
    "chunk_length": 1262
  },
  {
    "chunk_id": 707,
    "paper_filename": "dilukshi_2023_distribution_of_breast_density_in_woman_aged.pdf",
    "paper_title": "Dilukshi 2023 Distribution Of Breast Density In Woman Aged",
    "chunk_index": 11,
    "total_chunks": 46,
    "text_content": "27]. Briefly, the OBS device consists of one of four breast cups (sized to approxi- mately match bra cups A, B, C, and D) and the main control - ler board. Each cup comprises two light-emitting modules containing 13 lasers spanning the 660\u20131050 nm spectral range [27] (Supplementary Fig. 1A) and six sensitive pho- todetectors that capture the fractional wavelength-depend- ent light transmission from the two light-emitting modules (Supplementary Fig. 1B). Per breast, the light transmission between",
    "full_text_length": 43773,
    "chunk_length": 1282
  },
  {
    "chunk_id": 708,
    "paper_filename": "dilukshi_2023_distribution_of_breast_density_in_woman_aged.pdf",
    "paper_title": "Dilukshi 2023 Distribution Of Breast Density In Woman Aged",
    "chunk_index": 12,
    "total_chunks": 46,
    "text_content": "real-time quality control checks were performed by the research assistant and, when required, measurements were repeated. Women participating in Pilot Study 2 were asked to have an extra left breast scan to assess reliability. A second reference measurement of the silicon phantom was completed at the end of the breast scans for each participant. OBS data processing As described previously [22, 23, 26, 27], the resulting OBS output is processed to produce four main chromophore con - centrations r",
    "full_text_length": 43773,
    "chunk_length": 1291
  },
  {
    "chunk_id": 709,
    "paper_filename": "dilukshi_2023_distribution_of_breast_density_in_woman_aged.pdf",
    "paper_title": "Dilukshi 2023 Distribution Of Breast Density In Woman Aged",
    "chunk_index": 13,
    "total_chunks": 46,
    "text_content": "OBS-%water in the left breasts greater than 10% were examined manually and the closest measurement to the right breast was selected for inclusion (n = 23). Otherwise, the first measurement of the left breast was used. The correlation of %water between the left breasts was 0.78 after exclusion of the 23 outlying measures. The mean of the chosen left and right breasts was used to calculate the final measures for each participant. Measuring DXA breast density All DXA breast density measurements wer",
    "full_text_length": 43773,
    "chunk_length": 1227
  },
  {
    "chunk_id": 710,
    "paper_filename": "dilukshi_2023_distribution_of_breast_density_in_woman_aged.pdf",
    "paper_title": "Dilukshi 2023 Distribution Of Breast Density In Woman Aged",
    "chunk_index": 14,
    "total_chunks": 46,
    "text_content": "participants were asked to have an extra left breast scan to assess reliability. A MATLAB (MathWorks, Natick, MA) program (the University of Hawaii, version 5) was used to compute per - cent fibroglandular volume (%FGV) and absolute fibrog- landular volume (FGV) (cm3) from the DXA breast images [19]. A single trained reader (DP) measured all the DXA images. 524 Breast Cancer Research and Treatment (2024) 205:521\u2013531 Repeated DXA measures of the left breasts were assessed, and differences in %FGV",
    "full_text_length": 43773,
    "chunk_length": 1324
  },
  {
    "chunk_id": 711,
    "paper_filename": "dilukshi_2023_distribution_of_breast_density_in_woman_aged.pdf",
    "paper_title": "Dilukshi 2023 Distribution Of Breast Density In Woman Aged",
    "chunk_index": 15,
    "total_chunks": 46,
    "text_content": "views of the most recent full-field digital mammograms for Gen0/1 participants, obtained from BreastScreen Western Aus- tralia. The Cumulus software program was used (Sunny - brook Health Sciences Centre, Toronto, Canada), produc- ing measures of percent dense area (PDA) (%) and absolute dense area (DA) (cm2). A random selection of 10% of image measurements was repeated to assess the interclass correla- tion, which was > 0.9 for both measures. Statistical methods Descriptive statistics were used",
    "full_text_length": 43773,
    "chunk_length": 1289
  },
  {
    "chunk_id": 712,
    "paper_filename": "dilukshi_2023_distribution_of_breast_density_in_woman_aged.pdf",
    "paper_title": "Dilukshi 2023 Distribution Of Breast Density In Woman Aged",
    "chunk_index": 16,
    "total_chunks": 46,
    "text_content": "a subset of 935 women also had a DXA breast density scan. From Gen0/1, mammograms from 354 women (320 mothers and 34 grandmothers) were also available. The mean time dif- ference between the OBS scan date and mammogram date was approximately 1 year (SD 1.52 years). Table 1 describes the characteristics of participants stratified by measurement modality. Overall, the age of par - ticipants with OBS measures ranged from 18 to 97 years, with a median of 31.6 years (inter-quartile range (IQR) 27.5\u20134",
    "full_text_length": 43773,
    "chunk_length": 1184
  },
  {
    "chunk_id": 713,
    "paper_filename": "dilukshi_2023_distribution_of_breast_density_in_woman_aged.pdf",
    "paper_title": "Dilukshi 2023 Distribution Of Breast Density In Woman Aged",
    "chunk_index": 17,
    "total_chunks": 46,
    "text_content": "kg/m2 (IQR 21.8\u201328.7 kg/m2), similar to that of the subset with DXA measures (24.8 kg/m2 (IQR 21.9\u201328.9 kg/ m2)). The median of BMI was slightly higher in the older subset of women with mammograms (26.9 kg/m2 (IQR 23.9\u201331.1 kg/m2)). OBS measures by age: Raincloud plots for the OBS measures across age categories are shown in Figs. 2A\u2013C. From Fig. 2A, the histograms for OBS-%water were right skewed. Median OBS-%water was highest in the youngest age groups and steadily decreased (from 18\u201329 to 60\u20136",
    "full_text_length": 43773,
    "chunk_length": 1209
  },
  {
    "chunk_id": 714,
    "paper_filename": "dilukshi_2023_distribution_of_breast_density_in_woman_aged.pdf",
    "paper_title": "Dilukshi 2023 Distribution Of Breast Density In Woman Aged",
    "chunk_index": 18,
    "total_chunks": 46,
    "text_content": "%collagen, the distributions were more symmetrically distributed for all age categories except for 70 + (Fig. 2C). Median %water + %collagen was highest in the two youngest age groups (18\u201329 and 30\u201339) and steadily decreased with age (from 40\u201349 to 70 +). DXA measures by age: Figs. 3A\u2013B shows the raincloud plots for the DXA breast density measures across age cat- egories. There was considerable variation in %FGV for the younger age groups (< 50). However, the median steadily declined from 45.5% ",
    "full_text_length": 43773,
    "chunk_length": 1210
  },
  {
    "chunk_id": 715,
    "paper_filename": "dilukshi_2023_distribution_of_breast_density_in_woman_aged.pdf",
    "paper_title": "Dilukshi 2023 Distribution Of Breast Density In Woman Aged",
    "chunk_index": 19,
    "total_chunks": 46,
    "text_content": "median recorded for the 30\u201339 years age category (280.3), and the lowest for the 60 + age category (175.0) (Fig. 3B). Mammographic measures by age (at mammogram): Fig. 4A\u2013B shows the raincloud plots for the mammographic breast density measures in women over 50. Similar to OBS- %water, the clouds for both PDA and DA were mostly right skewed and the medians decreased with age (from 9.3% to 6.3% for PDA and 8.2 cm2 to 6.2 cm2 for DA). OBS measures by BMI : The median OBS measures were highest in th",
    "full_text_length": 43773,
    "chunk_length": 1175
  },
  {
    "chunk_id": 716,
    "paper_filename": "dilukshi_2023_distribution_of_breast_density_in_woman_aged.pdf",
    "paper_title": "Dilukshi 2023 Distribution Of Breast Density In Woman Aged",
    "chunk_index": 20,
    "total_chunks": 46,
    "text_content": "by BMI: Fig. 6A\u2013B shows the raincloud plots for the DXA breast density measures across BMI cat- egories. From Fig. 6A, there was larger variation in %FGV in the lowest BMI group, which reduced as BMI increased (IQRs from 37.3\u201361.9 kg/m2 to 20.1\u201327.8 kg/m2). The medi - ans also steadily declined from the lowest to highest BMI category (from 48.2 kg/m2 to 23.6 kg/m2). In contrast, for FGV, the distributions were largely symmetrical across all BMI categories and the medians steadily increased as BM",
    "full_text_length": 43773,
    "chunk_length": 1166
  },
  {
    "chunk_id": 717,
    "paper_filename": "dilukshi_2023_distribution_of_breast_density_in_woman_aged.pdf",
    "paper_title": "Dilukshi 2023 Distribution Of Breast Density In Woman Aged",
    "chunk_index": 21,
    "total_chunks": 46,
    "text_content": "the number of participants in each study group. a; The Breast Density Pilot Study had two groups\u2014Pilot Study 1 and Pilot Study 2. Both groups completed an OBS scan and questionnaire, and a subset of participants also had a DXA breast scan. b; The Raine Study had four groups of participants. Female participants from the Raine Study were recruited at age 27 (Gen2-27) and again at age 28 years (Gen2-28). Their biological mothers (Gen1) and grandmothers (Gen0) were recruited for the 28-year-old foll",
    "full_text_length": 43773,
    "chunk_length": 1250
  },
  {
    "chunk_id": 718,
    "paper_filename": "dilukshi_2023_distribution_of_breast_density_in_woman_aged.pdf",
    "paper_title": "Dilukshi 2023 Distribution Of Breast Density In Woman Aged",
    "chunk_index": 22,
    "total_chunks": 46,
    "text_content": "distribution of breast density for women aged 18\u201397 years measured using three different modalities\u2014OBS, DXA, and mammography. Despite the mix of volumetric and area measures (mam- mography) estimated from both imaging and non-imaging techniques, we have shown that the median breast density measures decrease with age and BMI for all three modalities. The exception is for DXA-FGV which only decreased after age 30 and increased with increasing BMI. Similarly, the variation in breast density measur",
    "full_text_length": 43773,
    "chunk_length": 1308
  },
  {
    "chunk_id": 719,
    "paper_filename": "dilukshi_2023_distribution_of_breast_density_in_woman_aged.pdf",
    "paper_title": "Dilukshi 2023 Distribution Of Breast Density In Woman Aged",
    "chunk_index": 23,
    "total_chunks": 46,
    "text_content": "density and age is well established [1 ]. Consistent with this literature, breast density measured from all three modali- ties decreased with age. The only exception is perhaps the appearance of a slight increase in the median of OBS-%water in the 70 + age group and of DXA-%FGV in the 60 + group compared to the preceding age category. This is supported by only a modest decrease in the median of mammographic PDA in the 60 + group compared to the 50\u201360 age group, suggesting that the oldest women i",
    "full_text_length": 43773,
    "chunk_length": 1161
  },
  {
    "chunk_id": 720,
    "paper_filename": "dilukshi_2023_distribution_of_breast_density_in_woman_aged.pdf",
    "paper_title": "Dilukshi 2023 Distribution Of Breast Density In Woman Aged",
    "chunk_index": 24,
    "total_chunks": 46,
    "text_content": "it is a pregnancy cohort. This potentially explains why their mammographic density measures are relatively low compared to a typical cross-sectional population that would have a mix of both parous and non-parous women. It could therefore also be possible that the rate of decline of Table 1 Characteristics of participants, stratified by breast density measuring methods N OBS DXA Mammograms 1961 935 354 Age (years) Median (IQR) 31.6 (27.5\u201345.9) 36.0 (28.2\u201354.2) 59.4 (55.6\u201364.0) N (%) N (%) N (%) 1",
    "full_text_length": 43773,
    "chunk_length": 1193
  },
  {
    "chunk_id": 721,
    "paper_filename": "dilukshi_2023_distribution_of_breast_density_in_woman_aged.pdf",
    "paper_title": "Dilukshi 2023 Distribution Of Breast Density In Woman Aged",
    "chunk_index": 25,
    "total_chunks": 46,
    "text_content": "(36.7) > 30 388 (19.8) 191 (20.4) 104 (29.4) Fig. 2 The distribution of OBS measures, stratified by age category. a. Distribution of OBS-%water stratified by age categories; b. distri- bution of OBS-%collagen stratified by age categories; c. distribution of OBS-%water + %collagen stratified by age categories. Within each age category all points are plotted, along with a histogram and a box- plot. The number labeled on the histogram is the median of the distri- bution. 527 Breast Cancer Research ",
    "full_text_length": 43773,
    "chunk_length": 1306
  },
  {
    "chunk_id": 722,
    "paper_filename": "dilukshi_2023_distribution_of_breast_density_in_woman_aged.pdf",
    "paper_title": "Dilukshi 2023 Distribution Of Breast Density In Woman Aged",
    "chunk_index": 26,
    "total_chunks": 46,
    "text_content": "stratified by age categories. Within each age category all points are plotted, along with a histogram and a boxplot. The number labeled on the histogram is the median of the distribu- tion. Fig. 4 The distribution of mammographic breast density measures, stratified by age category. a. Distribution of percent dense area (PDA) stratified by age categories; b. distribution dense area (DA) stratified by age categories. Within each age category, all points are plotted, along with a histogram and a bo",
    "full_text_length": 43773,
    "chunk_length": 1248
  },
  {
    "chunk_id": 723,
    "paper_filename": "dilukshi_2023_distribution_of_breast_density_in_woman_aged.pdf",
    "paper_title": "Dilukshi 2023 Distribution Of Breast Density In Woman Aged",
    "chunk_index": 27,
    "total_chunks": 46,
    "text_content": "to confirm the impacts of studying breast density within parous-only sample populations. The negative trend between age and OBS-%water is also consistent with an MRI study by Boyd and colleagues which also used percent water as an alternative measure for breast density in younger women [15]. They examined three possible models of change in breast density from early adult- hood. They concluded that it is most likely that there is large variation in breast density across women at early ages, and t",
    "full_text_length": 43773,
    "chunk_length": 1235
  },
  {
    "chunk_id": 724,
    "paper_filename": "dilukshi_2023_distribution_of_breast_density_in_woman_aged.pdf",
    "paper_title": "Dilukshi 2023 Distribution Of Breast Density In Woman Aged",
    "chunk_index": 28,
    "total_chunks": 46,
    "text_content": "OBS measures, stratified by BMI category. Distribution of OBS-%water stratified by BMI categories; b. distribu- tion of OBS-%collagen stratified by BMI categories; c. distribution of OBS-%water + %collagen stratified by BMI categories. Within each BMI category all points are plotted, along with a histogram and a boxplot. The number labeled on the histogram is the median of the distribution Fig. 6 The distribution of DXA breast density measures, stratified by BMI category. a. Distribution of perc",
    "full_text_length": 43773,
    "chunk_length": 1319
  },
  {
    "chunk_id": 725,
    "paper_filename": "dilukshi_2023_distribution_of_breast_density_in_woman_aged.pdf",
    "paper_title": "Dilukshi 2023 Distribution Of Breast Density In Woman Aged",
    "chunk_index": 29,
    "total_chunks": 46,
    "text_content": "variation with age for OBS- %water + %collagen and DXA-%FGV. The trend between median DXA-FGV and age was also negative but only after age 30. Absolute measures of breast density are restricted by total breast size. For example, Maskarinec and colleagues reported higher FGV in moth- ers than in adolescent daughters [20] who had not reached full breast maturity. Increasing BMI is associated with increasing breast size [9 ], and in the present study, women in the 18\u201329 age group had the lowest BMI",
    "full_text_length": 43773,
    "chunk_length": 1230
  },
  {
    "chunk_id": 726,
    "paper_filename": "dilukshi_2023_distribution_of_breast_density_in_woman_aged.pdf",
    "paper_title": "Dilukshi 2023 Distribution Of Breast Density In Woman Aged",
    "chunk_index": 30,
    "total_chunks": 46,
    "text_content": "density measures substantially decreased with clinical categories of BMI, consistent with the literature [1 , 9, 40]. The amount of absolute dense area is also thought to decrease with increased BMI; however, the underlying mechanism is not fully understood. Studies inves- tigating absolute dense volume, instead of area, have shown positive associations with BMI [15, 17]. This is consistent with our reported positive trend between median DXA-FGV and BMI. The three modalities used in this study a",
    "full_text_length": 43773,
    "chunk_length": 1388
  },
  {
    "chunk_id": 727,
    "paper_filename": "dilukshi_2023_distribution_of_breast_density_in_woman_aged.pdf",
    "paper_title": "Dilukshi 2023 Distribution Of Breast Density In Woman Aged",
    "chunk_index": 31,
    "total_chunks": 46,
    "text_content": "of fibroglandular tissue, respectively. Although the three modalities use different technology and units of measurement, they have all been reported to be highly correlated with each other [17, 21, 24, 41]. This study confirms that the distributions of the percentage-based breast density measures from all three modalities were consistently associated with age and BMI. The strengths of this study include the application of three breast density measuring modalities in overlap- ping samples of wome",
    "full_text_length": 43773,
    "chunk_length": 1314
  },
  {
    "chunk_id": 728,
    "paper_filename": "dilukshi_2023_distribution_of_breast_density_in_woman_aged.pdf",
    "paper_title": "Dilukshi 2023 Distribution Of Breast Density In Woman Aged",
    "chunk_index": 32,
    "total_chunks": 46,
    "text_content": "cally useful age-standardized measures to identify women with high breast density for their age or BMI. Fig. 7 The distribution of mammographic breast density measures, stratified by BMI category. a. Distribution of percent dense area (PDA) stratified by BMI categories; b. distribution dense area (DA) stratified by BMI categories. Within each BMI category all points are plotted, along with a histogram and a boxplot. The number labeled on the histogram is the median of the distribution 530 Breast",
    "full_text_length": 43773,
    "chunk_length": 1313
  },
  {
    "chunk_id": 729,
    "paper_filename": "dilukshi_2023_distribution_of_breast_density_in_woman_aged.pdf",
    "paper_title": "Dilukshi 2023 Distribution Of Breast Density In Woman Aged",
    "chunk_index": 33,
    "total_chunks": 46,
    "text_content": "women aged 18\u201397 years across categories of age and BMI using alternative and conventional modalities. This new evidence is an important first step toward ena- bling all adult women to understand their individual breast density compared to other women of similar age and BMI. Although breast cancer risk assessment tools are improving, inclusion of mammographic breast density information is restricting individual risk assessment to screen-aged women and ignores younger women, whose breast density ",
    "full_text_length": 43773,
    "chunk_length": 1324
  },
  {
    "chunk_id": 730,
    "paper_filename": "dilukshi_2023_distribution_of_breast_density_in_woman_aged.pdf",
    "paper_title": "Dilukshi 2023 Distribution Of Breast Density In Woman Aged",
    "chunk_index": 34,
    "total_chunks": 46,
    "text_content": "DP was in receipt of a Herta Massarik PhD Scholarship for Breast Cancer Research, a British Council IELTS Scholarship, and Australian Gov - ernment Research Training Program Scholarship at The University of Western Australia. Authors would like to thank: all the women who participated in the Breast Density Pilot Study and the Raine Study; BreastScreen Western Australia; and volunteer research assistance from Concetta Nocolletti-Lewins, Jessica Buttrum, and Shruti Patel. Author contribution CS, M",
    "full_text_length": 43773,
    "chunk_length": 1241
  },
  {
    "chunk_id": 731,
    "paper_filename": "dilukshi_2023_distribution_of_breast_density_in_woman_aged.pdf",
    "paper_title": "Dilukshi 2023 Distribution Of Breast Density In Woman Aged",
    "chunk_index": 35,
    "total_chunks": 46,
    "text_content": "to software; CS, SP, ED, GC and JSt. supervised the study; DP, ED, and SP contributed to validation; DP and JSt. contributed to writing\u2014origi- nal draft; DP, SP, JW, GC, ED, RL, MH, CS, MJH, DDS, JAS, LL and JSt. contributed to writing\u2014review & editing. All authors have read and agreed to the published version of the manuscript. Funding Open Access funding enabled and organized by CAUL and its Member Institutions. This study was funded by the National Breast Cancer Foundation-Pilot Study (Grant ",
    "full_text_length": 43773,
    "chunk_length": 1328
  },
  {
    "chunk_id": 732,
    "paper_filename": "dilukshi_2023_distribution_of_breast_density_in_woman_aged.pdf",
    "paper_title": "Dilukshi 2023 Distribution Of Breast Density In Woman Aged",
    "chunk_index": 36,
    "total_chunks": 46,
    "text_content": "vided by the University of Western Australia in accordance with its ethics review and approval procedures (2019/RA/5/15/1199, 2020/ ET000013). Consent to participate Informed consent was obtained from all indi- vidual participants included in the study. Open Access This article is licensed under a Creative Commons Attri- bution 4.0 International License, which permits use, sharing, adapta- tion, distribution and reproduction in any medium or format, as long as you give appropriate credit to the ",
    "full_text_length": 43773,
    "chunk_length": 1312
  },
  {
    "chunk_id": 733,
    "paper_filename": "dilukshi_2023_distribution_of_breast_density_in_woman_aged.pdf",
    "paper_title": "Dilukshi 2023 Distribution Of Breast Density In Woman Aged",
    "chunk_index": 37,
    "total_chunks": 46,
    "text_content": "obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/. References 1. BL Sprague RE Gangnon V Burt 2014 Prevalence of mammo- graphically dense breasts in the United States J Natl Cancer Inst 106 10 dju255 2. D Bond-Smith J Stone 2018 Methodological challenges and updated findings from a meta-analysis of the association between mammographic density and breast cancer Cancer Epidemiol Bio- markers Prev https:// doi. or",
    "full_text_length": 43773,
    "chunk_length": 1226
  },
  {
    "chunk_id": 734,
    "paper_filename": "dilukshi_2023_distribution_of_breast_density_in_woman_aged.pdf",
    "paper_title": "Dilukshi 2023 Distribution Of Breast Density In Woman Aged",
    "chunk_index": 38,
    "total_chunks": 46,
    "text_content": "breast density, and hormone replace- ment therapy use on the accuracy of screening mammography Ann Intern Med 138 3 168 175 6. TM Kolb J Lichy JH Newhouse 2002 Comparison of the perfor - mance of screening mammography, physical examination, and breast US and evaluation of factors that influence them: an analy - sis of 27,825 patient evaluations Radiology 225 1 165 175 7. LM Henderson RA Hubbard BL Sprague 2015 Increased risk of developing breast cancer after a false-positive screening mam- mogra",
    "full_text_length": 43773,
    "chunk_length": 1185
  },
  {
    "chunk_id": 735,
    "paper_filename": "dilukshi_2023_distribution_of_breast_density_in_woman_aged.pdf",
    "paper_title": "Dilukshi 2023 Distribution Of Breast Density In Woman Aged",
    "chunk_index": 39,
    "total_chunks": 46,
    "text_content": "J Liao 2014 Differential impact of body mass index on absolute and percent breast density: implications 531 Breast Cancer Research and Treatment (2024) 205:521\u2013531 regarding their use as breast cancer risk biomarkers Breast Cancer Res Treat 146 2 355 363 11. EC Atakpa AR Brentnall S Astley 2021 The relationship between body mass index and mammographic density during a premeno- pausal weight loss intervention study Cancers (Basel) 13 13 3245 12. S Hudson K Vik Hjerkind S Vinnicombe 2018 Adjusting",
    "full_text_length": 43773,
    "chunk_length": 1218
  },
  {
    "chunk_id": 736,
    "paper_filename": "dilukshi_2023_distribution_of_breast_density_in_woman_aged.pdf",
    "paper_title": "Dilukshi 2023 Distribution Of Breast Density In Woman Aged",
    "chunk_index": 40,
    "total_chunks": 46,
    "text_content": "Oncol 3 9 1228 1236 15. N Boyd L Martin S Chavez 2009 Breast-tissue composition and other risk factors for breast cancer in young women: a cross-sec- tional study Lancet Oncol 10 6 569 580 16. SJ Graham MJ Bronskill JW Byng 1996 Quantitative correlation of breast tissue parameters using magnetic resonance and X-ray mammography Br J Cancer 73 2 162 168 17. JA Shepherd KM Kerlikowske R Smith-Bindman 2002 Measure - ment of breast density with dual X-ray absorptiometry: feasibility Radiology 223 2 5",
    "full_text_length": 43773,
    "chunk_length": 1179
  },
  {
    "chunk_id": 737,
    "paper_filename": "dilukshi_2023_distribution_of_breast_density_in_woman_aged.pdf",
    "paper_title": "Dilukshi 2023 Distribution Of Breast Density In Woman Aged",
    "chunk_index": 41,
    "total_chunks": 46,
    "text_content": "comparison of breast density measures between mothers and adolescent daughters BMC Cancer 11 330 21. A Pereira ML Garmendia R Uauy 2017 Determinants of volumet- ric breast density in Chilean premenopausal women Breast Cancer Res Treat 162 2 343 352 22. R Lloyd J Walter S Pirikahu 2022 Assessment of repeated refer - ence measurements to inform the validity of optical breast spec- troscopy Rev Sci Instrum 93 4 044101 23. EJ Walter JA Knight L Lilge 2017 A multi-wavelength, laser- based optical spe",
    "full_text_length": 43773,
    "chunk_length": 1237
  },
  {
    "chunk_id": 738,
    "paper_filename": "dilukshi_2023_distribution_of_breast_density_in_woman_aged.pdf",
    "paper_title": "Dilukshi 2023 Distribution Of Breast Density In Woman Aged",
    "chunk_index": 42,
    "total_chunks": 46,
    "text_content": "women PLoS ONE 10 1 e0115851 26. R Lloyd S Pirikahu J Walter 2023 Alternative methods to measure breast density in younger women Br J Cancer 128 9 1701 1709 27. EJ Walter L Lilge 2018 Optical assessment of mammographic breast density by a 12-wavelength vs a continuous-spectrum opti- cal spectroscopy device J Biophotonics 11 2 e201700071 28. P Taroni A Pifferi G Quarto 2010 Noninvasive assessment of breast cancer risk using time-resolved diffuse optical spectroscopy J Biomed Opt 15 6 060501 29. P",
    "full_text_length": 43773,
    "chunk_length": 1155
  },
  {
    "chunk_id": 739,
    "paper_filename": "dilukshi_2023_distribution_of_breast_density_in_woman_aged.pdf",
    "paper_title": "Dilukshi 2023 Distribution Of Breast Density In Woman Aged",
    "chunk_index": 43,
    "total_chunks": 46,
    "text_content": "2019 Western Australian preg- nancy cohort (Raine) study: generation 1 BMJ Open 9 5 e026276 34. L Straker J Mountain A Jacques 2017 Cohort Profile: the West- ern Australian pregnancy cohort (Raine) study-generation 2 Int J Epidemiol 46 5 1384 1385j 35. T Li L Sun N Miller 2005 The association of measured breast tissue characteristics with mammographic density and other risk factors for breast cancer Cancer Epidemiol Biomarkers Prev 14 2 343 349 36. VA McCormack NM Perry SJ Vinnicombe SI Santos d",
    "full_text_length": 43773,
    "chunk_length": 1192
  },
  {
    "chunk_id": 740,
    "paper_filename": "dilukshi_2023_distribution_of_breast_density_in_woman_aged.pdf",
    "paper_title": "Dilukshi 2023 Distribution Of Breast Density In Woman Aged",
    "chunk_index": 44,
    "total_chunks": 46,
    "text_content": "and nondense breast tissue Cancer Epidemiol Biomarkers Prev 15 4 612 617 39. J Stone G Dite GG Giles 2012 Inference about causation from elimination of familial confounding: application to longitudinal twin data on mammographic density measures that predict breast cancer risk Cancer Epidemiol Biomarkers Prev 21 7 1149 1155 40. CM Vachon CC Kuni K Anderson 2000 Association of mammo- graphically defined percent breast density with epidemiologic risk factors for breast cancer (United States) Cancer",
    "full_text_length": 43773,
    "chunk_length": 1205
  },
  {
    "chunk_id": 741,
    "paper_filename": "dilukshi_2023_distribution_of_breast_density_in_woman_aged.pdf",
    "paper_title": "Dilukshi 2023 Distribution Of Breast Density In Woman Aged",
    "chunk_index": 45,
    "total_chunks": 46,
    "text_content": "22 43. VV Levenson 2007 Biomarkers for early detection of breast can- cer: what, when, and where? Biochim Biophys Acta 1770 6 847 856 44. J Wise 1999 Younger women might benefit from breast cancer screening BMJ. 318 7198 1575 Publisher's Note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.",
    "full_text_length": 43773,
    "chunk_length": 361
  },
  {
    "chunk_id": 742,
    "paper_filename": "eun_2024_dignostic_perdormance_without_ai_assitant_in_real_world_scanaris.pdf",
    "paper_title": "Eun 2024 Dignostic Perdormance Without Ai Assitant In Real World Scanaris",
    "chunk_index": 0,
    "total_chunks": 34,
    "text_content": "European Journal of Radiology Open 12 (2024) 100545 Available online 13 January 2024 2352-0477/\u00a9 2024 The Author(s). Published by Elsevier Ltd. This is an open access article under the CC BY-NC-ND license ( http://creativecommons.org/licenses/by- nc-nd/4.0/ ).Diagnostic performance with and without artificial intelligence assistance in real-world screening mammography Si Eun Lee1, Hanpyo Hong2, Eun-Kyung Kim*,3 Department of Radiology, Yongin Severance Hospital, Yonsei University College of Medi",
    "full_text_length": 36886,
    "chunk_length": 1535
  },
  {
    "chunk_id": 743,
    "paper_filename": "eun_2024_dignostic_perdormance_without_ai_assitant_in_real_world_scanaris.pdf",
    "paper_title": "Eun 2024 Dignostic Perdormance Without Ai Assitant In Real World Scanaris",
    "chunk_index": 1,
    "total_chunks": 34,
    "text_content": "in a single institution were included. Ra- diologists interpreted screening mammography in clinical practice with AI-CAD results being provided or withheld alternatively by month. The AI-CAD results were retrospectively obtained for analysis even when withheld from radiologists. The diagnostic performances of radiologists and stand-alone AI-CAD were compared and the performances of radiologists with and without AI-CAD assistance were also compared by cancer detection rate, recall rate, sensitivi",
    "full_text_length": 36886,
    "chunk_length": 1505
  },
  {
    "chunk_id": 744,
    "paper_filename": "eun_2024_dignostic_perdormance_without_ai_assitant_in_real_world_scanaris.pdf",
    "paper_title": "Eun 2024 Dignostic Perdormance Without Ai Assitant In Real World Scanaris",
    "chunk_index": 2,
    "total_chunks": 34,
    "text_content": "rates (8.6% vs 5.9%, all p D0.05) compared to stand-alone AI-CAD. Conclusion: Radiologists showed no significant difference in diagnostic performance when both screening mammography and ultrasound were performed with or without AI-CAD assistance for mammography. However, without AI-CAD assistance, radiologists showed lower specificity and accuracy and higher recall rates compared to stand-alone AI-CAD. 1.Introduction Mammography is the primary screening modality for breast cancer that improves s",
    "full_text_length": 36886,
    "chunk_length": 1592
  },
  {
    "chunk_id": 745,
    "paper_filename": "eun_2024_dignostic_perdormance_without_ai_assitant_in_real_world_scanaris.pdf",
    "paper_title": "Eun 2024 Dignostic Perdormance Without Ai Assitant In Real World Scanaris",
    "chunk_index": 3,
    "total_chunks": 34,
    "text_content": "PACS, , Picture archiving and communication system; PPV, , Positive predictive value; NPV, , Negative predictive value; AUC, , Area under the receiver-operating-characteristics curve. *Correspondence to: Department of Radiology, Yongin Severance Hospital, Yonsei University College of Medicine, 363, Dongbaekjukjeon-daero, Giheung-gul \u0325, Yongin-si, Gyeonggi-do, Korea. E-mail address: ekkim@yuhs.ac (E.-K. Kim). 1 0000-0002-3225-5484 2 0000-0002-0573-4527 3 0000-0002-3368-5013 Contents lists availab",
    "full_text_length": 36886,
    "chunk_length": 1476
  },
  {
    "chunk_id": 746,
    "paper_filename": "eun_2024_dignostic_perdormance_without_ai_assitant_in_real_world_scanaris.pdf",
    "paper_title": "Eun 2024 Dignostic Perdormance Without Ai Assitant In Real World Scanaris",
    "chunk_index": 4,
    "total_chunks": 34,
    "text_content": "In recent prospective trials, AI-CAD showed similar cancer detection rates without increasing recall rates when used to replace one of the radiologists in a double-reading system [17,18] . There is also an ongoing prospective trial on the role of AI assistance in a single reading system, but most current studies based on the retro- spective application of AI-CAD suffer from bias [19]. Simulation studies cannot completely replicate what occurs in clinical practice. In real life, radiologists deci",
    "full_text_length": 36886,
    "chunk_length": 1306
  },
  {
    "chunk_id": 747,
    "paper_filename": "eun_2024_dignostic_perdormance_without_ai_assitant_in_real_world_scanaris.pdf",
    "paper_title": "Eun 2024 Dignostic Perdormance Without Ai Assitant In Real World Scanaris",
    "chunk_index": 5,
    "total_chunks": 34,
    "text_content": "performed and has the potential to be more widely used with the introduction of automated breast ultrasound. When supplementary US is performed, the role of AI-CAD on mammography will differ from when only mammography is performed, and the advantages of AI-CAD may be partially diluted. In our institution, we integrated a commercial AI-CAD software into the picture archiving and communication system (PACS), and radiolo - gists could refer to the AI-CAD results during mammography interpre - tation",
    "full_text_length": 36886,
    "chunk_length": 1417
  },
  {
    "chunk_id": 748,
    "paper_filename": "eun_2024_dignostic_perdormance_without_ai_assitant_in_real_world_scanaris.pdf",
    "paper_title": "Eun 2024 Dignostic Perdormance Without Ai Assitant In Real World Scanaris",
    "chunk_index": 6,
    "total_chunks": 34,
    "text_content": "covers screening mammography every two years, and additional examinations can be covered by personal insurance. Digital mammography was obtained using one mammography unit (Senographe Pristina Mammography System, GE Healthcare, Milwau - kee, WI, United States) with cranio-caudal and medio-lateral-oblique views from each breast. AI-CAD was implemented on PACS and the AI-CAD results were shown on screen after the four mammographic views. Breast ultrasound was performed with either the conventional",
    "full_text_length": 36886,
    "chunk_length": 1398
  },
  {
    "chunk_id": 749,
    "paper_filename": "eun_2024_dignostic_perdormance_without_ai_assitant_in_real_world_scanaris.pdf",
    "paper_title": "Eun 2024 Dignostic Perdormance Without Ai Assitant In Real World Scanaris",
    "chunk_index": 7,
    "total_chunks": 34,
    "text_content": "provided and withheld the AI-CAD results monthly, with AI-CAD results being provided on even months. Four radiologists (dedicated to breast imaging for 1\u201328 years) interpreted mammography in clinical practice, and the radiologists could refer to the AI-CAD results every even month. Mammographic interpretations were based on the American College of Radiology Breast Imaging-Reporting and Data System (ACR BI-RADS). Mammographic density was assessed visually using the ACR BI-RADS 5th edition. As par",
    "full_text_length": 36886,
    "chunk_length": 1308
  },
  {
    "chunk_id": 750,
    "paper_filename": "eun_2024_dignostic_perdormance_without_ai_assitant_in_real_world_scanaris.pdf",
    "paper_title": "Eun 2024 Dignostic Perdormance Without Ai Assitant In Real World Scanaris",
    "chunk_index": 8,
    "total_chunks": 34,
    "text_content": "screening mammography and ultrasounds per- formed on the same day from August 2020 to May 2022, cases that were confirmed as cancer (n \u008829) or benign (n \u0088122) through histopatho - logical biopsy or operative results within 11 months from the date of the initial examination were deemed as having a definitive diagnosis. Stable findings on follow-up mammography or ultrasound for at least 1 year (n \u00881910) were considered as the standard reference for a negative result. Finally, we included 1819 cons",
    "full_text_length": 36886,
    "chunk_length": 1242
  },
  {
    "chunk_id": 751,
    "paper_filename": "eun_2024_dignostic_perdormance_without_ai_assitant_in_real_world_scanaris.pdf",
    "paper_title": "Eun 2024 Dignostic Perdormance Without Ai Assitant In Real World Scanaris",
    "chunk_index": 9,
    "total_chunks": 34,
    "text_content": "or grayscale map. Separate heatmaps are given for each craniocaudal and mediolateral oblique view, and the abnormality score is provided as the largest value per breast. When the abnormality score is less than 10%, the heatmap is not shown, and the malignancy risk is presented as \u201cLow\u201d with a corre - sponding score on PACS. AI-CAD results continued to be saved in the server even during the months that they were not automatically sent to PACS and withheld from radiologists, and we were able to us",
    "full_text_length": 36886,
    "chunk_length": 1284
  },
  {
    "chunk_id": 752,
    "paper_filename": "eun_2024_dignostic_perdormance_without_ai_assitant_in_real_world_scanaris.pdf",
    "paper_title": "Eun 2024 Dignostic Perdormance Without Ai Assitant In Real World Scanaris",
    "chunk_index": 10,
    "total_chunks": 34,
    "text_content": "4, and 5 as test-positive. For stand-alone AI-CAD, a score over 10 was regarded as test-positive. The diagnostic performances of the radiologists with or without AI-CAD assistance and of stand-alone AI-CAD were quantified in terms of sensitivity, speci - ficity, positive predictive value (PPV), negative predictive value (NPV), accuracy , area under the receiver-operating-characteristics curve (AUC), cancer detection rate, and recall rate. The cancer detection rate was defined as the proportion o",
    "full_text_length": 36886,
    "chunk_length": 1413
  },
  {
    "chunk_id": 753,
    "paper_filename": "eun_2024_dignostic_perdormance_without_ai_assitant_in_real_world_scanaris.pdf",
    "paper_title": "Eun 2024 Dignostic Perdormance Without Ai Assitant In Real World Scanaris",
    "chunk_index": 11,
    "total_chunks": 34,
    "text_content": "Ltd., Ostend, Belgium). Two-sided P values and 95% confidence intervals were reported, with a statistical significance threshold of.05. This retrospective study was approved by the institutional review board of ****, Yongin, Korea, with a waiver for informed consent. 3.Results Out of 1819 women (mean age 50.8\u00b19.4 years, range 30 to 82 years) with 2061 screening examinations, we categorized the breast S.E. Lee et al. European Journal of Radiology Open 12 (2024) 100545 3density of 25 as fatty, 223",
    "full_text_length": 36886,
    "chunk_length": 1326
  },
  {
    "chunk_id": 754,
    "paper_filename": "eun_2024_dignostic_perdormance_without_ai_assitant_in_real_world_scanaris.pdf",
    "paper_title": "Eun 2024 Dignostic Perdormance Without Ai Assitant In Real World Scanaris",
    "chunk_index": 12,
    "total_chunks": 34,
    "text_content": "1, p\u00880.036). Twenty-nine patients were diagnosed with breast cancer, consisting of 15 invasive cancers and 14 ductal carcinomas in situ (DCIS) on core biopsy. The overall cancer detection rate was 15.9 per 1000 women (29/ 1819). The average size of invasive cancers was 10.8 mm (range, 6\u201314mm) on initial imaging, and 6.7% of patients (2/29) had meta- static lymph nodes in the ipsilateral axilla. 3.1. Diagnostic performance of radiologists with or without AI-CAD assistance compared to stand-alone ",
    "full_text_length": 36886,
    "chunk_length": 1381
  },
  {
    "chunk_id": 755,
    "paper_filename": "eun_2024_dignostic_perdormance_without_ai_assitant_in_real_world_scanaris.pdf",
    "paper_title": "Eun 2024 Dignostic Perdormance Without Ai Assitant In Real World Scanaris",
    "chunk_index": 13,
    "total_chunks": 34,
    "text_content": "AI-CAD without statistical difference. Radiologists without AI-CAD assistance showed equal sensitivity (50.0%, 6/12), NPV (99.4%, 935/941), and cancer detection rate (6/ 1029) compared to stand-alone AI-CAD. However, PPV(6.8%, 6/88 vs 9.8%, 6/61), specificity (91.9%, 935/1017 vs 94.6%, 962/1017), and accuracy (91.5%, 941/1029 vs 94.1%, 968/1029) were significantly lower without AI-CAD assistance, and the recall rate(8.6%, 88/1029) vs 5.9%, 61/1029) was higher for radiologists. When we compared d",
    "full_text_length": 36886,
    "chunk_length": 1428
  },
  {
    "chunk_id": 756,
    "paper_filename": "eun_2024_dignostic_perdormance_without_ai_assitant_in_real_world_scanaris.pdf",
    "paper_title": "Eun 2024 Dignostic Perdormance Without Ai Assitant In Real World Scanaris",
    "chunk_index": 14,
    "total_chunks": 34,
    "text_content": "but again without statistical significance (8.6%, 88/1029 vs. 8.8%, 91/1032, p\u00880.830). 3.2. Performance of stand-alone AI-CAD according to breast density For the entire study period, stand-alone AI-CAD showed a sensitivity of 65.5% (19/29), specificity of 94.2% (1914/2032), PPV of 13.9% (19/ 137), NPV of 99.5% (1914/1924), AUC of 0.8 and recall rate of 6.6% (137/2061). When we classified patients by breast density into two groups (fatty and dense), in patients with fatty breasts, stand-alone AI-",
    "full_text_length": 36886,
    "chunk_length": 1357
  },
  {
    "chunk_id": 757,
    "paper_filename": "eun_2024_dignostic_perdormance_without_ai_assitant_in_real_world_scanaris.pdf",
    "paper_title": "Eun 2024 Dignostic Perdormance Without Ai Assitant In Real World Scanaris",
    "chunk_index": 15,
    "total_chunks": 34,
    "text_content": "Cancers missed by AI-CAD Among 29 breast cancers, 10 cancers (34%, 10/29) were missed by AI-CAD, consisting of 4 DCIS and 6 invasive cancers (5 invasive ductal carcinoma, 1 lobular carcinoma). The average size of the missed inva- sive cancers on initial imaging was 9.3mm (range 6\u201314), and one had axillary lymph node metastasis. Except for 1 case presenting as grouped microcalcifications, the other 9 lesions were not detected by radiologists and were diagnosed as BI- RADS 1 or 2 on screening mamm",
    "full_text_length": 36886,
    "chunk_length": 1413
  },
  {
    "chunk_id": 758,
    "paper_filename": "eun_2024_dignostic_perdormance_without_ai_assitant_in_real_world_scanaris.pdf",
    "paper_title": "Eun 2024 Dignostic Perdormance Without Ai Assitant In Real World Scanaris",
    "chunk_index": 16,
    "total_chunks": 34,
    "text_content": "and recall rates compared to the radiologists who did not refer to AI-CAD, although this improvement was not statistically significant. Interestingly, the radiologists with AI-CAD assistance showed similar performances with stand-alone AI-CAD. However, the radiologists without AI-CAD assistance showed significantly lower specificity, accuracy and higher recall rate compared to stand-alone AI- CAD. Table 1 Patient demographics and outcome characteristics. Total Months without AI- CAD Months with ",
    "full_text_length": 36886,
    "chunk_length": 1518
  },
  {
    "chunk_id": 759,
    "paper_filename": "eun_2024_dignostic_perdormance_without_ai_assitant_in_real_world_scanaris.pdf",
    "paper_title": "Eun 2024 Dignostic Perdormance Without Ai Assitant In Real World Scanaris",
    "chunk_index": 17,
    "total_chunks": 34,
    "text_content": "763 (37.0) 368(35.8) 395(38.3) Standard reference 0.488 Cancer by biopsy and/or surgery 29(1.4) 12(1.2) 17(1.6) 0.354 -Invasive 15(0.7) 6(0.6) 9(0.9) 0.440 -Ductal carcinoma in situ 14 (0.7) 6(0.6) 8(0.8) 0.595 Benign by biopsy and/or surgery 122 (5.9) 57(5.5) 65(6.3) 0.465 Benign by follow-up imaging 1910 (92.7) 960(93.3) 950(92.1) 0.280 Retrospective image review 0.158 Mammography-visible cancer 23 (79.3) 8(66.7) 15(88.2) Mammography-occult cancer 6(20.7) 4(33.3) 2(11.8) Note. \u2014Unless otherwis",
    "full_text_length": 36886,
    "chunk_length": 1425
  },
  {
    "chunk_id": 760,
    "paper_filename": "eun_2024_dignostic_perdormance_without_ai_assitant_in_real_world_scanaris.pdf",
    "paper_title": "Eun 2024 Dignostic Perdormance Without Ai Assitant In Real World Scanaris",
    "chunk_index": 18,
    "total_chunks": 34,
    "text_content": "(21.1, 78.9) 50.0 (6/12) (21.1, 78.9) F0.999 76.5 (13/17) (50.1, 93.2) 76.5 (13/17) (50.1, 93.2) F0.999 0.135 Specificity 91.9 (935/1017) (90.1, 93.5) 94.6 (962/1017) (93.0, 95.9) 0.017 92.3 (937/1015) (90.5, 93.9) 93.8 (952/1015) (92.1, 95.2) 0.190 0.752 PPV 6.8 (6/88) (3.9, 11.8) 9.8 (6/61) (5.5, 16.9) 0.518 14.3 (13/91) (10.6, 19.0) 17.1 (13/76) (12.6, 22.8) 0.619 0.101 NPV 99.4 (935/941) (98.9, 99.6) 99.4 (962/968) (98.9, 99.6) 0.961 99.6 (937/941)(99.0, 99.8) 99.6 (952/956) (99.0, 99.8) 0.9",
    "full_text_length": 36886,
    "chunk_length": 1401
  },
  {
    "chunk_id": 761,
    "paper_filename": "eun_2024_dignostic_perdormance_without_ai_assitant_in_real_world_scanaris.pdf",
    "paper_title": "Eun 2024 Dignostic Perdormance Without Ai Assitant In Real World Scanaris",
    "chunk_index": 19,
    "total_chunks": 34,
    "text_content": "AUC: Area under the receiver operating characteristic curve The number in parentheses indicates the 95% confidence interval. ]P-value, Radiologists without AI-CAD assistance (odd months) vs. Radiologists with AI-CAD assistance (even months) Fig. 2.Receiver operating characteristic curve analysis of radiologists with and without AI-CAD assistance. The blue line indicates radiologists with AI-CAD assistance, and the green dotted line indicates radiologists without AI-CAD assistance (AUC 0.84 vs 0.",
    "full_text_length": 36886,
    "chunk_length": 1393
  },
  {
    "chunk_id": 762,
    "paper_filename": "eun_2024_dignostic_perdormance_without_ai_assitant_in_real_world_scanaris.pdf",
    "paper_title": "Eun 2024 Dignostic Perdormance Without Ai Assitant In Real World Scanaris",
    "chunk_index": 20,
    "total_chunks": 34,
    "text_content": "a double-reading system by AI-CAD showed similar to higher cancer detection without increasing recall rates [17,18] ; however, this was not validated in a single-reading system with AI-CAD assistance. In our institution, AI-CAD has become part of all mammography examinations with its integration into PACS since the hospital first opened in March 2020. With the ubiquitous application of AI-CAD possible, we used a monthly alternate setting to provide and withhold the AI-CAD results from PACS durin",
    "full_text_length": 36886,
    "chunk_length": 1331
  },
  {
    "chunk_id": 763,
    "paper_filename": "eun_2024_dignostic_perdormance_without_ai_assitant_in_real_world_scanaris.pdf",
    "paper_title": "Eun 2024 Dignostic Perdormance Without Ai Assitant In Real World Scanaris",
    "chunk_index": 21,
    "total_chunks": 34,
    "text_content": "we recommend supplementary ultrasound for dense breasts, women with non-dense breasts also receive ultrasound for reasons other than medical condi - tions such as the availability of personal insurance, anxiety or a history of benign breast lesions. This could be the reason behind the relatively high cancer detection rate in our study population, and it is consistent with the findings of a recent article that reported a cancer detection rate of 9.3 per 1000 women in Korea, who underwent mammogra",
    "full_text_length": 36886,
    "chunk_length": 1354
  },
  {
    "chunk_id": 764,
    "paper_filename": "eun_2024_dignostic_perdormance_without_ai_assitant_in_real_world_scanaris.pdf",
    "paper_title": "Eun 2024 Dignostic Perdormance Without Ai Assitant In Real World Scanaris",
    "chunk_index": 22,
    "total_chunks": 34,
    "text_content": "failed to improve diagnostic performance [26]. Besides, available prior exami - nations were also reviewed for the best interpretation in routine prac- tice, which could increase the baseline diagnostic performance of mammography. While the use of supplementary ultrasound might be a confounding factor, findings that accept its routine use reflect what happens in actual clinical practice. Most mammograms (2015/2061, 97.8%) were interpreted by expe- rienced breast radiologists (5\u201328 years) in our ",
    "full_text_length": 36886,
    "chunk_length": 1428
  },
  {
    "chunk_id": 765,
    "paper_filename": "eun_2024_dignostic_perdormance_without_ai_assitant_in_real_world_scanaris.pdf",
    "paper_title": "Eun 2024 Dignostic Perdormance Without Ai Assitant In Real World Scanaris",
    "chunk_index": 23,
    "total_chunks": 34,
    "text_content": "- gists in actual clinical settings tend to confirm lesions that appear likely benign, with additional magnification views, ultrasound, or short-term follow-up to minimize the risk of missed cancers. This tendency can lower specificity and, consequently, reduce overall accuracy. A recent meta-analysis found that the pooled AUC of stand-alone AI-CAD was similar to better than radiologists [27]. NPV of AI-CAD remained very high, regardless of breast density (dense breast, 99.6%, 1678/1687 vs fatty",
    "full_text_length": 36886,
    "chunk_length": 1338
  },
  {
    "chunk_id": 766,
    "paper_filename": "eun_2024_dignostic_perdormance_without_ai_assitant_in_real_world_scanaris.pdf",
    "paper_title": "Eun 2024 Dignostic Perdormance Without Ai Assitant In Real World Scanaris",
    "chunk_index": 24,
    "total_chunks": 34,
    "text_content": "breast density [25]; however, as expected, it decreased the specificity of digital mammography. Since stand-alone AI-CAD showed higher speci - ficity and NPV, it may compensate for the decreased specificity of sup- plementary ultrasound. As the number of patients in our study was not enough to analyze how AI-CAD affected the performance of radiologists according to breast density, future research will need to elucidate this with a higher number of patients. Generally, for women who perform scree",
    "full_text_length": 36886,
    "chunk_length": 1334
  },
  {
    "chunk_id": 767,
    "paper_filename": "eun_2024_dignostic_perdormance_without_ai_assitant_in_real_world_scanaris.pdf",
    "paper_title": "Eun 2024 Dignostic Perdormance Without Ai Assitant In Real World Scanaris",
    "chunk_index": 25,
    "total_chunks": 34,
    "text_content": "was performed in a single institution with a single AI-CAD platform. Third, our results might Table 3 Comparison of diagnostic performance of mammography between radiologists with or without AI-CAD assistance and stand-alone AI-CAD during the whole period according to breast density. Fatty breasts Dense breasts P-value] Radiologists Stand-alone AI p-value Radiologists Stand-alone AI p-value Sensitivity 100.0 (2/2) (15.8100.0) 50.0 (1/2) (1.3, 98.7) 0.317 63.0 (17/27) (42.4, 80.6) 66.7 (18/27) (4",
    "full_text_length": 36886,
    "chunk_length": 1395
  },
  {
    "chunk_id": 768,
    "paper_filename": "eun_2024_dignostic_perdormance_without_ai_assitant_in_real_world_scanaris.pdf",
    "paper_title": "Eun 2024 Dignostic Perdormance Without Ai Assitant In Real World Scanaris",
    "chunk_index": 26,
    "total_chunks": 34,
    "text_content": "95.6 (237/248) (92.2, 97.8) 0.031 91.9 (1666/1813) (90.5, 93.1) 93.5 (1696/1813) (92.3, 94.6) 0.064 0.200 Detection rate 2/248 1/248 0.562 17/1813 18/1813 0.865 0.205 Recall rate 10.1 (25/248) (6.6, 14.5) 4.4 (11/248) (2.2, 7.8) 0.015 8.5 (154/1813) (7.3, 9.9) 6.9 (126/1813) (5.8, 8.2) 0.071 0.137 PPV: Positive predictive value, NPV: Negative predictive value, AUC: Area under the receiver operating characteristic curve The number in parentheses indicates the 95% confidence interval. ]P-value, co",
    "full_text_length": 36886,
    "chunk_length": 1419
  },
  {
    "chunk_id": 769,
    "paper_filename": "eun_2024_dignostic_perdormance_without_ai_assitant_in_real_world_scanaris.pdf",
    "paper_title": "Eun 2024 Dignostic Perdormance Without Ai Assitant In Real World Scanaris",
    "chunk_index": 27,
    "total_chunks": 34,
    "text_content": "to stand-alone AI- CAD. Ethical approval details This retrospective study was approved by the institutional review board (Yongin Severance Hospital, 9\u20132022-0059) with a waiver for informed consent. Funding sources This work was supported by Medical AI Clinic Program through the National IT Industry Promotion Agency (NIPA), funded by the Ministry of Science and ICT (MSIT). CRediT authorship contribution statement Kim Eun-Kyung: Writing \u2013 review & editing, Supervision, Data curation, Conceptualiza",
    "full_text_length": 36886,
    "chunk_length": 1418
  },
  {
    "chunk_id": 770,
    "paper_filename": "eun_2024_dignostic_perdormance_without_ai_assitant_in_real_world_scanaris.pdf",
    "paper_title": "Eun 2024 Dignostic Perdormance Without Ai Assitant In Real World Scanaris",
    "chunk_index": 28,
    "total_chunks": 34,
    "text_content": "breast carcinoma mortality, Cancer 91 (2001) 1724 \u20131731 . [2]L. Tabar, M.F. Yen, B. Vitak, H.H. Chen, R.A. Smith, S.W. Duffy, Mammography service screening and mortality in breast cancer patients: 20-year follow-up before and after introduction of screening, Lancet 361 (2003) 1405 \u20131410 . [3]S. Ciatto, N. Houssami, D. Bernardi, et al., Integration of 3D digital mammography with tomosynthesis for population breast-cancer screening (STORM): a prospective comparison study, Lancet Oncol. 14 (2013) 5",
    "full_text_length": 36886,
    "chunk_length": 1360
  },
  {
    "chunk_id": 771,
    "paper_filename": "eun_2024_dignostic_perdormance_without_ai_assitant_in_real_world_scanaris.pdf",
    "paper_title": "Eun 2024 Dignostic Perdormance Without Ai Assitant In Real World Scanaris",
    "chunk_index": 29,
    "total_chunks": 34,
    "text_content": "community mammography practice, J. Natl. Cancer Inst. 103 (2011) 1152 \u20131161 . [7]J.J. Fenton, S.H. Taplin, P.A. Carney, et al., Influence of computer-aided detection on performance of screening mammography, N. Engl. J. Med 356 (2007) 1399 \u20131409 . [8]J.H. Lee, K.H. Kim, E.H. Lee, et al., Improving the performance of radiologists using artificial intelligence-based detection support software for mammography: a multi- reader study, Korean J. Radio. 23 (2022) 505\u2013516. [9]S.M. McKinney, M. Sieniek, V",
    "full_text_length": 36886,
    "chunk_length": 1336
  },
  {
    "chunk_id": 772,
    "paper_filename": "eun_2024_dignostic_perdormance_without_ai_assitant_in_real_world_scanaris.pdf",
    "paper_title": "Eun 2024 Dignostic Perdormance Without Ai Assitant In Real World Scanaris",
    "chunk_index": 30,
    "total_chunks": 34,
    "text_content": "Changes in cancer detection and false-positive recall in mammography using artificial intelligence: a retrospective, multireader study, Lancet Digit Health 2 (2020) e138 \u2013e148. [13] K. L\u00e5ng, S. Hofvind, A. Rodr\u00edguez-Ruiz, I. Andersson, Can artificial intelligence reduce the interval cancer rate in mammography screening? Eur. Radiol. 31 (2021) 5940 \u20135947 . [14] K. L\u00e5ng, M. Dustler, V. Dahlblom, A. \u00c5kesson, I. Andersson, S. Zackrisson, Identifying normal mammograms in a large screening population ",
    "full_text_length": 36886,
    "chunk_length": 1419
  },
  {
    "chunk_id": 773,
    "paper_filename": "eun_2024_dignostic_perdormance_without_ai_assitant_in_real_world_scanaris.pdf",
    "paper_title": "Eun 2024 Dignostic Perdormance Without Ai Assitant In Real World Scanaris",
    "chunk_index": 31,
    "total_chunks": 34,
    "text_content": "et al., Artificial intelligence-supported screen reading versus standard double reading in the mammography screening with Artificial Intelligence trial (MASAI): a clinical safety analysis of a randomised, controlled, non-inferiority, single-blinded, screening accuracy study, Lancet Oncol. 24 (2023) 936\u2013944. [18] K. Dembrower, A. Crippa, E. Col\u02d8on, M. Eklund, F. Strand, Artificial intelligence for breast cancer detection in screening mammography in Sweden: a prospective, population-based, paired-",
    "full_text_length": 36886,
    "chunk_length": 1421
  },
  {
    "chunk_id": 774,
    "paper_filename": "eun_2024_dignostic_perdormance_without_ai_assitant_in_real_world_scanaris.pdf",
    "paper_title": "Eun 2024 Dignostic Perdormance Without Ai Assitant In Real World Scanaris",
    "chunk_index": 32,
    "total_chunks": 34,
    "text_content": "cancer detection among women aged 40-49 years with varying breast density undergoing screening mammography: a secondary analysis of a randomized clinical trial, JAMA Netw. Open 4 (2021) e2121505-e2121505 . [22] W.H. Yuan, H.C. Hsu, Y.Y. Chen, C.H. Wu, Supplemental breast cancer-screening ultrasonography in women with dense breasts: a systematic review and meta- analysis, Br. J. Cancer 123 (2020) 673\u2013688. [23] K. Dembrower, E. W\u00e5hlin, Y. Liu, et al., Effect of artificial intelligence-based triagi",
    "full_text_length": 36886,
    "chunk_length": 1349
  },
  {
    "chunk_id": 775,
    "paper_filename": "eun_2024_dignostic_perdormance_without_ai_assitant_in_real_world_scanaris.pdf",
    "paper_title": "Eun 2024 Dignostic Perdormance Without Ai Assitant In Real World Scanaris",
    "chunk_index": 33,
    "total_chunks": 34,
    "text_content": "307 (2023) e222435 . [26] S.E. Lee, J.H. Yoon, N.H. Son, K. Han, H.J. Moon, Screening in patients with dense breasts: comparison of mammography, artificial intelligence, and supplementary ultrasound, AJR Am. J. Roentgenol. (2023) . [27] Yoon J.H., Strand F., Baltzer P.A.T., et al. Standalone AI for Breast Cancer Detection at Screening Digital Mammography and Digital Breast Tomosynthesis: A Systematic Review and Meta-Analysis. Radiology. 0:222639. [28] S.E. Lee, K. Han, J.H. Yoon, J.H. Youk, E.K.",
    "full_text_length": 36886,
    "chunk_length": 704
  },
  {
    "chunk_id": 776,
    "paper_filename": "faseela_2023_advancing_histopathalogy_breast_cancer_dignosis_using_multimodal.pdf",
    "paper_title": "Faseela 2023 Advancing Histopathalogy Breast Cancer Dignosis Using Multimodal",
    "chunk_index": 0,
    "total_chunks": 72,
    "text_content": "Advancing Histopathology-Based Breast Cancer Diagnosis: Insights into Multi-Modality and Explainability Faseela Abdullakutty1*, Younes Akbari1*, Somaya Al-Maadeed1*, Ahmed Bouridane2and Rifat Hamoudi2 1*Qatar University, Qatar. 2University of Sharjah, United Arab Emirates. *Corresponding author(s). E-mail(s): faseela.abdullakutty@qu.edu.qa; akbari younes@yahoo.com; s alali@qu.edu.qa; Contributing authors: abouridane@sharjah.ac.ae; rhamoudi@sharjah.ac.ae; Abstract It is imperative that breast can",
    "full_text_length": 79667,
    "chunk_length": 1700
  },
  {
    "chunk_id": 777,
    "paper_filename": "faseela_2023_advancing_histopathalogy_breast_cancer_dignosis_using_multimodal.pdf",
    "paper_title": "Faseela 2023 Advancing Histopathalogy Breast Cancer Dignosis Using Multimodal",
    "chunk_index": 1,
    "total_chunks": 72,
    "text_content": "enhance diagnostic accuracy, clinician confidence, and patient engagement, ultimately fostering more personalized treatment strate- gies for breast cancer, while also identifying research gaps in multi-modality and explainability, guiding future studies, and contributing to the strategic direction of the field. Keywords: Breast Cancer detection, multi-modality, explainability 1arXiv:2406.12897v1 [cs.LG] 7 Jun 2024 1 Introduction In the realm of breast cancer diagnosis, the convergence of multi-m",
    "full_text_length": 79667,
    "chunk_length": 1522
  },
  {
    "chunk_id": 778,
    "paper_filename": "faseela_2023_advancing_histopathalogy_breast_cancer_dignosis_using_multimodal.pdf",
    "paper_title": "Faseela 2023 Advancing Histopathalogy Breast Cancer Dignosis Using Multimodal",
    "chunk_index": 2,
    "total_chunks": 72,
    "text_content": "the Computer-Aided Detection (CAD) framework. As depicted in Figure 2, breast can- cer detection can be performed using various data types, employing either unimodal or multimodal approaches. The process initiates with data pre-processing, followed by feature extraction. To enhance the learning of feature representations from image data, segmentation may be conducted prior to feature extraction. Subsequently, the detection model is applied to generate a diagnosis from the processed data. Based o",
    "full_text_length": 79667,
    "chunk_length": 1565
  },
  {
    "chunk_id": 779,
    "paper_filename": "faseela_2023_advancing_histopathalogy_breast_cancer_dignosis_using_multimodal.pdf",
    "paper_title": "Faseela 2023 Advancing Histopathalogy Breast Cancer Dignosis Using Multimodal",
    "chunk_index": 3,
    "total_chunks": 72,
    "text_content": "Fig. 1 : A generic representation of breast cancer diagnosis This review endeavours to illuminate the burgeoning field of multi-modal tech- niques in breast cancer diagnosis context, placing particular emphasis on the fusion of heterogeneous data streams encompassing both image and non-image modalities. Beyond the confines of traditional imaging modalities, such as mammography, mag- netic resonance imaging (MRI), ultrasound, and positron emission tomography (PET), 2 multi-modal approaches levera",
    "full_text_length": 79667,
    "chunk_length": 1580
  },
  {
    "chunk_id": 780,
    "paper_filename": "faseela_2023_advancing_histopathalogy_breast_cancer_dignosis_using_multimodal.pdf",
    "paper_title": "Faseela 2023 Advancing Histopathalogy Breast Cancer Dignosis Using Multimodal",
    "chunk_index": 4,
    "total_chunks": 72,
    "text_content": "data Furthermore, alongside the integration of multi-modal data, the exigency for explainability in breast cancer diagnosis emerges as a pivotal consideration. As machine learning and artificial intelligence algorithms increasingly permeate diagnostic workflows, the interpretability and transparency of decision-making processes assume paramount importance. Explainable AI (XAI) techniques endeavor to demystify the opaque nature of complex algorithms, elucidating the rationale behind diagnostic de",
    "full_text_length": 79667,
    "chunk_length": 1458
  },
  {
    "chunk_id": 781,
    "paper_filename": "faseela_2023_advancing_histopathalogy_breast_cancer_dignosis_using_multimodal.pdf",
    "paper_title": "Faseela 2023 Advancing Histopathalogy Breast Cancer Dignosis Using Multimodal",
    "chunk_index": 5,
    "total_chunks": 72,
    "text_content": "et al.[7] 2024 Breast Cancer Imaging \u00d7 \u2713 \u00d7 Rautela et al.[8] 2024Computational techniques for breast cancer\u00d7 \u2713 \u00d7 Singh et al.[9] 2024Breast Cancer Screening and Detection using Artificial Intelligence and Radiomics\u00d7 \u2713 \u00d7 Thakur et al .[10] 2024Identification and of breast cancer through medical image modalities\u2713 \u2713 \u00d7 Table 1 presents recent reviews on breast cancer diagnosis across various contexts. However, these reviews often overlook multi-modality and explainability, treating them as future re",
    "full_text_length": 79667,
    "chunk_length": 1523
  },
  {
    "chunk_id": 782,
    "paper_filename": "faseela_2023_advancing_histopathalogy_breast_cancer_dignosis_using_multimodal.pdf",
    "paper_title": "Faseela 2023 Advancing Histopathalogy Breast Cancer Dignosis Using Multimodal",
    "chunk_index": 6,
    "total_chunks": 72,
    "text_content": "that incorporate histopathology and non-image data, which are frequently overlooked in existing literature. \u2022A discussion on multi-modal techniques that utilize the aforementioned datasets, offering insights into their application and effectiveness in breast cancer diagnosis. \u2022An investigation of explainable multi-modal methods specifically within the context of histopathology-based breast cancer diagnosis, addressing a critical gap in current research. \u2022Identification research gaps in multi-mod",
    "full_text_length": 79667,
    "chunk_length": 1479
  },
  {
    "chunk_id": 783,
    "paper_filename": "faseela_2023_advancing_histopathalogy_breast_cancer_dignosis_using_multimodal.pdf",
    "paper_title": "Faseela 2023 Advancing Histopathalogy Breast Cancer Dignosis Using Multimodal",
    "chunk_index": 7,
    "total_chunks": 72,
    "text_content": "[11], strategies for ensuring prompt, pre- cise, and effective diagnostic processes can be elucidated, thereby fostering improved patient care and clinical outcomes. 2.1 Tasks Breast cancer diagnosis [12] involves several tasks that can utilize image and non- image data as shown in Figure. 3. Breast cancer detection entails pinpointing signs of cancer within breast imaging data, such as mammograms or ultrasound scans. By harnessing machine learning algorithms, it\u2019s possible to analyze these imag",
    "full_text_length": 79667,
    "chunk_length": 1430
  },
  {
    "chunk_id": 784,
    "paper_filename": "faseela_2023_advancing_histopathalogy_breast_cancer_dignosis_using_multimodal.pdf",
    "paper_title": "Faseela 2023 Advancing Histopathalogy Breast Cancer Dignosis Using Multimodal",
    "chunk_index": 8,
    "total_chunks": 72,
    "text_content": "learning mod- els can assist in this classification by analyzing features derived from imaging data, including characteristics like shape, texture, and intensity. By training these models on large datasets, they can provide predictions on the probability that an abnormality is cancerous, aiding healthcare professionals in making informed decisions regarding patient care [15]. Diagnosis Tasks Cancer Detection Malignanacy Grade Classification Image Segmentation Subtype Classification Reccurence an",
    "full_text_length": 79667,
    "chunk_length": 1439
  },
  {
    "chunk_id": 785,
    "paper_filename": "faseela_2023_advancing_histopathalogy_breast_cancer_dignosis_using_multimodal.pdf",
    "paper_title": "Faseela 2023 Advancing Histopathalogy Breast Cancer Dignosis Using Multimodal",
    "chunk_index": 9,
    "total_chunks": 72,
    "text_content": "categorizing cases into these subtypes, medical professionals can tailor treatment plans more effectively [17]. Machine learning models play a role in this by analyzing genomic data, gene expression profiles, and clinical information to predict the subtype, facilitating personalized and targeted therapeutic approaches. Table 2 : Recent research in breast cancer diagnosis including different tasks Method Dataset Modality Task Classifier-combined method[14] Proprietary MRIGrade Classification Deep",
    "full_text_length": 79667,
    "chunk_length": 1773
  },
  {
    "chunk_id": 786,
    "paper_filename": "faseela_2023_advancing_histopathalogy_breast_cancer_dignosis_using_multimodal.pdf",
    "paper_title": "Faseela 2023 Advancing Histopathalogy Breast Cancer Dignosis Using Multimodal",
    "chunk_index": 10,
    "total_chunks": 72,
    "text_content": "metastatis recurrence prediction [35] WPBC Clinical DataRecurrence and metastasis Semantic Segmentation [36]CBIS-DDSM [37], MIAS [33]Mammogram Segmentation Unet3+ [38] Proprietary Ultrasound Segmentation Yolo-Based Model [39]CBIS-DDSM [37], Inbreast [40], ProprietaryMammogram Detection Image segmentation [13] involves dividing an image into cell segmentation and distinct segments or regions of interest. Within the realm of breast cancer diagnosis, segmentation helps to demarcate the boundaries o",
    "full_text_length": 79667,
    "chunk_length": 1510
  },
  {
    "chunk_id": 787,
    "paper_filename": "faseela_2023_advancing_histopathalogy_breast_cancer_dignosis_using_multimodal.pdf",
    "paper_title": "Faseela 2023 Advancing Histopathalogy Breast Cancer Dignosis Using Multimodal",
    "chunk_index": 11,
    "total_chunks": 72,
    "text_content": "extending beyond initial diagnosis and treatment. This task involves assessing the risk of the cancer returning or spreading to other parts of the body. Machine learning models can combine multiple types of data\u2014such as imaging, genomic information, clinical variables (like patient demographics and medical his- tory), and treatment records\u2014to estimate the likelihood of recurrence or metastasis [35]. These predictions are valuable for clinicians, allowing them to customize follow- up care and cre",
    "full_text_length": 79667,
    "chunk_length": 1400
  },
  {
    "chunk_id": 788,
    "paper_filename": "faseela_2023_advancing_histopathalogy_breast_cancer_dignosis_using_multimodal.pdf",
    "paper_title": "Faseela 2023 Advancing Histopathalogy Breast Cancer Dignosis Using Multimodal",
    "chunk_index": 12,
    "total_chunks": 72,
    "text_content": "various tasks. A significant observation is the predominance of uni- modal approaches in current methodologies. While some existing multimodal methods incorporate different types of imaging, such as ultrasound and mammography, the integration of image data with non-image data remains significantly underexplored. In particular, the fusion of histopathology images with non-image data, including textual and clinical information, represents a largely untapped area. The potential ben- efits of this i",
    "full_text_length": 79667,
    "chunk_length": 1504
  },
  {
    "chunk_id": 789,
    "paper_filename": "faseela_2023_advancing_histopathalogy_breast_cancer_dignosis_using_multimodal.pdf",
    "paper_title": "Faseela 2023 Advancing Histopathalogy Breast Cancer Dignosis Using Multimodal",
    "chunk_index": 13,
    "total_chunks": 72,
    "text_content": "pathology images. Radiology images encompass modalities such as MRI, CT, thermal imaging, mammograms, and ultrasound, while pathology images include histopathology and pCLE [3]. The non-image data can be subdivided into clinical and non-clinical categories. Clinical data encompass radiology reports, pathology reports, including laboratory results, and narrative descriptions of patient status. Non-clinical data comprise patient profiles containing demographic information, patient history, age, ot",
    "full_text_length": 79667,
    "chunk_length": 1459
  },
  {
    "chunk_id": 790,
    "paper_filename": "faseela_2023_advancing_histopathalogy_breast_cancer_dignosis_using_multimodal.pdf",
    "paper_title": "Faseela 2023 Advancing Histopathalogy Breast Cancer Dignosis Using Multimodal",
    "chunk_index": 14,
    "total_chunks": 72,
    "text_content": "from the table that the number of multi-modal datasets is much less compared to the unimodal datasets. Also, the sample size is low in most of these datasets. The landscape of breast cancer histopathology research is enriched by a diverse array of datasets, each offering unique features and clinical insights. Uni-modal datasets, such as BRACS [48] and BreCaHAD [49], focus on a single type of data. The BRACS dataset provides 547 Whole-Slide Images (WSIs) and 4539 Regions Of Interest (ROIs), metic",
    "full_text_length": 79667,
    "chunk_length": 1418
  },
  {
    "chunk_id": 791,
    "paper_filename": "faseela_2023_advancing_histopathalogy_breast_cancer_dignosis_using_multimodal.pdf",
    "paper_title": "Faseela 2023 Advancing Histopathalogy Breast Cancer Dignosis Using Multimodal",
    "chunk_index": 15,
    "total_chunks": 72,
    "text_content": "datasets integrate various data types to provide a more comprehensive view of breast cancer pathology. The TCGA-BRCA [21] dataset, for instance, combines gene expression data, copy number variations (CNVs), and patho- logical images from 1098 breast cancer patients. This multi-dimensional approach allows for a deeper understanding of the molecular and histological characteristics of breast cancer. Similarly, the IMPRESS dataset includes Hematoxylin and Eosin (H&E) and immunohistochemistry (IHC) ",
    "full_text_length": 79667,
    "chunk_length": 1466
  },
  {
    "chunk_id": 792,
    "paper_filename": "faseela_2023_advancing_histopathalogy_breast_cancer_dignosis_using_multimodal.pdf",
    "paper_title": "Faseela 2023 Advancing Histopathalogy Breast Cancer Dignosis Using Multimodal",
    "chunk_index": 16,
    "total_chunks": 72,
    "text_content": "Modalities Post-NAT-BRCA [50] 2019 96 WSI ,Clinical data CPTAC-BRCA [51] 2020 642WSI, clinical, proteomic, genomic data Pathological EMR [52] 2021 WSI, patient profile BCNB [53] 2022 1,058 Clinical Data IMPRESS [54] 2023 126 WSI ,Clinical data GTEx-Breast dataset [55] 2023 894 WSI, pathology notes TCGA-BRCA dataset [21] 2023 1098WSI, gene expression, CNV 8 The IMPRESS dataset [54] consists of 126 breast H&E WSIs from 62 female patients with HER2-positive breast cancer and 64 female patients with",
    "full_text_length": 79667,
    "chunk_length": 1349
  },
  {
    "chunk_id": 793,
    "paper_filename": "faseela_2023_advancing_histopathalogy_breast_cancer_dignosis_using_multimodal.pdf",
    "paper_title": "Faseela 2023 Advancing Histopathalogy Breast Cancer Dignosis Using Multimodal",
    "chunk_index": 17,
    "total_chunks": 72,
    "text_content": "offers gene expression data across 44 human tissues. It includes 894 breast tissue histology images, comprising 306 WSIs of female breast tissue and 588 WSIs of male breast tissue, collected from the central subareolar region of the right breast at various centers in the United States. The images are accompanied by brief pathology notes and an annotation file with detailed sample information. The CPTAC-BRCA dataset [51], from the Clinical Proteomic Tumor Analysis Con- sortium, includes 642 WSIs ",
    "full_text_length": 79667,
    "chunk_length": 1371
  },
  {
    "chunk_id": 794,
    "paper_filename": "faseela_2023_advancing_histopathalogy_breast_cancer_dignosis_using_multimodal.pdf",
    "paper_title": "Faseela 2023 Advancing Histopathalogy Breast Cancer Dignosis Using Multimodal",
    "chunk_index": 18,
    "total_chunks": 72,
    "text_content": "scanner. Tumor regions in each image are anno- tated by two pathologists. The dataset also includes extensive clinical data such as patient age, tumour size, histological and molecular subtypes, number of lymph node metastases, and HER2, ER, and PR status. 3 Exploring multi-modality Multi-modal techniques are increasingly significant in histopathology-based breast cancer detection due to their capability to enhance diagnostic accuracy, provide com- prehensive insights, and improve patient outcom",
    "full_text_length": 79667,
    "chunk_length": 1516
  },
  {
    "chunk_id": 795,
    "paper_filename": "faseela_2023_advancing_histopathalogy_breast_cancer_dignosis_using_multimodal.pdf",
    "paper_title": "Faseela 2023 Advancing Histopathalogy Breast Cancer Dignosis Using Multimodal",
    "chunk_index": 19,
    "total_chunks": 72,
    "text_content": "larly, in breast cancer detection, artificial intelligence (AI) provides an automated and objective means to incorporate complementary information and clinical context from diverse datasets, thereby enhancing predictive accuracy. As shown in Figure. 4, the multi-modal fusion can be categorised as stage-based and method-based techniques. Stage-based fusion strategies can be further categorized into early, late, and intermediate fusion approaches [56], each offering unique advantages in breast can",
    "full_text_length": 79667,
    "chunk_length": 1556
  },
  {
    "chunk_id": 796,
    "paper_filename": "faseela_2023_advancing_histopathalogy_breast_cancer_dignosis_using_multimodal.pdf",
    "paper_title": "Faseela 2023 Advancing Histopathalogy Breast Cancer Dignosis Using Multimodal",
    "chunk_index": 20,
    "total_chunks": 72,
    "text_content": "in tasks like video captioning and object detection. Attention mechanism methods use mechanisms like co-attention and cross-attention to enhance each modality with information from other modalities, allowing the model to fuse features and learn interdependencies among them. Graph Neural Network methods use GNN to capture long-range dependencies among different modalities, categorizing tasks into different classes based on data types. Generative Neural Network methods include models like VAE-base",
    "full_text_length": 79667,
    "chunk_length": 1570
  },
  {
    "chunk_id": 797,
    "paper_filename": "faseela_2023_advancing_histopathalogy_breast_cancer_dignosis_using_multimodal.pdf",
    "paper_title": "Faseela 2023 Advancing Histopathalogy Breast Cancer Dignosis Using Multimodal",
    "chunk_index": 21,
    "total_chunks": 72,
    "text_content": "heterogeneity, and predict potential treatment responses. They also contribute to better prognostication and personalized medicine by providing a holistic view of tumour biology and patient condition. Advanced imaging and computational tools, such as machine learning and artificial intelligence (AI), have revolutionized the anal- ysis of histopathological data, automating the detection and classification of cancerous cells, extracting and analyzing complex patterns, and providing decision suppor",
    "full_text_length": 79667,
    "chunk_length": 1564
  },
  {
    "chunk_id": 798,
    "paper_filename": "faseela_2023_advancing_histopathalogy_breast_cancer_dignosis_using_multimodal.pdf",
    "paper_title": "Faseela 2023 Advancing Histopathalogy Breast Cancer Dignosis Using Multimodal",
    "chunk_index": 22,
    "total_chunks": 72,
    "text_content": "more effective and widely adopted in clinical practice. However, challenges persist, such as the rich- ness of feature representation [52] in images and the inadequacy of information fusion, which can lead to the loss of high-dimensional information and partially missing data in real-world scenarios. Each modality within multimodal data possesses distinct char- acteristics, adding to the complexity of heterogeneous data and further complicating multimodal fusion methods. 3.1 Multi-Modal Breast C",
    "full_text_length": 79667,
    "chunk_length": 1558
  },
  {
    "chunk_id": 799,
    "paper_filename": "faseela_2023_advancing_histopathalogy_breast_cancer_dignosis_using_multimodal.pdf",
    "paper_title": "Faseela 2023 Advancing Histopathalogy Breast Cancer Dignosis Using Multimodal",
    "chunk_index": 23,
    "total_chunks": 72,
    "text_content": "Table. 4 shows recent multi-modal research in breast cancer diagnosis. Combining heterogeneous data without losing information from high-dimensional images has posed a significant challenge in data fusion. To address this issue, the authors of [52] proposed a multi-modal fusion technique that increased the dimen- sionality of structured data to align with the high-dimensional image features from histopathology WSI. They employed VGG-16 as the backbone model for image fea- ture extraction and uti",
    "full_text_length": 79667,
    "chunk_length": 1490
  },
  {
    "chunk_id": 800,
    "paper_filename": "faseela_2023_advancing_histopathalogy_breast_cancer_dignosis_using_multimodal.pdf",
    "paper_title": "Faseela 2023 Advancing Histopathalogy Breast Cancer Dignosis Using Multimodal",
    "chunk_index": 24,
    "total_chunks": 72,
    "text_content": "Generalized Pathological Text (GPT) to distill and summarize complex text reports, thereby establishing an effective conditioning mechanism. The authors achieved a state-of-the-art Fr\u00b4 echet Inception Distance (FID) score of 7.64 for text-to-image generation on the TCGA-BRCA dataset, surpassing the closest text-conditioned competitor. The study compared PathLDM against alternative meth- ods such as Moghadam et al., Medfusion, and Stable Diffusion, demonstrating its superiority in generating high",
    "full_text_length": 79667,
    "chunk_length": 1518
  },
  {
    "chunk_id": 801,
    "paper_filename": "faseela_2023_advancing_histopathalogy_breast_cancer_dignosis_using_multimodal.pdf",
    "paper_title": "Faseela 2023 Advancing Histopathalogy Breast Cancer Dignosis Using Multimodal",
    "chunk_index": 25,
    "total_chunks": 72,
    "text_content": "framework, which utilized contrastively aligned image and text models for zero-shot transfer on gigapixel histopathology whole slide images. Reformu- lating zero-shot transfer through multiple instance learning for large images, the text encoder was pre-trained with over 550,000 pathology reports and in-domain text cor- pora. Evaluated on three WSI datasets from Brigham and Women\u2019s Hospital, MI-Zero used independent datasets to prevent information leakage and employed a graph-based representatio",
    "full_text_length": 79667,
    "chunk_length": 1507
  },
  {
    "chunk_id": 802,
    "paper_filename": "faseela_2023_advancing_histopathalogy_breast_cancer_dignosis_using_multimodal.pdf",
    "paper_title": "Faseela 2023 Advancing Histopathalogy Breast Cancer Dignosis Using Multimodal",
    "chunk_index": 26,
    "total_chunks": 72,
    "text_content": "input features to form stacked features. These stacked features were subsequently fed into random forest classifiers enhanced with adaptive boosting for the final survival clas- sification task. The fusion strategy leveraged feature-level fusion, combining features from different modalities to augment the model\u2019s predictive power, thereby utilizing the strengths of each modality and improving the accuracy of breast cancer prognosis prediction. A hybrid deep learning [65] model was developed to p",
    "full_text_length": 79667,
    "chunk_length": 1412
  },
  {
    "chunk_id": 803,
    "paper_filename": "faseela_2023_advancing_histopathalogy_breast_cancer_dignosis_using_multimodal.pdf",
    "paper_title": "Faseela 2023 Advancing Histopathalogy Breast Cancer Dignosis Using Multimodal",
    "chunk_index": 27,
    "total_chunks": 72,
    "text_content": "model outperformed both the DNN and CNN models in terms of accuracy and AUC values, demonstrating the effectiveness of the fusion strategy in improving subtype prediction accuracy. 12 Table 4 : Existing research in multi-modal breast cancer diagnosis Author Year DatasetsFusion strategyModality Sun et al. [62] 2018 METABRIC Late fusionClinical data, Gene expression Tong et al. [63] 2020 TCGA-BRCAEncoder-decoder methodGene Expressions, CNV Arya and Saha [61] 2021METABRIC, TCGA-BRCAEarly fusionClin",
    "full_text_length": 79667,
    "chunk_length": 1567
  },
  {
    "chunk_id": 804,
    "paper_filename": "faseela_2023_advancing_histopathalogy_breast_cancer_dignosis_using_multimodal.pdf",
    "paper_title": "Faseela 2023 Advancing Histopathalogy Breast Cancer Dignosis Using Multimodal",
    "chunk_index": 28,
    "total_chunks": 72,
    "text_content": "Gene expressions Kayikci et al. [71] 2023 METABRIC Attention-basedClinical data, Gene expression Arya et al.[72] 2023 TCGA-BRCA Early fusionClinical data, Gene expression Mondol et al. [73] 2024 TCGA-BRCA Attention-basedHistopathology images, Clinical data, Gene Expressions Huang et al. [74] 2024TCGA-BRCA, GMUCH-BRCAEarly fusionHistopathology images, Clinical data Li and Nabavi [75] 2024 TCGA-BRCAGraph-neural network methodGene Expressions, CNV A deep learning model [66] was employed to predict ",
    "full_text_length": 79667,
    "chunk_length": 1476
  },
  {
    "chunk_id": 805,
    "paper_filename": "faseela_2023_advancing_histopathalogy_breast_cancer_dignosis_using_multimodal.pdf",
    "paper_title": "Faseela 2023 Advancing Histopathalogy Breast Cancer Dignosis Using Multimodal",
    "chunk_index": 29,
    "total_chunks": 72,
    "text_content": "of tumor presence in each tile. This fusion strategy led to improved accuracy in pre- dicting recurrence assay results and risk of recurrence, surpassing traditional clinical nomograms. The application of the fusion strategy facilitated enhanced accuracy in predicting recurrence assay results and risk of recurrence in breast cancer patients. Canonical Correlation Analysis (CCA) and its penalized variants (pCCA) were employed for multi-modality fusion in breast cancer prediction [64]. CCA identif",
    "full_text_length": 79667,
    "chunk_length": 1505
  },
  {
    "chunk_id": 806,
    "paper_filename": "faseela_2023_advancing_histopathalogy_breast_cancer_dignosis_using_multimodal.pdf",
    "paper_title": "Faseela 2023 Advancing Histopathalogy Breast Cancer Dignosis Using Multimodal",
    "chunk_index": 30,
    "total_chunks": 72,
    "text_content": "The model demonstrated superior per- formance compared to Principal Components Analysis (PCA) embeddings in survival prediction tasks. A deep learning approach was proposed for survival risk stratification in breast cancer, integrating histopathological imaging, genetic, and clinical data [73]. The MaxViT model was employed for image feature extraction, with self-attention mechanisms capturing intricate image relationships at the patient level. A dual cross- attention mechanism fused image featu",
    "full_text_length": 79667,
    "chunk_length": 1487
  },
  {
    "chunk_id": 807,
    "paper_filename": "faseela_2023_advancing_histopathalogy_breast_cancer_dignosis_using_multimodal.pdf",
    "paper_title": "Faseela 2023 Advancing Histopathalogy Breast Cancer Dignosis Using Multimodal",
    "chunk_index": 31,
    "total_chunks": 72,
    "text_content": "cancer prognosis prediction, incorporating multi-dimensional data [62]. The method\u2019s archi- tecture design and fusion of multi-dimensional data represented innovative aspects. The METABRIC dataset, obtained from 1,980 breast cancer patients, contained gene expression profiles, CNA profiles, and clinical information. Patients were classified as short-term (491) or long-term survivors based on a 5-year survival threshold, with each patient\u2019s data comprising 27 clinical features. The MDNNMD method ",
    "full_text_length": 79667,
    "chunk_length": 1518
  },
  {
    "chunk_id": 808,
    "paper_filename": "faseela_2023_advancing_histopathalogy_breast_cancer_dignosis_using_multimodal.pdf",
    "paper_title": "Faseela 2023 Advancing Histopathalogy Breast Cancer Dignosis Using Multimodal",
    "chunk_index": 32,
    "total_chunks": 72,
    "text_content": "data was generated using view-specific generative adversarial networks conditioned on shared representations and encoded features from other views. The effectiveness of the method was evaluated on the TCGA-BRCA and METABRIC datasets, commonly uti- lized in cancer research, to demonstrate its superiority over state-of-the-art methods. Fusion in the model was achieved through the generation of missing view data using view-specific generative adversarial networks. A multimodal siamese model was pro",
    "full_text_length": 79667,
    "chunk_length": 1468
  },
  {
    "chunk_id": 809,
    "paper_filename": "faseela_2023_advancing_histopathalogy_breast_cancer_dignosis_using_multimodal.pdf",
    "paper_title": "Faseela 2023 Advancing Histopathalogy Breast Cancer Dignosis Using Multimodal",
    "chunk_index": 33,
    "total_chunks": 72,
    "text_content": "prediction accuracy. This comprehensive representation of features facilitated improved survival risk assessment. A method was utilized to group patient graphs into batches for training and updat- ing graph embeddings in a Graph Neural Network (GNN) framework [69], employing the cross-entropy loss function. The datasets incorporated in the study comprised The Cancer Genome Atlas Breast Invasive Carcinoma (TCGA-BRCA) dataset, encompass- ing clinical, genomic, and radiological data from 1,040 pati",
    "full_text_length": 79667,
    "chunk_length": 1496
  },
  {
    "chunk_id": 810,
    "paper_filename": "faseela_2023_advancing_histopathalogy_breast_cancer_dignosis_using_multimodal.pdf",
    "paper_title": "Faseela 2023 Advancing Histopathalogy Breast Cancer Dignosis Using Multimodal",
    "chunk_index": 34,
    "total_chunks": 72,
    "text_content": "cancer detection, particularly with the increasing use of complex machine learning and deep learning models. Explain- ability is essential for clinical decision-making, trust and transparency, regulatory compliance, and error detection and correction. Explainable models enable clinicians to understand the rationale behind a diagnosis, thereby facilitating more informed decision-making and increasing trust in automated systems through transparent conclusions. Explanation Scope StageText Feature A",
    "full_text_length": 79667,
    "chunk_length": 1502
  },
  {
    "chunk_id": 811,
    "paper_filename": "faseela_2023_advancing_histopathalogy_breast_cancer_dignosis_using_multimodal.pdf",
    "paper_title": "Faseela 2023 Advancing Histopathalogy Breast Cancer Dignosis Using Multimodal",
    "chunk_index": 35,
    "total_chunks": 72,
    "text_content": "be in terms of feature attributes and textual format. In scope-based categorisation, there are local and global methods. Post-hoc and ante-hoc are the stage-based XAI methods. To enhance explainability, techniques such as feature importance and saliency maps can provide insights into which aspects of the input are driving the model\u2019s pre- dictions. Model-agnostic methods like LIME and SHAP allow for the explanation of any machine learning model, offering flexibility in creating explainable outpu",
    "full_text_length": 79667,
    "chunk_length": 1449
  },
  {
    "chunk_id": 812,
    "paper_filename": "faseela_2023_advancing_histopathalogy_breast_cancer_dignosis_using_multimodal.pdf",
    "paper_title": "Faseela 2023 Advancing Histopathalogy Breast Cancer Dignosis Using Multimodal",
    "chunk_index": 36,
    "total_chunks": 72,
    "text_content": "and transparent. 4.1 An explainability analysis on multi-modal techniques In the domain of uni-modal breast cancer detection, significant advancements have been made in integrating explainable artificial intelligence (AI) techniques to enhance the interpretability and reliability of predictive models. Gu et al [76] developed an auxiliary decision support system that combines ensemble learning with case-based reasoning (CBR) to predict breast cancer recurrence. Using XGBoost for predictions and C",
    "full_text_length": 79667,
    "chunk_length": 1503
  },
  {
    "chunk_id": 813,
    "paper_filename": "faseela_2023_advancing_histopathalogy_breast_cancer_dignosis_using_multimodal.pdf",
    "paper_title": "Faseela 2023 Advancing Histopathalogy Breast Cancer Dignosis Using Multimodal",
    "chunk_index": 37,
    "total_chunks": 72,
    "text_content": "models used in breast cancer histopathology. Authors of [78] utilized pre- trained models combined with gradient-boosting classifiers to achieve high accuracy in classifying breast cancer images from the BreakHis dataset. Similarly, Peta et al. [79] introduced an explainable deep learning technique involving adaptive unsharp mask filtering and the Explainable Soft Attentive EfficientNet (ESAE-Net), which provided improved visualization and understanding of classification decisions. Jaume et al. ",
    "full_text_length": 79667,
    "chunk_length": 1471
  },
  {
    "chunk_id": 814,
    "paper_filename": "faseela_2023_advancing_histopathalogy_breast_cancer_dignosis_using_multimodal.pdf",
    "paper_title": "Faseela 2023 Advancing Histopathalogy Breast Cancer Dignosis Using Multimodal",
    "chunk_index": 38,
    "total_chunks": 72,
    "text_content": "clinical application. Explainability is a critical factor in radio genomics [83], as it fosters trust with end- users like physicians and patients, driving the deployment of deep learning models in research and clinical practice. It increases confidence in the model\u2019s decision-making process, enabling better understanding and acceptance of results. Explainability also serves as a debugging process for model training and fine-tuning, identifying potential errors or biases. It also helps bypass ma",
    "full_text_length": 79667,
    "chunk_length": 1479
  },
  {
    "chunk_id": 815,
    "paper_filename": "faseela_2023_advancing_histopathalogy_breast_cancer_dignosis_using_multimodal.pdf",
    "paper_title": "Faseela 2023 Advancing Histopathalogy Breast Cancer Dignosis Using Multimodal",
    "chunk_index": 39,
    "total_chunks": 72,
    "text_content": "The method\u2019s objective was to con- struct a multi-modal feature representation space, utilizing knowledge bases as initial connectors for the development of novel explanation interface techniques. Essential components included intra-modal feature extraction and multi-modal embedding. Var- ious GNN architectures and graph embeddings, such as GCNN, Graph Isomorphism Network (GIN), and SchNet, were considered viable options. Additionally, dynamic GNN architectures like Pointer Graph Networks (PGN) ",
    "full_text_length": 79667,
    "chunk_length": 1513
  },
  {
    "chunk_id": 816,
    "paper_filename": "faseela_2023_advancing_histopathalogy_breast_cancer_dignosis_using_multimodal.pdf",
    "paper_title": "Faseela 2023 Advancing Histopathalogy Breast Cancer Dignosis Using Multimodal",
    "chunk_index": 40,
    "total_chunks": 72,
    "text_content": "comprehensive reasoning and fusion of multimodal relations. Additionally, an explanation generation module was incorporated to provide justifications for predicted answers. Experimen- tal findings demonstrated the model\u2019s effectiveness in achieving both quantitative and qualitative performance improvements. Authors of [86] introduced a segmentation framework with an interpretation module that highlights critical features from each modality, guided by a novel interpre- tation loss with strengthen",
    "full_text_length": 79667,
    "chunk_length": 1566
  },
  {
    "chunk_id": 817,
    "paper_filename": "faseela_2023_advancing_histopathalogy_breast_cancer_dignosis_using_multimodal.pdf",
    "paper_title": "Faseela 2023 Advancing Histopathalogy Breast Cancer Dignosis Using Multimodal",
    "chunk_index": 41,
    "total_chunks": 72,
    "text_content": "branch performed image classification through a fully connected layer. Training and validation utilized over 7,000 breast cancer biopsy slide images from the BreaKHis dataset, resulting in a binary classification accuracy of 98.7%. Notably, the model offered enhanced clinical interpretability, with highlighted cancer regions corresponding well with expert pathologist findings. The ABN-DCN model effectively combined an attention mechanism with a CNN feature extractor, thereby improving both diagn",
    "full_text_length": 79667,
    "chunk_length": 1530
  },
  {
    "chunk_id": 818,
    "paper_filename": "faseela_2023_advancing_histopathalogy_breast_cancer_dignosis_using_multimodal.pdf",
    "paper_title": "Faseela 2023 Advancing Histopathalogy Breast Cancer Dignosis Using Multimodal",
    "chunk_index": 42,
    "total_chunks": 72,
    "text_content": "approaches exist within this field as well. As part of a contemporary focus in multimodal methodologies, image and textual data are inte- grated, manifesting in applications including report generation [88], Visual Question Answering (VQA) [89], cross-modal retrieval [90], and semantic segmentation [91]. It has been noted that substantial scholarly attention has been devoted to the leveraging of medical image and text data through these methodologies[1]. Nevertheless, there still remains a need ",
    "full_text_length": 79667,
    "chunk_length": 1472
  },
  {
    "chunk_id": 819,
    "paper_filename": "faseela_2023_advancing_histopathalogy_breast_cancer_dignosis_using_multimodal.pdf",
    "paper_title": "Faseela 2023 Advancing Histopathalogy Breast Cancer Dignosis Using Multimodal",
    "chunk_index": 43,
    "total_chunks": 72,
    "text_content": "models could offer comprehensible diagnostic support, thereby enhancing the accuracy and transparency of the diagnostic process. The LeGrad [93] explainability method, which employs Vision Transformers (ViTs) [94], utilizes techniques such as GradCAM [95] and AttentionCAM [96] to provide granular insights into feature for- mation. These explainability methods are crucial for breast cancer detection, offering transparent interpretations of model decisions. By adapting this method to a multi- moda",
    "full_text_length": 79667,
    "chunk_length": 1524
  },
  {
    "chunk_id": 820,
    "paper_filename": "faseela_2023_advancing_histopathalogy_breast_cancer_dignosis_using_multimodal.pdf",
    "paper_title": "Faseela 2023 Advancing Histopathalogy Breast Cancer Dignosis Using Multimodal",
    "chunk_index": 44,
    "total_chunks": 72,
    "text_content": "ensured that relevant diagnostic infor- mation was accessible and interpretable to pathologists. Through precise alignment and interpretation of multimodal diagnostic data, including histopathology images and clinical and textual reports, the method enhances transparency and interpretability in breast cancer diagnosis. This approach can provide clear insights into the decision- making process of diagnostic models, thereby enhancing trust and clinical acceptance in the application of multimodal A",
    "full_text_length": 79667,
    "chunk_length": 1480
  },
  {
    "chunk_id": 821,
    "paper_filename": "faseela_2023_advancing_histopathalogy_breast_cancer_dignosis_using_multimodal.pdf",
    "paper_title": "Faseela 2023 Advancing Histopathalogy Breast Cancer Dignosis Using Multimodal",
    "chunk_index": 45,
    "total_chunks": 72,
    "text_content": "learning models, researchers explored the use of VisualBERT and UNITER networks to generate mul- timodal visual and language explanations [98]. The potential of these models to mimic domain expertise underscores the value of explainable AI techniques in breast cancer detection. By providing clear and understandable rationales for automated decisions, such methods enhance clinical trust and support informed decision-making in diagnostic processes. A framework named LangXAI [99] was introduced, in",
    "full_text_length": 79667,
    "chunk_length": 1515
  },
  {
    "chunk_id": 822,
    "paper_filename": "faseela_2023_advancing_histopathalogy_breast_cancer_dignosis_using_multimodal.pdf",
    "paper_title": "Faseela 2023 Advancing Histopathalogy Breast Cancer Dignosis Using Multimodal",
    "chunk_index": 46,
    "total_chunks": 72,
    "text_content": "Ensuring the transparency and comprehensibility of model decisions can play a crucial role in facilitating regula- tory compliance and fostering clinical acceptance of such models in diagnostic settings. A tool called LVLM-Interpret [101] was developed to interpret responses from large vision-language models, employing techniques such as raw attention and relevancy maps. This tool\u2019s capacity to visualize and comprehend model outputs can be utilized in breast cancer detection to improve the inter",
    "full_text_length": 79667,
    "chunk_length": 1455
  },
  {
    "chunk_id": 823,
    "paper_filename": "faseela_2023_advancing_histopathalogy_breast_cancer_dignosis_using_multimodal.pdf",
    "paper_title": "Faseela 2023 Advancing Histopathalogy Breast Cancer Dignosis Using Multimodal",
    "chunk_index": 47,
    "total_chunks": 72,
    "text_content": "across diverse tasks, holds the potential for adaptation in breast cancer detection. By providing both diagnostic conclusions and 19 their explanations, the NLX-GPT method enhances the usability and trustworthiness of AI models in clinical settings. These advancements indicate a promising future for the integration of multi- modal fusion and explainability in breast cancer diagnostics. By leveraging these innovative approaches, histopathology-based breast cancer diagnosis can be signifi- cantly ",
    "full_text_length": 79667,
    "chunk_length": 1623
  },
  {
    "chunk_id": 824,
    "paper_filename": "faseela_2023_advancing_histopathalogy_breast_cancer_dignosis_using_multimodal.pdf",
    "paper_title": "Faseela 2023 Advancing Histopathalogy Breast Cancer Dignosis Using Multimodal",
    "chunk_index": 48,
    "total_chunks": 72,
    "text_content": "extracted through computer vision techniques. Clinical data, includ- ing patient history and laboratory results, are integrated into the report generation pipeline to ensure contextually relevant diagnostic reports. Pathology Report Generation Histopathology ImagesPre-trained r eport generation modelPathology Reports Explainable multi-modal br east cancer detection Report generation ExplainabilityMulti-modal framework DecisionFinal decision Crowdsour cing & Humn-in-the Loop Pathology r eport Fig",
    "full_text_length": 79667,
    "chunk_length": 1629
  },
  {
    "chunk_id": 825,
    "paper_filename": "faseela_2023_advancing_histopathalogy_breast_cancer_dignosis_using_multimodal.pdf",
    "paper_title": "Faseela 2023 Advancing Histopathalogy Breast Cancer Dignosis Using Multimodal",
    "chunk_index": 49,
    "total_chunks": 72,
    "text_content": "through crowdsourcing or expert consultations to validate and refine model predictions, ensuring clinical relevance and accuracy. This human-in-the-loop approach facilitates informed decision-making and iterative refine- ment based on feedback from clinicians and patients. Ultimately, the framework supports diagnostic support by providing transparent and understandable diagnostic conclusions, along with explanations for model predictions. It is integrated into exist- ing clinical workflows to st",
    "full_text_length": 79667,
    "chunk_length": 1481
  },
  {
    "chunk_id": 826,
    "paper_filename": "faseela_2023_advancing_histopathalogy_breast_cancer_dignosis_using_multimodal.pdf",
    "paper_title": "Faseela 2023 Advancing Histopathalogy Breast Cancer Dignosis Using Multimodal",
    "chunk_index": 50,
    "total_chunks": 72,
    "text_content": "data. This review has delved into the burgeoning field of multimodal methodologies, with a specific focus on the fusion of histopathology images with non-image data. Furthermore, the incorporation of Explainable AI (XAI) serves to illuminate the decision-making processes of intricate algorithms, underlining the importance of transparency in diagnostic procedures. By leveraging multi-modal data and emphasizing explainability, this review advocates for enhancing diagnostic accuracy, bolstering cli",
    "full_text_length": 79667,
    "chunk_length": 1516
  },
  {
    "chunk_id": 827,
    "paper_filename": "faseela_2023_advancing_histopathalogy_breast_cancer_dignosis_using_multimodal.pdf",
    "paper_title": "Faseela 2023 Advancing Histopathalogy Breast Cancer Dignosis Using Multimodal",
    "chunk_index": 51,
    "total_chunks": 72,
    "text_content": "patient outcomes and a more effective healthcare landscape. Acknowledgment Research reported in this publication was supported by the Qatar Research Devel- opment and Innovation Council [ARG01-0513-230141]. The content is solely the responsibility of the authors and does not necessarily represent the official views of Qatar Research Development and Innovation Council. 21 References [1] Sun, Z., Lin, M., Zhu, Q., Xie, Q., Wang, F., Lu, Z., Peng, Y.: A scoping review on multimodal deep learning in",
    "full_text_length": 79667,
    "chunk_length": 1419
  },
  {
    "chunk_id": 828,
    "paper_filename": "faseela_2023_advancing_histopathalogy_breast_cancer_dignosis_using_multimodal.pdf",
    "paper_title": "Faseela 2023 Advancing Histopathalogy Breast Cancer Dignosis Using Multimodal",
    "chunk_index": 52,
    "total_chunks": 72,
    "text_content": "in breast cancer segmen- tation: A comprehensive review. Acadlore Transactions on AI and Machine Learning 3(2), 70\u201383 (2024) [5] Hussain, S., Ali, M., Naseem, U., Nezhadmoghadam, F., Jatoi, M.A., Gulliver, T.A., Tamez-Pe\u02dc na, J.G.: Breast cancer risk prediction using machine learning: a systematic review. Frontiers in Oncology 14, 1343627 (2024) [6] Brodhead, M., Woods, R.W., Fowler, A.M., Roy, M., Neuman, H., Gegios, A.: Multimodality imaging review of metastatic melanoma involving the breast. ",
    "full_text_length": 79667,
    "chunk_length": 1356
  },
  {
    "chunk_id": 829,
    "paper_filename": "faseela_2023_advancing_histopathalogy_breast_cancer_dignosis_using_multimodal.pdf",
    "paper_title": "Faseela 2023 Advancing Histopathalogy Breast Cancer Dignosis Using Multimodal",
    "chunk_index": 53,
    "total_chunks": 72,
    "text_content": "D., Singh, G.: Technical review of breast cancer screening and detection using artificial intelligence and radiomics. In: 2024 11th International Conference on Computing for Sustainable Global Development (INDIACom), pp. 1171\u20131176 (2024). IEEE [10] Thakur, N., Kumar, P., Kumar, A.: A systematic review of machine and deep learning techniques for the identification and classification of breast cancer through medical image modalities. Multimedia Tools and Applications 83(12), 35849\u201335942 (2024) [11",
    "full_text_length": 79667,
    "chunk_length": 1380
  },
  {
    "chunk_id": 830,
    "paper_filename": "faseela_2023_advancing_histopathalogy_breast_cancer_dignosis_using_multimodal.pdf",
    "paper_title": "Faseela 2023 Advancing Histopathalogy Breast Cancer Dignosis Using Multimodal",
    "chunk_index": 54,
    "total_chunks": 72,
    "text_content": "Song, X., Qin, J., et al. : A classifier-combined method for grading breast cancer based on dempster-shafer evidence theory. Quantitative Imaging in Medicine and Surgery 13(5), 3288 (2023) [15] Kumaraswamy, E., Kumar, S., Sharma, M.: An invasive ductal carcinomas breast cancer grade classification using an ensemble of convolutional neural networks. Diagnostics 13(11), 1977 (2023) [16] Huang, Y., Zeng, P., Zhong, C.: Classifying breast cancer subtypes on multi- omics data via sparse canonical cor",
    "full_text_length": 79667,
    "chunk_length": 1419
  },
  {
    "chunk_id": 831,
    "paper_filename": "faseela_2023_advancing_histopathalogy_breast_cancer_dignosis_using_multimodal.pdf",
    "paper_title": "Faseela 2023 Advancing Histopathalogy Breast Cancer Dignosis Using Multimodal",
    "chunk_index": 55,
    "total_chunks": 72,
    "text_content": "ultrasound images. Data in brief 28, 104863 (2020) [20] Paulo, S.: Breast ultrasound image. Mendeley data (2017) [21] The Cancer Genome Atlas (TCGA): Genomic Data Commons Data Portal (GDC). https://portal.gdc.cancer.gov/projects/TCGA-BRCA. Accessed 07 Jul. 2023 [22] Parshionikar, S., Bhattacharyya, D.: An enhanced multi-scale deep convolu- tional orchard capsule neural network for multi-modal breast cancer detection. Healthcare Analytics 5, 100298 (2024) [23] Spanhol, F.A., Oliveira, L.S., Petit",
    "full_text_length": 79667,
    "chunk_length": 1524
  },
  {
    "chunk_id": 832,
    "paper_filename": "faseela_2023_advancing_histopathalogy_breast_cancer_dignosis_using_multimodal.pdf",
    "paper_title": "Faseela 2023 Advancing Histopathalogy Breast Cancer Dignosis Using Multimodal",
    "chunk_index": 56,
    "total_chunks": 72,
    "text_content": "cancer using histogram k-means segmentation technique. Multimedia Tools and Applications 82(9), 14055\u201314075 (2023) [27] Sahu, A., Das, P.K., Meher, S.: High accuracy hybrid cnn classifiers for breast cancer detection using mammogram and ultrasound datasets. Biomedical Signal Processing and Control 80, 104292 (2023) [28] Lekamlage, C.D., Afzal, F., Westerberg, E., Cheddad, A.: Mini-ddsm: Mammography-based automatic age estimation. In: 2020 3rd International Conference on Digital Medicine and Imag",
    "full_text_length": 79667,
    "chunk_length": 1434
  },
  {
    "chunk_id": 833,
    "paper_filename": "faseela_2023_advancing_histopathalogy_breast_cancer_dignosis_using_multimodal.pdf",
    "paper_title": "Faseela 2023 Advancing Histopathalogy Breast Cancer Dignosis Using Multimodal",
    "chunk_index": 57,
    "total_chunks": 72,
    "text_content": "Multi-modal fusion network with intra-and inter-modality attention for prognosis prediction in breast cancer. Computers in Biology and Medicine 168, 107796 (2024) [32] Sivamurugan, J., Sureshkumar, G.: Applying dual models on optimized lstm with u-net segmentation for breast cancer diagnosis using mammogram images. Artificial Intelligence in Medicine 143, 102626 (2023) [33] Kendall, E.J., Barnett, M.G., Chytyk-Praznik, K.: Automatic detection of anomalies in screening mammograms. BMC Medical Ima",
    "full_text_length": 79667,
    "chunk_length": 1405
  },
  {
    "chunk_id": 834,
    "paper_filename": "faseela_2023_advancing_histopathalogy_breast_cancer_dignosis_using_multimodal.pdf",
    "paper_title": "Faseela 2023 Advancing Histopathalogy Breast Cancer Dignosis Using Multimodal",
    "chunk_index": 58,
    "total_chunks": 72,
    "text_content": "data practices for semantic segmentation of breast cancer using deep neural network. Journal of Ambient Intelligence and Humanized Computing 14(11), 15227\u201315243 (2023) [37] Lee, R.S., Gimenez, F., Hoogi, A., Miyake, K.K., Gorovoy, M., Rubin, D.L.: A curated mammography data set for use in computer-aided detection and diagnosis research. Scientific data 4(1), 1\u20139 (2017) [38] Alam, T., Shia, W.-C., Hsu, F.-R., Hassan, T.: Improving breast cancer detection and diagnosis through semantic segmentatio",
    "full_text_length": 79667,
    "chunk_length": 1386
  },
  {
    "chunk_id": 835,
    "paper_filename": "faseela_2023_advancing_histopathalogy_breast_cancer_dignosis_using_multimodal.pdf",
    "paper_title": "Faseela 2023 Advancing Histopathalogy Breast Cancer Dignosis Using Multimodal",
    "chunk_index": 59,
    "total_chunks": 72,
    "text_content": "threshold image segmentation based on an improved salp swarm algorithm: Case study of breast cancer pathology images. Computers in Biology and Medicine 168, 107769 (2024) [42] Rajoub, B., Qusa, H., Abdul-Rahman, H., Mohamed, H.: Segmentation of breast tissue structures in mammographic images. Artificial Intelligence and Image Processing in Medical Imaging, 115\u2013146 (2024) [43] Soliman, A., Li, Z., Parwani, A.V.: Artificial intelligence\u2019s impact on breast cancer pathology: a literature review. Dia",
    "full_text_length": 79667,
    "chunk_length": 1450
  },
  {
    "chunk_id": 836,
    "paper_filename": "faseela_2023_advancing_histopathalogy_breast_cancer_dignosis_using_multimodal.pdf",
    "paper_title": "Faseela 2023 Advancing Histopathalogy Breast Cancer Dignosis Using Multimodal",
    "chunk_index": 60,
    "total_chunks": 72,
    "text_content": "Kleesiek, J.: Beyond medical imaging-a review of multimodal deep learning in radiology. Authorea Preprints (2023) [47] Laokulrath, N., Gudi, M.A., Deb, R., Ellis, I.O., Tan, P.H.: Invasive breast 25 cancer reporting guidelines: Iccr, cap, rcpath, rcpa datasets and future directions. Diagnostic Histopathology (2024) [48] Brancati, N., Anniciello, A.M., Pati, P., Riccio, D., Scognamiglio, G., Jaume, G., De Pietro, G., Di Bonito, M., Foncubierta, A., Botti, G., et al. : Bracs: A dataset for breast ",
    "full_text_length": 79667,
    "chunk_length": 1413
  },
  {
    "chunk_id": 837,
    "paper_filename": "faseela_2023_advancing_histopathalogy_breast_cancer_dignosis_using_multimodal.pdf",
    "paper_title": "Faseela 2023 Advancing Histopathalogy Breast Cancer Dignosis Using Multimodal",
    "chunk_index": 61,
    "total_chunks": 72,
    "text_content": "Clinical Proteomic Tumor Analysis Consortium Breast Invasive Carcinoma Col- lection (CPTAC-BRCA). The Cancer Imaging Archive. Accessed 07 Jul. 2023 (2020). https://wiki.cancerimagingarchive.net/pages/viewpage.action?pageId= 70227748 [52] Yan, R., Zhang, F., Rao, X., Lv, Z., Li, J., Zhang, L., Liang, S., Li, Y., Ren, F., Zheng, C., et al. : Richer fusion network for breast cancer classification based on multimodal data. BMC Medical Informatics and Decision Making 21, 1\u201315 (2021) [53] vEarly Breas",
    "full_text_length": 79667,
    "chunk_length": 1452
  },
  {
    "chunk_id": 838,
    "paper_filename": "faseela_2023_advancing_histopathalogy_breast_cancer_dignosis_using_multimodal.pdf",
    "paper_title": "Faseela 2023 Advancing Histopathalogy Breast Cancer Dignosis Using Multimodal",
    "chunk_index": 62,
    "total_chunks": 72,
    "text_content": "M., Shao, D., Vaidya, A.J., Chen, C., Zhuang, L., Williamson, D.F., et al. : Artificial intelligence for multimodal data integration in oncology. Cancer cell 40(10), 1095\u20131110 (2022) [57] Zhao, F., Zhang, C., Geng, B.: Deep multimodal data fusion. ACM Computing Surveys (2024) [58] Yellapragada, S., Graikos, A., Prasanna, P., Kurc, T., Saltz, J., Samaras, D.: 26 Pathldm: Text conditioned latent diffusion model for histopathology. In: Pro- ceedings of the IEEE/CVF Winter Conference on Applications",
    "full_text_length": 79667,
    "chunk_length": 1437
  },
  {
    "chunk_id": 839,
    "paper_filename": "faseela_2023_advancing_histopathalogy_breast_cancer_dignosis_using_multimodal.pdf",
    "paper_title": "Faseela 2023 Advancing Histopathalogy Breast Cancer Dignosis Using Multimodal",
    "chunk_index": 63,
    "total_chunks": 72,
    "text_content": "Saha, S.: Multi-modal advanced deep learning architectures for breast cancer survival prediction. Knowledge-Based Systems 221, 106965 (2021) [62] Sun, D., Wang, M., Li, A.: A multimodal deep neural network for human breast cancer prognosis prediction by integrating multi-dimensional data. IEEE/ACM transactions on computational biology and bioinformatics 16(3), 841\u2013850 (2018) [63] Tong, L., Mitchel, J., Chatlin, K., Wang, M.D.: Deep learning based feature- level integration of multi-omics data fo",
    "full_text_length": 79667,
    "chunk_length": 1444
  },
  {
    "chunk_id": 840,
    "paper_filename": "faseela_2023_advancing_histopathalogy_breast_cancer_dignosis_using_multimodal.pdf",
    "paper_title": "Faseela 2023 Advancing Histopathalogy Breast Cancer Dignosis Using Multimodal",
    "chunk_index": 64,
    "total_chunks": 72,
    "text_content": "S., Khramtsova, G., Vickery, J., Srisuwananukorn, A., Woodard, A., Chen, N., Nanda, R., Perou, C.M., et al.: Multimodal prediction of breast cancer recurrence assays and risk of recurrence. bioRxiv, 2022\u201307 (2022) [67] Arya, N., Saha, S.: Generative incomplete multi-view prognosis predictor for breast cancer: Gimpp. IEEE/ACM Transactions on Computational Biology and Bioinformatics 19(4), 2252\u20132263 (2021) [68] Arya, N., Saha, S.: Multi-modal classification for human breast cancer prognosis predic",
    "full_text_length": 79667,
    "chunk_length": 1455
  },
  {
    "chunk_id": 841,
    "paper_filename": "faseela_2023_advancing_histopathalogy_breast_cancer_dignosis_using_multimodal.pdf",
    "paper_title": "Faseela 2023 Advancing Histopathalogy Breast Cancer Dignosis Using Multimodal",
    "chunk_index": 65,
    "total_chunks": 72,
    "text_content": "prediction using gated attentive multimodal deep learning. Journal of Big Data 10(1), 62 (2023) [72] Arya, N., Saha, S., Mathur, A., Saha, S.: Improving the robustness and stability of a machine learning model for breast cancer prognosis through the use of multi-modal classifiers. Scientific Reports 13(1), 4079 (2023) [73] Mondol, R.K., Millar, E.K., Sowmya, A., Meijering, E.: Mm-survnet: Deep learning-based survival risk stratification in breast cancer through multimodal data fusion. arXiv prep",
    "full_text_length": 79667,
    "chunk_length": 1390
  },
  {
    "chunk_id": 842,
    "paper_filename": "faseela_2023_advancing_histopathalogy_breast_cancer_dignosis_using_multimodal.pdf",
    "paper_title": "Faseela 2023 Advancing Histopathalogy Breast Cancer Dignosis Using Multimodal",
    "chunk_index": 66,
    "total_chunks": 72,
    "text_content": "107, 101858 (2020) [77] Kabak\u00b8 c\u0131, K.A., C \u00b8ak\u0131r, A., T\u00a8 urkmen, \u02d9I., T\u00a8 oreyin, B.U., C \u00b8apar, A.: Automated scoring of cerbb2/her2 receptors using histogram based analysis of immunohisto- chemistry breast cancer tissue images. Biomedical Signal Processing and Control 69, 102924 (2021) [78] Maleki, A., Raahemi, M., Nasiri, H.: Breast cancer diagnosis from histopathology images using deep neural network and xgboost. Biomedical Signal Processing and Control 86, 105152 (2023) [79] Peta, J., Koppu,",
    "full_text_length": 79667,
    "chunk_length": 1457
  },
  {
    "chunk_id": 843,
    "paper_filename": "faseela_2023_advancing_histopathalogy_breast_cancer_dignosis_using_multimodal.pdf",
    "paper_title": "Faseela 2023 Advancing Histopathalogy Breast Cancer Dignosis Using Multimodal",
    "chunk_index": 67,
    "total_chunks": 72,
    "text_content": "Transactions on Biomedical Engineering (2023) [82] Altini, N., Puro, E., Taccogna, M.G., Marino, F., De Summa, S., Saponaro, C., Mattioli, E., Zito, F.A., Bevilacqua, V.: Tumor cellularity assessment of breast histopathological slides via instance segmentation and pathomic features explainability. Bioengineering 10(4), 396 (2023) [83] Liu, Q., Hu, P.: Extendable and explainable deep learning for pan-cancer radiogenomics research. Current opinion in chemical biology 66, 102111 (2022) [84] Holzing",
    "full_text_length": 79667,
    "chunk_length": 1411
  },
  {
    "chunk_id": 844,
    "paper_filename": "faseela_2023_advancing_histopathalogy_breast_cancer_dignosis_using_multimodal.pdf",
    "paper_title": "Faseela 2023 Advancing Histopathalogy Breast Cancer Dignosis Using Multimodal",
    "chunk_index": 68,
    "total_chunks": 72,
    "text_content": "(2023) [87] Krishna, S., Suganthi, S., Bhavsar, A., Yesodharan, J., Krishnamoorthy, S.: An interpretable decision-support model for breast cancer diagnosis using histopathology images. Journal of Pathology Informatics 14, 100319 (2023) [88] Guo, Z., Ma, J., Xu, Y., Wang, Y., Wang, L., Chen, H.: Histgen: Histopathol- ogy report generation via local-global feature encoding and cross-modal context interaction. arXiv preprint arXiv:2403.05396 (2024) [89] Hartsock, I., Rasool, G.: Vision-language mod",
    "full_text_length": 79667,
    "chunk_length": 1466
  },
  {
    "chunk_id": 845,
    "paper_filename": "faseela_2023_advancing_histopathalogy_breast_cancer_dignosis_using_multimodal.pdf",
    "paper_title": "Faseela 2023 Advancing Histopathalogy Breast Cancer Dignosis Using Multimodal",
    "chunk_index": 69,
    "total_chunks": 72,
    "text_content": "(2021) [92] Held, J., Itani, H., Cioppa, A., Giancola, S., Ghanem, B., Van Droogenbroeck, M.: X-vars: Introducing explainability in football refereeing with multi-modal 29 large language model. arXiv preprint arXiv:2404.06332 (2024) [93] Bousselham, W., Boggust, A., Chaybouti, S., Strobelt, H., Kuehne, H.: Legrad: An explainability method for vision transformers via feature formation sensitiv- ity. arXiv preprint arXiv:2404.03214 (2024) [94] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., J",
    "full_text_length": 79667,
    "chunk_length": 1438
  },
  {
    "chunk_id": 846,
    "paper_filename": "faseela_2023_advancing_histopathalogy_breast_cancer_dignosis_using_multimodal.pdf",
    "paper_title": "Faseela 2023 Advancing Histopathalogy Breast Cancer Dignosis Using Multimodal",
    "chunk_index": 70,
    "total_chunks": 72,
    "text_content": "and Control 96, 106497 (2024) [97] Tizhoosh, H., Pantanowitz, L.: On image search in histopathology. Journal of Pathology Informatics, 100375 (2024) [98] Werner, P., Zapaishchykova, A., Ratan, U.: The ability of image- language explainable models to resemble domain expertise. arXiv preprint arXiv:2209.09310 (2022) [99] Nguyen, T.T.H., Clement, T., Nguyen, P.T.L., Kemmerzell, N., Truong, V.B., Nguyen, V.T.K., Abdelaal, M., Cao, H.: Langxai: Integrating large vision models for generating textual e",
    "full_text_length": 79667,
    "chunk_length": 1457
  },
  {
    "chunk_id": 847,
    "paper_filename": "faseela_2023_advancing_histopathalogy_breast_cancer_dignosis_using_multimodal.pdf",
    "paper_title": "Faseela 2023 Advancing Histopathalogy Breast Cancer Dignosis Using Multimodal",
    "chunk_index": 71,
    "total_chunks": 72,
    "text_content": "language explainer. IEEE Transactions on Multimedia (2023) [103] Sammani, F., Deligiannis, N.: Uni-nlx: Unifying textual explanations for vision and vision-language tasks. In: Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 4634\u20134639 (2023) 30 [104] Biswal, S., Xiao, C., Glass, L.M., Westover, B., Sun, J.: Clara: clinical report auto-completion. In: Proceedings of The Web Conference 2020, pp. 541\u2013550 (2020) 31",
    "full_text_length": 79667,
    "chunk_length": 442
  },
  {
    "chunk_id": 848,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 0,
    "total_chunks": 150,
    "text_content": "Accepted: 26 September 2024 / Published online: 12 October 2024 \u00a9 The Author(s) 2024 Extended author information available on the last page of the articleA comprehensive investigation of multimodal deep learning fusion strategies for breast cancer classification Fatima-Zahrae Nakach1 \u00b7 Ali Idri2 \u00b7 Evgin Goceri3Artificial Intelligence Review (2024) 57:327 https://doi.org/10.1007/s10462-024-10984-z Abstract In breast cancer research, diverse data types and formats, such as radiological images, cli",
    "full_text_length": 151955,
    "chunk_length": 1445
  },
  {
    "chunk_id": 849,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 1,
    "total_chunks": 150,
    "text_content": "systematic literature review encompasses various aspects, including the medical modalities combined, the datasets utilized in these studies, the techniques, models, and architectures used in MDLF and it also discusses the advantages and limitations of each approach. The analysis of selected papers has revealed a compelling trend: the emergence of new modalities and combina - tions that were previously unexplored in the context of breast cancer classification. This exploration has not only expand",
    "full_text_length": 151955,
    "chunk_length": 1405
  },
  {
    "chunk_id": 850,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 2,
    "total_chunks": 150,
    "text_content": "and future directions in this field, including the need for larger datasets, the use of ensemble learning methods, and the interpretation of multimodal models. Keywords Breast cancer \u00b7 Classification \u00b7 Fusion \u00b7 Multimodality \u00b7 Deep learning \u00b7 Review 1 3 F.-Z. Nakach et al.1 Introduction Breast cancer (BC) is characterized by the uncontrolled growth of malignant cells in breast tissue; this disease is widespread and life-altering (Arnold et al. 2022 ). It not only affects millions of women worldw",
    "full_text_length": 151955,
    "chunk_length": 1257
  },
  {
    "chunk_id": 851,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 3,
    "total_chunks": 150,
    "text_content": "and enhance the overall quality of life for individu - als affected by BC (Lu et al. 2009 ). Artificial intelligence (AI) has emerged as a power - ful tool for predicting BC (Bahl and Bahl 2022 ; Sugimoto 2023 ). By leveraging advanced machine learning (ML) algorithms and analyzing a vast array of patient data, AI systems can significantly improve cancer diagnosis, treatment, and prevention (Hanahan et al. 2011 ). The mortality rates associated with BC could decrease by offering physicians accur",
    "full_text_length": 151955,
    "chunk_length": 1308
  },
  {
    "chunk_id": 852,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 4,
    "total_chunks": 150,
    "text_content": "treatment, and prevention using different modalities, including gene expression, clinical records, and medical images (James et al. 2014 ). Although one of those modalities alone is not accurate enough, their combination can significantly improve diagnostic accu - racy and treatment decisions due to their complementary nature (Yuan et al. 2010 ). Recently, multimodal data fusion based on DL has undergone rapid growth in the devel - opment of healthcare AI systems (Lahat et al. 2015 ). Unimodal s",
    "full_text_length": 151955,
    "chunk_length": 1307
  },
  {
    "chunk_id": 853,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 5,
    "total_chunks": 150,
    "text_content": "medical modalities, each characterized by its own distinct set of features, shapes, and dimensions, leads to the generation of high-dimensional data that represent a complex interplay of information and a significant hurdle in the implementation of multimodal DL (Metzger-Filho et al. 2013 ). The main challenge lies in identifying the optimal combination of medical modalities, feature processing methods, feature extraction techniques, and decision-fusion algorithms tailored to address a particula",
    "full_text_length": 151955,
    "chunk_length": 1348
  },
  {
    "chunk_id": 854,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 6,
    "total_chunks": 150,
    "text_content": "paramount sig - nificance of these systems, leading to a series of comprehensive reviews (Huang et al. 2020 ; Lipkova et al. 2022 ; Stahlschmidt et al. 2022 ; Salvi et al. 2024 ; Steyaert et al. 2023 ; Pei et al. 2023 ). These reviews, conducted by esteemed scholars in the field, aim to elucidate the 1 3327 Page 2 of 53 A comprehensive investigation of multimodal deep learning fusion\u2026current state of knowledge, identify gaps, and provide insights into the evolving landscape of multimodal ML appl",
    "full_text_length": 151955,
    "chunk_length": 1279
  },
  {
    "chunk_id": 855,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 7,
    "total_chunks": 150,
    "text_content": "for a more in-depth analysis of the applications, challenges, and implications of BC within its context, resulting in a review that is targeted, valuable, and contextually relevant. This paper explores the primary studies published in six digital libraries\u2014ScienceDi - rect, SpringerLink, Wiley, IEEE Xplore, ACM Digital Library, and Google Scholar\u2014until December 2023 and presents a comprehensive review in the field of multimodal DL fusion applied for BC classification, emphasizing its pivotal rol",
    "full_text_length": 151955,
    "chunk_length": 1430
  },
  {
    "chunk_id": 856,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 8,
    "total_chunks": 150,
    "text_content": "used. \u25cfEnumerating the medical tasks, classification target and the combinations of modalities that are most frequently addressed. \u25cfDescribing the fusion strategies and concatenation techniques employed. \u25cfExploring the application of DL and ML models. \u25cfIdentifying he most commonly used validation methods and metrics for evaluation. \u25cfAssessing the performance of multimodal DL models. \u25cfComparing the performance of models based on multimodal data and on an individual modality. \u25cfIdentifying the stre",
    "full_text_length": 151955,
    "chunk_length": 1332
  },
  {
    "chunk_id": 857,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 9,
    "total_chunks": 150,
    "text_content": "examination of approaches to multimodal fusion is provided, along with a detailed evaluation of the concatenation methods and ML/DL models employed. Section 7 outlines the patterns discovered in the fused features learned from the studies that were selected. In Section 8, a thorough analysis is presented, discussing the strengths and weaknesses of each multimodal fusion approach and providing a comparative assessment of the selected works. Section 9 elaborates on the performance metrics utilized",
    "full_text_length": 151955,
    "chunk_length": 1352
  },
  {
    "chunk_id": 858,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 10,
    "total_chunks": 150,
    "text_content": "provides an overview of multimodal DL fusion and its relevance in BC research. It also summarizes related reviews to highlight key insights and identify gaps in current knowledge. 2.1 Multimodal fusion taxonomy The accuracy of BC classification using a single modality falls short of meeting therapeutic requirements; due to the complexity of natural factors, a single modality struggles to pro - vide comprehensive information for analysis (Abhisheka et al. 2023 ). Consequently, lever - aging multi",
    "full_text_length": 151955,
    "chunk_length": 1335
  },
  {
    "chunk_id": 859,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 11,
    "total_chunks": 150,
    "text_content": "establishing a uniform nomenclature for its diverse types. The absence of standardized terminology across the literature makes it challenging to consis - tently categorize and define different fusion approaches, particularly when specific details are not explicitly articulated in the papers. In our investigation of multimodal ML fusion techniques, we underscore a paper (Holste et al. 2021 ) that criticizes an existing survey\u2019s taxonomy (Baltru\u0161aitis et al. 2019 ), deeming it limited in expressiv",
    "full_text_length": 151955,
    "chunk_length": 1326
  },
  {
    "chunk_id": 860,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 12,
    "total_chunks": 150,
    "text_content": "emphasizing the nature of the features involved and the fusion methodol - ogy employed. This process encompasses three distinct fusion approaches: \u2013Decision fusion , also known as late fusion or probability fusion, refers to the process of utilizing predictions from multiple models to generate a final prediction. This procedure employs distinct models that are trained using various modalities, and a consolidation function merges their decisions (D). The available functions include averaging, maj",
    "full_text_length": 151955,
    "chunk_length": 1316
  },
  {
    "chunk_id": 861,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 13,
    "total_chunks": 150,
    "text_content": "- tion, pooling, or the utilization of a gated unit (Huang et al. 2020 ). Within feature fusion, subtypes emerge based on the nature of the features: \u2013Semantic features (S): Represents the raw, original data. \u2013Learned Features (L): Extracted features based on a DL model. 1 3327 Page 4 of 53 A comprehensive investigation of multimodal deep learning fusion\u2026When semantic features are fused, the fusion approach is referred to as \u201cearly fusion.\u201d When the features are learned, the fusion is known as \u201c",
    "full_text_length": 151955,
    "chunk_length": 1228
  },
  {
    "chunk_id": 862,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 14,
    "total_chunks": 150,
    "text_content": "modalities on top of layers of modality-specific networks. In the DL literature, researchers often focus on innovating by introducing new layers, optimization techniques, or novel methods tailored to specific data types (Akkus et al. 2023 ). However, in the field of biomedical AI, the emphasis is primarily on determining the most suitable architecture for the task at hand. Research - ers are attempting to experiment with the number and configuration of layers and explore different loss functions",
    "full_text_length": 151955,
    "chunk_length": 1295
  },
  {
    "chunk_id": 863,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 15,
    "total_chunks": 150,
    "text_content": "diverse networks and integrating them at different levels. This subsection reviews the most impactful models utilized in the reviewed works, facilitat - ing a quicker understanding of new research and, ideally, aiding them in developing models tailored to their specific tasks. New developments in DL are highly important for making diagnoses more accurate and faster: \u2013Deep Neural Networks A Deep Neural Networks (DNN) is a form of artificial neural network that consists of multiple layers, such as",
    "full_text_length": 151955,
    "chunk_length": 1285
  },
  {
    "chunk_id": 864,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 16,
    "total_chunks": 150,
    "text_content": "is the mul - tilayer perceptron (MLP), which comprises three or more tiers of nodes, encompassing an input tier, one or more concealed tiers, and an output tier. The network establishes connections between each node in one layer and every node in the following layer while incorporating activation functions to introduce nonlinear behavior. MLPs are commonly used for supervised learning tasks such as classification and regression, and they often incorporate fully connected layers (FCLs) to facilit",
    "full_text_length": 151955,
    "chunk_length": 1388
  },
  {
    "chunk_id": 865,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 17,
    "total_chunks": 150,
    "text_content": "the most commonly used DL networks for BC detection and use multiple modalities. They have greatly advanced the field of computer vision by greatly enhancing the precision of image recognition tasks. CNNs are highly proficient at capturing hierarchical representations of visual features, which makes them particularly suitable for tasks such as object detection, segmentation, and classification (Battleday et al. 2021 ). CNNs have demonstrated significant efficacy not only in image classification ",
    "full_text_length": 151955,
    "chunk_length": 1370
  },
  {
    "chunk_id": 866,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 18,
    "total_chunks": 150,
    "text_content": "information while decreasing computational complexity. Fully connected layers establish connections between every neuron in one layer and every neuron in the subsequent layer, allowing the network to generate predictions by utiliz - ing the acquired features (Alzubaidi et al. 2021 ). Beyond their ability to extract features 1 3327 Page 6 of 53 A comprehensive investigation of multimodal deep learning fusion\u2026from image data, CNNs have emerged as popular choices for information fusion in predictio",
    "full_text_length": 151955,
    "chunk_length": 1365
  },
  {
    "chunk_id": 867,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 19,
    "total_chunks": 150,
    "text_content": "class of neural networks that possess a form of memory, allowing them to access previous information during processing. Unlike typical neural networks where inputs and outputs are independent, RNNs can leverage their memory to consider past data, albeit within a limited temporal scope. However, RNNs face challenges, particularly short-term memory issues, as they struggle to retain information over extended sequences (Yin et al. 2017 ). To address this limitation, long short-term memory (LSTM) ne",
    "full_text_length": 151955,
    "chunk_length": 1335
  },
  {
    "chunk_id": 868,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 20,
    "total_chunks": 150,
    "text_content": "). GRUs employ three primary gates and an internal cell state, making them more efficient than LSTMs in certain contexts. GRUs outperform LSTMs due to parameter reduction, resulting in a higher conver - gence rate and requiring less computational time. The simplicity of the GRU structure, compared to that of LSTMs, contributes to reduced matrix multiplication, saving time without compromising performance (Dey 2017 ). \u2013Autoencoders An autoencoder is an unsupervised neural network composed of an e",
    "full_text_length": 151955,
    "chunk_length": 1344
  },
  {
    "chunk_id": 869,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 21,
    "total_chunks": 150,
    "text_content": "versions of the raw data. This approach enhances the robustness of feature representation by preventing the model from simply learning identity function mapping (Gondara 2016 ). \u2013Conditional autoencoder A conditional autoencoder is designed to extract related features from different modalities. This approach typically involves the introduction of specific encoders, such as a gene encoder for dimension reduction, and aims to perform correlated feature extraction. This type of autoencoder is commo",
    "full_text_length": 151955,
    "chunk_length": 1367
  },
  {
    "chunk_id": 870,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 22,
    "total_chunks": 150,
    "text_content": "- tion (Cheng et al. 2021 ). \u2013 Generative Adversarial Networks. Generative Adversarial Networks (GANs) hold a distinctive position among researchers owing to their ability to produce high-quality generated data (Goodfellow et al. 2020 ). The fundamental concept behind GANs involves the simultaneous training of two mod - els\u2014a discriminator and a generator. The discriminator aims to distinguish between real and generated images, while the generator endeavors to deceive the discriminator by produc",
    "full_text_length": 151955,
    "chunk_length": 1384
  },
  {
    "chunk_id": 871,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 23,
    "total_chunks": 150,
    "text_content": "methods for data generation and augmentation. How - ever, as the dimensionality of the data increases, the quality of the generated synthetic data may also decrease (Goodfellow et al. 2020 ). \u2013Graph Neural Networks. Graphs are fundamental data structures representing sets of interconnected objects (nodes) and their relationships (edges). In the realm of ML, the analysis of graphs has gained substantial attention due to their exceptional expressive power, which has made them versatile representat",
    "full_text_length": 151955,
    "chunk_length": 1375
  },
  {
    "chunk_id": 872,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 24,
    "total_chunks": 150,
    "text_content": "span from modeling physical systems to predicting molecular finger - prints and classifying diseases, demonstrating the adaptability of GNNs. 2.3 Related work Several review papers have been published on the application of ML and DL in BC research (Yassin et al. 2018 ; Abhisheka et al. 2023 ; Thakur et al. 2024 ). These reviews provide valu - able insights into the advancements and trends in the field. However, they lack a com - prehensive analysis of recent developments of multimodal DL fusion,",
    "full_text_length": 151955,
    "chunk_length": 1239
  },
  {
    "chunk_id": 873,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 25,
    "total_chunks": 150,
    "text_content": "address this gap by specifically examining the fusion of multiple modalities in the context of BC, providing a more comprehensive understanding of the advancements and challenges in this specific domain. Table 1 summarizes the contributions and limitations of these existing review papers, underscoring the necessity for this SLR to provide a more holistic and updated overview. 3 Research methodology The present mapping and review process follows the guidelines set forth by Kitchenham and Charters",
    "full_text_length": 151955,
    "chunk_length": 1282
  },
  {
    "chunk_id": 874,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 26,
    "total_chunks": 150,
    "text_content": "chosen for the current review. Subsequently, we evaluated the cal - iber of the chosen papers by employing a questionnaire and scoring mechanism. Ultimately, information was extracted from these papers and combined to generate answers to the map - ping and review inquiries. Each of these steps is further explained in the following sections. 3.1 Mapping and review questions The mapping questions (MQs) aim to identify, describe, and classify the primary studies published in the field of multimodal",
    "full_text_length": 151955,
    "chunk_length": 1218
  },
  {
    "chunk_id": 875,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 27,
    "total_chunks": 150,
    "text_content": "To respond to the MQs and RQs, we implemented a four-stage search procedure. (1) We developed a search query from the earliest available studies up to December 2023; (2) This query was used in an automated search across six chosen digital libraries to find primary papers; (3) As part of a second search, the bibliographies of relevant papers (those meeting certain inclusion and exclusion criteria) were examined to ensure that all relevant literature was included; and (4) Finally, the same query w",
    "full_text_length": 151955,
    "chunk_length": 1240
  },
  {
    "chunk_id": 876,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 28,
    "total_chunks": 150,
    "text_content": "(\u201cmachine learning\u201d OR \u201cdeep Learning\u201d OR \u201cartificial intelligence\u201d OR prediction 1 3Page 9 of 53 327 F.-Z. Nakach et al.Table 1 Summary of review papers published on multimodal DL for BC classification Paper Scope Contributions Limitations (Luo et al. 2024 )The paper provided an extensive review of DL-based BC imaging research over the past decade.-Introduction of widely applied DL para - digms, including supervised learning, semi-supervised learning, weakly-super - vised learning, unsupervised",
    "full_text_length": 151955,
    "chunk_length": 1464
  },
  {
    "chunk_id": 877,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 29,
    "total_chunks": 150,
    "text_content": "dimensionality reduction, variations in survival prediction windows, handling of minority classes, and the utility of multi- modal approaches over single modalities like genomics.-Not a systematic review. -The review primarily focused on prognosis, particularly molecular subtype and survival prognosis predictions, which may limit the scope of its applicability to other aspects of BC research. (Abdul - lakutty et al. 2024 )The review explored the integration of histo - pathology images with non-i",
    "full_text_length": 151955,
    "chunk_length": 1400
  },
  {
    "chunk_id": 878,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 30,
    "total_chunks": 150,
    "text_content": "studies and contribut - ing to the strategic development of the field.-Not a systematic review. -The review focused solely on whole slide imaging (WSI) histopa - thology as the imaging modality fused with clinical or genomic data. (Thakur et al. 2024 )The authors analyzed various ML and DL techniques in the con - text of BC. The study explored the classifica - tion of BC using differ - ent image modalities, alongside discussions on diagnosis methodol - ogies utilizing publicly and privately avai",
    "full_text_length": 151955,
    "chunk_length": 1331
  },
  {
    "chunk_id": 879,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 31,
    "total_chunks": 150,
    "text_content": "investigation of multimodal deep learning fusion\u2026Table 2 The mapping/review questions and their motivations Mapping/Review Questions Motivations MQ1 : What were the publication years, publication channels, and sources of the selected papers in the field of multimodal ML for BC?To ascertain the presence of a particular publica - tion channel and determine the frequency of stud - ies on multimodal fusion for BC over the years. MQ2 : What are the types of papers that use multi - modal ML for BC cla",
    "full_text_length": 151955,
    "chunk_length": 1316
  },
  {
    "chunk_id": 880,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 32,
    "total_chunks": 150,
    "text_content": "of multimodal ML for BC?To discover the most investigated classification target in the literature of multimodal fusion for BC. RQ1 : What were the attributes and structures of the multimodal datasets employed for assessing and validating the explored multimodal fusion techniques in BC research?-To enumerate the different multimodal datasets used for BC, and identify the most commonly utilized modalities and combinations. RQ2 : What were the key multimodal fusion approach - es for integrating div",
    "full_text_length": 151955,
    "chunk_length": 1318
  },
  {
    "chunk_id": 881,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 33,
    "total_chunks": 150,
    "text_content": "- sure the performance for the classification of BC using multimodal fusion?To define the measures/metrics and validation methods used to evaluate the multimodal fusion techniques. RQ5 : Did the reviewed studies propose any interpret - ability methods to enhance understanding of multi - modal ML models in the context of BC?To discover if interpretability was considered when fusing different medical modalities for the classification of BC. RQ6 : What is the overall performance of the multi - moda",
    "full_text_length": 151955,
    "chunk_length": 1303
  },
  {
    "chunk_id": 882,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 34,
    "total_chunks": 150,
    "text_content": "context of BC, aiming for a broad and comprehensive selection of candidate papers. This is why the search string incorporates terms such as \u201cmodalities\u201d and \u201cfusion.\u201d In the selection process, we applied inclusion and exclusion criteria to filter studies, specifically focusing on those where diverse modalities were integrated to formu - late the ultimate ML prediction rather than being individually evaluated within the same study. We conducted an automated search using the specified search strin",
    "full_text_length": 151955,
    "chunk_length": 1320
  },
  {
    "chunk_id": 883,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 35,
    "total_chunks": 150,
    "text_content": "yet been recognized among the candidate studies obtained from digital libraries. 3.3 Study selection procedure In this phase, relevant studies that addressed the RQs were selected from a group of poten - tial studies based on their titles, abstracts, and keywords. To achieve this goal, every can - didate study identified in the initial search was subjected to evaluation by the authors. The authors employed inclusion and exclusion criteria to ascertain whether a study should be retained or discar",
    "full_text_length": 151955,
    "chunk_length": 1337
  },
  {
    "chunk_id": 884,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 36,
    "total_chunks": 150,
    "text_content": "paper provides a summary of current multimodal fusion techniques employed for BC classification. 2. Papers presenting an improvement of existing multimodal fusion techniques for BC classification. 3. Papers introducing novel multimodal fusion techniques applied to the classification of BC. 4. Papers assessing or contrasting existing multimodal fusion techniques applied to BC classification. Exclusion criteria (ECs): 1. Papers dealing with other types of cancer (not focusing on BC). 2. Different ",
    "full_text_length": 151955,
    "chunk_length": 1332
  },
  {
    "chunk_id": 885,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 37,
    "total_chunks": 150,
    "text_content": "Study quality assessment To enhance the selection criteria and ensure the pertinence of the papers, we devised a ques - tionnaire to evaluate the quality of the 53 relevant papers. The quality assessment process was applied to each paper, following the checklist specified in Table 3. The questionnaire consisted of four questions designed to evaluate the pertinent papers. The evaluation of QA1 is contingent upon the pertinence of the empirical findings presented in the paper to address the resear",
    "full_text_length": 151955,
    "chunk_length": 1329
  },
  {
    "chunk_id": 886,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 38,
    "total_chunks": 150,
    "text_content": "using Journal Citation Reports (JCR 2022). The authors autonomously performed the quality assurance process, and any inconsistencies were resolved through a collaborative meeting to arrive at a definitive con - clusion. The quality score for each pertinent study was calculated by summing the scores of the quality assurance questions. A study was selected if its quality score exceeded 3.5, which is 50% of the ideal quality score of 7. This criterion was used to ensure the reliability and strength",
    "full_text_length": 151955,
    "chunk_length": 1173
  },
  {
    "chunk_id": 887,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 39,
    "total_chunks": 150,
    "text_content": "and recognized source?For conference: (1.5) Rank CORE A or A* (1) Rank Core B (0.5) Rank Core C (0) if not Core ranking For journals: (2) Rank JCR 2023 Q1 (1.5) Rank JCR 2023 Q2 (1) Rank JCR 2023 Q3 or Q4 (0) if not in JCR 2023 rankingTable 3 Quality assessment checklist 1 3Page 13 of 53 327 F.-Z. Nakach et al.3.5 Data extraction strategy and synthesis To handle the mapping and review inquiries, a data extraction form was created and com - pleted for each of the chosen papers. The data obtained ",
    "full_text_length": 151955,
    "chunk_length": 1219
  },
  {
    "chunk_id": 888,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 40,
    "total_chunks": 150,
    "text_content": "extraction of the data, a synthesis was conducted, and the data were organized in a way that corresponded to the research questions to gather evidence for their resolution. Three methodologies were utilized for data synthesis: (1) vote counting, which entailed calculat - ing the frequency of different outcomes across the chosen studies; (2) narrative synthesis, which involved creating a descriptive summary of the findings from the selected papers; and (3) visualization tools such as bar charts, ",
    "full_text_length": 151955,
    "chunk_length": 1373
  },
  {
    "chunk_id": 889,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 41,
    "total_chunks": 150,
    "text_content": "- HBE: Incorporating historical existing data in the evaluation. - Case study: An empirical assessment based on real-world datasets from hospitals/clinics. MQ3 Medical tasks encompass various stages to ensure the well-being of patients: -Screening: Involves examining individuals who outwardly show no symptoms of an un - derlying illness, yet may harbor a concealed condition, presenting as overall good health. -Diagnosis: Focuses on the identification of a disease by analyzing its observable sign",
    "full_text_length": 151955,
    "chunk_length": 1430
  },
  {
    "chunk_id": 890,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 42,
    "total_chunks": 150,
    "text_content": "-Management: Involves activities related to health promotion and the provision of neces - sary medical services to support overall well-being. MQ4 The targets to predict with classification. RQ1 Datasets used, modalities employed and the combinations that were exploited. RQ2 The multimodal fusion approaches that were predominantly employed for BC classification. RQ3 Aggregation and concatenation methods, with the ML and DL models used. RQ4 The performance measures used (accuracy, sensitivity, sp",
    "full_text_length": 151955,
    "chunk_length": 1388
  },
  {
    "chunk_id": 891,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 43,
    "total_chunks": 150,
    "text_content": "we specifically focused on fusion techniques applied for classification. However, we acknowledge the potential appli - cability of fusion in other compelling medical tasks, such as regression, segmentation and registration. Differences in the goals, input methods, and performance metrics were reported in the studies that were included, but some did not provide confidence limits. This makes it difficult to combine or statistically compare performance gains through meta-analysis. Fur - thermore, t",
    "full_text_length": 151955,
    "chunk_length": 1426
  },
  {
    "chunk_id": 892,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 44,
    "total_chunks": 150,
    "text_content": "to validity are outlined below: \u2013A comprehensive search string was formulated with appropriate keywords, and multiple iterations were conducted to cover a maximum of primary studies from the chosen digi - tal libraries. Duplicate papers were removed. \u2013To prevent the exclusion of relevant papers, the authors rigorously applied selection cri - teria to titles, abstracts, and keywords. In cases of uncertainty, a thorough analysis of the full article was undertaken, with disagreements resolved throu",
    "full_text_length": 151955,
    "chunk_length": 1320
  },
  {
    "chunk_id": 893,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 45,
    "total_chunks": 150,
    "text_content": "are summa - rized in this section, along with a discussion of the findings. Figure 3 displays the flowchart of articles acquired during each phase of the selection procedure. The exploration of six electronic databases yielded a total of 838 potential papers from both searches. The inclu - sion and exclusion criteria were meticulously applied to identify pertinent studies, resulting in the selection of 53 articles. The selection process involved a thorough evaluation of titles, abstracts, and ke",
    "full_text_length": 151955,
    "chunk_length": 1301
  },
  {
    "chunk_id": 894,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 46,
    "total_chunks": 150,
    "text_content": "53 327 F.-Z. Nakach et al.extraction, answers to MQs, and overall review, interested readers can request additional information from the corresponding author via email. 4.1 Publications\u2019 trends By examining different sources and using mapping and review questions, it seems that jour - nals have published more in-depth studies on multimodal data fusion for BC classification than did conferences. There was a total of 38 journal articles and 9 conference articles on this topic. Figure 4 illustrates",
    "full_text_length": 151955,
    "chunk_length": 1316
  },
  {
    "chunk_id": 895,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 47,
    "total_chunks": 150,
    "text_content": "to 2018. A substantial increase in publication rate was evident Fig. 3 PRISMA (preferred reporting items for systematic reviews and meta-analyses) flow diagram of the literature selection scheme 1 3327 Page 16 of 53 A comprehensive investigation of multimodal deep learning fusion\u2026from 2018 onward, with 2021 emerging as a standout year, encompassing 27.7% of the total selected studies. The large increase in publications in 2021 occurred because people sud - denly realized how important it is to c",
    "full_text_length": 151955,
    "chunk_length": 1234
  },
  {
    "chunk_id": 896,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 48,
    "total_chunks": 150,
    "text_content": "published per year and publication channel Fig. 4 Distribution of the digital libraries of the selected papers 1 3Page 17 of 53 327 F.-Z. Nakach et al.ity of the selected papers were indeed published in 2023, indicating that this acknowledg - ment was not an abrupt occurrence; rather, it was facilitated by the accessibility of ample computational power and extensive datasets. These enabling factors paved the way for the development of potent yet intricate models, such as DNNs. As illustrated in ",
    "full_text_length": 151955,
    "chunk_length": 1347
  },
  {
    "chunk_id": 897,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 49,
    "total_chunks": 150,
    "text_content": "Imaging and the International Conference on Medical Image Computing and Computer-Assisted Intervention were the most common. 4.2 Contribution type The examination identified two distinct types of empirical studies: human-based evalua - tion (HBE) and case studies. Figure 6 illustrates that 55.3% and 38.3% of the articles were categorized as HBE and case studies, respectively. Notably, three papers employed both HBE and case study methodologies, resulting in 26 papers exclusively utilizing HBE an",
    "full_text_length": 151955,
    "chunk_length": 1337
  },
  {
    "chunk_id": 898,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 50,
    "total_chunks": 150,
    "text_content": "(33 papers), and diag - nosis received 25.5% (12 papers). Screening and treatment were the least explored, each comprising 4.3% (2 papers), while none of the selected papers investigated the monitoring or management task. Publication source #Of papers Conference International Workshop on Breast Imaging (IWBI) 2 International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI)2 Journal Scientific reports 3 IEEE/ACM Transactions on Computational Biology and Bioinforma",
    "full_text_length": 151955,
    "chunk_length": 1368
  },
  {
    "chunk_id": 899,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 51,
    "total_chunks": 150,
    "text_content": "research. Within the realm of classification, various objectives were explored, as shown in Fig. 8, with the long-term and short-term survival of patients emerging as the predomi - nant ones, accounting for 44.2% of the selected papers. Similarly, binary classification dis - tinguishing between malignant and benign tumors was the second most common (21.2%), while multiclassification of BC molecular subtypes was the third most common. The recur - rence of BC was another area of investigation, rep",
    "full_text_length": 151955,
    "chunk_length": 1341
  },
  {
    "chunk_id": 900,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 52,
    "total_chunks": 150,
    "text_content": "a predominant trend emerged, with the majority being private (20 papers); only four public datasets were identified, namely, the META - BRIC, TCGA-BRCA, the PathoEMR and BCNB cohorts, as detailed in Table 6. Seven studies evaluated their models with two datasets (TCGA-BRCA and METABRIC). Distinguishing between three distinct types of modalities is a key aspect of this SLR, as it focuses on their application in BC research. The first modality, clinical data, encompasses various forms of tabular d",
    "full_text_length": 151955,
    "chunk_length": 1404
  },
  {
    "chunk_id": 901,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 53,
    "total_chunks": 150,
    "text_content": "genetic factors influenc - ing BC. The third modality, medical images, encompasses visual representations obtained through techniques such as mammography, magnetic resonance imaging, and computed tomography. These images play a crucial role in diagnosing and monitoring BC, provid - ing detailed insights into anatomical structures and aiding in treatment decision-making. Figure 9 illustrates the distribution of studies for each modality combination. Notably, combinations involving clinical and ge",
    "full_text_length": 151955,
    "chunk_length": 1430
  },
  {
    "chunk_id": 902,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 54,
    "total_chunks": 150,
    "text_content": "imaging (MRI) and ultrasound (US). Figure 10 depicts the distribution of various modalities within the chosen studies. Clinical data emerged as the most predominant, constituting 28.6%. Similarly, the gene expression was 20.5%. histopathology and CNV/CNA shared the third position at 14.3%. US followed up at 8.9%, followed by mammography at 7.1%, and finally MRI at 6.3%. In Table 7, a comprehensive breakdown of the combinations explored by the studies is presented, highlighting the intended class",
    "full_text_length": 151955,
    "chunk_length": 1317
  },
  {
    "chunk_id": 903,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 55,
    "total_chunks": 150,
    "text_content": "the combination of mammography with US were observed. Table 6 Public datasets used across the selected studies Dataset N# of PapersOverview Modalities TCGA- BRCA (The 2023 )19 Subset of The Cancer Genome Atlas (TCGA) project focus - ing on BC. TCGA was developed between the NCI and National Human Genome project starting in 2006. The data for BC is standardized to 1031 patients.Clinical Data, Gene Expres - sion, CNV , Histopatholog - ical Imaging METABRIC (cBioPortal 2023 )13 The METABRIC (Molecu",
    "full_text_length": 151955,
    "chunk_length": 1319
  },
  {
    "chunk_id": 904,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 56,
    "total_chunks": 150,
    "text_content": "(Xu et al. 2021 )1 the Institutional Ethical Committees of Beijing Chaoyang Hospital affiliated to Capital Medical University released the Early BC Core-Needle Biopsy WSI dataset for micro- metastatic ALN preoperatively of 1058 patients.Clinical Data, H&E Stained 1 3Page 21 of 53 327 F.-Z. Nakach et al. Fig. 10 Distribution of the different modalities across the selected studies Fig. 9 Distribution of the combinations of the different modalities 1 3327 Page 22 of 53 A comprehensive investigation",
    "full_text_length": 151955,
    "chunk_length": 882
  },
  {
    "chunk_id": 905,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 57,
    "total_chunks": 150,
    "text_content": "X X X O 2 C9 X X X X O 1 C10 X X X O 2 C11 X X O 3 C12 X X O 1 C13 X X 1 C14 X X O O 2 C15 X X O 4 C16 X X O 1 C17 X X O 1 C18 X X O 1 C19 X X O 1 C20 X X O 1 Clinical Data (CD) , Gene expression (GE) , Copy Number Variation (CN) , histopathology (HI) , Mammography (MG) , magnetic resonance imaging (MRI) , Ultrasound (US) , digital breast tomosynthesis (DBT) , Imaging Mass Cytometry (IMC) , and diffuse Optical Tomography (DOT) and classification targets: benign or malignant tumor (B/M), multiple",
    "full_text_length": 151955,
    "chunk_length": 1018
  },
  {
    "chunk_id": 906,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 58,
    "total_chunks": 150,
    "text_content": "preva - lent combination was clinical data with gene expression or CNV/CNA, appearing in 12 studies. The following closely followed: histopathology and clinical data (6 studies); MG and US (4 studies); and combinations of histopathology and gene expression, as well as MRI and clinical data, each featured in 3 studies. All the other combinations were repre - sented in one or two studies. Specifically, in one study, the clinical data was presented in the form of BIRADS scoring data. Furthermore, a",
    "full_text_length": 151955,
    "chunk_length": 1328
  },
  {
    "chunk_id": 907,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 59,
    "total_chunks": 150,
    "text_content": "Notably, for the predicted targets, the binary classification of benign and malignant tumors (sometimes with normal types) exhibited the highest diversity, followed by the mul - ticlassification of molecular subtypes with four combinations, the long-term and short-term survival of patients with five combinations, and the recurrence of BC with only three com - binations. The references of the papers that used combinations of modalities can be found in Tables 8 and 9. 6 Multimodal fusion In the co",
    "full_text_length": 151955,
    "chunk_length": 1366
  },
  {
    "chunk_id": 908,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 60,
    "total_chunks": 150,
    "text_content": "decision-fusion implementations. How - ever, it is crucial to acknowledge that only two papers directly compared these approaches (decision-fusion and feature-fusion), limiting the robustness of this comparison. Interest - ingly, as of 2023, no studies have reported using the decision-fusion approach. This absence in recent literature may suggest a growing recognition among researchers regarding the effi - cacy of feature fusion over decision fusion. The indication here is that the field is evol",
    "full_text_length": 151955,
    "chunk_length": 1256
  },
  {
    "chunk_id": 909,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 61,
    "total_chunks": 150,
    "text_content": "of another modality, such as histopathological images with clini - cal data in Yang et al. ( 2022 ), Yan et al. ( 2019 ), Wang et al. ( 2023 ), and histopathological images with US images (Qu et al. 2023 ). The remaining papers focused on fusing extracted features from both modalities. Moreover, one paper used hybrid fusion and combined both decision fusion and feature fusion approaches. 1 3327 Page 24 of 53 A comprehensive investigation of multimodal deep learning fusion\u20266.1 Decision fusion Dec",
    "full_text_length": 151955,
    "chunk_length": 1264
  },
  {
    "chunk_id": 910,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 62,
    "total_chunks": 150,
    "text_content": "implemented distinct aggregation strategies such as averaging, weighted voting, and employing a meta-classifier. Liu ( 2021 ) integrates predictions from a DNN model focused on gene expression, along with CNV , and a CNN based on the VGG16 architecture for histopathological images. The fusion process involves meticulous adjustment of the fusion parameters and employs a weighted linear aggregation method to ascertain the weights assigned to the probability of each modality (Sun et al. 2019 ). int",
    "full_text_length": 151955,
    "chunk_length": 1284
  },
  {
    "chunk_id": 911,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 63,
    "total_chunks": 150,
    "text_content": "logistic regression, and support vector machine (SVM), with random forest emerging as the most effective in this context (Mullen et al. 2023 ). extracted image texture features from both MR and DBT images and inputted them into an SVM, generating likelihoods or cancer-likeness scores. These scores from each modality were then fed into a joint Bayesian classifier for data integration. Subsequently, the joint Bayesian classifier was employed to calculate the probability of malignancy. The authors ",
    "full_text_length": 151955,
    "chunk_length": 1258
  },
  {
    "chunk_id": 912,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 64,
    "total_chunks": 150,
    "text_content": "final prediction. Instead of relying on a single classifier for each modality to obtain the final class, the authors of Rabinovici-Cohen et al. (2022 ) relied on the ensemble learning method to generate the final decision by calculating the mean value of the six classifiers. They implemented three classifiers for each modality: three CNNs with different hyperparameters for MRI and logistic regression, random forest, and XGBoost for clinical data. 6.2 Feature fusion Table 8 provides a detailed ov",
    "full_text_length": 151955,
    "chunk_length": 1392
  },
  {
    "chunk_id": 913,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 65,
    "total_chunks": 150,
    "text_content": "modalities, CNNs remained prevalent (9 papers), followed by DNNs (7 papers) and autoencoders (5 papers). Various other methods, including decision trees, logistic regression, random forest, and XGBoost, were employed sporadically. Nota - bly, 12 papers integrated an attention mechanism into the feature extraction process, select - ing the best features for each modality, and multi-instance learning (Li 2021 ) was explored in 3 papers (Xu et al. 2021 ; Wang et al. 2023 ; Li et al. 2021 ) mainly t",
    "full_text_length": 151955,
    "chunk_length": 1273
  },
  {
    "chunk_id": 914,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 66,
    "total_chunks": 150,
    "text_content": "fuzzy classifier, XGBoost, decision tree, CNNs, extreme learning machine, extra random trees, and LSTM) were employed sporadically, with some papers comparing multiple classifiers to assess performance. The Fig. 12 Distribution of the different ML and DL models across the selected studies 1 3327 Page 26 of 53 A comprehensive investigation of multimodal deep learning fusion\u2026distribution of models across selected papers reveals prevalent usage, with certain models emerging as dominant choices. Thi",
    "full_text_length": 151955,
    "chunk_length": 1335
  },
  {
    "chunk_id": 915,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 67,
    "total_chunks": 150,
    "text_content": "exhibit a diverse array of tech - niques for feature fusion. The predominant method across studies involves concatenation, a widely adopted strategy. However, a subset of papers, including Holste et al. ( 2021) , Wang et al. ( 2023 ), and Mokni et al. ( 2021 ), explore alternative operations to integrate information from multiple modalities. Notably, variants of the learned feature fusion model implement elementwise addition and multiplication, demonstrating comparable performance to that of the",
    "full_text_length": 151955,
    "chunk_length": 1322
  },
  {
    "chunk_id": 916,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 68,
    "total_chunks": 150,
    "text_content": "2020 ), adopts a similarity network fusion algorithm, further diversifying the feature fusion methodologies explored across the reviewed studies. 6.3 Hybrid fusion In the domain of multimodal fusion, an impactful hybrid strategy is evident, as illustrated by the research detailed in Wu et al. ( 2023 ), where the authors introduce the innovative mul - timodal DL radiomic nomogram (DLRN) crafted for predicting metastatic BC. The DLRN seamlessly integrates dual-modality US images, radiomic features",
    "full_text_length": 151955,
    "chunk_length": 1344
  },
  {
    "chunk_id": 917,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 69,
    "total_chunks": 150,
    "text_content": "from clinical features, the DL score, and the radiomics score. This pioneering hybrid fusion approach represents a promis - ing avenue for advancing the ability of multimodal models to predict the prognosis of BC patients. 6.4 DL models In examining recent trends in multimodal fusion for BC classification, our analysis of selected studies revealed a significant shift toward the utilization of DL models, with 70% of the studies relying on DL techniques, as shown in Fig. 13. This dominance undersc",
    "full_text_length": 151955,
    "chunk_length": 1261
  },
  {
    "chunk_id": 918,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 70,
    "total_chunks": 150,
    "text_content": "datasets across various domains (Akkus et al. 2023 ). It is crucial to recognize that the choice between DL and ML models hinges on factors such as the nature of the specific problem, dataset characteristics, and interpretability importance (Hakkoum et al. 2022 ). While traditional ML models remain relevant in scenarios with smaller datasets or where interpretability is paramount, the dominance of DL models per - sists in tasks demanding complex data analysis. The findings from our pie chart not",
    "full_text_length": 151955,
    "chunk_length": 1338
  },
  {
    "chunk_id": 919,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 71,
    "total_chunks": 150,
    "text_content": "of FCL in multimodal feature classification underscores its effectiveness in capturing intricate relationships within diverse datasets. \u2013Notably, 70% of the papers opt for pretrained CNN models, such as ResNet50 (Szegedy 2017 ), trained on ImageNet. The straightforward fine-tuning procedure through transfer learning, which facilitates the ease of adaptation to medical imaging, favors this option (Hemant Kumar et al. 2021 ). While pretraining provides an initial convergence direc - tion by levera",
    "full_text_length": 151955,
    "chunk_length": 1246
  },
  {
    "chunk_id": 920,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 72,
    "total_chunks": 150,
    "text_content": "et al. ( 2023 ), LSM and GRU were employed to classify stacked features, while Li ( 2020 )utilized an LSTM network to derive a fixed-length image feature from TR and ROI features. Additionally, in Atrey et al. ( 2023 ), the LSTM, which is equipped with the capability to handle variable-length inputs and capture long-term dependencies through the incorporation of a forget gate, proved well suited for the classification task. Consequently, LSTM was implemented Fig. 13 Distribution of DL models for",
    "full_text_length": 151955,
    "chunk_length": 1252
  },
  {
    "chunk_id": 921,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 73,
    "total_chunks": 150,
    "text_content": "of 1D-CNN in processing CNA and gene expression data at the feature extraction layer. \u2013One notable application of autoencoders is seen in Yan et al. ( 2021 ), where denoising autoencoders (DAEs) are utilized to increase the dimensionality of the clinical data. While basic autoencoders are commonly used for dimensionality reduction, DAE, by training on noisy versions of raw data, enhances robust feature representation and prevents overfitting. Another instance is observed in Li ( 2020 ), which em",
    "full_text_length": 151955,
    "chunk_length": 1316
  },
  {
    "chunk_id": 922,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 74,
    "total_chunks": 150,
    "text_content": "space, and are shown to extract features of very low dimensions. Finally, the authors of Furtney et al. ( 2023 ) leverages an autoencoder to represent genomic variant results or microarray expression features in a condensed latent space, with the low-dimensional vectors extracted from the latent space providing inputs into the GNN. \u2013 The paper Du et al. ( 2023 ) is the sole one among the selected studies that employed a GAN to create a multimodal adversarial representation framework for predicti",
    "full_text_length": 151955,
    "chunk_length": 1392
  },
  {
    "chunk_id": 923,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 75,
    "total_chunks": 150,
    "text_content": "the alignment of representation distributions across modalities, fostering effective modality invariance. The optimized representation subspace, achieved through the convergence of this process, ensures cross-modal associations. Additionally, a clas - sifier learns to categorize encoded representations into correct labels, preserving the underlying cross-modal semantic structure for predictive tasks. Furthermore, individual decoders for each modality prevent the loss of unimodal information. \u2013No",
    "full_text_length": 151955,
    "chunk_length": 1450
  },
  {
    "chunk_id": 924,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 76,
    "total_chunks": 150,
    "text_content": "the effectiveness and versatility of GNNs, offering solutions across a spectrum of domains, from cancer prognosis to multimodal graph- based reasoning in medical data. 1 3Page 29 of 53 327 F.-Z. Nakach et al.7 Multimodal fusion architectures After scrutinizing the 47 selected papers, several distinct patterns in learned feature fusion emerged, deviating from the traditional feature fusion approach outlined in Sect. 2. Four noteworthy patterns are identified: \u2013Feature selection based on the atten",
    "full_text_length": 151955,
    "chunk_length": 1416
  },
  {
    "chunk_id": 925,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 77,
    "total_chunks": 150,
    "text_content": "ignorance of data heterogeneity (Stahlschmidt et al. 2022 ). To address this, a recent study applied shallow attention nets to each feature, effectively extracting key information from multimodal data while accounting for the distinction and uniformity of heterogeneous data. Additionally, various papers have leveraged attention mechanisms in innovative ways. For example Guo et al. ( 2021 ), employs a shallow attention net for each feature, accommodating the need for multimodal data by effectivel",
    "full_text_length": 151955,
    "chunk_length": 1297
  },
  {
    "chunk_id": 926,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 78,
    "total_chunks": 150,
    "text_content": "combining image features with clinical and gene expression data to predict the risk of recurrence and metastasis. Finally, a spatial attention convolution network automati - cally was developed in Luo et al. ( 2023 ) to focus on key regions of images, while a fully connected neural network performed feature correlation coding and extraction, combin - ing the scoring data with image features in high-dimensional space. These examples underscore the versatility and widespread applicability of atten",
    "full_text_length": 151955,
    "chunk_length": 1361
  },
  {
    "chunk_id": 927,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 79,
    "total_chunks": 150,
    "text_content": "and disease introduces redundancy, which can be addressed only through feature selection on fused multimodal features. Papers such as Luo et al. ( 2023 ) exem - plify this approach by addressing key challenges in classification tasks involving US images and BIRADS features. This research addresses the challenge of redundant infor - mation that arises when directly concatenating features extracted from US images and 1 3327 Page 30 of 53 A comprehensive investigation of multimodal deep learning fu",
    "full_text_length": 151955,
    "chunk_length": 1358
  },
  {
    "chunk_id": 928,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 80,
    "total_chunks": 150,
    "text_content": "classifier on a stacked multimodal feature vector for each patient, allowing the model to continually update its learned feature space on a nonstationary multimodal data stream. This unique approach demonstrates the model\u2019s ability to learn complex relationships between different multimodal attributes. The authors of Wang et al. ( 2021 ) proposed a genomic and pathological deep bilinear network (GPDBN) to establish a unified frame - work for BC prognosis prediction by integrating both genomic da",
    "full_text_length": 151955,
    "chunk_length": 1479
  },
  {
    "chunk_id": 929,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 81,
    "total_chunks": 150,
    "text_content": "improved predictive capabilities in diverse applications. Additionally, two key attention mechanisms, namely, intramodal - ity attention and inter-modality attention, were suggested by Zhang et al. ( 2023 ) for enhancing the overall feature representation. Intramodality attention focuses on refining features within the same modality. Concurrently, inter-modality attention extends its influence beyond a single modality. This attention module is designed to interactively fuse information across di",
    "full_text_length": 151955,
    "chunk_length": 1476
  },
  {
    "chunk_id": 930,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 82,
    "total_chunks": 150,
    "text_content": "extraction of features from a single modality is influenced by interactions with another modality, such as through loss (before actual fusion occurs). First, to address the assumption of equal contributions from all modalities, the authors of Arya et al. ( 2021 ) designed a cross-modality attention-based architecture called the SiGaAtCNN Bi-Attention for three modalities. The model employs a bimodal- based attention mechanism, with cross-modal attention performed pairwise, prioritiz - ing distin",
    "full_text_length": 151955,
    "chunk_length": 1348
  },
  {
    "chunk_id": 931,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 83,
    "total_chunks": 150,
    "text_content": "31 of 53 327 F.-Z. Nakach et al.using an attention mechanism. The gene expression profile was decomposed using five algorithms, and the attention mechanism calculates the weight of each representation based on clinical data. The weighted summation of these representations is concatenated with clinical data to create the final feature representation, which is subsequently input into the DNN for the classification task. This approach, by considering the intricate relationships between modalities, ",
    "full_text_length": 151955,
    "chunk_length": 1394
  },
  {
    "chunk_id": 932,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 84,
    "total_chunks": 150,
    "text_content": "classifier. This approach includes what can be termed intermediate fusion, where multiple fusion steps are performed before multimodal representations are ultimately fused. Two studies have employed this meth - odology. In the study by Qiao et al. ( 2022 ), the authors present a discriminative feature learning method utilizing a min\u2013max feature loss training strategy for BC classification. This method ensures the separation of modality-agnostic and modality-specific features, focusing on tumor r",
    "full_text_length": 151955,
    "chunk_length": 1442
  },
  {
    "chunk_id": 933,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 85,
    "total_chunks": 150,
    "text_content": "By utilizing a stack-based shal - low self-attention network, the model amplifies survival-related features within lesion regions. The affinity fusion module enhances structured information mapping between patients and multimodal data, leading to a robust fusion feature representation. The resulting fusion feature embedding, combined with a specific feature embedding from a triple modal network, is employed for accurate classification of long-term or short-term survival. This method not only con",
    "full_text_length": 151955,
    "chunk_length": 1465
  },
  {
    "chunk_id": 934,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 86,
    "total_chunks": 150,
    "text_content": "by delving into its various approaches, each characterized by distinct strengths and weaknesses. 1 3327 Page 32 of 53 A comprehensive investigation of multimodal deep learning fusion\u20268.1 Decision fusion Decision fusion has been recognized for its various advantages: it minimizes the risk of suboptimal predictions that might result from a model trained on a single modality, lever - aging the fact that errors from different modalities are typically uncorrelated (Baltru\u0161aitis et al. 2019 ). This ap",
    "full_text_length": 151955,
    "chunk_length": 1362
  },
  {
    "chunk_id": 935,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 87,
    "total_chunks": 150,
    "text_content": "mitigates the \u201ccurse of dimensionality\u201d by using specialized models for each modality, limiting the input feature vector size for each model (Aremu et al. 2020 ). In scenarios with missing or incomplete data, decision fusion retains the ability to make pre - dictions. It employs separate models for each modality, allowing for aggregation functions even when predictions from a modality are missing. Decision fusion is advantageous when dealing with modalities having different numbers of features, ",
    "full_text_length": 151955,
    "chunk_length": 1319
  },
  {
    "chunk_id": 936,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 88,
    "total_chunks": 150,
    "text_content": "applications, feature fusion is the initial choice for multimodal learning since it can learn shared representations, facilitating the model\u2019s ability to understand correla - tions across modalities and ultimately leading to improved overall performance (Singh et al. 2022 ). It is a straightforward approach that does not require training multiple models when semantic features are used; in that case, it was found that the combined modalities have the same dimension (different types of medical ima",
    "full_text_length": 151955,
    "chunk_length": 1310
  },
  {
    "chunk_id": 937,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 89,
    "total_chunks": 150,
    "text_content": "preference is attributed to the quality of their features, these networks encounter a signifi - cant decline in performance when confronted with shifts in dataset features, especially with smaller datasets. The authors of Muramatsu et al. ( 2022 ) fused two imaging modalities using semantic features, and the learned features revealed that the fusion of learned features outperformed the fusion of raw images since the extracted features represent high-level abstractions learned by the DL models, h",
    "full_text_length": 151955,
    "chunk_length": 1307
  },
  {
    "chunk_id": 938,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 90,
    "total_chunks": 150,
    "text_content": "features of different modalities and not semantic features. 8.3 Hybrid fusion The decision to employ a hybrid fusion approach, as demonstrated in Wu et al. ( 2023 ), stems from thoughtful consideration of the characteristics and dimensions of the involved modalities\u2014dual-modality US images and radiomic features. In many cases, these imaging modalities generate high-dimensional data, and directly combining all the features could lead to challenges related to the curse of dimensionality. The curse",
    "full_text_length": 151955,
    "chunk_length": 1387
  },
  {
    "chunk_id": 939,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 91,
    "total_chunks": 150,
    "text_content": "If all the raw features from the imaging modalities were directly combined, the resulting high-dimensional representation might overshadow or dominate the clinical features, leading to suboptimal model performance. The introduction of scores allows for a condensed yet informative representation of the complex information contained in the imaging modalities. The fusion of these scores with clinical features provides a comprehensive prediction model that leverages the strengths of each modality wh",
    "full_text_length": 151955,
    "chunk_length": 1434
  },
  {
    "chunk_id": 940,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 92,
    "total_chunks": 150,
    "text_content": "distributions were utilized for these methods: 49% (24) for K-fold cross-validation, 49% (23) for Data split, and 2% (1) for the Holdout technique. Additionally, one paper utilized both K-fold cross-validation and the holdout technique simultaneously. In the case of K-fold cross-validation, various K-folds were implemented, including K = 2, 3, 4, 5, and 10. The most prevalent choices were 5-fold (8 papers) and 10-fold (13 papers), while other values were each represented in just one paper. For t",
    "full_text_length": 151955,
    "chunk_length": 1286
  },
  {
    "chunk_id": 941,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 93,
    "total_chunks": 150,
    "text_content": "to measures such as the Cohen kappa, Jaccard index, G-measure, t value and p value. While these mea - sures are straightforward, they may be less comprehensive in covering various aspects of the evaluation process. On the other hand, graphical evaluation criteria, exemplified by the 1 3327 Page 34 of 53 A comprehensive investigation of multimodal deep learning fusion\u2026receiver operating characteristic (ROC) curve, confusion matrix or Kaplan\u2013Meier survival curves, are recognized for their complexi",
    "full_text_length": 151955,
    "chunk_length": 1301
  },
  {
    "chunk_id": 942,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 94,
    "total_chunks": 150,
    "text_content": "the rationale behind its relevance. Clinicians are unlikely to embrace a system that they cannot comprehend; thus, the interpretability of the model plays a crucial role in persuading medical professionals to trust the recommenda - tions provided by the predictive system (Nunnari 2021 ). In our SLR, we found that out of 47 papers, only 13 considered interpretability, with feature importance for clinical data or gene expression using SHAP or random forest being the most common approach (6 papers)",
    "full_text_length": 151955,
    "chunk_length": 1332
  },
  {
    "chunk_id": 943,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 95,
    "total_chunks": 150,
    "text_content": "explaining the inner workings of ML/DL models. Notably, the absence of interpretability was recognized as a limitation in studies where XAI was not employed. This heightened awareness underscores the significance of ensuring interpret - ability in model predictions. Fig. 14 Distribution of the different validation metrics across the selected studies 1 3Page 35 of 53 327 F.-Z. Nakach et al.10 Overall performance Figure 15 illustrates the quantity of papers at each performance level; the x-axis re",
    "full_text_length": 151955,
    "chunk_length": 1330
  },
  {
    "chunk_id": 944,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 96,
    "total_chunks": 150,
    "text_content": "achieve high accuracy in BC classification, reflecting the effectiveness of multimodal fusion approaches. Notably, a peak is observed at an interval of approximately 90% accuracy, where a substantial number of papers demonstrate com - mendable results. Conversely, only four papers were found in the lower accuracy ranges (between 80% and 85% accuracy), and one paper had 68% accuracy, indicating a focus on achieving higher precision and efficiency in the multimodal fusion models examined in the li",
    "full_text_length": 151955,
    "chunk_length": 1399
  },
  {
    "chunk_id": 945,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 97,
    "total_chunks": 150,
    "text_content": "the integration of diverse modalities. Of the 47 papers analyzed, 38 specifically addressed the performance of models in the context of a single modality. In this examination, we sought to compare the effectiveness of the best-performing single-modality model with that of the optimal multimodal model proposed in each paper. Notably, our findings consistently demonstrated that multimodal models consistently outperformed their single-modality counterparts. Remarkably, the per - Fig. 15 Distributio",
    "full_text_length": 151955,
    "chunk_length": 1403
  },
  {
    "chunk_id": 946,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 98,
    "total_chunks": 150,
    "text_content": "potential of multimodal learning fusion to enhance the predictive capabilities of ML and DL models. By leveraging information from multiple modalities, these models can achieve superior performance compared to relying on a single modality alone, opening avenues for more robust and accurate predictions in vari - ous applications. Notably, among the papers employing multiple fusion approaches, feature fusion consistently outperformed decision fusion. 11 Challenges and recommendations for researche",
    "full_text_length": 151955,
    "chunk_length": 1353
  },
  {
    "chunk_id": 947,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 99,
    "total_chunks": 150,
    "text_content": "number of papers in our cohort (14 papers) underscores the limited scope and exploration of multimodal approaches in BC research, emphasizing the need for more comprehensive investigations into the synergistic benefits and challenges posed by the fusion of diverse modalities. This work aims to shed light on the complexities researchers encounter when integrating diverse data modalities for BC classification. By addressing these challenges, the present review aims to provide a comprehensive resou",
    "full_text_length": 151955,
    "chunk_length": 1361
  },
  {
    "chunk_id": 948,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 100,
    "total_chunks": 150,
    "text_content": "of extensive datasets that encompass various modali - ties for each patient, including genetic information, age, images, therapy, and clinical records for training the DL algorithms. The creation of such comprehensive datasets poses significant challenges given the expense associated with collecting sufficient data, especially when dealing with multiple modalities (Lahat et al. 2015 ). Notably, studies involving mammography, MRI, or US modalities predominantly utilized private datas - ets. This ",
    "full_text_length": 151955,
    "chunk_length": 1448
  },
  {
    "chunk_id": 949,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 101,
    "total_chunks": 150,
    "text_content": "and treatment procedures (Tan et al. 2022 ). Consequently, future research should focus on handling the problem of missing modalities in incomplete data. The availability of an open-source multimodal BC dataset would significantly pro - pel advancements in this area (Luo et al. 2024 ). \u2013The predominant reliance on private datasets, often treated as confidential, poses a chal - lenge when attempting to compare the effectiveness of models across different studies. To address the issue of limited d",
    "full_text_length": 151955,
    "chunk_length": 1283
  },
  {
    "chunk_id": 950,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 102,
    "total_chunks": 150,
    "text_content": "demonstrated effectiveness, albeit contingent on the dissimilarities between the source and target dataset features. Several studies have adopted data augmentation methods (Holste et al. 2021 ; Muramatsu et al. 2022 ; Atrey et al. 2023 ; Li et al. 2020 ; Zhang et al. 2023 ; Zhang et al. 2023 ; Qiao et al. 2022 ; Zhang 2024 ; Rabinovici-Cohen et al. 2022 ) to artificially expand the dataset, enhancing the prediction results. However, unlike new independent images, data augmentation contributes on",
    "full_text_length": 151955,
    "chunk_length": 1309
  },
  {
    "chunk_id": 951,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 103,
    "total_chunks": 150,
    "text_content": "learn a modality- invariant embedding space that aligns diverse modality distributions. In contrast, some approaches involve assigning a subnetwork to each modality and immediately pro - ceeding with the merger (James et al. 2014 ). Consequently, the typical modality devia - tion significantly impacts the effectiveness of the fusion. To address this issue, recent studies have concentrated on feature fusion, primarily based on learned features, using 1 3327 Page 38 of 53 A comprehensive investiga",
    "full_text_length": 151955,
    "chunk_length": 1348
  },
  {
    "chunk_id": 952,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 104,
    "total_chunks": 150,
    "text_content": "respond to various treatment options is a complex endeavor (Garc\u00eda-Aranda et al. 2019 ). Clinical validation and the need for extensive patient outcome data make developing accurate treatment prediction models challenging (El Haji et al. 2023 ). While numerous studies have made significant strides in the prediction of BC diagnosis and prognosis using multimodal methods, a notable disparity exists in regard to addressing treatment out - come prediction. This approach provides a promising perspect",
    "full_text_length": 151955,
    "chunk_length": 1331
  },
  {
    "chunk_id": 953,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 105,
    "total_chunks": 150,
    "text_content": "three modalities at once. Furthermore, the scarcity of extensive combinations when integrating more than two modalities reveals a significant gap in the literature. Out of the 18 studies that involved more than two modalities, only 11 papers used the comprehensive approach of explor - ing all possible combinations. This indicates a notable trend where a subset of studies has systematically examined the performance of models with varying combinations of multiple modalities, showcasing a thoughtfu",
    "full_text_length": 151955,
    "chunk_length": 1320
  },
  {
    "chunk_id": 954,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 106,
    "total_chunks": 150,
    "text_content": "The advantage of this approach is that it can potentially reduce the loss of information caused by using a single modality or a single classifier (Osman and Aljahdali 2020 ), resulting in a more efficient and effective prediction of BC. In our exploration of the selected papers, we found that only 15 out of 47 considered ensemble learning. RF emerged as the most popular choice and was employed in 12 papers, while XGBoost was utilized twice and extreme learning machines (ELMs) were utilized once.",
    "full_text_length": 151955,
    "chunk_length": 1351
  },
  {
    "chunk_id": 955,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 107,
    "total_chunks": 150,
    "text_content": "findings of these studies underscore the versatility of ensemble methods in enhancing classification outcomes in comparison with single classifiers. However, research employing ensemble learning for multimodal classification predominantly centers on the application of the widely used \u201crandom forest\u201d technique. This focus overlooks the exploration and evaluation of alternative ensemble methods utilizing 1 3Page 39 of 53 327 F.-Z. Nakach et al.different base learners or different ensemble methods ",
    "full_text_length": 151955,
    "chunk_length": 1368
  },
  {
    "chunk_id": 956,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 108,
    "total_chunks": 150,
    "text_content": "of why specific predictions are made. It also becomes essential for unraveling the intricate relationships that exist between different modalities within a multimodal learning frame - work. Addressing this challenge can lead to more holistic interpretability (Hakkoum et al. 2022 ), providing insights not only into the predictions themselves but also into the interplay and synergies between various modalities, contributing to a deeper under - standing of the multimodal learning process. This emer",
    "full_text_length": 151955,
    "chunk_length": 1371
  },
  {
    "chunk_id": 957,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 109,
    "total_chunks": 150,
    "text_content": "involves the conversion or mapping of infor - mation between modalities to facilitate joint understanding (Baltru\u0161aitis et al. 2019 ). An example of translation can be found in He et al. ( 2020 ), where the authors propose a BC immunohistochemical benchmark attempting to synthesize immunohistochemical technique data directly with paired hematoxylin and eosin-stained images. Moreover He et al. (2020 ), demonstrated the development of a DL algorithm that can predict local gene expression from hist",
    "full_text_length": 151955,
    "chunk_length": 1346
  },
  {
    "chunk_id": 958,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 110,
    "total_chunks": 150,
    "text_content": "al. 2020 ) aligns with the co-learning approach, indicating the relevance and applicability of such methods in the field of multimodal ML. In the future, there is a potential avenue for researchers to employ translation and co-learning approaches in multimodal learning scenarios for BC classification. The scarcity of stud - ies utilizing these methods adds to their intrigue, making them promising and compel - ling avenues for exploration. 12 Conclusion and future work This SLR aimed to provide a",
    "full_text_length": 151955,
    "chunk_length": 1260
  },
  {
    "chunk_id": 959,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 111,
    "total_chunks": 150,
    "text_content": "learning fusion\u2026libraries were carefully selected, analyzed and classified. The exploration of the literature has unveiled a dynamic landscape in the realm of BC classification, where the fusion of different modalities stands at the forefront of innovative research. Across all the studies, the multimodal models based on data fusion demonstrated enhanced performance com - pared to models utilizing only single modalities; leveraging multiple modalities enhances the prediction accuracy and provides",
    "full_text_length": 151955,
    "chunk_length": 1419
  },
  {
    "chunk_id": 960,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 112,
    "total_chunks": 150,
    "text_content": "among multiple heterogeneous data distri - butions poses a significant challenge. The semantic correlation and complementary nature of information from these various modalities underscore the need for capturing high-level associations through a compact set of latent variables that decision fusion may fall short in capturing. Thus, when fusing features, the conventional approach of concatenating data descriptors from different sources into a single high-dimensional feature vector has proven ineff",
    "full_text_length": 151955,
    "chunk_length": 1469
  },
  {
    "chunk_id": 961,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 113,
    "total_chunks": 150,
    "text_content": "proposed solutions and architectures to enhance the fusion of learned features. These innovative architectures, although diverse, can be cat - egorized into four distinct patterns. The integration of previously untapped modalities and the pursuit of diverse task objectives promise to contribute significantly to ongoing efforts in advancing cancer prediction methodologies. Nevertheless, researchers still grapple with the challenge of handling diverse multimodal data through the integration of var",
    "full_text_length": 151955,
    "chunk_length": 1453
  },
  {
    "chunk_id": 962,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 114,
    "total_chunks": 150,
    "text_content": "the adoption of new DL-based methodologies. By narrowing the research scope and delving deeper into other specific multimodal DL aspects, our future work will aim to provide a comprehensive exploration of the literature on translation and co-learning methods based on multiple modalities for BC research. Appendix Details on the extracted data from the selected studies and abbreviations list. See Tables 8 and 9. 1 3Page 41 of 53 327 F.-Z. Nakach et al. Paper Comb Dataset Feature extraction for ima",
    "full_text_length": 151955,
    "chunk_length": 1267
  },
  {
    "chunk_id": 963,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 115,
    "total_chunks": 150,
    "text_content": "Com - pact BilinearFCL AUC: 72% Recall: 67% Specificity: 83% (Guo et al. 2021 )C6 TCGA-BRCA METABRIC- DNN and affinity fusion module for genomicsAttention module Concat. FCL; SVM; LR; RFAccuracy: 89% AUC: 94% Precision: 91% Recall: 94% F1-Score: 92% (Arya and Saha 2020 )C6 TCGA-BRCA METABRIC- CNN Concat. RF; SVM; NB; LRAccuracy: 88% AUC: 97% Precision: 95% Recall: 95% (Arya et al. 2021 )C6 TCGA-BRCA METABRIC- CNN Gated-attention layerConcat. RF; SVM; NB; LRAccuracy: 91% AUC: 95% Precision: 84% R",
    "full_text_length": 151955,
    "chunk_length": 1310
  },
  {
    "chunk_id": 964,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 116,
    "total_chunks": 150,
    "text_content": "86%Table 8 Data extracted from selected papers using feature fusion 1 3327 Page 42 of 53 A comprehensive investigation of multimodal deep learning fusion\u2026 Paper Comb Dataset Feature extraction for imageFeature extrac - tion for nonimageSelection of multi - modal featuresFusion strategy Classifier Performance (Yan et al. 2019 )C2 Private VGG16 Feature maps of 3 FCLsConcat. FCL Accuracy: 90% AUC: 94% (Mura - matsu et al. 2022 )C15 Private EfficientNetB3 - Concat. FCL Accuracy: 77% F1-score: 59% (A",
    "full_text_length": 151955,
    "chunk_length": 1241
  },
  {
    "chunk_id": 965,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 117,
    "total_chunks": 150,
    "text_content": "87% Specificity: 90% (Wang et al. 2021 )C11 TCGA-BRCA Bilinear feature encoding moduleCapture the com - plex intra and inter - modality relationsConcat. Multilayer DNN Accuracy: 80% AUC: 82% C-Index: 72% (Yao et al. 2022 )C7 Private DNN; Attention module Concat. FCL Accuracy: 80% AUC: 75% (Joo et al. 2021 )C4 Private 3D-ResNet FCL Concat. FCL Accuracy: 85% AUC: 89% Recall: 67% Specificity: 93%Table 8 (continued) 1 3Page 43 of 53 327 F.-Z. Nakach et al. Paper Comb Dataset Feature extraction for i",
    "full_text_length": 151955,
    "chunk_length": 1324
  },
  {
    "chunk_id": 966,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 118,
    "total_chunks": 150,
    "text_content": "imag - ing featuresChannel attention Concat. FCL Accuracy: 91% AUC: 95% Recall: 93% Specificity: 89% MCC: 92% (Wang et al. 2023 )C2 Private TCGA-BRCAResNet50; K-meansMultiple instance learningGraph attention net - works; Multihead attention networksConcat.; Linear add; Outer productFCL C-Index: 73% (Xu et al. 2021 )C2 BCNB VGG16 Multi-instance learningConcat. FCL Accuracy: 76% AUC: 83% Recall: 89% C-Index: 67% (Du et al. 2023 )C6 TCGA-BRCA; METABRIC;- Multiscale bilin - ear CNNDecoder and discri",
    "full_text_length": 151955,
    "chunk_length": 1381
  },
  {
    "chunk_id": 967,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 119,
    "total_chunks": 150,
    "text_content": "- modal featuresFusion strategy Classifier Performance (Yuan and Xu 2023 )C6 METABRIC CNN; LSTM; The attention mechanismGated multimodal unitMLP; RF Accuracy: 94% AUC: 96% Recall: 85% C-Index: 87% (Qu et al. 2023 )C12 Private Quantum CNN - Quantum ML model Concat. Variational quan - tum classifierAccuracy: 97% AUC: 97% Recall: 97% (Wang et al. 2020 )C6 TCGA-BRCA - Sample similar - ity; Feature matrix;Similarity network fusion algorithmGraph convolu - tional networkAccuracy: 79% AUC: 93% Precisio",
    "full_text_length": 151955,
    "chunk_length": 1266
  },
  {
    "chunk_id": 968,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 120,
    "total_chunks": 150,
    "text_content": "2023 )C3 Private CNN XGBoost + SHAP Concat. XGBoost AUC: 85% (Kayikci and Khosh - goftaar 2022 )C6 METABRIC - 3 CNNs Concat. RF; DT; SVM Accuracy: 90% C-Index: 67% MCC: 93%Table 8 (continued) 1 3Page 45 of 53 327 F.-Z. Nakach et al. Paper Comb Dataset Feature extraction for imageFeature extrac - tion for nonimageSelection of multi - modal featuresFusion strategy Classifier Performance (Arya et al. 2023 )C8 TCGA-BRCA ResNet152 ; PCA Concat. SVM Utility KernelC-Index: 94% (Qiao et al. 2022 )C14 Pr",
    "full_text_length": 151955,
    "chunk_length": 1247
  },
  {
    "chunk_id": 969,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 121,
    "total_chunks": 150,
    "text_content": "et al. 2021 )C2 Private EfficientNet-B0 TabNet Attention-based Multi-instance Learning fusion moduleConcat. FCL AUC: 88% Precision: 75% Recall: 83% MCC: 79% (Jadoon et al. 2023 )C6 Private CNN CNN; DNN; Concat. RF Accuracy: 97% AUC: 90% Precision: 98% Recall: 97% F1-score: 98% (Zhang 2024 )C15 Private ResNet50 - Attention Concat. MLP AUC: 86% MCC: 79% (Fu et al. 2023 )C18 Private CNN Clinical embed - ding moduleGraph attentional layerConcat. FCL C-index: 75% (Furtney et al. 2023 )C9 TCGA-BRCA Gr",
    "full_text_length": 151955,
    "chunk_length": 1389
  },
  {
    "chunk_id": 970,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 122,
    "total_chunks": 150,
    "text_content": "DNN Weighted linear aggregationAccuracy: 83% AUC: 85% Precision: 75% Specificity: 95% F1-Score: 49% (Arya and Saha 2020 )C6 Short-term/long-term survivalTCGA METABRIC- -CNN CNNRF Accuracy: 90% AUC: 93% Precision: 84% Recall: 75% F1-Score: 73% (Mullen et al. 2023 )C16 + C17 Short-term/long-term survivalPrivate SVM Joint Bayesian classifierAccuracy: 98% AUC: 91% (Holste et al. 2021 )C4 Benign/malignant tumorsPrivate ResNet50 DNN FCL AUC: 90% Specificity: 95% C-Index: 49% (Rabinovici-Co - hen et al",
    "full_text_length": 151955,
    "chunk_length": 1407
  },
  {
    "chunk_id": 971,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 123,
    "total_chunks": 150,
    "text_content": "conducted under the research project \u201cML based Breast Cancer Diagno - sis and Treatment\u201d, 2023\u20132025. The authors would like to thank the Moroccan Ministry of Higher Education and Scientific Research, Digital Development Agency (ADD), CNRST, and UM6P for their support. Author contributions All authors contributed equally to this article. Data availability No datasets were generated or analysed during the current study. Declarations Competing interests The authors declare no competing interests. O",
    "full_text_length": 151955,
    "chunk_length": 1343
  },
  {
    "chunk_id": 972,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 124,
    "total_chunks": 150,
    "text_content": "images or other third party material in this article are included in the article\u2019s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article\u2019s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by-nc-nd/4.0/ . Refere",
    "full_text_length": 151955,
    "chunk_length": 1393
  },
  {
    "chunk_id": 973,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 125,
    "total_chunks": 150,
    "text_content": "30(8):5023\u20135052 Akkus C, Chu L, Djakovic V , Jauch-Walser S, Koch P, Loss G et al (2023) Multimodal deep learning. arXiv. http://arxiv.org/abs/2301.04856 . Accessed 21 Sep 2023. Aliper A, Plis S, Artemov A, Ulloa A, Mamoshina P, Zhavoronkov A (2016) Deep learning applications for predicting pharmacological properties of drugs and drug repurposing using transcriptomic data. Mol Pharm 13(7):2524\u20132530 Alzubaidi L, Zhang J, Humaidi AJ, Al-Dujaili A, Duan Y , Al-Shamma O et al (2021) Review of deep l",
    "full_text_length": 151955,
    "chunk_length": 1310
  },
  {
    "chunk_id": 974,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 126,
    "total_chunks": 150,
    "text_content": "N, Saha S (2020) Multi-modal classification for human breast cancer prognosis prediction: proposal of deep-learning based stacked ensemble model. IEEE/ACM Trans Comput Biol Bioinf 1\u20131 Arya N, Saha S (2021) Multi-modal advanced deep learning architectures for breast cancer survival predic - tion. Knowl Based Syst 221:106965 Arya N, Saha S, Mathur A, Saha S (2023) Improving the robustness and stability of a machine learning model for breast cancer prognosis through the use of multi-modal classifie",
    "full_text_length": 151955,
    "chunk_length": 1305
  },
  {
    "chunk_id": 975,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 127,
    "total_chunks": 150,
    "text_content": "T, Ahuja C, Morency LP (2019) Multimodal machine learning: a survey and taxonomy. IEEE Trans Pattern Anal Mach Intell 41(2):423\u2013443 1 3327 Page 48 of 53 A comprehensive investigation of multimodal deep learning fusion\u2026Barros V , Tlusty T, Barkan E, Hexter E, Gruen D, Guindy M et al (2023) Virtual biopsy by using artificial intelligence\u2013based multimodal modeling of binational mammography data. Radiology 306(3):e220027 Battleday RM, Peterson JC, Griffiths TL (2021) From convolutional neural networ",
    "full_text_length": 151955,
    "chunk_length": 1347
  },
  {
    "chunk_id": 976,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 128,
    "total_chunks": 150,
    "text_content": "Accessed 26 Dec 2023 Chen H, Gao M, Zhang Y , Liang W, Zou X (2019) Attention-based multi-NMF deep neural network with multimodality data for breast cancer prognosis model. Biomed Res Int 2019:e9523719 Cheng J, Gao M, Liu J, Yue H, Kuang H, Liu J et al (2021) Multimodal disentangled variational autoencoder with game theoretic interpretability for glioma grading. IEEE J Biomedical Health Inf Chharia A, Kumar N (2021) Foreseeing survival through \u2018fuzzy intelligence\u2019: a cognitively-inspired incre -",
    "full_text_length": 151955,
    "chunk_length": 1456
  },
  {
    "chunk_id": 977,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 129,
    "total_chunks": 150,
    "text_content": "image compression with a conditional autoencoder. pp. 3146\u201354. https://openaccess.thecvf.com/content_ICCV_2019/html/Choi_Variable_Rate_Deep_ Image_Compression_With_a_Conditional_Autoencoder_ICCV_2019_paper.html . Accessed 28 Dec 2023 Dey R, Salem FM (2017) Gate-variants of gated recurrent unit (GRU) neural networks. In: 2017 IEEE 60th International Midwest Symposium on Circuits and Systems (MWSCAS). pp. 1597\u2013600. https://ieeex - plore.ieee.org/abstract/document/8053243 . Accessed 28 Dec 2023 Dhi",
    "full_text_length": 151955,
    "chunk_length": 1510
  },
  {
    "chunk_id": 978,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 130,
    "total_chunks": 150,
    "text_content": "Yang JYH, Feng DD, Kim J (2023) Deep multimodal graph-based network for survival prediction from highly multiplexed images and patient variables. Comput Biol Med 154:106576 Furtney I, Bradley R, Kabuka MR (2023) Patient graph deep learning to predict breast cancer molecular subtype. IEEE/ACM Trans Comput Biol Bioinform 20(5):3117\u20133127 Ganaie MA, Hu M, Tanveer* M, Suganthan* PN (2021) Ensemble deep learning: a review. arXiv:210402395. http://arxiv.org/abs/2104.02395 . Accessed 9 Sep 2021. Garc\u00eda-",
    "full_text_length": 151955,
    "chunk_length": 1398
  },
  {
    "chunk_id": 979,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 131,
    "total_chunks": 150,
    "text_content": "et al (2020) Generative adversarial networks. Commun ACM 63(11):139\u2013144 Guo Y , Shi H, Kumar A, Grauman K, Rosing T, Feris R (2018) SpotTune transfer learning through adaptive fine-tuning.10 Guo W, Liang W, Deng Q, Zou X (2021) A multimodal affinity fusion network for predicting the survival of breast cancer patients. Front Genet. 12. https://www.frontiersin.org/articles/ https://doi.org/10.3389/ fgene.2021.709027 . Accessed 28 Oct 2023 Hakkoum H, Abnane I, Idri A (2022) Interpretability in the ",
    "full_text_length": 151955,
    "chunk_length": 1321
  },
  {
    "chunk_id": 980,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 132,
    "total_chunks": 150,
    "text_content": "machine learn - ing approach for detection of skin cancer: performance analysis and comparison. dcth 10(1):1845\u20131860 Holste G, Partridge SC, Rahbar H, Biswas D, Lee CI, Alessio AM (2021) End-to-end learning of fused image and non-image features for improved breast cancer classification from MRI. In: 2021 IEEE/CVF inter - national conference on computer vision workshops (ICCVW). pp. 3287\u201396 Huang SC, Pareek A, Seyyedi S, Banerjee I, Lungren MP (2020) Fusion of medical imaging and electronic healt",
    "full_text_length": 151955,
    "chunk_length": 1260
  },
  {
    "chunk_id": 981,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 133,
    "total_chunks": 150,
    "text_content": "Dasarathy BV (2014) Medical image fusion: a survey of the state of the art. Inform Fusion 19:4\u201319 Jetley S, Lord NA, Lee N, Torr PHS (2018) Learn to pay attention. arXiv; 2018. http://arxiv.org/ abs/1804.02391 . Accessed 20 Mar 2023 Joo S, Ko ES, Kwon S, Jeon E, Jung H, Kim JY et al (2021) Multimodal deep learning models for the predic - tion of pathologic response to neoadjuvant chemotherapy in breast cancer. Sci Rep 11(1):18800 Kayikci S, Khoshgoftaar T (2022) A stack based multimodal machine ",
    "full_text_length": 151955,
    "chunk_length": 1255
  },
  {
    "chunk_id": 982,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 134,
    "total_chunks": 150,
    "text_content": "(2015) Multimodal data fusion: an overview of methods, challenges, and pros - pects. Proc IEEE 103(9):1449\u20131477 Li X, Qin G, He Q, Sun L, Zeng H, He Z et al (2020) Digital breast tomosynthesis versus digital mammog - raphy: integration of image modalities enhances deep learning-based breast mass classification. Eur Radiol 30(2):778\u2013788 Li H, Yang F, Xing X, Zhao Y , Zhang J, Liu Y et al (2021) Multi-modal multi-instance learning using weakly correlated histopathological images and tabular clinic",
    "full_text_length": 151955,
    "chunk_length": 1352
  },
  {
    "chunk_id": 983,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 135,
    "total_chunks": 150,
    "text_content": "J, Ding D et al (2021) Multi-modal multi-instance learning for retinal disease recognition. In: Proceedings of the 29th ACM international conference on multimedia. New York, NY , USA: Association for Computing Machinery; pp. 2474\u201382. https://doi.org/10.1145/3474085.3475418 . Accessed 2 May 2022 Li S, Shi H, Sui D, Hao A, Qin H (2020) A Novel Pathological Images and Genomic Data Fusion Framework for Breast Cancer Survival Prediction. In: 2020 42nd Annual International Conference of the IEEE Engin",
    "full_text_length": 151955,
    "chunk_length": 1335
  },
  {
    "chunk_id": 984,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 136,
    "total_chunks": 150,
    "text_content": "ence/article/pii/S1959031820301858 . Accessed 31 Aug 2021 Logan R, Williams BG, Ferreira da Silva M, Indani A, Schcolnicov N, Ganguly A et al (2021) Deep convo - lutional neural networks with ensemble learning and generative adversarial networks for Alzheimer\u2019s disease image data classification. Front Aging Neurosci 13:497 Lu J, Steeg PS, Price JE, Krishnamurthy S, Mani SA, Reuben J et al (2009) Breast cancer metastasis: chal - lenges and opportunities. Cancer Res 69(12):4951\u20134953 Luo Y , Lu Z, ",
    "full_text_length": 151955,
    "chunk_length": 1244
  },
  {
    "chunk_id": 985,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 137,
    "total_chunks": 150,
    "text_content": "using different imaging modalities: a systematic review. Cancers 14(21):5334 1 3327 Page 50 of 53 A comprehensive investigation of multimodal deep learning fusion\u2026Mahmood T, Li J, Pei Y , Akhtar F, Imran A, Rehman KU (2020) A brief survey on breast cancer diagnostic with deep learning schemes using multi-image modalities. IEEE Access 8:165779\u2013165809 Mathur A, Arya N, Pasupa K, Saha S, Roy Dey S, Saha S (2024) Breast cancer prognosis through the use of multi-modal classifiers: current state of th",
    "full_text_length": 151955,
    "chunk_length": 1295
  },
  {
    "chunk_id": 986,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 138,
    "total_chunks": 150,
    "text_content": "system based on the multimodal fusion of breast cancer (MF-CAD). Biomed Signal Process Control 69:102914 Mullen LA, Walton WC, Williams MP, Peyton KS, Porter DW (2023) Breast cancer detection with upstream data fusion, machine learning, and automated registration: initial results. J Med Imaging (Bellingham) 10(Suppl 2):S22409 Muramatsu C, Iwasaki T, Oiwa M, Kawasaki T, Fujita H (2022) Classification of intrinsic subtypes and histological grade for breast cancers by multimodality images. In: 16th",
    "full_text_length": 151955,
    "chunk_length": 1604
  },
  {
    "chunk_id": 987,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 139,
    "total_chunks": 150,
    "text_content": "Diagnostics 13(10):1688 Nakach FZ (2024) Hybrid deep boosting ensembles for histopathological breast cancer classification. Health Technol 18 Nakach FZ, Idri A, Zerouaoui H (2023) Deep hybrid bagging ensembles for classifying histopathological breast cancer images. pp. 289\u2013300. https://www.scitepress.org/Link.aspx?doi =10.5220/0011704200003393 . Accessed 23 May 2023 Nassif AB, Talib MA, Nasir Q, Afadar Y , Elgendy O (2022) Breast cancer detection using artificial intel - ligence techniques: a sy",
    "full_text_length": 151955,
    "chunk_length": 1420
  },
  {
    "chunk_id": 988,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 140,
    "total_chunks": 150,
    "text_content": "AT (2023) A hybrid deep learning framework with decision-level fusion for breast cancer survival prediction. Big Data Cogn Comput 7(1):50 Page MJ, McKenzie JE, Bossuyt PM, Boutron I, Hoffmann TC, Mulrow CD et al (2021) The PRISMA 2020 statement: an updated guideline for reporting systematic reviews. BMJ 372:n71 Pei X, Zuo K, Li Y , Pang Z (2023) A review of the application of multi-modal deep learning in medicine: bibliometrics and future directions. Int J Comput Intell Syst 16(1):44 Qiao M, Liu",
    "full_text_length": 151955,
    "chunk_length": 1263
  },
  {
    "chunk_id": 989,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 141,
    "total_chunks": 150,
    "text_content": "et al (2022) Multimodal prediction of five-year breast cancer recurrence in women who receive neoadjuvant che - motherapy. Cancers (Basel) 14(16):3848 Rahate A, Walambe R, Ramanna S, Kotecha K (2022) Multimodal co-learning: challenges, applications with datasets, recent advances and future directions. Inform Fusion 81:203\u2013239 Romeo V , Accardo G, Perillo T, Basso L, Garbino N, Nicolai E et al (2021) Assessment and prediction of response to neoadjuvant chemotherapy in breast cancer: a comparison ",
    "full_text_length": 151955,
    "chunk_length": 1333
  },
  {
    "chunk_id": 990,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 142,
    "total_chunks": 150,
    "text_content": "for biomedical data fusion: a review. Brief Bioinform 23(2):bbab569 1 3Page 51 of 53 327 F.-Z. Nakach et al.Steyaert S, Pizurica M, Nagaraj D, Khandelwal P, Hernandez-Boussard T, Gentles AJ et al (2023) Multi - modal data fusion for cancer biomarker discovery with deep learning. Nat Mach Intell 5(4):351\u2013362 Sugimoto M, Hikichi S, Takada M, Toi M (2023) Machine learning techniques for breast cancer diagnosis and treatment: a narrative review. Ann Breast Surg 7(0). https://abs.amegroups.org/articl",
    "full_text_length": 151955,
    "chunk_length": 1369
  },
  {
    "chunk_id": 991,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 143,
    "total_chunks": 150,
    "text_content": "the Impact of Resid - ual Connections on Learning. In: Thirty-First AAAI Conference on Artificial Intelligence. https://www. aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14806 . Accessed 25 Feb 2022 Tan XJ, Cheor WL, Lim LL, Ab Rahman KS, Bakrin IH (2022) Artificial Intelligence (AI) in breast imaging: a scientometric umbrella review. Diagnostics 12(12):3111 Taud H, Mas JF (2018) Multilayer perceptron (MLP). In: Camacho Olmedo MT, Paegelow M, Mas JF, Escobar F (Eds) Geomatic approaches for mode",
    "full_text_length": 151955,
    "chunk_length": 1445
  },
  {
    "chunk_id": 992,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 144,
    "total_chunks": 150,
    "text_content": "of multi-omics data for breast cancer patients survival analysis. BMC Med Inf Decis Mak 20(1):225 Wan Y , Shu J, Sui Y , Xu G, Zhao Z, Wu J et al (2019) Multi-modal attention network learning for seman - tic source code retrieval. In: Proceedings of the 34th IEEE/ACM international conference on auto - mated software engineering. San Diego, California: IEEE Press. pp. 13\u201325. (ASE \u201919). https://doi. org/10.1109/ASE.2019.00012 . Accessed 23 May 2022 Wang F, Han J (2009) Multimodal biometric authent",
    "full_text_length": 151955,
    "chunk_length": 1227
  },
  {
    "chunk_id": 993,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 145,
    "total_chunks": 150,
    "text_content": "Wang Y , Zhang L, Li Y , Wu F, Cao S, Ye F (2023) Predicting the prognosis of HER2-positive breast cancer patients by fusing pathological whole slide images and clinical features using multiple instance learn - ing. Math Biosci Eng 20(6):11196\u201311211 Weng L (2019) From From GAN to WGAN. arXiv. http://arxiv.org/abs/1904.08994 . Accessed 28 Dec 2023 Wu Y , Wei L, Duan Y (2019) Deep spatiotemporal LSTM network with temporal pattern feature for 3D human action recognition. Comput Intell 35(3):535\u2013554",
    "full_text_length": 151955,
    "chunk_length": 1209
  },
  {
    "chunk_id": 994,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 146,
    "total_chunks": 150,
    "text_content": "biopsy slides. Front Oncol. https://doi.org/10.3389/ fonc.2021.759007 Yala A, Lehman C, Schuster T, Portnoi T, Barzilay R (2019) A deep learning mammography-based model for improved breast cancer risk prediction. Radiology 292(1):60\u201366 Yan R, Ren F, Rao X, Shi B, Xiang T, Zhang L et al (2019) Integration of multimodal data for breast cancer classification using a hybrid deep learning method. In: Huang DS, Bevilacqua V , Premaratne P (Eds) Intelligent computing theories and application (Lecture N",
    "full_text_length": 151955,
    "chunk_length": 1287
  },
  {
    "chunk_id": 995,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 147,
    "total_chunks": 150,
    "text_content": "350:109019 Yang J, Ju J, Guo L, Ji B, Shi S, Yang Z et al (2022) Prediction of HER2-positive breast cancer recurrence and metastasis risk from histopathological images and clinical information via multimodal deep learning. Comput Struct Biotechnol J 20:333\u2013342 1 3327 Page 52 of 53 A comprehensive investigation of multimodal deep learning fusion\u2026Yao Y , Lv Y , Tong L, Liang Y , Xi S, Ji B et al (2022) ICSDA: a multi-modal deep learning model to predict breast cancer recurrence and metastasis risk",
    "full_text_length": 151955,
    "chunk_length": 1246
  },
  {
    "chunk_id": 996,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 148,
    "total_chunks": 150,
    "text_content": "IEEE Access 5:21954\u201321961 Yuan H, Xu H (2023) Deep multi-modal fusion network with gated unit for breast cancer survival prediction. Comput Methods Biomech BioMed Eng 0(0):1\u201314 Yuan Y , Giger ML, Li H, Bhooshan N, Sennett CA (2010) Multimodality computer-aided breast cancer diag - nosis with FFDM and DCE-MRI. Acad Radiol 17(9):1158\u20131167 Zerouaoui H, Idri A (2021) Reviewing machine learning and image Processing Based decision-making sys - tems for breast Cancer imaging. J Med Syst 45(1):8 Zhang D",
    "full_text_length": 151955,
    "chunk_length": 1216
  },
  {
    "chunk_id": 997,
    "paper_filename": "fatima_2024_comprehensive_investigation_of_multimodal_deep_learning_fusion_Strategies_for_breadt_cancer_classification.pdf",
    "paper_title": "Fatima 2024 Comprehensive Investigation Of Multimodal Deep Learning Fusion Strategies For Breadt Cancer Classification",
    "chunk_index": 149,
    "total_chunks": 150,
    "text_content": "deep learning approach combining diffuse optical tomog - raphy and ultrasound for improving breast cancer classification. Biomed Opt Express 14(4):1636\u20131646 Zhou J, Cui G, Hu S, Zhang Z, Yang C, Liu Z et al (2020) Graph neural networks: a review of methods and applications. AI Open 1:57\u201381 Zhang T, Han L, Gao Y , Wang X, Beets-Tan R, Mann R (2024) Predicting molecular subtypes of breast can - cer using multimodal deep learning and incorporation of the attention mechanism Publisher\u2019s note Springe",
    "full_text_length": 151955,
    "chunk_length": 1119
  },
  {
    "chunk_id": 998,
    "paper_filename": "gisella_2024_performance_of_dual_energy-subtraction_in_contrast_enhanced_mammography.pdf",
    "paper_title": "Gisella 2024 Performance Of Dual Energy-Subtraction In Contrast Enhanced Mammography",
    "chunk_index": 0,
    "total_chunks": 46,
    "text_content": "Gennaro et al .European Radiology Experimental https://doi.org/10.1186/s41747-024-00516-3 ORIGINAL ARTICLE Open Access Performance of dual-energy subtraction in contrast-enhanced mammography forthree different manufacturers: a phantom study Gisella Gennaro1*, Giulia Vatteroni2,3, Daniela Bernardi2,3and Francesca Caumo1 Abstract Background Dual-energy subtraction (DES) imaging is critical in contrast-enhanced mammography (CEM), as the recombination of low-energy (LE) and high-energy (HE) images p",
    "full_text_length": 44020,
    "chunk_length": 1428
  },
  {
    "chunk_id": 999,
    "paper_filename": "gisella_2024_performance_of_dual_energy-subtraction_in_contrast_enhanced_mammography.pdf",
    "paper_title": "Gisella 2024 Performance Of Dual Energy-Subtraction In Contrast Enhanced Mammography",
    "chunk_index": 1,
    "total_chunks": 46,
    "text_content": "(CNR) was obtained from LE and HE images and DES images and used asan image quality metric. Results On average, the CNR of LE images of CEM1 was 2.3 times higher than that of CEM2 and 2.7 times higher than that of CEM3. For HE images, the CNR of CEM1 was 2.7 and 3.5 times higher than that of CEM2 and CEM3, respectively.The CNR remained predominantly higher for CEM1 even when measured from DES images, followed by CEM2 andthen CEM3. CEM1 delivered the lowest MGD (2.34 \u00b1 0.03 mGy), followed by CEM3",
    "full_text_length": 44020,
    "chunk_length": 1182
  },
  {
    "chunk_id": 1000,
    "paper_filename": "gisella_2024_performance_of_dual_energy-subtraction_in_contrast_enhanced_mammography.pdf",
    "paper_title": "Gisella 2024 Performance Of Dual Energy-Subtraction In Contrast Enhanced Mammography",
    "chunk_index": 2,
    "total_chunks": 46,
    "text_content": "by different CEM systems, showing that these differences can be translated in terms of variations in contrastenhancement and radiation dose. Key Points \u25cfDES images, obtained by recombining LE and HE images, have a major role in CEM. \u25cfDifferences in radiation dose among CEM systems were between 8.0% and 49.6%. \u25cfOne DES algorithm achieved superior technical performance, providing higher CNR values at a lower radiation dose. Keywords Contrast media, Mammography, Phantoms (imaging), Radiographic ima",
    "full_text_length": 44020,
    "chunk_length": 1306
  },
  {
    "chunk_id": 1001,
    "paper_filename": "gisella_2024_performance_of_dual_energy-subtraction_in_contrast_enhanced_mammography.pdf",
    "paper_title": "Gisella 2024 Performance Of Dual Energy-Subtraction In Contrast Enhanced Mammography",
    "chunk_index": 3,
    "total_chunks": 46,
    "text_content": "\u2019s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article \u2019s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyrightholder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/ .*Correspondence: Gisella Gennaro gisella.gennaro@iov.veneto.itFull list of author information",
    "full_text_length": 44020,
    "chunk_length": 1399
  },
  {
    "chunk_id": 1002,
    "paper_filename": "gisella_2024_performance_of_dual_energy-subtraction_in_contrast_enhanced_mammography.pdf",
    "paper_title": "Gisella 2024 Performance Of Dual Energy-Subtraction In Contrast Enhanced Mammography",
    "chunk_index": 4,
    "total_chunks": 46,
    "text_content": "right.Contrast-to-noise ratio (CNR), grouped by iodine concentration, measured from DES images for (a) the adipose half and (b) the glandular half of the phantom for the three CEM systems and all the automatic exposure contr ol (AEC) modes; (c) total mean glandular dose (MGD) derived from the phantom exposures with the three CEM systems and all AEC modes. ThisphantomstudyhighlightedthevariabilityinperformanceamongtheDESalgorithms usedbydifferentCEMsystems,impactingoncontrast-enhancementandradiat",
    "full_text_length": 44020,
    "chunk_length": 1728
  },
  {
    "chunk_id": 1003,
    "paper_filename": "gisella_2024_performance_of_dual_energy-subtraction_in_contrast_enhanced_mammography.pdf",
    "paper_title": "Gisella 2024 Performance Of Dual Energy-Subtraction In Contrast Enhanced Mammography",
    "chunk_index": 5,
    "total_chunks": 46,
    "text_content": "of acquiring pairs ofmammography images, called low-energy (LE) and high-energy (HE) images, at least two minutes after intravenousadministration of iodinated contrast agent to the patient.LE and HE images are recombined to generate a new image, called the following dual-energy subtraction (DES) image [ 1]. Compared with the soft tissues of which the breast is composed, iodine has a higher atomic number, whichincreases its ability to attenuate x-rays, with an absorptionpeak, called the k-edge, a",
    "full_text_length": 44020,
    "chunk_length": 1310
  },
  {
    "chunk_id": 1004,
    "paper_filename": "gisella_2024_performance_of_dual_energy-subtraction_in_contrast_enhanced_mammography.pdf",
    "paper_title": "Gisella 2024 Performance Of Dual Energy-Subtraction In Contrast Enhanced Mammography",
    "chunk_index": 6,
    "total_chunks": 46,
    "text_content": "the anatomical noise of the surrounding background[1,2]. CEM has demonstrated ef \ufb01cacy in preoperative staging [3\u20135], monitoring of neoadjuvant therapy [ 6,7], andworkup of screening recalls [ 8\u201310], especially in dense breasts [ 11,12]. It has also demonstrated comparable performance to breast MRI as a screening tool for womenat increased risk for breast cancer, but more studies areongoing to prospectively investigate its clinical role [ 13]. From a technical point of view, CEM has been devel- ",
    "full_text_length": 44020,
    "chunk_length": 1465
  },
  {
    "chunk_id": 1005,
    "paper_filename": "gisella_2024_performance_of_dual_energy-subtraction_in_contrast_enhanced_mammography.pdf",
    "paper_title": "Gisella 2024 Performance Of Dual Energy-Subtraction In Contrast Enhanced Mammography",
    "chunk_index": 7,
    "total_chunks": 46,
    "text_content": "algorithms producing DESimages while maximizing contrast and minimizing arti-facts [ 1,13\u201315]. The quality of the recombination process, which effectively removes the background tissue signalwhile enhancing the iodine contrast, is particularly crucialfor clear visualization of lesions. Variations in thesealgorithms among manufacturers can lead to signi \ufb01cant differences in the appearance of the \ufb01nal image. Thus, physical differences between CEM units, resulting fromGennaro et al .European Radiol",
    "full_text_length": 44020,
    "chunk_length": 1444
  },
  {
    "chunk_id": 1006,
    "paper_filename": "gisella_2024_performance_of_dual_energy-subtraction_in_contrast_enhanced_mammography.pdf",
    "paper_title": "Gisella 2024 Performance Of Dual Energy-Subtraction In Contrast Enhanced Mammography",
    "chunk_index": 8,
    "total_chunks": 46,
    "text_content": "a dual-track x-raytube (molybdenum, Mo, and rhodium, Rh) paired with ascintillator-based \ufb02at panel detector (FPD) with a 100-\u03bcm pixel pitch. In contrast, both Hologic and Siemens sys- tems use a single-track x-ray tube made of tungsten (W)and a photoconductor-based FPD, with pixel pitches of70 \u03bcm and 85 \u03bcm, respectively. Various \ufb01lters are used for image acquisition. The GE system employs 30-\u03bcm Mocombined with the Mo anode and 25-\u03bcm silver (Ag)combined with the Rh anode. Hologic \ufb01lters consist o",
    "full_text_length": 44020,
    "chunk_length": 1226
  },
  {
    "chunk_id": 1007,
    "paper_filename": "gisella_2024_performance_of_dual_energy-subtraction_in_contrast_enhanced_mammography.pdf",
    "paper_title": "Gisella 2024 Performance Of Dual Energy-Subtraction In Contrast Enhanced Mammography",
    "chunk_index": 9,
    "total_chunks": 46,
    "text_content": "four additional modes: two to increase the dose from the default mode (labeled \u201c112% \u201dand \u201c125% \u201ddose levels) and two to decrease the dose (labeled \u201c89% \u201dand \u201c80% \u201ddose levels). The technical speci \ufb01cations of the three CEM systems are summarized in Table 1. Phantom The CIRS phantom Model 022 (Sun Nuclear, Melbourne, FL, USA) consists of four plates, as shown in Fig. 1a. The target plate (Fig. 1b) is constructed of breast-equivalent material in a 50/50 ratio of gland to adipose tissue and athick",
    "full_text_length": 44020,
    "chunk_length": 1210
  },
  {
    "chunk_id": 1008,
    "paper_filename": "gisella_2024_performance_of_dual_energy-subtraction_in_contrast_enhanced_mammography.pdf",
    "paper_title": "Gisella 2024 Performance Of Dual Energy-Subtraction In Contrast Enhanced Mammography",
    "chunk_index": 10,
    "total_chunks": 46,
    "text_content": "mm thick, consists of half 100% adipose material and halfTable 1 Main technical speci \ufb01cations of the three CEM systems compared CEM system Senographe PristinaSelenia 3DimensionsMammomat Revelation Manufacturer GE Healthcare Hologic Siemens Detector type CsI FPD a-Se FPD a-Se FPD Pixel pitch, (\u03bcm) 100 70 85 Anode, (s) Mo or Rh W WFilters LE Mo or Ag Rh or Ag RhHE Cu Cu Ti N\u00b0 AEC modes 1 1 5AEC AOP/STD AutoFilter Dose level 100% (default) Dose level 125% Dose level 112%Dose level 89%Dose level 80",
    "full_text_length": 44020,
    "chunk_length": 1251
  },
  {
    "chunk_id": 1009,
    "paper_filename": "gisella_2024_performance_of_dual_energy-subtraction_in_contrast_enhanced_mammography.pdf",
    "paper_title": "Gisella 2024 Performance Of Dual Energy-Subtraction In Contrast Enhanced Mammography",
    "chunk_index": 11,
    "total_chunks": 46,
    "text_content": "detector, LELow energy, HEHigh energy 2.0 mg/cm22.0 mg/cm20.2 mg/cm20.2 mg/cm21.0 mg/cm21.0 mg/cm20.5 mg/cm20.5 mg/cm2 100% glandular(a) (b) Fig. 1 CIRS phantom Model 022 for CEM. aPhantom stack comprising the target plate (1) of breast-equivalent material in a 50/50 ratio of the gland to adipose tissue and a thickness of 10 mm, the contrast plate (2) of25 mm thickness, composed half of 100% adipose material and half of100% glandular material, the top (3) and bottom (4) plates, each 10 mm thick,",
    "full_text_length": 44020,
    "chunk_length": 1247
  },
  {
    "chunk_id": 1010,
    "paper_filename": "gisella_2024_performance_of_dual_energy-subtraction_in_contrast_enhanced_mammography.pdf",
    "paper_title": "Gisella 2024 Performance Of Dual Energy-Subtraction In Contrast Enhanced Mammography",
    "chunk_index": 12,
    "total_chunks": 46,
    "text_content": "separation of iodine from the background over a widerange of densities. The top and bottom plates, each 10 mm thick, are composed entirely of 100% adipose material, with rounded edges to emulate the realisticshape of a compressed breast. In total, the phantommeasures 55-mm thick. In CEM mode, during the acquisition of the CIRS Model 022 phantom, the two central plugs composed of100% glandular tissue are more visible than the plug withhigher iodine concentration (2.0 mg/cm\u00b2) in the LE image. In c",
    "full_text_length": 44020,
    "chunk_length": 1253
  },
  {
    "chunk_id": 1011,
    "paper_filename": "gisella_2024_performance_of_dual_energy-subtraction_in_contrast_enhanced_mammography.pdf",
    "paper_title": "Gisella 2024 Performance Of Dual Energy-Subtraction In Contrast Enhanced Mammography",
    "chunk_index": 13,
    "total_chunks": 46,
    "text_content": "CEM study:the two \u201cDICOM For Processing \u201dLE and HE images, the \u201cDICOM For Presentation \u201dLE image, and the DES image. For each study, the two DICOM For Processing LE and HE were used to obtain the technical factors (anode/ \ufb01lter combination, tube voltage, and exposure) selected by theAEC, and the resulting mean glandular dose (MGD) wascalculated using the model proposed by Dance et al [ 19]. For each CEM study, the total dose was calculated as thesum of MGDs for LE and HE images. LE and HE images",
    "full_text_length": 44020,
    "chunk_length": 1206
  },
  {
    "chunk_id": 1012,
    "paper_filename": "gisella_2024_performance_of_dual_energy-subtraction_in_contrast_enhanced_mammography.pdf",
    "paper_title": "Gisella 2024 Performance Of Dual Energy-Subtraction In Contrast Enhanced Mammography",
    "chunk_index": 14,
    "total_chunks": 46,
    "text_content": "in signal between an object or area of interest and the surrounding background divided by the background noise [ 20]. The CNR was obtained by measur- ing the signal strength as the mean pixel value (MPV) of acircular region of interest (ROI) placed within the target area(iodinated contrast-enhanced detail or area where the 100%glandular plug is located), and the signal and noise from thesurrounding background as MPV and standard deviation(SD) of a circular ring placed around the target area. CNR",
    "full_text_length": 44020,
    "chunk_length": 1303
  },
  {
    "chunk_id": 1013,
    "paper_filename": "gisella_2024_performance_of_dual_energy-subtraction_in_contrast_enhanced_mammography.pdf",
    "paper_title": "Gisella 2024 Performance Of Dual Energy-Subtraction In Contrast Enhanced Mammography",
    "chunk_index": 15,
    "total_chunks": 46,
    "text_content": "noise measurements of background for contrast-enhanced detailsCircular ROI for measurement of residual signal from normaltissue detailsCircular ring for signal and noise of background for normal tissue details2.0 mg/cm2 2.0 mg/cm21.0 mg/cm2 1.0 mg/cm20.5 mg/cm2 0.5 mg/cm2 0.2 mg/cm2 0.2 mg/cm2 Fig. 2 Illustration of the circular ROIs and circular rings used to measure the MPV within each target area (representing iodine concentrations and glandular plugs), as well as the MPV and SDs of the surro",
    "full_text_length": 44020,
    "chunk_length": 1299
  },
  {
    "chunk_id": 1014,
    "paper_filename": "gisella_2024_performance_of_dual_energy-subtraction_in_contrast_enhanced_mammography.pdf",
    "paper_title": "Gisella 2024 Performance Of Dual Energy-Subtraction In Contrast Enhanced Mammography",
    "chunk_index": 16,
    "total_chunks": 46,
    "text_content": "used to calculate theCNR of details with iodinated contrast and those used to assess the residual CNR of 100% glandular plugs. Statistical analysis The MGD for both LE and HE images was compared among the three CEM systems and AEC modes. For eachsystem/AEC mode, the average MGD from the threerepeated acquisitions was calculated, with the maximumhalf-dispersion used as an estimator of the repeatability error. Next, the CNR associated with the LE and HE images was compared. CNR values were obtaine",
    "full_text_length": 44020,
    "chunk_length": 1307
  },
  {
    "chunk_id": 1015,
    "paper_filename": "gisella_2024_performance_of_dual_energy-subtraction_in_contrast_enhanced_mammography.pdf",
    "paper_title": "Gisella 2024 Performance Of Dual Energy-Subtraction In Contrast Enhanced Mammography",
    "chunk_index": 17,
    "total_chunks": 46,
    "text_content": "CNR values and the maximum half-dispersionof the three repeated acquisitions were calculated toestimate the radiation dose and an image quality metricand used for comparison. In addition, to evaluate the performance of the removal of normal anatomical structures of the DES algorithm, theresidual CNR was measured, i.e., the CNR measured inthe two areas of phantom images corresponding to the position of the two central 100% glandular plugs. ImageJ2 was used to measure the MPV and SD values used fo",
    "full_text_length": 44020,
    "chunk_length": 1237
  },
  {
    "chunk_id": 1016,
    "paper_filename": "gisella_2024_performance_of_dual_energy-subtraction_in_contrast_enhanced_mammography.pdf",
    "paper_title": "Gisella 2024 Performance Of Dual Energy-Subtraction In Contrast Enhanced Mammography",
    "chunk_index": 18,
    "total_chunks": 46,
    "text_content": "standardmammography; for the HE image, it opted for Rh/Cu atthe maximum available voltage of 49 kVp. The Hologicsystem, on the other hand, used W/Ag and 30 kVp for theLE image, deviating from the W/Rh and 29 kVp settings used for mammography [ 21], and used W/Cu with 49 kVp for the HE image. Meanwhile, the Siemens system con-sistently chose W/Rh and 29 kVp for the LE image andW/Ti at 49 kVp for the HE image, regardless of the AECmodes selected; variations among the \ufb01ve AEC modes were limited to ",
    "full_text_length": 44020,
    "chunk_length": 1106
  },
  {
    "chunk_id": 1017,
    "paper_filename": "gisella_2024_performance_of_dual_energy-subtraction_in_contrast_enhanced_mammography.pdf",
    "paper_title": "Gisella 2024 Performance Of Dual Energy-Subtraction In Contrast Enhanced Mammography",
    "chunk_index": 19,
    "total_chunks": 46,
    "text_content": "HE images selected by the AEC of the three CEM systems compared System AEC CEM image A/F Tube voltage, (kV p) Exposure, (mAs) MGD, (mGy) GE AOP/STD LE Rh/Ag 34 45.0 \u00b1 0.9 1.64 \u00b1 0.03 HE Rh/Cu 49 111.3 \u00b1 0.4 0.70 \u00b1 0.00 Hologic AutoFilter LE W/Ag 30 156.6 \u00b1 2.5 2.66 \u00b1 0.05 HE W/Cu 49 97.6 \u00b1 2.0 0.84 \u00b1 0.02 Siemens Dose level 125% LE W/Rh 29 215.0 \u00b1 0.6 2.39 \u00b1 0.02 HE W/Ti 49 46.8 \u00b1 0.0 0.75 \u00b1 0.00 Dose level 112% LE W/Rh 29 191.0 \u00b1 1.6 2.12 \u00b1 0.02 HE W/Ti 49 42.3 \u00b1 0.2 0.68 \u00b1 0.00 Dose level 100%",
    "full_text_length": 44020,
    "chunk_length": 943
  },
  {
    "chunk_id": 1018,
    "paper_filename": "gisella_2024_performance_of_dual_energy-subtraction_in_contrast_enhanced_mammography.pdf",
    "paper_title": "Gisella 2024 Performance Of Dual Energy-Subtraction In Contrast Enhanced Mammography",
    "chunk_index": 20,
    "total_chunks": 46,
    "text_content": "Dose level 80% LE W/Rh 29 138.6 \u00b1 1.2 1.54 \u00b1 0.01 HE W/Ti 49 31.7 \u00b1 0.2 0.51 \u00b1 0.00 The last column shows the mean value and maximum half-dispersion of the MGD associated with the two images AEC Automatic exposure control, A/FAnode/ \ufb01lter, CEM Contrast-enhanced mammography, HEHigh energy, LELow energy, MGD Mean glandular doseGennaro et al .European Radiology Experimental Page 5 of 12 Siemens AEC mode operating at the lowest dose (dose level 80%) used a lower MGD for LE images than the GEsystem. ",
    "full_text_length": 44020,
    "chunk_length": 1161
  },
  {
    "chunk_id": 1019,
    "paper_filename": "gisella_2024_performance_of_dual_energy-subtraction_in_contrast_enhanced_mammography.pdf",
    "paper_title": "Gisella 2024 Performance Of Dual Energy-Subtraction In Contrast Enhanced Mammography",
    "chunk_index": 21,
    "total_chunks": 46,
    "text_content": "depending on the AEC mode selected. CNR of LE and HE images Table 3shows the CNR (mean and maximum half-dis- persion) measured on LE and HE images for the threeCEM systems, and for all AEC modes. CNR values are given for each nominal iodine concentration and sepa- rately for the two halves of the phantom, adipose, andglandular. The GE system demonstrated a higher CNR than the Hologic and Siemens systems for both LE and HE imagesfor all four iodine concentrations included in thephantom. The ratio",
    "full_text_length": 44020,
    "chunk_length": 1134
  },
  {
    "chunk_id": 1020,
    "paper_filename": "gisella_2024_performance_of_dual_energy-subtraction_in_contrast_enhanced_mammography.pdf",
    "paper_title": "Gisella 2024 Performance Of Dual Energy-Subtraction In Contrast Enhanced Mammography",
    "chunk_index": 22,
    "total_chunks": 46,
    "text_content": "HE images was 2.7 times that of Hologicand 3.5 that of Siemens. DES algorithm performance A sample image for each CEM system was shown in Fig. 3, with LE images on the left and DES images on the right;for Siemens CEM the sample image was taken from theseries obtained with the default AEC mode (100% doselevel). It can be seen that the appearance of the phantomis different among manufacturers in both LE and DESimages. Table 5shows the CNR (mean and maximum half- dispersion) measured on DES images ",
    "full_text_length": 44020,
    "chunk_length": 1227
  },
  {
    "chunk_id": 1021,
    "paper_filename": "gisella_2024_performance_of_dual_energy-subtraction_in_contrast_enhanced_mammography.pdf",
    "paper_title": "Gisella 2024 Performance Of Dual Energy-Subtraction In Contrast Enhanced Mammography",
    "chunk_index": 23,
    "total_chunks": 46,
    "text_content": "DES images for each iodine concentration, all CEMTable 3 Mean CNR and maximum half-dispersion measured from the DICOM for processing (raw) LE and HE images acquired by the three CEM systems for all available AEC modes Image typePhantom half \u2014iodine conc., (mg/cm2)GE Hologic Siemens 125%Siemens 112%Siemens 100%Siemens 89% Siemens 80% LE raw Adip \u20142.0 4.84 \u00b1 0.10 2.15 \u00b1 0.04 2.56 \u00b1 0.05 2.43 \u00b1 0.02 2.34 \u00b1 0.02 2.23 \u00b1 0.01 2.16 \u00b1 0.02 Adip \u20141.0 2.63 \u00b1 0.10 1.12 \u00b1 0.01 1.34 \u00b1 0.01 1.25 \u00b1 0.03 1.25 \u00b1",
    "full_text_length": 44020,
    "chunk_length": 949
  },
  {
    "chunk_id": 1022,
    "paper_filename": "gisella_2024_performance_of_dual_energy-subtraction_in_contrast_enhanced_mammography.pdf",
    "paper_title": "Gisella 2024 Performance Of Dual Energy-Subtraction In Contrast Enhanced Mammography",
    "chunk_index": 24,
    "total_chunks": 46,
    "text_content": "\u00b1 0.00 1.82 \u00b1 0.05 1.76 \u00b1 0.03 1.69 \u00b1 0.05 1.62 \u00b1 0.02 1.56 \u00b1 0.04 Gland \u20141.0 1.91 \u00b1 0.02 0.81 \u00b1 0.02 1.04 \u00b1 0.01 0.97 \u00b1 0.03 0.95 \u00b1 0.01 0.91 \u00b1 0.01 0.91 \u00b1 0.03 Gland \u20140.5 1.09 \u00b1 0.03 0.45 \u00b1 0.01 0.52 \u00b1 0.01 0.46 \u00b1 0.01 0.48 \u00b1 0.01 0.42 \u00b1 0.01 0.40 \u00b1 0.02 Gland \u20140.2 0.55 \u00b1 0.03 0.23 \u00b1 0.0 0.18 \u00b1 0.02 0.16 \u00b1 0.01 0.18 \u00b1 0.03 0.16 \u00b1 0.01 0.20 \u00b1 0.01 HE raw Adip \u20142.0 9.51 \u00b1 0.24 3.83 \u00b1 0.06 3.16 \u00b1 0.01 3.00 \u00b1 0.04 2.77 \u00b1 0.05 2.67 \u00b1 0.03 2.57 \u00b1 0.08 Adip \u20141.0 4.80 \u00b1 0.06 1.75 \u00b1 0.05 1.41 \u00b1 0.03 1.",
    "full_text_length": 44020,
    "chunk_length": 817
  },
  {
    "chunk_id": 1023,
    "paper_filename": "gisella_2024_performance_of_dual_energy-subtraction_in_contrast_enhanced_mammography.pdf",
    "paper_title": "Gisella 2024 Performance Of Dual Energy-Subtraction In Contrast Enhanced Mammography",
    "chunk_index": 25,
    "total_chunks": 46,
    "text_content": "0.01 0.70 \u00b1 0.01 0.67 \u00b1 0.01 Adip \u20140.2 1.25 \u00b1 0.06 0.45 \u00b1 0.03 0.39 \u00b1 0.02 0.36 \u00b1 0.04 0.33 \u00b1 0.01 0.30 \u00b1 0.04 0.28 \u00b1 0.04 Gland \u20142.0 8.72 \u00b1 0.19 3.61 \u00b1 0.03 2.86 \u00b1 0.04 2.75 \u00b1 0.03 2.56 \u00b1 0.01 2.44 \u00b1 0.051 2.33 \u00b1 0.01 Gland \u20141.0 4.21 \u00b1 0.05 1.68 \u00b1 0.01 1.40 \u00b1 0.02 1.36 \u00b1 0.03 1.28 \u00b1 0.04 0.22 \u00b1 0.02 1.18 \u00b1 0.02 Gland \u20140.5 2.63 \u00b1 0.05 0.93 \u00b1 0.01 0.72 \u00b1 0.02 0.67 \u00b1 0.01 0.65 \u00b1 0.03 0.64 \u00b1 0.02 0.59 \u00b1 0.01 Gland \u20140.2 1.16 \u00b1 0.08 0.42 \u00b1 0.03 0.34 \u00b1 0.04 0.30 \u00b1 0.03 0.29 \u00b1 0.01 0.26 \u00b1 0.03 0.28 \u00b1 0",
    "full_text_length": 44020,
    "chunk_length": 1024
  },
  {
    "chunk_id": 1024,
    "paper_filename": "gisella_2024_performance_of_dual_energy-subtraction_in_contrast_enhanced_mammography.pdf",
    "paper_title": "Gisella 2024 Performance Of Dual Energy-Subtraction In Contrast Enhanced Mammography",
    "chunk_index": 26,
    "total_chunks": 46,
    "text_content": "ratio, HEHigh-energy, LELow-energyGennaro et al .European Radiology Experimental Page 6 of 12 systems, and AEC modes for the two adipose (panel a) and glandular (panel b) phantom halves, along with thetotal MGD (c). Overall, the CNR values rema in higher for GE images also when measured from the processed DES images, followedby Hologic and then Siemens. The largest difference wasfound for the two highest iodine concentrations (2.0 mg/cm 2 and 1.0 mg/cm2), while the difference decreased for the l",
    "full_text_length": 44020,
    "chunk_length": 1174
  },
  {
    "chunk_id": 1025,
    "paper_filename": "gisella_2024_performance_of_dual_energy-subtraction_in_contrast_enhanced_mammography.pdf",
    "paper_title": "Gisella 2024 Performance Of Dual Energy-Subtraction In Contrast Enhanced Mammography",
    "chunk_index": 27,
    "total_chunks": 46,
    "text_content": "showed a sligh t l yh i g h e rC N Rf o rt h el o w e r iodine concentrations (0.5 mg/cm 2and 0.2 mg/cm2)i nt h e glandular half of the phantom than in the adipose half. The Siemens system showed lower CNR values com- pared to the other two CEM systems across all \ufb01ve available AEC modes. The CNR increased as expectedfrom the lowest dose level (80%) to the highest (125%), with relative differences ranging from a minimum of-9.8% to a maximum of 12.9% compared to the default dose level (100%). On a",
    "full_text_length": 44020,
    "chunk_length": 1089
  },
  {
    "chunk_id": 1026,
    "paper_filename": "gisella_2024_performance_of_dual_energy-subtraction_in_contrast_enhanced_mammography.pdf",
    "paper_title": "Gisella 2024 Performance Of Dual Energy-Subtraction In Contrast Enhanced Mammography",
    "chunk_index": 28,
    "total_chunks": 46,
    "text_content": "GE (2.34 \u00b1 0.03 mGy), followed by Siemens at 100% dose level(2.53 \u00b1 0.02 mGy) and then Hologic (3.50 \u00b1 0.05 mGy).This resulted in an increase of 8.0% for Siemens and 49.6%for Hologic over GE. However, Siemens AEC allows \ufb01ve different options, ranging from a maximum dose of(3.14 \u00b1 0.01 mGy) to a minimum dose of (2.05 \u00b10.01 mGy), with one AEC mode using a dose level very close to GE (-3.4%) and another slightly lower than GE (-12.4%). The GE system and the Siemens system in three of the \ufb01ve AEC mo",
    "full_text_length": 44020,
    "chunk_length": 1156
  },
  {
    "chunk_id": 1027,
    "paper_filename": "gisella_2024_performance_of_dual_energy-subtraction_in_contrast_enhanced_mammography.pdf",
    "paper_title": "Gisella 2024 Performance Of Dual Energy-Subtraction In Contrast Enhanced Mammography",
    "chunk_index": 29,
    "total_chunks": 46,
    "text_content": "for processing LE and HE images produced by the GE CEM system to that produced by the Hologic and Siemens systems (the latter for all AEC modes) Image typePhantom half \u2014iodine conc., (mg/cm2)GE/Hologic GE/Siemens 125%GE/Siemens 112%GE/Siemens 100%GE/Siemens 89%GE/Siemens 80% LE raw Adip \u20142.0 2.2 1.9 2.0 2.1 2.2 2.2 Adip \u20141.0 2.4 2.0 2.1 2.1 2.3 2.3 Adip \u20140.5 2.3 2.0 2.1 2.3 2.3 2.3 Adip \u20140.2 2.0 2.0 2.3 2.1 2.2 2.4 Gland \u20142.0 2.1 1.9 2.0 2.1 2.1 2.2 Gland \u20141.0 2.3 1.8 2.0 2.0 2.1 2.1 Gland \u20140.5 ",
    "full_text_length": 44020,
    "chunk_length": 1046
  },
  {
    "chunk_id": 1028,
    "paper_filename": "gisella_2024_performance_of_dual_energy-subtraction_in_contrast_enhanced_mammography.pdf",
    "paper_title": "Gisella 2024 Performance Of Dual Energy-Subtraction In Contrast Enhanced Mammography",
    "chunk_index": 30,
    "total_chunks": 46,
    "text_content": "3.2 3.4 3.6 3.7 Gland \u20141.0 2.5 3.0 3.1 3.3 3.4 3.6 Gland \u20140.5 2.8 3.7 3.9 4.0 4.1 4.5 Gland \u20140.2 2.8 3.4 3.8 4.0 4.5 4.2 Mean 2.7 3.3 3.5 3.7 3.9 4.0 AEC Automatic exposure control, CEM Contrast-enhanced mammography, CNR Contrast-to-noise ratio, HEHigh-energy, LELow-energyGennaro et al .European Radiology Experimental Page 7 of 12 Figure 5depicts the residual signal (indicated by double arrows) for the three CEM systems (default AEC mode forSiemens) and the mean value of the residual CNR mea-sur",
    "full_text_length": 44020,
    "chunk_length": 1281
  },
  {
    "chunk_id": 1029,
    "paper_filename": "gisella_2024_performance_of_dual_energy-subtraction_in_contrast_enhanced_mammography.pdf",
    "paper_title": "Gisella 2024 Performance Of Dual Energy-Subtraction In Contrast Enhanced Mammography",
    "chunk_index": 31,
    "total_chunks": 46,
    "text_content": "lowest MGD (2.34 \u00b1 0.03 mGy), while the Siemens system, operating in the default AEC mode, had a slightly higherdose (2.53 \u00b1 0.02 mGy). In contrast, the Hologic systemadministered the highest dose (3.50 \u00b1 0.05 mGy), sur-passing GE by 49.6% and Siemens by 38.3%. Analysis based on the raw images indicated that the GE system outperformed the Hologic and Siemens systems interms of CNR for both LE and HE images in all iodine concentrations.The GE system maintained its superior CNR perfor- mance in DE",
    "full_text_length": 44020,
    "chunk_length": 1300
  },
  {
    "chunk_id": 1030,
    "paper_filename": "gisella_2024_performance_of_dual_energy-subtraction_in_contrast_enhanced_mammography.pdf",
    "paper_title": "Gisella 2024 Performance Of Dual Energy-Subtraction In Contrast Enhanced Mammography",
    "chunk_index": 32,
    "total_chunks": 46,
    "text_content": "maintaining higher CNR levels than theother systems. Finally, residual CNR values provided insight into the effectiveness of DES algorithms in removing signals fromnormal breast tissue, with Hologic and Siemens out-performing GE in reducing residual signals. However, overall cancellation performance was good for all three CEM systems, with CNRs of contrast-enhanced details1.4\u20133.4 times higher than the residual CNR of normal tissue. Recently, two other research groups have published results compa",
    "full_text_length": 44020,
    "chunk_length": 1307
  },
  {
    "chunk_id": 1031,
    "paper_filename": "gisella_2024_performance_of_dual_energy-subtraction_in_contrast_enhanced_mammography.pdf",
    "paper_title": "Gisella 2024 Performance Of Dual Energy-Subtraction In Contrast Enhanced Mammography",
    "chunk_index": 33,
    "total_chunks": 46,
    "text_content": "glandular half and 100% adipose half) can be distinguished, while they appear very similar to theDES images, indicating the overall effectiveness of the DES algorithmGennaro et al .European Radiology Experimental Page 8 of 12 phantoms with iodine inserts [ 16], while Ghetti et al used the same phantom to compare two systems included inthis study (GE and Hologic), plus a Fuji \ufb01lm Amulet Innovality and an IMS Giotto Class system [ 23]. In both papers, the focus was on the physical characterization",
    "full_text_length": 44020,
    "chunk_length": 1276
  },
  {
    "chunk_id": 1032,
    "paper_filename": "gisella_2024_performance_of_dual_energy-subtraction_in_contrast_enhanced_mammography.pdf",
    "paper_title": "Gisella 2024 Performance Of Dual Energy-Subtraction In Contrast Enhanced Mammography",
    "chunk_index": 34,
    "total_chunks": 46,
    "text_content": "CEM [ 18]. For the future development of protocols for CEM qualitycontrol, the assurance of linearity between signal differ-ence and iodine concentration could serve as a generalcriterion for verifying the correct operation of DESalgorithms. In this study, CNR measured from DES images proved to be an effective metric for comparing the contrast- enhancement capabilities of different DES algorithms. Although measured from processed images, CNR remainsa relevant indicator of image quality because i",
    "full_text_length": 44020,
    "chunk_length": 1453
  },
  {
    "chunk_id": 1033,
    "paper_filename": "gisella_2024_performance_of_dual_energy-subtraction_in_contrast_enhanced_mammography.pdf",
    "paper_title": "Gisella 2024 Performance Of Dual Energy-Subtraction In Contrast Enhanced Mammography",
    "chunk_index": 35,
    "total_chunks": 46,
    "text_content": "should be demonstrated through appropriately designed clinical performance studies. This study has limitations. First, the phantom used represents a single breast thickness/composition, whichmay not capture the full range of potential differencesbetween CEM systems in various breast thicknesses andcompositions. Second, the comparison of DES algorithmswas limited to three CEM systems, while other commer- cial CEM systems were not evaluated. Finally, since this is a phantom study, the clinical rel",
    "full_text_length": 44020,
    "chunk_length": 1183
  },
  {
    "chunk_id": 1034,
    "paper_filename": "gisella_2024_performance_of_dual_energy-subtraction_in_contrast_enhanced_mammography.pdf",
    "paper_title": "Gisella 2024 Performance Of Dual Energy-Subtraction In Contrast Enhanced Mammography",
    "chunk_index": 36,
    "total_chunks": 46,
    "text_content": "0.15 3.55 \u00b1 0.20 1.53 \u00b1 0.07 0.69 \u00b1 0.01 Adip \u20140.5 3.44 \u00b1 0.08 2.92 \u00b1 0.06 1.63 \u00b1 0.07 1.54 \u00b1 0.04 1.53 \u00b1 0.07 1.49 \u00b1 0.04 1.45 \u00b1 0.05 Adip \u20140.2 1.54 \u00b1 0.12 1.54 \u00b1 0.07 0.82 \u00b1 0.06 0.75 \u00b1 0.14 0.69 \u00b1 0.01 0.64 \u00b1 0.10 0.64 \u00b1 0.12 Gland \u20142.0 12.07 \u00b1 0.18 9.58 \u00b1 0.30 6.61 \u00b1 0.10 6.30 \u00b1 0.17 6.15 \u00b1 0.04 5.70 \u00b1 0.15 5.53 \u00b1 0.02 Gland \u20141.0 5.61 \u00b1 0.11 4.88 \u00b1 0.14 3.61 \u00b1 0.09 3.44 \u00b1 0.10 3.35 \u00b1 0.18 3.07 \u00b1 0.10 2.90 \u00b1 0.10 Gland \u20140.5 2.82 \u00b1 0.13 3.12 \u00b1 0.04 1.55 \u00b1 0.02 1.49 \u00b1 0.06 1.41 \u00b1 0.08 1.45 \u00b1 0.",
    "full_text_length": 44020,
    "chunk_length": 847
  },
  {
    "chunk_id": 1035,
    "paper_filename": "gisella_2024_performance_of_dual_energy-subtraction_in_contrast_enhanced_mammography.pdf",
    "paper_title": "Gisella 2024 Performance Of Dual Energy-Subtraction In Contrast Enhanced Mammography",
    "chunk_index": 37,
    "total_chunks": 46,
    "text_content": "the ratio between GE CNR and CNR by other CEM systems Adip \u20142.0 1.0 (ref) 1.3 1.9 2.0 2.2 2.2 2.3 Adip \u20141.0 1.0 (ref) 1.4 1.7 1.7 1.8 2.0 2.0 Adip \u20140.5 1.0 (ref) 1.2 2.1 2.2 2.2 2.3 2.4 Adip \u20140.2 1.0 (ref) 1.0 1.9 2.1 2.2 2.4 2.4 Gland \u20142.0 1.0 (ref) 1.3 1.8 1.9 2.0 2.1 2.2 Gland \u20141.0 1.0 (ref) 1.1 1.6 1.6 1.7 1.8 1.9 Gland \u20140.5 1.0 (ref) 0.9 1.8 1.9 2.0 1.9 2.2 Gland \u20140.2 1.0 (ref) 0.7 1.6 1.7 1.6 1.9 1.7 Mean 1.1 1.8 1.9 2.0 2.1 2.1 Data are shown for the two phantom halves, adipose (Adip) and",
    "full_text_length": 44020,
    "chunk_length": 1081
  },
  {
    "chunk_id": 1036,
    "paper_filename": "gisella_2024_performance_of_dual_energy-subtraction_in_contrast_enhanced_mammography.pdf",
    "paper_title": "Gisella 2024 Performance Of Dual Energy-Subtraction In Contrast Enhanced Mammography",
    "chunk_index": 38,
    "total_chunks": 46,
    "text_content": "control, CEM Contrast-enhanced mammography, CNR Contrast-to-noise ratio, DES Dual-energy subtractionGennaro et al .European Radiology Experimental Page 9 of 12 Fig. 4 aCNR measured from DES images for the adipose half of phantom, grouped by iodine concentration, for all CEM systems and AEC modes. bCNR was measured from DES images for the glandular half of the phantom, grouped by iodine concentration, for all CEM systems and AEC modes. cStacked column chart of the total MGD, calculated as the sum",
    "full_text_length": 44020,
    "chunk_length": 1273
  },
  {
    "chunk_id": 1037,
    "paper_filename": "gisella_2024_performance_of_dual_energy-subtraction_in_contrast_enhanced_mammography.pdf",
    "paper_title": "Gisella 2024 Performance Of Dual Energy-Subtraction In Contrast Enhanced Mammography",
    "chunk_index": 39,
    "total_chunks": 46,
    "text_content": "three systems of the residual signal in the adipose half of the phantom; for each system, the contrast-to-noise value is the average value obtained from three DES images and the error is the maximum half-dispersionGennaro et al .European Radiology Experimental Page 10 of 12 CNR differences needs to be further validated in the clinical setting. In conclusion, this phantom study demonstrated that, among the three CEM systems compared, the DES algo- rithm of one system performed better than the oth",
    "full_text_length": 44020,
    "chunk_length": 1378
  },
  {
    "chunk_id": 1038,
    "paper_filename": "gisella_2024_performance_of_dual_energy-subtraction_in_contrast_enhanced_mammography.pdf",
    "paper_title": "Gisella 2024 Performance Of Dual Energy-Subtraction In Contrast Enhanced Mammography",
    "chunk_index": 40,
    "total_chunks": 46,
    "text_content": "study. GG performed the dataanalysis and interpretation, and was a major contributor in writing themanuscript. FC supervised the overall research project and secured \ufb01nancial support. All authors read and approved the \ufb01nal manuscript. Funding This research is \ufb01nancially supported by \u201cFondo Ricerca Corrente 2024 \u201d, which provides funding for open-access publication. Data availability The datasets analyzed during the current study are available in the Zenodorepository https://doi.org/10.5281/zenod",
    "full_text_length": 44020,
    "chunk_length": 1568
  },
  {
    "chunk_id": 1039,
    "paper_filename": "gisella_2024_performance_of_dual_energy-subtraction_in_contrast_enhanced_mammography.pdf",
    "paper_title": "Gisella 2024 Performance Of Dual Energy-Subtraction In Contrast Enhanced Mammography",
    "chunk_index": 41,
    "total_chunks": 46,
    "text_content": "HPJF, Alcantara R et al (2021) Contrast-enhanced mammography: what the radiologist needs to know. BJR Open3:20210034. https://doi.org/10.1259/bjro.202100343. \u00c5hsberg K, Gardfjell A, Nimeus E et al (2021) The PROCEM study pro- tocol: added value of preoperative contrast-enhanced mammographyin staging of malignant breast lesions \u2014a prospective randomized multicenter study. BMC Cancer 21:1115. https://doi.org/10.1186/ s12885-021-08832-2 4. Lobbes MBI, Neeter LMFH, Raat F et al (2023) The performanc",
    "full_text_length": 44020,
    "chunk_length": 1630
  },
  {
    "chunk_id": 1040,
    "paper_filename": "gisella_2024_performance_of_dual_energy-subtraction_in_contrast_enhanced_mammography.pdf",
    "paper_title": "Gisella 2024 Performance Of Dual Energy-Subtraction In Contrast Enhanced Mammography",
    "chunk_index": 42,
    "total_chunks": 46,
    "text_content": "of contrast-enhanced mammography and contrast-enhanced MRI for assessing pathological complete response to neoadjuvant therapy in patients with breast cancer: a meta-analysis. Breast Cancer Res Treat202:1 \u20139.https://doi.org/10.1007/s10549-023-07034-7 8. Lalji UC, Houben IPL, Prevos R et al (2016) Contrast-enhanced spectral mammography in recalls from the Dutch breast cancer screening pro-gram: validation of results in a large multireader, multicase study. Eur Radiol 26:4371 \u20134379. https://doi.or",
    "full_text_length": 44020,
    "chunk_length": 1683
  },
  {
    "chunk_id": 1041,
    "paper_filename": "gisella_2024_performance_of_dual_energy-subtraction_in_contrast_enhanced_mammography.pdf",
    "paper_title": "Gisella 2024 Performance Of Dual Energy-Subtraction In Contrast Enhanced Mammography",
    "chunk_index": 43,
    "total_chunks": 46,
    "text_content": "Korean J Radiol 21:442 \u2013449. https://doi.org/ 10.3348/kjr.2019.0393 13. Kornecki A (2022) Current status of contrast-enhanced mammography: a comprehensive review. Can Assoc Radiol J 73:141 \u2013156. https://doi.org/10. 1177/08465371211029047 14. Gennaro G, Baldan E, Bezzon E, Caumo F (2022) Artifact reduction in contrast-enhanced mammography. Insights Imaging 13:90. https://doi. org/10.1186/s13244-022-01211-w 15. van Nijnatten TJA, Morscheid S, Baltzer PAT et al (2024) Contrast- enhanced breast imag",
    "full_text_length": 44020,
    "chunk_length": 1681
  },
  {
    "chunk_id": 1042,
    "paper_filename": "gisella_2024_performance_of_dual_energy-subtraction_in_contrast_enhanced_mammography.pdf",
    "paper_title": "Gisella 2024 Performance Of Dual Energy-Subtraction In Contrast Enhanced Mammography",
    "chunk_index": 44,
    "total_chunks": 46,
    "text_content": "2018 (EPOS). https://epos.myesr.org/poster/esr/ecr2018/C-2650 19. Dance DR, Young KC (2014) Estimation of mean glandular dose for contrast-enhanced digital mammography: factors for use with the UK,European and IAEA breast dosimetry protocols. Phys Med Biol59:2127 \u20132137. https://doi.org/10.1088/0031-9155/59/9/2127 20. Gennaro G, Avramova-Cholakova S, Azzalini A et al (2018) Quality controls in digital mammography protocol of the EFOMP Mammo Working group. Phys Med 48:55 \u201364.https://doi.org/10.101",
    "full_text_length": 44020,
    "chunk_length": 1596
  },
  {
    "chunk_id": 1043,
    "paper_filename": "gisella_2024_performance_of_dual_energy-subtraction_in_contrast_enhanced_mammography.pdf",
    "paper_title": "Gisella 2024 Performance Of Dual Energy-Subtraction In Contrast Enhanced Mammography",
    "chunk_index": 45,
    "total_chunks": 46,
    "text_content": "Netherlands), pp 1 \u201382. https://euref.org/ download-section/physico-technical-protocol/ 23. Ghetti C, Ortenzia O, Pagan L et al (2024) Physical and dosimetric char- acterisation of different contrast-enhanced digital mammographic sys-tems: a multicentric study. Phys Med 120:103334. https://doi.org/10.1016/ j.ejmp.2024.103334 24. Grand DJ, Beland M, Dupuy D, Mayo-Smith WW (2009) Contrast-to-noise ratios of liver lesions using subtraction imaging on multiphase64-detector row CT. Clin Radiol 64:107",
    "full_text_length": 44020,
    "chunk_length": 1039
  },
  {
    "chunk_id": 1044,
    "paper_filename": "gweonson_2013_radiologist_assessment_of_breast_density_by_bi_rads_categories_versus_fully_automated_volumetric.pdf",
    "paper_title": "Gweonson 2013 Radiologist Assessment Of Breast Density By Bi Rads Categories Versus Fully Automated Volumetric",
    "chunk_index": 0,
    "total_chunks": 35,
    "text_content": "692 AJR:201, September 2013The limitations of the area-based method have led several research groups to develop volumetric measures of breast density. The vol- umetric method quantifies breast density by us- ing the pixel value to derive information about the x-ray attenuation properties of the column of breast tissue above that pixel [10, 11]. In re- cent years, a fully automated method of mea- suring breast volume to estimate breast density has been developed [11, 12]. Its advantages in-clude ",
    "full_text_length": 34097,
    "chunk_length": 1315
  },
  {
    "chunk_id": 1045,
    "paper_filename": "gweonson_2013_radiologist_assessment_of_breast_density_by_bi_rads_categories_versus_fully_automated_volumetric.pdf",
    "paper_title": "Gweonson 2013 Radiologist Assessment Of Breast Density By Bi Rads Categories Versus Fully Automated Volumetric",
    "chunk_index": 1,
    "total_chunks": 35,
    "text_content": "to estimate mam- mographic breast density using a fully auto- mated volumetric breast density measurement Radiologist Assessment of Breast Density by BI-RADS Categories Versus Fully Automated Volumetric Assessment Hye Mi Gweon1 Ji Hyun Youk Jeong-Ah KimEun Ju Son Gweon HM, Youk JH, Kim JA, Son EJ 1All authors: Department of Radiology, Gangnam Severance Hospital, Yonsei University College of Medicine, 211 Eonju-ro, Dogok-Dong, Gangnam-Gu, Seoul 135-720, South Korea. Address correspondence to J. H",
    "full_text_length": 34097,
    "chunk_length": 1408
  },
  {
    "chunk_id": 1046,
    "paper_filename": "gweonson_2013_radiologist_assessment_of_breast_density_by_bi_rads_categories_versus_fully_automated_volumetric.pdf",
    "paper_title": "Gweonson 2013 Radiologist Assessment Of Breast Density By Bi Rads Categories Versus Fully Automated Volumetric",
    "chunk_index": 2,
    "total_chunks": 35,
    "text_content": "vari-ety of visual computer-assisted and fully auto- mated methods [4\u20136]. There are two methods of breast composition measures: area and vol-umetric. The area-based method works direct-ly from the image of the compressed and pro - jected breast and attempts to quantify breast density by segmenting areas of the mammo-graphic images [7, 8]. However, this method is intrinsically subjective and can be limited by substantial inter- and intraobserver vari- ability. Also, it requires additional decisio",
    "full_text_length": 34097,
    "chunk_length": 1428
  },
  {
    "chunk_id": 1047,
    "paper_filename": "gweonson_2013_radiologist_assessment_of_breast_density_by_bi_rads_categories_versus_fully_automated_volumetric.pdf",
    "paper_title": "Gweonson 2013 Radiologist Assessment Of Breast Density By Bi Rads Categories Versus Fully Automated Volumetric",
    "chunk_index": 3,
    "total_chunks": 35,
    "text_content": "radiologists as BI-RADS density categories 1\u20134. For fully automated volumetric analysis, volumetric breast density was calcu - lated with fully automated software. The volume of fibroglandular tissue, the volume of the breast, and the volumetric percentage density were provided. RESULTS. The weighted overall kappa was 0.48 (moderate agreement) for the three ra - diologists\u2019 estimates of BI-RADS density. Pairwise comparisons of the radiologists\u2019 mea - surements of BI-RADS density revealed moderat",
    "full_text_length": 34097,
    "chunk_length": 1379
  },
  {
    "chunk_id": 1048,
    "paper_filename": "gweonson_2013_radiologist_assessment_of_breast_density_by_bi_rads_categories_versus_fully_automated_volumetric.pdf",
    "paper_title": "Gweonson 2013 Radiologist Assessment Of Breast Density By Bi Rads Categories Versus Fully Automated Volumetric",
    "chunk_index": 4,
    "total_chunks": 35,
    "text_content": "automated volumetric method with radiologist-assigned BI-RADS density categories. Mammographic density as - sessment with the fully automated volumetric method may be used to assign BI-RADS den - sity categories.Gweon et al. Breast Density Assessment Women\u2019s Imaging Original Research Downloaded from ajronline.org by 223.123.41.71 on 08/11/25 from IP address 223.123.41.71. Copyright ARRS. For personal use only; all rights reserved AJR:201, September 2013 693Breast Density Assessment method in com",
    "full_text_length": 34097,
    "chunk_length": 1386
  },
  {
    "chunk_id": 1049,
    "paper_filename": "gweonson_2013_radiologist_assessment_of_breast_density_by_bi_rads_categories_versus_fully_automated_volumetric.pdf",
    "paper_title": "Gweonson 2013 Radiologist Assessment Of Breast Density By Bi Rads Categories Versus Fully Automated Volumetric",
    "chunk_index": 5,
    "total_chunks": 35,
    "text_content": "size of 1914 \u00d7 2294 pixels. The Lorad Selenia unit is equipped with 24 \u00d7 29 cm amorphous selenium de- tectors with a pixel size of 70 \u00b5m. The two differ-ent digital mammography systems were randomly assigned to each patient according to the mammog-raphy suite schedule. BI-RADS Density Classification All mammographic images were downloaded to a soft-copy review workstation (Selenia Softco - py Workstation, Hologic) with soft-copy reading software (MeVis BreastCare version 6.0.5, MeVis Medical Sol",
    "full_text_length": 34097,
    "chunk_length": 1379
  },
  {
    "chunk_id": 1050,
    "paper_filename": "gweonson_2013_radiologist_assessment_of_breast_density_by_bi_rads_categories_versus_fully_automated_volumetric.pdf",
    "paper_title": "Gweonson 2013 Radiologist Assessment Of Breast Density By Bi Rads Categories Versus Fully Automated Volumetric",
    "chunk_index": 6,
    "total_chunks": 35,
    "text_content": "tations: category 1, almost fatty (< 25% glandular); category 2, scattered fibroglandular densities (25\u201350% glandular); category 3, heterogeneously dense (51\u201375% glandular); and category 4, extremely dense (> 75% glandular). Mammographic Density Analysis by Fully Automated Volumetric Software For fully automated volumetric analysis, Volpara software (version 1.5.1, M\u0101 takina Technology) was used. By using the DICOM for processing image data generated by the digital mammography system, the Volpar",
    "full_text_length": 34097,
    "chunk_length": 1310
  },
  {
    "chunk_id": 1051,
    "paper_filename": "gweonson_2013_radiologist_assessment_of_breast_density_by_bi_rads_categories_versus_fully_automated_volumetric.pdf",
    "paper_title": "Gweonson 2013 Radiologist Assessment Of Breast Density By Bi Rads Categories Versus Fully Automated Volumetric",
    "chunk_index": 7,
    "total_chunks": 35,
    "text_content": "adding up all the val-ues in the density map, the software can compute the volume of fibroglandular tissue in cubic centi-meters, the volume of breast in cubic centimeters, and the percentage of the breast that consists of fi-broglandular tissue. The software uses these data to determine the volumetric density. The volu-metric density ranges from 0% to 40%. The breast density information is provided per breast (by av-eraging the craniocaudal and mediolateral oblique values). For each patient, a ",
    "full_text_length": 34097,
    "chunk_length": 1293
  },
  {
    "chunk_id": 1052,
    "paper_filename": "gweonson_2013_radiologist_assessment_of_breast_density_by_bi_rads_categories_versus_fully_automated_volumetric.pdf",
    "paper_title": "Gweonson 2013 Radiologist Assessment Of Breast Density By Bi Rads Categories Versus Fully Automated Volumetric",
    "chunk_index": 8,
    "total_chunks": 35,
    "text_content": "pro-cessing data and is able to send out DICOM sec-ondary capture images with the breast density in-formation (Fig. 1). Data and Statistical Analysis Each patient\u2019s medical record was reviewed and data about age and personal history of breast augmentation, breast-conserving surgery, or mas - tectomy were compiled. Also, radiologic reports and mammograms were reviewed for any mass or calcification larger than 1 mm. Weighted kappa values were calculated to as - sess the proportion of interobserver",
    "full_text_length": 34097,
    "chunk_length": 1350
  },
  {
    "chunk_id": 1053,
    "paper_filename": "gweonson_2013_radiologist_assessment_of_breast_density_by_bi_rads_categories_versus_fully_automated_volumetric.pdf",
    "paper_title": "Gweonson 2013 Radiologist Assessment Of Breast Density By Bi Rads Categories Versus Fully Automated Volumetric",
    "chunk_index": 9,
    "total_chunks": 35,
    "text_content": "- justment for multiple comparisons for continuous variables and the chi-square or Fisher exact test for categoric variables. The correlation between the BI-RADS density category and volumetric breast density provided by the fully automated software was estimated using the Spearman rank correlation coefficient ( \u03c1). Statistical analysis was performed using statistics software (SAS, version 9.1.3, SAS Institute). Differences were considered to be statistically significant at p < 0.05. Results Of ",
    "full_text_length": 34097,
    "chunk_length": 1288
  },
  {
    "chunk_id": 1054,
    "paper_filename": "gweonson_2013_radiologist_assessment_of_breast_density_by_bi_rads_categories_versus_fully_automated_volumetric.pdf",
    "paper_title": "Gweonson 2013 Radiologist Assessment Of Breast Density By Bi Rads Categories Versus Fully Automated Volumetric",
    "chunk_index": 10,
    "total_chunks": 35,
    "text_content": "be- cause of technical error (n = 1). Among the 778 patients, 178 (22.9%) had a history of breast cancer surgery (unilateral mas- tectomy in 115 and breast-conserving surgery in 63). A review of the patients\u2019 mammograms showed that 44 patients (5.6%) had abnormal mammographic findings: masses in 27 and cal-cifications in 17. In cases in which the mass or calcification was visible on the mammo- grams, no significant difference was found in the mean volumetric breast density between the abnormal b",
    "full_text_length": 34097,
    "chunk_length": 1249
  },
  {
    "chunk_id": 1055,
    "paper_filename": "gweonson_2013_radiologist_assessment_of_breast_density_by_bi_rads_categories_versus_fully_automated_volumetric.pdf",
    "paper_title": "Gweonson 2013 Radiologist Assessment Of Breast Density By Bi Rads Categories Versus Fully Automated Volumetric",
    "chunk_index": 11,
    "total_chunks": 35,
    "text_content": "ajronline.org by 223.123.41.71 on 08/11/25 from IP address 223.123.41.71. Copyright ARRS. For personal use only; all rights reserved 694 AJR:201, September 2013Gweon et al. tion: 21.0% \u00b1 10.2% vs 20.9% \u00b1 10.1%, p = 0.84). For the breasts iatrogenically altered by cancer surgery, however, the mean volumet- ric breast density of the altered breast (16.7% \u00b1 7.3%) was significantly higher than that of the contralateral normal breast (14.0% \u00b1 7.8%) (p < 0.001). Because of these results, mammo- grams ",
    "full_text_length": 34097,
    "chunk_length": 1293
  },
  {
    "chunk_id": 1056,
    "paper_filename": "gweonson_2013_radiologist_assessment_of_breast_density_by_bi_rads_categories_versus_fully_automated_volumetric.pdf",
    "paper_title": "Gweonson 2013 Radiologist Assessment Of Breast Density By Bi Rads Categories Versus Fully Automated Volumetric",
    "chunk_index": 12,
    "total_chunks": 35,
    "text_content": "data from the automated software, and BI-RADS breast density category between the two different systems (Table 1). The overall weighted kappa of the three ra- diologists\u2019 estimates of BI-RADS density cat- egories showed moderate agreement (\u03ba = 0.48). Pairwise estimates of the weighted kappa be- tween two different observers showed moder- ate to substantial agreement (\u03ba = 0.51\u20130.64). Table 2 summarizes the results of each observ-er\u2019s estimate of BI-RADS density category and VDG. There were no sig",
    "full_text_length": 34097,
    "chunk_length": 1293
  },
  {
    "chunk_id": 1057,
    "paper_filename": "gweonson_2013_radiologist_assessment_of_breast_density_by_bi_rads_categories_versus_fully_automated_volumetric.pdf",
    "paper_title": "Gweonson 2013 Radiologist Assessment Of Breast Density By Bi Rads Categories Versus Fully Automated Volumetric",
    "chunk_index": 13,
    "total_chunks": 35,
    "text_content": "the cases and BI-RADS category 4, in 20.2%. Regarding the automated results from the Volpara soft- ware, VDG 3 was found in 44.7% and VDG 4 in 41.6%. Of the 778 mammographic examinations, 497 examinations (63.9%) showed agreement on BI-RADS density category among the three observers and all examinations (100%) showed agreement on BI-RADS density cat-egory among at least two observers. Even in cases of disagreement about the BI-RADS density category, the differences were with-in one category. Tab",
    "full_text_length": 34097,
    "chunk_length": 1269
  },
  {
    "chunk_id": 1058,
    "paper_filename": "gweonson_2013_radiologist_assessment_of_breast_density_by_bi_rads_categories_versus_fully_automated_volumetric.pdf",
    "paper_title": "Gweonson 2013 Radiologist Assessment Of Breast Density By Bi Rads Categories Versus Fully Automated Volumetric",
    "chunk_index": 14,
    "total_chunks": 35,
    "text_content": "[p > 0.99]) (Fig. 2A). A signif- icant positive correlation was found between BI-RADS categories and fully automated vol- umetric breast density (\u03c1 = 0.765 overall, \u03c1 = 0.785 for Senographe 2000D, \u03c1 = 0.745 for Lorad Selenia; p < 0.001). Regarding the 497 cases in which agreement of BI-RADS den- sity categories was shown among all three ra- diologists, there was also a significant differ- ence in mean volumetric breast density among BI-RADS density categories and mean volu-metric breast density ",
    "full_text_length": 34097,
    "chunk_length": 1224
  },
  {
    "chunk_id": 1059,
    "paper_filename": "gweonson_2013_radiologist_assessment_of_breast_density_by_bi_rads_categories_versus_fully_automated_volumetric.pdf",
    "paper_title": "Gweonson 2013 Radiologist Assessment Of Breast Density By Bi Rads Categories Versus Fully Automated Volumetric",
    "chunk_index": 15,
    "total_chunks": 35,
    "text_content": "Mammographic breast density is consid - ered to be an important independent risk fac - tor for breast cancer [15\u201319]. The relationship TABLE 1: Patient Age and Breast Density Results Overall and by Digital Mammography System Characteristic or Result All Examinations ( n = 778) Senographe 2000Da (n = 346) Lorad Seleniab (n = 432) pc Patient age (y), mean \u00b1 SD 51.7 \u00b1 9.2 51.9 \u00b1 9.6 51.6 \u00b1 8.9 0.75 Volparad results, mean \u00b1 SD Fibroglandular tissue volume (cm3) 51.0 \u00b1 25.7 49.7 \u00b1 39.6 52.0 \u00b1 28.3 0.",
    "full_text_length": 34097,
    "chunk_length": 1062
  },
  {
    "chunk_id": 1060,
    "paper_filename": "gweonson_2013_radiologist_assessment_of_breast_density_by_bi_rads_categories_versus_fully_automated_volumetric.pdf",
    "paper_title": "Gweonson 2013 Radiologist Assessment Of Breast Density By Bi Rads Categories Versus Fully Automated Volumetric",
    "chunk_index": 16,
    "total_chunks": 35,
    "text_content": "(43.1) Radiologists\u2019 resultse BI-RADS density category, no. (%) of patients 0.47 1 19 (2.4) 8 (2.3) 11 (2.5) 2 131 (16.8) 59 (17.1) 72 (16.7) 3 482 (62.0) 206 (59.5) 276 (63.9) 4 146 (18.8) 73 (21.1) 73 (16.9) Note\u2014VDG = Volpara density grade. aGE Healthcare. bHologic. cThe p values indicate comparison between Senographe 2000D and Lorad Selenia. dVersion 1.5.1, M \u0101takina Technology. eAt least two radiologists agreed about the BI-RADS category in all cases. Downloaded from ajronline.org by 223.12",
    "full_text_length": 34097,
    "chunk_length": 1344
  },
  {
    "chunk_id": 1061,
    "paper_filename": "gweonson_2013_radiologist_assessment_of_breast_density_by_bi_rads_categories_versus_fully_automated_volumetric.pdf",
    "paper_title": "Gweonson 2013 Radiologist Assessment Of Breast Density By Bi Rads Categories Versus Fully Automated Volumetric",
    "chunk_index": 17,
    "total_chunks": 35,
    "text_content": "important for cancer risk predictive models and epidemiologic studies. The meth- od most commonly used to classify mammog-raphy density is BI-RADS; however, classi-fication is commonly determined on a visual basis, so it is subjective and has been associ- ated with suboptimal reproducibility [2\u20134]. The suboptimal reproducibility of this method could affect the ability of the BI-RADS den-sity categories, even when incorporated into a larger model, to predict breast cancer risk. In our study, inte",
    "full_text_length": 34097,
    "chunk_length": 1370
  },
  {
    "chunk_id": 1062,
    "paper_filename": "gweonson_2013_radiologist_assessment_of_breast_density_by_bi_rads_categories_versus_fully_automated_volumetric.pdf",
    "paper_title": "Gweonson 2013 Radiologist Assessment Of Breast Density By Bi Rads Categories Versus Fully Automated Volumetric",
    "chunk_index": 18,
    "total_chunks": 35,
    "text_content": "a computer-assist- ed threshold method [7]. A limitation of this method is that it does not take into account the thickness of the dense tissue. Moreover, tech-nical characteristics such as radiation dose and projection angle are seldom registered but can influence the density measurement. Another disadvantage is the subjectivity introduced by the reader who sets the threshold manual-ly, which could cause observer bias. The idea of automated volumetric assessment of breast density is definitely ",
    "full_text_length": 34097,
    "chunk_length": 1296
  },
  {
    "chunk_id": 1063,
    "paper_filename": "gweonson_2013_radiologist_assessment_of_breast_density_by_bi_rads_categories_versus_fully_automated_volumetric.pdf",
    "paper_title": "Gweonson 2013 Radiologist Assessment Of Breast Density By Bi Rads Categories Versus Fully Automated Volumetric",
    "chunk_index": 19,
    "total_chunks": 35,
    "text_content": "the advantage that the pixel value is known to be linearly related to expo-sure. As a result, estimates of volumetric breast density are expected to be more accurate and reproducible for FFDM than for film-screen mammography. However, considering that all we know about breast density as a risk factor or as a determinant of mammography sensitivity is based on visual classification, breast density values obtained by automated software should be adapted to match commonly used BI-RADS categories. Ou",
    "full_text_length": 34097,
    "chunk_length": 1370
  },
  {
    "chunk_id": 1064,
    "paper_filename": "gweonson_2013_radiologist_assessment_of_breast_density_by_bi_rads_categories_versus_fully_automated_volumetric.pdf",
    "paper_title": "Gweonson 2013 Radiologist Assessment Of Breast Density By Bi Rads Categories Versus Fully Automated Volumetric",
    "chunk_index": 20,
    "total_chunks": 35,
    "text_content": "in vol- umetric breast density between BI-RADS categories 1 and 2 was not significant after adjustment for multiple comparisons. Also, volumetric breast densities ranged widely for single BI-RADS categories and overlapped with different BI-RADS categories, especial-ly BI-RADS categories 3 and 4. For example, BI-RADS category 3 included mammograms with volumetric breast densities that ranged from 5.7% to 32.5% (Fig. 2). In a study about a comparison of qualitative estimates based on BI-RADS categ",
    "full_text_length": 34097,
    "chunk_length": 1343
  },
  {
    "chunk_id": 1065,
    "paper_filename": "gweonson_2013_radiologist_assessment_of_breast_density_by_bi_rads_categories_versus_fully_automated_volumetric.pdf",
    "paper_title": "Gweonson 2013 Radiologist Assessment Of Breast Density By Bi Rads Categories Versus Fully Automated Volumetric",
    "chunk_index": 21,
    "total_chunks": 35,
    "text_content": "and compared with the BI-RADS (categories 1\u20134), a significant differ-ence was found (Table 2). The fully automat- ed volumetric software classified more mam- mographic examinations as density grade 4 (41.6%) than the three observers (20.2%) (Ta - ble 2). This overestimation can be reduced by means of a correction in the mapping be- tween average volumetric breast density and VDGs 1\u20134. In this study, two different systems were used, Senographe 2000D and Lorad Selenia. In previous studies [23\u201325],",
    "full_text_length": 34097,
    "chunk_length": 1277
  },
  {
    "chunk_id": 1066,
    "paper_filename": "gweonson_2013_radiologist_assessment_of_breast_density_by_bi_rads_categories_versus_fully_automated_volumetric.pdf",
    "paper_title": "Gweonson 2013 Radiologist Assessment Of Breast Density By Bi Rads Categories Versus Fully Automated Volumetric",
    "chunk_index": 22,
    "total_chunks": 35,
    "text_content": "(41.6) aObservers assessed breast density according to BI-RADS density categories 1\u20134, whereas Volpara software assigned density grades, which it refers to as \u201cVolpara density grade\u201d or \u201cVDG,\u201d 1\u20134. bVersion 1.5.1, M \u0101takina Technology. TABLE 3: Frequency of Volparaa Density Grades (VDGs) for Each BI-RADS Category Assigned by Agreement of at Least Two Radiologists BI-RADS CategoryNo. (%) of Mammographic Examinations ( n = 778)Total No. of Examinations VDG 1 VDG 2 VDG 3 VDG 4 1 3 (15.8) 12 (63.2) ",
    "full_text_length": 34097,
    "chunk_length": 1221
  },
  {
    "chunk_id": 1067,
    "paper_filename": "gweonson_2013_radiologist_assessment_of_breast_density_by_bi_rads_categories_versus_fully_automated_volumetric.pdf",
    "paper_title": "Gweonson 2013 Radiologist Assessment Of Breast Density By Bi Rads Categories Versus Fully Automated Volumetric",
    "chunk_index": 23,
    "total_chunks": 35,
    "text_content": "for vol- umetric measurements than Senographe 2000D images because of a major difference between the two mammography systems: The Lorad Selenia compression paddle is designed to tilt during compression, whereas the compression paddle that is used in the Senographe 2000D is more rigid. The use of a flexible paddle re- sults in variation in breast thickness measure- ments from the chest wall to the breast margin. However, our study showed no significant dif-ference for volumetric breast density da",
    "full_text_length": 34097,
    "chunk_length": 1351
  },
  {
    "chunk_id": 1068,
    "paper_filename": "gweonson_2013_radiologist_assessment_of_breast_density_by_bi_rads_categories_versus_fully_automated_volumetric.pdf",
    "paper_title": "Gweonson 2013 Radiologist Assessment Of Breast Density By Bi Rads Categories Versus Fully Automated Volumetric",
    "chunk_index": 24,
    "total_chunks": 35,
    "text_content": "iatrogeni-cally altered by cancer surgery also could af- fect automated measurements of volumetric density, but these associations have not yet been investigated. We assessed the effect of lesions (mass and calcification) and of breast cancer surgery on volumetric breast densi- ty by comparing breast density between the abnormal breast (mass or calcification) and normal contralateral breast in single patients, and no significant differences were shown in patients with breast lesions. However, vo",
    "full_text_length": 34097,
    "chunk_length": 1357
  },
  {
    "chunk_id": 1069,
    "paper_filename": "gweonson_2013_radiologist_assessment_of_breast_density_by_bi_rads_categories_versus_fully_automated_volumetric.pdf",
    "paper_title": "Gweonson 2013 Radiologist Assessment Of Breast Density By Bi Rads Categories Versus Fully Automated Volumetric",
    "chunk_index": 25,
    "total_chunks": 35,
    "text_content": "the auto- mated volumetric software may show increased density measurements for small breasts. How-ever, the assignment of BI-RADS category 4 by observers is usually not influenced by breast size. Only a few studies have assessed the abso-lute dense breast area or volume [23, 26]. Fur-ther study will be needed for evaluation of ab-solute breast volume, fibroglandular volume, and percentage breast density as well as the association of breast size with breast density and breast cancer risk. This s",
    "full_text_length": 34097,
    "chunk_length": 1323
  },
  {
    "chunk_id": 1070,
    "paper_filename": "gweonson_2013_radiologist_assessment_of_breast_density_by_bi_rads_categories_versus_fully_automated_volumetric.pdf",
    "paper_title": "Gweonson 2013 Radiologist Assessment Of Breast Density By Bi Rads Categories Versus Fully Automated Volumetric",
    "chunk_index": 26,
    "total_chunks": 35,
    "text_content": "different mammography units (Senographe 2000D and Lorad Selenia) were used. However, no significant differenc-es in patient age, data from the automated soft-ware, and BI-RADS breast density category between the two different systems were not- ed (Table 1). Finally, the association of vol-umetric density with many factors affecting breast density was not investigated. A high- percentage breast density was strongly related to younger age, lower body mass index (BMI), nulliparity, late age at firs",
    "full_text_length": 34097,
    "chunk_length": 1338
  },
  {
    "chunk_id": 1071,
    "paper_filename": "gweonson_2013_radiologist_assessment_of_breast_density_by_bi_rads_categories_versus_fully_automated_volumetric.pdf",
    "paper_title": "Gweonson 2013 Radiologist Assessment Of Breast Density By Bi Rads Categories Versus Fully Automated Volumetric",
    "chunk_index": 27,
    "total_chunks": 35,
    "text_content": "conclusion, our study showed good cor- relation of the fully automated volumetric TABLE 4: Correlation of Results for 778 Examinations Obtained Using Fully Automated Volumetric Method With BI-RADS Category Assigned by Agreement of at Least Two Radiologists BI-RADS CategoryFully Automated Volumetric Methoda (Mean \u00b1 SD) Fibroglandular Tissue Volume (cm3) Breast Tissue Volume (cm3) Breast Density (%) 1 33.7 \u00b1 7.3 551.8 \u00b1 76.4 6.1 \u00b1 0.9 2 40.7 \u00b1 13.3 560.3 \u00b1 218.3 7.8 \u00b1 2.3 3 50.7 \u00b1 23.7 394.9 \u00b1 179",
    "full_text_length": 34097,
    "chunk_length": 1239
  },
  {
    "chunk_id": 1072,
    "paper_filename": "gweonson_2013_radiologist_assessment_of_breast_density_by_bi_rads_categories_versus_fully_automated_volumetric.pdf",
    "paper_title": "Gweonson 2013 Radiologist Assessment Of Breast Density By Bi Rads Categories Versus Fully Automated Volumetric",
    "chunk_index": 28,
    "total_chunks": 35,
    "text_content": "volumetric breast density assessed by fully automated volumetric method using Volpara software (version 1.5.1, M \u0101takina Technology). Whiskers show most extreme values within 1.5 interquartile ranges. Thick horizontal lines = median value, \uf0a1 = outliers. A, Two radiologists in agreement about BI-RADS category and volumetric breast density by fully automated volumetric measurement. B, Three radiologists in agreement about BI-RADS category and volumetric breast density by fully automated volumetric",
    "full_text_length": 34097,
    "chunk_length": 1420
  },
  {
    "chunk_id": 1073,
    "paper_filename": "gweonson_2013_radiologist_assessment_of_breast_density_by_bi_rads_categories_versus_fully_automated_volumetric.pdf",
    "paper_title": "Gweonson 2013 Radiologist Assessment Of Breast Density By Bi Rads Categories Versus Fully Automated Volumetric",
    "chunk_index": 29,
    "total_chunks": 35,
    "text_content": "Mendelson EB, Ikeda DM, et al. Breast Imaging Reporting and Data System: ACR BI-RADS\u2014breast imaging atlas . Reston, VA: American College of Radiology, 2003 2. Kerlikowske K, Grady D, Barclay J, et al. Vari - ability and accuracy in mammographic interpre - tation using the American College of Radiology Breast Imaging Reporting and Data System. J Natl Cancer Inst 1998; 90:1801\u20131809 3. Berg WA, Campassi C, Langenberg P, Sexton MJ. Breast Imaging Reporting and Data System: inter- and intraobserver v",
    "full_text_length": 34097,
    "chunk_length": 1302
  },
  {
    "chunk_id": 1074,
    "paper_filename": "gweonson_2013_radiologist_assessment_of_breast_density_by_bi_rads_categories_versus_fully_automated_volumetric.pdf",
    "paper_title": "Gweonson 2013 Radiologist Assessment Of Breast Density By Bi Rads Categories Versus Fully Automated Volumetric",
    "chunk_index": 30,
    "total_chunks": 35,
    "text_content": "mammographic parenchymal pat - terns. Eur J Radiol 1997; 24:131\u2013136 7. Byng JW, Boyd NF, Fishell E, Jong RA, Yaffe MJ. The quantitative analysis of mammographic den - sities. Phys Med Biol 1994; 39:1629\u20131638 8. Sivaramakrishna R, Obuchowski NA, Chilcote WA, Powell KA. Automatic segmentation of mam- mographic density. Acad Radiol 2001; 8:250\u2013256 9. Prevrhal S, Shepherd JA, Smith-Bindman R, Cum-mings SR, Kerlikowske K. Accuracy of mammo- graphic breast density analysis: results of formal operator ",
    "full_text_length": 34097,
    "chunk_length": 1310
  },
  {
    "chunk_id": 1075,
    "paper_filename": "gweonson_2013_radiologist_assessment_of_breast_density_by_bi_rads_categories_versus_fully_automated_volumetric.pdf",
    "paper_title": "Gweonson 2013 Radiologist Assessment Of Breast Density By Bi Rads Categories Versus Fully Automated Volumetric",
    "chunk_index": 31,
    "total_chunks": 35,
    "text_content": "standard mammo- gram form. Br J Radiol 2006; 79:378\u2013382 13. Tagliafico A, Tagliafico G, Astengo D, et al. Mammographic density estimation: one-to-one comparison of digital mammography and digital breast tomosynthesis using fully automated soft - ware. Eur Radiol 2012; 22:1265\u20131270 14. Landis JR, Koch GG. The measurement of ob - server agreement for categorical data. Biometrics 1977; 33:159\u2013174 15. Brisson J, Diorio C, Masse B. Wolfe\u2019s parenchy - mal pattern and percentage of the breast with mamm",
    "full_text_length": 34097,
    "chunk_length": 1289
  },
  {
    "chunk_id": 1076,
    "paper_filename": "gweonson_2013_radiologist_assessment_of_breast_density_by_bi_rads_categories_versus_fully_automated_volumetric.pdf",
    "paper_title": "Gweonson 2013 Radiologist Assessment Of Breast Density By Bi Rads Categories Versus Fully Automated Volumetric",
    "chunk_index": 32,
    "total_chunks": 35,
    "text_content": "18. Ursin G, Ma H, Wu AH, et al. Mammographic den- sity and breast cancer in three ethnic groups. Can- cer Epidemiol Biomarkers Prev 2003; 12:332\u2013338 19. Chen Z, Wu AH, Gauderman WJ, et al. Does mammographic density reflect ethnic differences in breast cancer incidence rates? Am J Epidemiol 2004; 159:140\u2013147 20. Boyd NF, Guo H, Martin LJ, et al. Mammograph - ic density and the risk and detection of breast can - cer. N Engl J Med 2007; 356:227\u2013236 21. Harvey JA, Bovbjerg VE. Quantitative assess -",
    "full_text_length": 34097,
    "chunk_length": 1250
  },
  {
    "chunk_id": 1077,
    "paper_filename": "gweonson_2013_radiologist_assessment_of_breast_density_by_bi_rads_categories_versus_fully_automated_volumetric.pdf",
    "paper_title": "Gweonson 2013 Radiologist Assessment Of Breast Density By Bi Rads Categories Versus Fully Automated Volumetric",
    "chunk_index": 33,
    "total_chunks": 35,
    "text_content": "breast density from full-field digital mammo-grams and its association with breast cancer risk fac- tors: a comparison with a threshold method. Cancer Epidemiol Biomarkers Prev 2010; 19:3096\u20133105 24. Tyson AH, Mawdsley GE, Yaffe MJ. Measure - ment of compressed breast thickness by optical stereoscopic photogrammetry. Med Phys 2009; 36:569\u2013576 25. Mawdsley GE, Tyson AH, Peressotti CL, Jong RA, Yaffe MJ. Accurate estimation of com - pressed breast thickness in mammography. Med Phys 2009; 36:577\u201358",
    "full_text_length": 34097,
    "chunk_length": 1302
  },
  {
    "chunk_id": 1078,
    "paper_filename": "gweonson_2013_radiologist_assessment_of_breast_density_by_bi_rads_categories_versus_fully_automated_volumetric.pdf",
    "paper_title": "Gweonson 2013 Radiologist Assessment Of Breast Density By Bi Rads Categories Versus Fully Automated Volumetric",
    "chunk_index": 34,
    "total_chunks": 35,
    "text_content": "stan-dard mammogram form and the interactive thresh- old measurement methods. Cancer Epidemiol Bio- markers Prev 2010; 19:418\u2013428 29. Jeffreys M, Warren R, Highnam R, Davey Smith G. Breast cancer risk factors and a novel measure of volumetric breast density: cross-sectional study. Br J Cancer 2008; 98:210\u2013216 Downloaded from ajronline.org by 223.123.41.71 on 08/11/25 from IP address 223.123.41.71. Copyright ARRS. For personal use only; all rights reserved",
    "full_text_length": 34097,
    "chunk_length": 459
  },
  {
    "chunk_id": 1079,
    "paper_filename": "Han_2024_breas_cancer_basen_on_BIRADS_catagory.pdf",
    "paper_title": "Han 2024 Breas Cancer Basen On Birads Catagory",
    "chunk_index": 0,
    "total_chunks": 42,
    "text_content": "Academic Editor: Luca Nicosia Received: 21 November 2024 Revised: 2 January 2025 Accepted: 15 January 2025 Published: 17 January 2025 Citation: Mun, H.S.; Ko, E.Y.; Han, B.-K.; Ko, E.S.; Choi, J.S.; Kim, H.; Kim, M.K.; Kim, J. Comparative Analysis of Automated and Handheld Breast Ultrasound Findings for Small ( \u22641 cm) Breast Cancers Based on BI-RADS Category. Diagnostics 2025 ,15, 212. https://doi.org/10.3390/ diagnostics15020212 Copyright: \u00a9 2025 by the authors. Licensee MDPI, Basel, Switzerlan",
    "full_text_length": 41917,
    "chunk_length": 1471
  },
  {
    "chunk_id": 1080,
    "paper_filename": "Han_2024_breas_cancer_basen_on_BIRADS_catagory.pdf",
    "paper_title": "Han 2024 Breas Cancer Basen On Birads Catagory",
    "chunk_index": 1,
    "total_chunks": 42,
    "text_content": "of Medicine, The Catholic University of Korea, Seoul 06591, Republic of Korea; im_hsm@catholic.ac.kr 2Department of Radiology, Center for Imaging Science, Samsung Medical Center, Sungkyunkwan University School of Medicine, Seoul 06351, Republic of Korea; bkhan@skku.edu (B.-K.H.); mathilda0330@gmail.com (E.S.K.); jisoo.choi@samsung.com (J.S.C.); hjk220@naver.com (H.K.); myoungkk@gmail.com (M.K.K.) 3Department of Radiology, Inje University Haeundae Paik Hospital, Busan 48108, Republic of Korea; dm",
    "full_text_length": 41917,
    "chunk_length": 1551
  },
  {
    "chunk_id": 1081,
    "paper_filename": "Han_2024_breas_cancer_basen_on_BIRADS_catagory.pdf",
    "paper_title": "Han 2024 Breas Cancer Basen On Birads Catagory",
    "chunk_index": 2,
    "total_chunks": 42,
    "text_content": "posterior characteristics and assigned BI-RADS categories. Lesion sizes were compared between US and pathological findings. Statistical analyses were performed using Bowker\u2019s test of symmetry, a paired t-test, and a cumulative link mixed model. Results : ABUS assigned lower BI-RADS categories than HHUS while still maintaining malignancy suspicion in categories 4A or higher (54.8% consistent with HHUS; 37.3% downcategorized in ABUS, p= 0.005). While ABUS demonstrated less aggressive margins in so",
    "full_text_length": 41917,
    "chunk_length": 1426
  },
  {
    "chunk_id": 1082,
    "paper_filename": "Han_2024_breas_cancer_basen_on_BIRADS_catagory.pdf",
    "paper_title": "Han 2024 Breas Cancer Basen On Birads Catagory",
    "chunk_index": 3,
    "total_chunks": 42,
    "text_content": "assessment categories on ABUS compared to HHUS. Therefore, when conducting breast cancer screening with ABUS, it is important to remain attentive to even subtle suspicious findings, and active consideration for biopsy may be warranted. Keywords: breast cancer; cancer screening; ultrasound; automated breast ultrasound 1. Introduction Automated breast ultrasound (ABUS) is a specialized breast US technique that auto- matically scans the entire breast in transverse sections using a wide transducer. ",
    "full_text_length": 41917,
    "chunk_length": 1428
  },
  {
    "chunk_id": 1083,
    "paper_filename": "Han_2024_breas_cancer_basen_on_BIRADS_catagory.pdf",
    "paper_title": "Han 2024 Breas Cancer Basen On Birads Catagory",
    "chunk_index": 4,
    "total_chunks": 42,
    "text_content": "[ 3\u20135]. This is a key reason why complemen- tary imaging techniques like ABUS and handheld breast ultrasound (HHUS) are used to improve detection accuracy in such cases. Several prospective studies have indicated that the supplementation of mammography with ABUS screening yields comparable favorable outcomes to HHUS screening. These outcomes include the additional detection of cancer, ranging from 1.9 to 2.4 per 1000 screened women with heterogeneously dense or extremely dense breasts. The major",
    "full_text_length": 41917,
    "chunk_length": 1369
  },
  {
    "chunk_id": 1084,
    "paper_filename": "Han_2024_breas_cancer_basen_on_BIRADS_catagory.pdf",
    "paper_title": "Han 2024 Breas Cancer Basen On Birads Catagory",
    "chunk_index": 5,
    "total_chunks": 42,
    "text_content": "However, there is limited research directly comparing lesion sizes in final pathological reports after breast cancer surgery using US measurements from both ABUS and HHUS [12,13]. Over time, the prevalence of small breast cancers has increased due to evolving screen- ing strategies [ 14]. The most frequently diagnosed invasive breast cancers in developed countries are T1 tumors, encompassing T1a ( \u22645 mm), T1b (>0.5 but \u22641 cm), and T1c (>1 but \u22642 cm) tumors [ 15,16]. These smaller tumors exhibit ",
    "full_text_length": 41917,
    "chunk_length": 1327
  },
  {
    "chunk_id": 1085,
    "paper_filename": "Han_2024_breas_cancer_basen_on_BIRADS_catagory.pdf",
    "paper_title": "Han 2024 Breas Cancer Basen On Birads Catagory",
    "chunk_index": 6,
    "total_chunks": 42,
    "text_content": "using breast imaging reporting and data system (BI-RADS) categories, especially for small breast masses ( \u22641 cm), and whether the corresponding results align with those obtained through HHUS [ 10]. Accurate charac- terization of suspicious breast masses is essential in clinical practice as it provides crucial information for interpreting biopsy results in conformance with imaging findings. Dis- crepancies in BI-RADS categorization, depending on the type of US used, can significantly impact the q",
    "full_text_length": 41917,
    "chunk_length": 1329
  },
  {
    "chunk_id": 1086,
    "paper_filename": "Han_2024_breas_cancer_basen_on_BIRADS_catagory.pdf",
    "paper_title": "Han 2024 Breas Cancer Basen On Birads Catagory",
    "chunk_index": 7,
    "total_chunks": 42,
    "text_content": "evaluation of patients with early-stage breast cancer. Pa- tients were enrolled between October 2019 and December 2020 and were included if they had newly detected, biopsy-confirmed breast cancer and had voluntarily consented to participate in the study. Patients were not eligible for participation if they had undergone neoadjuvant chemotherapy or any form of breast cancer treatment prior to the US examinations, or if Diagnostics 2025 ,15, 212 3 of 11 they had previously undergone breast surgery",
    "full_text_length": 41917,
    "chunk_length": 1272
  },
  {
    "chunk_id": 1087,
    "paper_filename": "Han_2024_breas_cancer_basen_on_BIRADS_catagory.pdf",
    "paper_title": "Han 2024 Breas Cancer Basen On Birads Catagory",
    "chunk_index": 8,
    "total_chunks": 42,
    "text_content": "analysis. Ultimately, 51 patients with 51 breast masses were analyzed in this study. The results of the prospective study (NCT04607473) have not yet been published. 2.2. US Examinations 2.2.1. ABUS All ABUS examinations were conducted using the Invenia ABUS system (Reverse Curve \u2122Ultra-broadband Transducer, GE Healthcare, Sunnyvale, CA, USA) equipped with an automated 6\u201315 MHz, 15.3 cm wide-field view transducer. During the examination, the patients were placed in the supine position with a spon",
    "full_text_length": 41917,
    "chunk_length": 1360
  },
  {
    "chunk_id": 1088,
    "paper_filename": "Han_2024_breas_cancer_basen_on_BIRADS_catagory.pdf",
    "paper_title": "Han 2024 Breas Cancer Basen On Birads Catagory",
    "chunk_index": 9,
    "total_chunks": 42,
    "text_content": "to enhance the diagnostic information quality based on nipple location. The obtained volume data were automatically transmitted from the ABUS scanner to a review workstation. Subsequently, the volume data were assessed in the axial, sagittal, and coronal planes on a review workstation, with a 0.5-mm slice interval. 2.2.2. HHUS Bilateral whole-breast HHUS was conducted by one of the five breast imaging radiolo- gists, each with 10\u201325 years of experience in breast US. They utilized an IU-22 unit w",
    "full_text_length": 41917,
    "chunk_length": 1267
  },
  {
    "chunk_id": 1089,
    "paper_filename": "Han_2024_breas_cancer_basen_on_BIRADS_catagory.pdf",
    "paper_title": "Han 2024 Breas Cancer Basen On Birads Catagory",
    "chunk_index": 10,
    "total_chunks": 42,
    "text_content": "chest wall muscle. A focal zone band was placed at the center of the breast parenchyma. Mild manual compression with a transducer was applied during the examination. Each breast was systematically examined in four quadrants, along with the subareolar area and both axillae, and the findings were recorded for each patient. 2.3. Analysis of US Features and Pathologic Data For comparative analysis, ABUS and HHUS images of breast cancers were jointly reviewed and re-assessed by two board-certified ra",
    "full_text_length": 41917,
    "chunk_length": 1334
  },
  {
    "chunk_id": 1090,
    "paper_filename": "Han_2024_breas_cancer_basen_on_BIRADS_catagory.pdf",
    "paper_title": "Han 2024 Breas Cancer Basen On Birads Catagory",
    "chunk_index": 11,
    "total_chunks": 42,
    "text_content": "applied uniformly to both ABUS and HHUS images, facilitating an unbiased comparison [ 20]. ABUS findings were analyzed using multiplanar images in three different planes (axial, sagittal, and coronal) with volume data on a dedicated Invenia ABUS review workstation (GE Healthcare, Chicago, IL, USA). Diagnostics 2025 ,15, 212 4 of 11 For HHUS image analysis, the radiologists were blinded to Doppler and elastography data, relying solely on B-mode static images. To ensure unbiased evaluation, a sepa",
    "full_text_length": 41917,
    "chunk_length": 1389
  },
  {
    "chunk_id": 1091,
    "paper_filename": "Han_2024_breas_cancer_basen_on_BIRADS_catagory.pdf",
    "paper_title": "Han 2024 Breas Cancer Basen On Birads Catagory",
    "chunk_index": 12,
    "total_chunks": 42,
    "text_content": "this study were masses. We analyzed the US features of the lesions, including the shape (oval, round, or irregular), orientation (height\u2013 width ratio), margin (circumscribed, microlobulated, indistinct/angular, or spiculated), echo pattern (hyperechoic, isoechoic, heterogeneous echoic, hypoechoic, complex solid, or cystic), and posterior features (no feature, enhancement, shadowing, or combined pattern). Furthermore, they re-assigned the BI-RADS final assessment categories just on the basis of U",
    "full_text_length": 41917,
    "chunk_length": 1452
  },
  {
    "chunk_id": 1092,
    "paper_filename": "Han_2024_breas_cancer_basen_on_BIRADS_catagory.pdf",
    "paper_title": "Han 2024 Breas Cancer Basen On Birads Catagory",
    "chunk_index": 13,
    "total_chunks": 42,
    "text_content": "t-tests were used to compare the mass orientation (height\u2013width ratio) and size measurements obtained using ABUS and HHUS. The size measurements were then assessed individually for each modality (ABUS vs. pathology and HHUS vs. pathology). Differences between the imaging modalities (ABUS and HHUS) and their corresponding pathological results were also evaluated using paired t-tests. Considering that the final BI-RADS assessment categories for ABUS and HHUS represented ordered categorical respons",
    "full_text_length": 41917,
    "chunk_length": 1386
  },
  {
    "chunk_id": 1093,
    "paper_filename": "Han_2024_breas_cancer_basen_on_BIRADS_catagory.pdf",
    "paper_title": "Han 2024 Breas Cancer Basen On Birads Catagory",
    "chunk_index": 14,
    "total_chunks": 42,
    "text_content": "carcinoma in 41 patients (80.4%), ductal carcinoma in situ in 7 patients (13.7%), invasive lobular carcinoma in 2 patients (3.9%), and mixed invasive ductal and lobular carcinoma in 1 patient (1.96%). Additional cancers beyond the index cancer were detected in specimens from 8 patients, though these were not included in the analysis. A comparison of US findings between ABUS and HHUS is summarized in Table 1. All lesions evaluated with ABUS were classified as having an irregular shape, resulting ",
    "full_text_length": 41917,
    "chunk_length": 1362
  },
  {
    "chunk_id": 1094,
    "paper_filename": "Han_2024_breas_cancer_basen_on_BIRADS_catagory.pdf",
    "paper_title": "Han 2024 Breas Cancer Basen On Birads Catagory",
    "chunk_index": 15,
    "total_chunks": 42,
    "text_content": "significant Diagnostics 2025 ,15, 212 5 of 11 (p= 0.221). Regarding the height\u2013width ratio, ABUS showed a slightly higher median value (0.98, interquartile range [IQR]: 0.72\u20131.12) compared to HHUS (0.86, IQR: 0.74\u20131.10), although this difference was also not statistically significant ( p= 0.166). Additionally, there were no significant differences between ABUS and HHUS in terms of echo patterns and posterior features ( p> 0.05), indicating comparable performance in these aspects. Table 1. A Comp",
    "full_text_length": 41917,
    "chunk_length": 1345
  },
  {
    "chunk_id": 1095,
    "paper_filename": "Han_2024_breas_cancer_basen_on_BIRADS_catagory.pdf",
    "paper_title": "Han 2024 Breas Cancer Basen On Birads Catagory",
    "chunk_index": 16,
    "total_chunks": 42,
    "text_content": "28 (54.8) 0.005 ABUS more suspicious 4 (7.8) ABUS less suspicious 19 (37.3) NA: Not Applicable\u2014All lesions on ABUS were assessed as having an irregular shape; therefore, statistics cannot be extracted. No.: Number of lesions, Q1: first quartile, Q3: third quartile, H-W ratio: height\u2013width ratio. Diagnostics 2025 , 15, x FOR PEER REVIEW 6 of 12 Figure 1. Surgical histopathology revealed a 0.9 cm invasi ve ductal carcinoma in the right breast of a 45-year-old woman. ( A,B) The 0.8 cm mass with an ",
    "full_text_length": 41917,
    "chunk_length": 1270
  },
  {
    "chunk_id": 1096,
    "paper_filename": "Han_2024_breas_cancer_basen_on_BIRADS_catagory.pdf",
    "paper_title": "Han 2024 Breas Cancer Basen On Birads Catagory",
    "chunk_index": 17,
    "total_chunks": 42,
    "text_content": "cm, and the height\u2013 width ratio was calculated to be 1.8. Table 2 presents the analysis of mass margins according to the BI-RADS lexicon, cat- egorizing margins as circumscribed, microlobulated, indistinct/angular, or spiculated, with each classi \ufb01cation representing increasing su spicion for malignancy. The overall agreement between ABUS and HHUS was 61.3% (3 1 of 51 masses), with both modalities showing consistent \ufb01ndings in many cases. However, notable discrepancies were ob- served: seven cas",
    "full_text_length": 41917,
    "chunk_length": 1320
  },
  {
    "chunk_id": 1097,
    "paper_filename": "Han_2024_breas_cancer_basen_on_BIRADS_catagory.pdf",
    "paper_title": "Han 2024 Breas Cancer Basen On Birads Catagory",
    "chunk_index": 18,
    "total_chunks": 42,
    "text_content": "with indis- tinct/angular margins on HHUS were reclassi \ufb01ed as microlobulated on ABUS. Figure 1. Surgical histopathology revealed a 0.9 cm invasive ductal carcinoma in the right breast of a 45-year-old woman. ( A,B) The 0.8 cm mass with an indistinct margin exhibited isoechogenicity on automated breast ultrasound (white arrows). The green and blue lines served as crossing directional lines to indicate the position and direction of the lesion, and the yellow dot represents the nipple location in ",
    "full_text_length": 41917,
    "chunk_length": 1329
  },
  {
    "chunk_id": 1098,
    "paper_filename": "Han_2024_breas_cancer_basen_on_BIRADS_catagory.pdf",
    "paper_title": "Han 2024 Breas Cancer Basen On Birads Catagory",
    "chunk_index": 19,
    "total_chunks": 42,
    "text_content": "malignancy. The overall agreement between ABUS and HHUS was 61.3% (31 of 51 masses), with both modalities showing consistent findings in many cases. However, notable discrepancies were observed: seven cases (12.9%) had more suspicious margins on ABUS compared to HHUS. Specifically, five lesions that had indistinct/angular margins on HHUS were classified as spiculated on ABUS, and two lesions with microlobulated margins on HHUS were reclassified as indistinct/angular on ABUS. On the other hand, 1",
    "full_text_length": 41917,
    "chunk_length": 1293
  },
  {
    "chunk_id": 1099,
    "paper_filename": "Han_2024_breas_cancer_basen_on_BIRADS_catagory.pdf",
    "paper_title": "Han 2024 Breas Cancer Basen On Birads Catagory",
    "chunk_index": 20,
    "total_chunks": 42,
    "text_content": "%) Circumscribed 2 0 0 0 2 (3.9) Microlobulated 0 0 2 0 2 (3.9) Indistinct/angular 0 2 8 11 21 (41.2) Spiculated 0 0 5 21 26 (51.0) Total (n, %) 2 (3.9) 2 (3.9) 15 (29.4) 32 (62.7) 51 n: number of masses. The only statistically significant difference between ABUS and HHUS was found in the final BI-RADS assessment categories based on US findings (Table 3). Cross-tabulation of the re-assigned BI-RADS categories revealed that 54.9% (28/51) of the masses had identical classifications between ABUS an",
    "full_text_length": 41917,
    "chunk_length": 1166
  },
  {
    "chunk_id": 1100,
    "paper_filename": "Han_2024_breas_cancer_basen_on_BIRADS_catagory.pdf",
    "paper_title": "Han 2024 Breas Cancer Basen On Birads Catagory",
    "chunk_index": 21,
    "total_chunks": 42,
    "text_content": "cases (7.8%) were classified with a higher BI-RADS category on ABUS than on HHUS. Table 3. Breast imaging reporting and data system final assessment categorization by automated breast ultrasound (ABUS) and handheld breast ultrasound (HHUS). ABUSHHUS 4A 4B 4C 5 Total ( n, %) 4A 3 2 2 0 7 (13.7) 4B 0 2 5 0 7 (13.7) 4C 0 0 13 10 23 (45.1) 5 0 0 4 10 14 (27.5) Total ( n, %) 3 (5.9) 4 (7.8) 24 (47.1) 20 (39.2) 51 n: number of masses. Diagnostics 2025 ,15, 212 7 of 11 Diagnostics 2025 , 15, x FOR PEER",
    "full_text_length": 41917,
    "chunk_length": 1035
  },
  {
    "chunk_id": 1101,
    "paper_filename": "Han_2024_breas_cancer_basen_on_BIRADS_catagory.pdf",
    "paper_title": "Han 2024 Breas Cancer Basen On Birads Catagory",
    "chunk_index": 22,
    "total_chunks": 42,
    "text_content": "2 0 0 0 2 (3.9) Microlobulated 0 0 2 0 2 (3.9) Indistinct/angular 0 2 8 11 21 (41.2) Spiculated 0 0 5 21 26 (51.0) Total (n , %) 2 (3.9) 2 (3.9) 15 (29.4) 32 (62.7) 51 n: number of masses. The only statistically signi \ufb01cant di\ufb00erence between ABUS and HHUS was found in the \ufb01nal BI-RADS assessment ca tegories based on US \ufb01ndings (Table 3). Cross-tabulation of the re-assigned BI-RADS categories revealed that 54.9% (28/51) of the masses had iden- tical classi \ufb01cations between ABUS and HHUS. However,",
    "full_text_length": 41917,
    "chunk_length": 1125
  },
  {
    "chunk_id": 1102,
    "paper_filename": "Han_2024_breas_cancer_basen_on_BIRADS_catagory.pdf",
    "paper_title": "Han 2024 Breas Cancer Basen On Birads Catagory",
    "chunk_index": 23,
    "total_chunks": 42,
    "text_content": "ev en by ABUS. In contrast, four cases (7.8%) were classi \ufb01ed with a higher BI-RADS cate gory on ABUS than on HHUS. Table 3. Breast imaging reporting and data system \ufb01nal assessment categorization by automated breast ultrasound (ABUS) and hand held breast ultr asound (HHUS). ABUS HHUS 4A 4B 4C 5 Total ( n, %) 4A 3 2 2 0 7 (13.7) 4B 0 2 5 0 7 (13.7) 4C 0 0 13 10 23 (45.1) 5 0 0 4 10 14 (27.5) Total ( n, %) 3 (5.9) 4 (7.8) 24 (47.1) 20 (39.2) 51 n: number of masses. Figure 2. Surgical histopatholo",
    "full_text_length": 41917,
    "chunk_length": 1096
  },
  {
    "chunk_id": 1103,
    "paper_filename": "Han_2024_breas_cancer_basen_on_BIRADS_catagory.pdf",
    "paper_title": "Han 2024 Breas Cancer Basen On Birads Catagory",
    "chunk_index": 24,
    "total_chunks": 42,
    "text_content": "served as crossing directional lines to indicate the position and direction of the lesion in (B). ( C,D) The mass was measured at 1.1 cm and showed more heterogeneous echogenicity. The assessment was upgraded to BI-RADS 4C on handheld breast ultrasound (HHUS) (yellow arrows). The lesion received a lower category rating on ABUS compared to HHUS. Size measurements of small breast masses were similar across both ABUS and HHUS modalities. The maximal diameters were 0.92 \u00b10.30 cm for ABUS and 0.93 \u00b10",
    "full_text_length": 41917,
    "chunk_length": 1259
  },
  {
    "chunk_id": 1104,
    "paper_filename": "Han_2024_breas_cancer_basen_on_BIRADS_catagory.pdf",
    "paper_title": "Han 2024 Breas Cancer Basen On Birads Catagory",
    "chunk_index": 25,
    "total_chunks": 42,
    "text_content": "( p> 0.05). 4. Discussion To our knowledge, this study is the first to compare the morphological characteristics, BI-RADS category assessments, and pathological size between ABUS and HHUS in small malignant breast masses ( \u22641 cm). Our findings demonstrated that ABUS tended to assign lower BI-RADS categories compared to HHUS, with a significant number of lesions being downcategorized, while maintaining malignancy suspicion within categories 4A or higher. Both ABUS and HHUS are US modalities that ",
    "full_text_length": 41917,
    "chunk_length": 1344
  },
  {
    "chunk_id": 1105,
    "paper_filename": "Han_2024_breas_cancer_basen_on_BIRADS_catagory.pdf",
    "paper_title": "Han 2024 Breas Cancer Basen On Birads Catagory",
    "chunk_index": 26,
    "total_chunks": 42,
    "text_content": "23] indicated the superior sensitivity of ABUS for lesions < 1 cm, it has shown a tendency to identify smaller lesions less frequently and to assign them lower final assessment categories. Nevertheless, ABUS has successfully detected all malignant lesions identified on HHUS [ 11]. Due to these characteristics, ABUS is widely accepted as an effective screening tool, helping to reduce false positives while enhancing breast cancer detection. In our study, ABUS demonstrated fewer suspicious margins ",
    "full_text_length": 41917,
    "chunk_length": 1347
  },
  {
    "chunk_id": 1106,
    "paper_filename": "Han_2024_breas_cancer_basen_on_BIRADS_catagory.pdf",
    "paper_title": "Han 2024 Breas Cancer Basen On Birads Catagory",
    "chunk_index": 27,
    "total_chunks": 42,
    "text_content": "an irregular shape was recognized as an independent predictor of breast cancer in both ABUS and HHUS, aligning with previous investigations that utilized the fourth and fifth editions of the BI-RADS lexicon for HHUS assessment [24,25]. The height\u2013width ratio, while an objective measure, could be influenced by differences between ABUS and HHUS. Factors such as imaging plane selection, manual compression in HHUS, operator variability, and transducer resolution may all affect this ratio. ABUS emplo",
    "full_text_length": 41917,
    "chunk_length": 1367
  },
  {
    "chunk_id": 1107,
    "paper_filename": "Han_2024_breas_cancer_basen_on_BIRADS_catagory.pdf",
    "paper_title": "Han 2024 Breas Cancer Basen On Birads Catagory",
    "chunk_index": 28,
    "total_chunks": 42,
    "text_content": "This discrepancy in compression techniques may explain why ABUS tends to demonstrate a higher height\u2013width ratio compared to HHUS in our study. Distinguishing characteristics in very small masses can be particularly challenging. Larger masses typically exhibit features that are easier to identify according to the BI- RADS lexicon, while small masses require more meticulous assessment. Variations in US compression techniques may unveil different features, highlighting the importance of understand",
    "full_text_length": 41917,
    "chunk_length": 1376
  },
  {
    "chunk_id": 1108,
    "paper_filename": "Han_2024_breas_cancer_basen_on_BIRADS_catagory.pdf",
    "paper_title": "Han 2024 Breas Cancer Basen On Birads Catagory",
    "chunk_index": 29,
    "total_chunks": 42,
    "text_content": "defaulting to follow-up. This proactive approach can significantly reduce the risk of false-negative results and enhance the early detection of breast cancer. The role of ABUS as an adjunctive breast cancer screening tool for mammography has gained increasing recognition in multicenter clinical studies [ 6,7]. Its utility in patients without clinically and mammographically suspected enlarged axillary lymph nodes is worthy of further consideration. Several studies have reported image\u2013pathology co",
    "full_text_length": 41917,
    "chunk_length": 1408
  },
  {
    "chunk_id": 1109,
    "paper_filename": "Han_2024_breas_cancer_basen_on_BIRADS_catagory.pdf",
    "paper_title": "Han 2024 Breas Cancer Basen On Birads Catagory",
    "chunk_index": 30,
    "total_chunks": 42,
    "text_content": "ABUS and HHUS matched precisely, irrespective of shape and elasticity. This study also revealed excellent inter- and intra-observer agreements in size measurements between ABUS and HHUS. Consequently, our results, which indicate no significant differences in tumor size assessments across ABUS, HHUS, and pathological reports, may be readily applicable for clinical use. Our study had several limitations. First, we analyzed the comparative US features of small breast cancers without the assistance ",
    "full_text_length": 41917,
    "chunk_length": 1333
  },
  {
    "chunk_id": 1110,
    "paper_filename": "Han_2024_breas_cancer_basen_on_BIRADS_catagory.pdf",
    "paper_title": "Han 2024 Breas Cancer Basen On Birads Catagory",
    "chunk_index": 31,
    "total_chunks": 42,
    "text_content": "lesion types, particularly those \u22641 cm. Third, this study relied solely on grayscale images without incorporating elastography or Doppler imaging in the assessment of HHUS. We posited that this would maintain a consistent setting for comparison with ABUS. Fourth, a potential limitation is that the reviewers were aware the US images were from breast cancer patients, which may have influenced their evaluation of lesion features. However, as both HHUS and ABUS images were analyzed with this prior k",
    "full_text_length": 41917,
    "chunk_length": 1312
  },
  {
    "chunk_id": 1111,
    "paper_filename": "Han_2024_breas_cancer_basen_on_BIRADS_catagory.pdf",
    "paper_title": "Han 2024 Breas Cancer Basen On Birads Catagory",
    "chunk_index": 32,
    "total_chunks": 42,
    "text_content": "provide further insights into the performance of ABUS and HHUS in the preoperative evaluation of early-stage breast cancer. In conclusion, small breast cancers measuring \u22641 cm exhibited suspicious US char- acteristics on both ABUS and HHUS and yet were assigned lower BI-RADS assessment categories on ABUS compared to HHUS. Recognizing the discrepancies in US features for small breast cancers between ABUS and HHUS and actively recommending biopsies for even subtle suspicious findings on ABUS can e",
    "full_text_length": 41917,
    "chunk_length": 1455
  },
  {
    "chunk_id": 1112,
    "paper_filename": "Han_2024_breas_cancer_basen_on_BIRADS_catagory.pdf",
    "paper_title": "Han 2024 Breas Cancer Basen On Birads Catagory",
    "chunk_index": 33,
    "total_chunks": 42,
    "text_content": "grants from GE Healthcare and the Korean Society of Breast Imaging & Korean Society for Breast Screening (KSBI&KSFBS-2020-03). Institutional Review Board Statement: This retrospective cohort study was approved by the Institu- tional Review Board of the Samsung Medical Center (IRB No. 2023-12-012-001) on 6 December 2023. The study adhered to the principles outlined in the Declaration of Helsinki. Informed Consent Statement: The requirement for written informed consent was waived due to its retros",
    "full_text_length": 41917,
    "chunk_length": 1365
  },
  {
    "chunk_id": 1113,
    "paper_filename": "Han_2024_breas_cancer_basen_on_BIRADS_catagory.pdf",
    "paper_title": "Han 2024 Breas Cancer Basen On Birads Catagory",
    "chunk_index": 34,
    "total_chunks": 42,
    "text_content": "M.; Girometti, R.; Zuiani, C. Automated breast ultrasound: Basic principles and emerging clinical applications. Radiol. Med. 2018 ,123, 1\u201312. [CrossRef] [PubMed] 2. Ko, K.H.; Jung, H.K.; Kim, S.J.; Kim, H.; Yoon, J.H. Potential role of shear-wave ultrasound elastography for the differential diagnosis of breast non-mass lesions: Preliminary report. Eur. Radiol. 2014 ,24, 305\u2013311. [CrossRef] [PubMed] 3. Kolb, T.M.; Lichy, J.; Newhouse, J.H. Comparison of the performance of screening mammography, p",
    "full_text_length": 41917,
    "chunk_length": 1372
  },
  {
    "chunk_id": 1114,
    "paper_filename": "Han_2024_breas_cancer_basen_on_BIRADS_catagory.pdf",
    "paper_title": "Han 2024 Breas Cancer Basen On Birads Catagory",
    "chunk_index": 35,
    "total_chunks": 42,
    "text_content": "Venet, L.; Roeser, R. Ten- to fourteen-year effect of screening on breast cancer mortality. J. Natl. Cancer Inst. 1982 ,69, 349\u2013355. 6. Brem, R.F.; Tab\u00e1r, L.; Duffy, S.W.; Inciardi, M.F.; Guingrich, J.A.; Hashimoto, B.E.; Lander, M.R.; Lapidus, R.L.; Peterson, M.K.; Rapelyea, J.A.; et al. Assessing improvement in detection of breast cancer with three-dimensional automated breast US in women with dense breast tissue: The SomoInsight Study. Radiology 2015 ,274, 663\u2013673. [CrossRef] 7. Wilczek, B.; ",
    "full_text_length": 41917,
    "chunk_length": 1375
  },
  {
    "chunk_id": 1115,
    "paper_filename": "Han_2024_breas_cancer_basen_on_BIRADS_catagory.pdf",
    "paper_title": "Han 2024 Breas Cancer Basen On Birads Catagory",
    "chunk_index": 36,
    "total_chunks": 42,
    "text_content": "Choi, H.; Park, E.H.; Song, J.S.; Youk, J.H. Evaluation of an automated breast volume scanner according to the fifth edition of BI-RADS for breast ultrasound compared with hand-held ultrasound. Eur. J. Radiol. 2018 ,99, 138\u2013145. [CrossRef] 10. Vourtsis, A.; Kachulis, A. The performance of 3D ABUS versus HHUS in the visualisation and BI-RADS characterisation of breast lesions in a large cohort of 1886 women. Eur. Radiol. 2018 ,28, 592\u2013601. [CrossRef] 11. Jeh, S.K.; Kim, S.H.; Choi, J.J.; Jung, S.",
    "full_text_length": 41917,
    "chunk_length": 1313
  },
  {
    "chunk_id": 1116,
    "paper_filename": "Han_2024_breas_cancer_basen_on_BIRADS_catagory.pdf",
    "paper_title": "Han 2024 Breas Cancer Basen On Birads Catagory",
    "chunk_index": 37,
    "total_chunks": 42,
    "text_content": "of automated versus handheld breast ultrasound examinations of suspicious breast masses. Ultrasonography 2019 ,38, 264\u2013271. [CrossRef] [PubMed] 14. Fracheboud, J.; Otto, S.; Van Dijck, J.; Broeders, M.; Verbeek, A.; De Koning, H. Decreased rates of advanced breast cancer due to mammography screening in The Netherlands. Br. J. Cancer 2004 ,91, 861\u2013867. [CrossRef] [PubMed] 15. Bland, K.I.; Menck, H.R.; Scott-Conner, C.E.; Morrow, M.; Winchester, D.J.; Winchester, D.P . The National Cancer Data Bas",
    "full_text_length": 41917,
    "chunk_length": 1391
  },
  {
    "chunk_id": 1117,
    "paper_filename": "Han_2024_breas_cancer_basen_on_BIRADS_catagory.pdf",
    "paper_title": "Han 2024 Breas Cancer Basen On Birads Catagory",
    "chunk_index": 38,
    "total_chunks": 42,
    "text_content": "Valero, V . Overall survival and cause-specific mortality of patients with stage T1a, bN0M0 breast carcinoma. J. Clin. Oncol. 2007 ,25, 4952\u20134960. [CrossRef] 18. Hanrahan, E.O.; Valero, V .; Gonzalez-Angulo, A.M.; Hortobagyi, G.N. Prognosis and management of patients with node-negative invasive breast carcinoma that is 1 cm or smaller in size (stage 1; T1a, bN0M0): A review of the literature. J. Clin. Oncol. 2006 ,24, 2113\u20132122. [CrossRef] 19. Kennedy, T.; Stewart, A.K.; Bilimoria, K.Y.; Patel-P",
    "full_text_length": 41917,
    "chunk_length": 1351
  },
  {
    "chunk_id": 1118,
    "paper_filename": "Han_2024_breas_cancer_basen_on_BIRADS_catagory.pdf",
    "paper_title": "Han 2024 Breas Cancer Basen On Birads Catagory",
    "chunk_index": 39,
    "total_chunks": 42,
    "text_content": "ultrasound: Detection performance of radiologists using automated breast ultrasound data. Acta Radiol. 2011 ,52, 8\u201314. [CrossRef] 22. Berg, W.A.; Vourtsis, A. Screening breast ultrasound using handheld or automated technique in women with dense breasts. J. Breast Imaging 2019 ,1, 283\u2013296. [CrossRef] [PubMed] 23. Wang, H.-Y.; Jiang, Y.-X.; Zhu, Q.-L.; Zhang, J.; Dai, Q.; Liu, H.; Lai, X.-J.; Sun, Q. Differentiation of benign and malignant breast lesions: A comparison between automatically generat",
    "full_text_length": 41917,
    "chunk_length": 1432
  },
  {
    "chunk_id": 1119,
    "paper_filename": "Han_2024_breas_cancer_basen_on_BIRADS_catagory.pdf",
    "paper_title": "Han 2024 Breas Cancer Basen On Birads Catagory",
    "chunk_index": 40,
    "total_chunks": 42,
    "text_content": "Han, B.-K.; Kim, E.-K.; Choi, W.J.; Choi, Y.; Kim, H.H.; Moon, W.K. Breast Cancer Detected at Screening US: Survival Rates and Clinical-Pathologic and Imaging Factors Associated with Recurrence. Radiology 2017 ,284, 354\u2013364. [CrossRef] 27. Berg, W.A.; Gilbreath, P .L. Multicentric and multifocal cancer: Whole-breast US in preoperative evaluation. Radiology 2000 ,214, 59\u201366. [CrossRef] Diagnostics 2025 ,15, 212 11 of 11 28. Costantini, M.; Belli, P .; Bufi, E.; Asunis, A.M.; Ferra, E.; Bitti, G.T",
    "full_text_length": 41917,
    "chunk_length": 1452
  },
  {
    "chunk_id": 1120,
    "paper_filename": "Han_2024_breas_cancer_basen_on_BIRADS_catagory.pdf",
    "paper_title": "Han 2024 Breas Cancer Basen On Birads Catagory",
    "chunk_index": 41,
    "total_chunks": 42,
    "text_content": "Arakawa, A.; Yanagisawa, N.; Iijima, K.; Saito, M. Clinicopathological features of breast cancer without mammographic findings suggesting malignancy. Breast 2020 ,54, 335\u2013342. [CrossRef] 32. Park, K.W.; Ko, E.Y.; Park, S.; Han, B.-K.; Choi, J.S.; Kwon, M.-R. Reproducibility of Automated Breast Ultrasonography and Handheld Ultrasonography for Breast Lesion Size Measurement. Ultrasound Q. 2022 ,38, 13\u201317. [CrossRef] Disclaimer/Publisher\u2019s Note: The statements, opinions and data contained in all pu",
    "full_text_length": 41917,
    "chunk_length": 787
  },
  {
    "chunk_id": 1121,
    "paper_filename": "hartsock_2024_vision_language-models_for_reports_generation_visual_quesitons_answeriing.pdf",
    "paper_title": "Hartsock 2024 Vision Language-Models For Reports Generation Visual Quesitons Answeriing",
    "chunk_index": 0,
    "total_chunks": 124,
    "text_content": "TYPEReview PUBLISHED /one.tnum/nine.tnum November /two.tnum/zero.tnum/two.tnum/four.tnum DOI/one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/four.tnum/three.tnum/zero.tnum/nine.tnum/eight.tnum/four.tnum OPENACCESS EDITEDBY Tuan D. Pham, Queen Mary University of London, United Kingdom REVIEWEDBY Geng Chen, Northwestern Polytechnical University, China Kaya Kuru, University of Central Lancashire, United Kingdom Wajahat Akbar, Chan",
    "full_text_length": 160559,
    "chunk_length": 2137
  },
  {
    "chunk_id": 1122,
    "paper_filename": "hartsock_2024_vision_language-models_for_reports_generation_visual_quesitons_answeriing.pdf",
    "paper_title": "Hartsock 2024 Vision Language-Models For Reports Generation Visual Quesitons Answeriing",
    "chunk_index": 1,
    "total_chunks": 124,
    "text_content": "terms.Vision-language models for medical report generation and visual question answering: a review Iryna Hartsock *and Ghulam Rasool Department of Machine Learning, H. Lee Mo\ufb03tt Cancer Center an d Research Institute, Tampa, FL, United States Medical vision-language models (VLMs) combine computer visio n (CV) and natural language processing (NLP) to analyze visual and textual medical data. Our paper reviews recent advancements in developing VLMs special ized for healthcare, focusing on publicly a",
    "full_text_length": 160559,
    "chunk_length": 1787
  },
  {
    "chunk_id": 1123,
    "paper_filename": "hartsock_2024_vision_language-models_for_reports_generation_visual_quesitons_answeriing.pdf",
    "paper_title": "Hartsock 2024 Vision Language-Models For Reports Generation Visual Quesitons Answeriing",
    "chunk_index": 2,
    "total_chunks": 124,
    "text_content": "We also h ighlight currentchallenges facingmedicalVLMdevelopment,includinglimiteddataavailability, concernswithdataprivacy,andlackofproperevaluationmetrics,amongothe rs,whilealso proposingfuturedirectionstoaddresstheseobstacles. Overall, our review summarizes the recent progress in developing VLMs to harness multimodal medical data for improved healthcare applications. KEYWORDS vision-language models, report generation, visual question a nswering, datasets, evaluationmetrics,healthcare /one.tnum",
    "full_text_length": 160559,
    "chunk_length": 2102
  },
  {
    "chunk_id": 1124,
    "paper_filename": "hartsock_2024_vision_language-models_for_reports_generation_visual_quesitons_answeriing.pdf",
    "paper_title": "Hartsock 2024 Vision Language-Models For Reports Generation Visual Quesitons Answeriing",
    "chunk_index": 3,
    "total_chunks": 124,
    "text_content": "focused on digital pathology using histopathology and immunohistochemistry data have also shown signi\ufb01cant advances in accurate disease diagnosis, prognosis, and biomarkeridenti\ufb01cation( Waqasetal.,2023 ,2024a).Ontheotherhand,bytrainingmodels using large datasets of medical literature, clinical notes, and other healthcare-related text, Frontiersin Arti\ufb01cialIntelligence /zero.tnum/one.tnum frontiersin.org Hartsock and Rasool /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two",
    "full_text_length": 160559,
    "chunk_length": 1782
  },
  {
    "chunk_id": 1125,
    "paper_filename": "hartsock_2024_vision_language-models_for_reports_generation_visual_quesitons_answeriing.pdf",
    "paper_title": "Hartsock 2024 Vision Language-Models For Reports Generation Visual Quesitons Answeriing",
    "chunk_index": 4,
    "total_chunks": 124,
    "text_content": "such as race, gender, and others; Acosta et al., 2022 ;Shrestha et al., 2023 ;Waqas et al., 2024b ;Tripathi et al., 2024a ;Mohsan etal.,2023 ;Waqasetal.,2024c ,a;Tripathietal.,2024b ).Inroutine clinical practice, healthcare professionals utilize a combination of these data modalities for diagnosing and treating various conditions. Integrating information from diverse data modalities enhances the precision and thoroughness of disease assessments, diagnoses, treatment planning, and post-treatment ",
    "full_text_length": 160559,
    "chunk_length": 1534
  },
  {
    "chunk_id": 1126,
    "paper_filename": "hartsock_2024_vision_language-models_for_reports_generation_visual_quesitons_answeriing.pdf",
    "paper_title": "Hartsock 2024 Vision Language-Models For Reports Generation Visual Quesitons Answeriing",
    "chunk_index": 5,
    "total_chunks": 124,
    "text_content": "and Vision Assistant; Liu et al., 2023c ), andFlamingo (Alayrac et al., 2022) are tailored to healthcare domain through training on extensive medical datasets. Adapting VLMs for medical visual question-answering (VQA; Lin et al., 2023b ) enables healthcare professionals to query medical images such as CT scans, MRIs, mammograms, ultrasounds, X-rays, and more. The question- answering capability elevates the interactive nature of the AI/ML models in healthcare, facilitating dynamic exchanges betwe",
    "full_text_length": 160559,
    "chunk_length": 1440
  },
  {
    "chunk_id": 1127,
    "paper_filename": "hartsock_2024_vision_language-models_for_reports_generation_visual_quesitons_answeriing.pdf",
    "paper_title": "Hartsock 2024 Vision Language-Models For Reports Generation Visual Quesitons Answeriing",
    "chunk_index": 6,
    "total_chunks": 124,
    "text_content": "VQA. The speci\ufb01c objectives of this review are as follows: \u2022Provide essential background on arti\ufb01cial neural networks, CV, and NLP, to ensure the accessibility of this review for readers from medical \ufb01elds and promote collaboration and knowledge exchange between the AI/ML community and the medicalprofessionals(seeSection2).\u2022Explore the integration of CV and NLP in VLMs, including modelarchitectures,trainingstrategies,anddownstreamtasks (seeSection3). \u2022Analyze recent advances in VLMs, datasets, a",
    "full_text_length": 160559,
    "chunk_length": 1756
  },
  {
    "chunk_id": 1128,
    "paper_filename": "hartsock_2024_vision_language-models_for_reports_generation_visual_quesitons_answeriing.pdf",
    "paper_title": "Hartsock 2024 Vision Language-Models For Reports Generation Visual Quesitons Answeriing",
    "chunk_index": 7,
    "total_chunks": 124,
    "text_content": "Deep learning (DL), a sub\ufb01eld of ML, involves algorithms that learn to recognize patterns and make decisions by analyzing large amounts of data. In this section, we review the fundamental principles of DL and explore two main areas of DL relevant to medicalVLMs:CVandNLP.FormoredetailedinformationonDL, wereferthereaderto LeCunetal.(2015) ,Goodfellowetal.(2016) , andBaldi(2021) . /two.tnum./one.tnum Principles of deep learning MLandAIoriginatedinthe1940\u20131950\u2019s,withneuralnetworks (NNs) emerging as ",
    "full_text_length": 160559,
    "chunk_length": 1912
  },
  {
    "chunk_id": 1129,
    "paper_filename": "hartsock_2024_vision_language-models_for_reports_generation_visual_quesitons_answeriing.pdf",
    "paper_title": "Hartsock 2024 Vision Language-Models For Reports Generation Visual Quesitons Answeriing",
    "chunk_index": 8,
    "total_chunks": 124,
    "text_content": "between predicted and actual outputs, with the goal of minimizing this scalar value during training. DL leverages NNs but extends them into deeper architectures with many hidden layers. Backpropagation, short for Frontiersin Arti\ufb01cialIntelligence /zero.tnum/two.tnum frontiersin.org Hartsock and Rasool /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/four.tnum/three.tnum/zero.tnum/nine.tnum/eight.tnum/four.tnum FIGURE/one.tnum O",
    "full_text_length": 160559,
    "chunk_length": 1695
  },
  {
    "chunk_id": 1130,
    "paper_filename": "hartsock_2024_vision_language-models_for_reports_generation_visual_quesitons_answeriing.pdf",
    "paper_title": "Hartsock 2024 Vision Language-Models For Reports Generation Visual Quesitons Answeriing",
    "chunk_index": 9,
    "total_chunks": 124,
    "text_content": "). These methods iteratively update the weights to improve the model\u2019s performanceduringtraining./two.tnum./two.tnum Natural language processing NLP is the analysis of linguistic data, most commonly in the form of textual data such as documents or publications, using computational methods ( Verspoor and Cohen, 2013 ). NLP encompasses a variety of tasks aimed at understanding, processing, and generating human language. The common NLP tasks include machine translation, named entity recognition, te",
    "full_text_length": 160559,
    "chunk_length": 1702
  },
  {
    "chunk_id": 1131,
    "paper_filename": "hartsock_2024_vision_language-models_for_reports_generation_visual_quesitons_answeriing.pdf",
    "paper_title": "Hartsock 2024 Vision Language-Models For Reports Generation Visual Quesitons Answeriing",
    "chunk_index": 10,
    "total_chunks": 124,
    "text_content": "fundamental property that the future state depends only on the current state and not on the sequence of events that preceded it. This property, known as the Markov property, allowed Markov chainstomodelthelikelihoodofsequencesofwordsorcharacters by capturing statistical dependencies between adjacent elements. They facilitated tasks such as text generation, next-element prediction, and part-of-speech tagging in early NLP research and applications, providing a foundational framework for subsequent",
    "full_text_length": 160559,
    "chunk_length": 1745
  },
  {
    "chunk_id": 1132,
    "paper_filename": "hartsock_2024_vision_language-models_for_reports_generation_visual_quesitons_answeriing.pdf",
    "paper_title": "Hartsock 2024 Vision Language-Models For Reports Generation Visual Quesitons Answeriing",
    "chunk_index": 11,
    "total_chunks": 124,
    "text_content": ") shares similarities with WordPiecebutfollowsamoredeterministicmergingstrategy.BPE merges the most frequent pair of adjacent characters or subword unitsineachiteration,progressingtowardaprede\ufb01nedvocabulary size.Byte-level BPE (Wang et al., 2020 ) operates at an even \ufb01ner granularity,consideringindividualbytesinsteadofcharacters.This extension allows it to capture more nuanced patterns at the byte level. /two.tnum./two.tnum./three.tnum Token embeddings Tokens are often transformed into numerical",
    "full_text_length": 160559,
    "chunk_length": 1719
  },
  {
    "chunk_id": 1133,
    "paper_filename": "hartsock_2024_vision_language-models_for_reports_generation_visual_quesitons_answeriing.pdf",
    "paper_title": "Hartsock 2024 Vision Language-Models For Reports Generation Visual Quesitons Answeriing",
    "chunk_index": 12,
    "total_chunks": 124,
    "text_content": "connections. FastText(Bojanowskiet al., 2017 ) is e\ufb00ective for handling out-of-vocabulary words and morphologically rich languages. It adopts a sub-word approach, breaking words into n-grams, and uses a skip-gram training method similar to Word2Vec to learn embeddings for these sub- wordunits. Specialized embeddings are available for biomedical and clinical terms. BioWordVec (Zhang et al., 2019 ) incorporates MeSH terms and text from PubMed abstracts to learn improved biomedical word embeddings.",
    "full_text_length": 160559,
    "chunk_length": 1877
  },
  {
    "chunk_id": 1134,
    "paper_filename": "hartsock_2024_vision_language-models_for_reports_generation_visual_quesitons_answeriing.pdf",
    "paper_title": "Hartsock 2024 Vision Language-Models For Reports Generation Visual Quesitons Answeriing",
    "chunk_index": 13,
    "total_chunks": 124,
    "text_content": "facilitating the prediction of subsequent layer outputs. This mechanism empowers RNNs to adeptly model sequential and temporal dependencies, capturing information from preceding time steps within hidden states. However,theyfacechallengesinretaininglong-termdependencies duetothevanishinggradientproblem.Toaddressthis,variantslike Long Short-Term Memory (LSTM; Hochreiter and Schmidhuber, 1997)andGatedRecurrentUnit(GRU; Choetal.,2014 )havebeen developedtobettercaptureandutilizelong-rangedependencies",
    "full_text_length": 160559,
    "chunk_length": 2184
  },
  {
    "chunk_id": 1135,
    "paper_filename": "hartsock_2024_vision_language-models_for_reports_generation_visual_quesitons_answeriing.pdf",
    "paper_title": "Hartsock 2024 Vision Language-Models For Reports Generation Visual Quesitons Answeriing",
    "chunk_index": 14,
    "total_chunks": 124,
    "text_content": "has been pivotal in developing large languagemodels(LLMs).Thesemodelsaremeticulouslytrainedon vast datasets with many parameters. BERT (Bidirectional Encoder RepresentationsfromTransformers; Devlinetal.,2019 )markedthe inceptionofLLMs.TheeraofevenlargerLLMsbeganin2020with the introduction of models like GPT-3 (the 3rd generation of the GenerativePre-trainedTransformermodel; Brownetal.,2020 )and Frontiersin Arti\ufb01cialIntelligence /zero.tnum/four.tnum frontiersin.org Hartsock and Rasool /one.tnum/z",
    "full_text_length": 160559,
    "chunk_length": 1891
  },
  {
    "chunk_id": 1136,
    "paper_filename": "hartsock_2024_vision_language-models_for_reports_generation_visual_quesitons_answeriing.pdf",
    "paper_title": "Hartsock 2024 Vision Language-Models For Reports Generation Visual Quesitons Answeriing",
    "chunk_index": 15,
    "total_chunks": 124,
    "text_content": "a signi\ufb01cant advancement in CV ( Yamashita et al., 2018 ). Besides pooling and fully connected layers, CNNs also have convolution layers, which apply convolution operations to input data. A small \ufb01lter or kernel slides over the input data during a convolution operation, performing element-wise multiplicationswithlocalregionsoftheinputateachposition.The results are summed to create a new value in the output feature map. This process is repeated across the entire input, capturing patternsandfeatur",
    "full_text_length": 160559,
    "chunk_length": 1560
  },
  {
    "chunk_id": 1137,
    "paper_filename": "hartsock_2024_vision_language-models_for_reports_generation_visual_quesitons_answeriing.pdf",
    "paper_title": "Hartsock 2024 Vision Language-Models For Reports Generation Visual Quesitons Answeriing",
    "chunk_index": 16,
    "total_chunks": 124,
    "text_content": "layer, enriching the patches for a more expressive representation. Positional encodings are then incorporated to convey spatial arrangement information. ViTs also introduce a special token capturing global image information, represented by a learnable token embedding with unique parameters. ViTs have excelled in semantic segmentation ( Ranftl et al., 2021 ), anomaly detection (Mishra et al., 2021 ), medical image classi\ufb01cation ( Manzari et al., 2023;Barhoumietal.,2023 ),andevenoutperformedCNNsin",
    "full_text_length": 160559,
    "chunk_length": 1677
  },
  {
    "chunk_id": 1138,
    "paper_filename": "hartsock_2024_vision_language-models_for_reports_generation_visual_quesitons_answeriing.pdf",
    "paper_title": "Hartsock 2024 Vision Language-Models For Reports Generation Visual Quesitons Answeriing",
    "chunk_index": 17,
    "total_chunks": 124,
    "text_content": "VisualBERT; Li et al., 2019 and UNITERorUNiversalImage-TExtRepresentationLearning; Chen etal.,2020b ),and(2) dual-stream models(e.g.,ViLBERTorVision- and-Language BERT; Lu et al., 2019 and CLIP or Contrastive Language-ImagePre-training; Radfordetal.,2021 ). Asingle-stream VLM adopts an e\ufb03cient architecture for processing visual and textual information within a uni\ufb01ed module (seeFigure2A andFigure3A ). This architecture incorporates an early fusion of distinct data modalities, concatenating featu",
    "full_text_length": 160559,
    "chunk_length": 1774
  },
  {
    "chunk_id": 1139,
    "paper_filename": "hartsock_2024_vision_language-models_for_reports_generation_visual_quesitons_answeriing.pdf",
    "paper_title": "Hartsock 2024 Vision Language-Models For Reports Generation Visual Quesitons Answeriing",
    "chunk_index": 18,
    "total_chunks": 124,
    "text_content": "(e.g., PubMedCLIP; Eslami et al., 2023 ). These features are then integrated using a multimodal fusion module , often leveraging attentionmechanisms,tocapturecross-modaldependencies. /three.tnum./one.tnum./two.tnum Encoder vs. encoder-decoder VLMs The learned cross-modal representations can be optionally processed by a decoder before producing the \ufb01nal output. Consequently, VLMs are classi\ufb01ed into two groups: (1) encoder-only [e.g., ALIGN (A Large-scale ImaGe and Noisy- text embedding; Jia et al",
    "full_text_length": 160559,
    "chunk_length": 1980
  },
  {
    "chunk_id": 1140,
    "paper_filename": "hartsock_2024_vision_language-models_for_reports_generation_visual_quesitons_answeriing.pdf",
    "paper_title": "Hartsock 2024 Vision Language-Models For Reports Generation Visual Quesitons Answeriing",
    "chunk_index": 19,
    "total_chunks": 124,
    "text_content": "meaningful outputs. However, this versatility comes at the cost of increased computational demand andcomplexity. /three.tnum./two.tnum Model training /three.tnum./two.tnum./one.tnum Transfer learning A widely used strategy in ML is transfer learning, where pre- trained models are customized for speci\ufb01c downstream tasks. This involves \ufb01ne-tuning the model\u2019s parameters using smaller task- speci\ufb01c datasets to address the intricacies of the target task rather than starting with random initialization",
    "full_text_length": 160559,
    "chunk_length": 1483
  },
  {
    "chunk_id": 1141,
    "paper_filename": "hartsock_2024_vision_language-models_for_reports_generation_visual_quesitons_answeriing.pdf",
    "paper_title": "Hartsock 2024 Vision Language-Models For Reports Generation Visual Quesitons Answeriing",
    "chunk_index": 20,
    "total_chunks": 124,
    "text_content": "It strategically presents training examples or tasks in a designed order, often based on di\ufb03culty or complexity measures ( Soviany et al., 2021 ). For instance, LLaVa-Med, a recent medical VLM ( Li et al., 2023a),employscurriculumlearningduringtraining.Thisgradual learning approach starts with simpler examples and progresses to more complex ones, enhancing the model\u2019s adaptability and performance. /three.tnum./two.tnum./three.tnum Self-supervised learning SSL provides a potent alternative to tra",
    "full_text_length": 160559,
    "chunk_length": 1707
  },
  {
    "chunk_id": 1142,
    "paper_filename": "hartsock_2024_vision_language-models_for_reports_generation_visual_quesitons_answeriing.pdf",
    "paper_title": "Hartsock 2024 Vision Language-Models For Reports Generation Visual Quesitons Answeriing",
    "chunk_index": 21,
    "total_chunks": 124,
    "text_content": "common approach involves extensivepre-trainingondatasetspairingimages/videoswiththeir corresponding textual descriptions. Throughout pre-training, the model engages in various tasks to acquire versatile representations for downstream applications. The following paragraphs describe commonlyusedpre-trainingtechniques.Contrastive learning (CL) trains the model to distinguish positive pairs from negative pairs of visual and textual data ( Li et al., 2021 ). Positive pairs contain related visual and ",
    "full_text_length": 160559,
    "chunk_length": 1478
  },
  {
    "chunk_id": 1143,
    "paper_filename": "hartsock_2024_vision_language-models_for_reports_generation_visual_quesitons_answeriing.pdf",
    "paper_title": "Hartsock 2024 Vision Language-Models For Reports Generation Visual Quesitons Answeriing",
    "chunk_index": 22,
    "total_chunks": 124,
    "text_content": "in BERT ( Devlin et al., 2019 ). MLM randomly replaces a percentage of tokens in textual data with a special token, usually denoted as MASK. The model then predicts these masked tokens, considering the context on both sides, enabling it to capture detailed contextual information. VLMs like UNITER (Chen et al., 2020b ) and VisualBERT ( Li et al., 2019 ) utilize MLM duringpre-training. Masked image modeling (MIM) , extending the idea of MLM to images, emerged as a novel approach ( Xie et al., 2022",
    "full_text_length": 160559,
    "chunk_length": 1283
  },
  {
    "chunk_id": 1144,
    "paper_filename": "hartsock_2024_vision_language-models_for_reports_generation_visual_quesitons_answeriing.pdf",
    "paper_title": "Hartsock 2024 Vision Language-Models For Reports Generation Visual Quesitons Answeriing",
    "chunk_index": 23,
    "total_chunks": 124,
    "text_content": "The cross-entropy loss is employed in MLM and MIM tasks to measure the di\ufb00erence between predicted and actual probability distributions for the maskedelements.Additionally,MLMcanbecombinedwithMIM, allowing the reconstruction of the masked signal in one modality withsupportfromanothermodality( Kwonetal.,2023 ). Image-text matching (ITM) is another common vision- language pre-training task. Throughout the training, the model learns to map images and corresponding textual descriptions into a shared",
    "full_text_length": 160559,
    "chunk_length": 1573
  },
  {
    "chunk_id": 1145,
    "paper_filename": "hartsock_2024_vision_language-models_for_reports_generation_visual_quesitons_answeriing.pdf",
    "paper_title": "Hartsock 2024 Vision Language-Models For Reports Generation Visual Quesitons Answeriing",
    "chunk_index": 24,
    "total_chunks": 124,
    "text_content": "In VLM pre-training, multiple tasks are often combined to enable models to understand nuanced contextual information across modalities. Tasks like contrastive loss, cross-entropy loss for masked token prediction, and others can be integrated into the \ufb01nal loss function. This approach equips VLMs with versatile representationsfordiversedownstreamtasks.Forinstance,ALBEF (Lietal.,2021 )adoptsapre-trainingobjectiveinvolvingCL,MLM, and ITM tasks, with the overall loss computed as the sum of thesecomp",
    "full_text_length": 160559,
    "chunk_length": 1803
  },
  {
    "chunk_id": 1146,
    "paper_filename": "hartsock_2024_vision_language-models_for_reports_generation_visual_quesitons_answeriing.pdf",
    "paper_title": "Hartsock 2024 Vision Language-Models For Reports Generation Visual Quesitons Answeriing",
    "chunk_index": 25,
    "total_chunks": 124,
    "text_content": "these blocks. /three.tnum./two.tnum./five.tnum Fine-tuning techniques Followingthetraining,acommonpracticeinvolves\ufb01ne-tuning VLMsonsmallerdatasetstailoredtospeci\ufb01cdownstreamtasks.In the following, we present well-known techniques for \ufb01ne-tuning VLMs. Supervised \ufb01ne-tuning (SFT) involves meticulous \ufb01ne- tuning of a model on a dataset curated to match the nuances of the targeted application. However, before engaging in SFT, the VLM undergoes pre-training on an extensive image- text dataset to esta",
    "full_text_length": 160559,
    "chunk_length": 1726
  },
  {
    "chunk_id": 1147,
    "paper_filename": "hartsock_2024_vision_language-models_for_reports_generation_visual_quesitons_answeriing.pdf",
    "paper_title": "Hartsock 2024 Vision Language-Models For Reports Generation Visual Quesitons Answeriing",
    "chunk_index": 26,
    "total_chunks": 124,
    "text_content": "methodology, allowing for \ufb01ne- tuninginalignmentwithhumanpreferences,ultimatelyimproving modeloutcomes.Instruction \ufb01ne-tuning (IFT) refers to re\ufb01ning a pre-trained language model by providing speci\ufb01c instructions or guidance tailored to a particular task or application ( Ren et al., 2024 ). This process typically involves exposing the model to examples or prompts related to the desired instructions and updating its parametersbasedonthefeedbackreceivedduringthistask-speci\ufb01c training phase. Medica",
    "full_text_length": 160559,
    "chunk_length": 2011
  },
  {
    "chunk_id": 1148,
    "paper_filename": "hartsock_2024_vision_language-models_for_reports_generation_visual_quesitons_answeriing.pdf",
    "paper_title": "Hartsock 2024 Vision Language-Models For Reports Generation Visual Quesitons Answeriing",
    "chunk_index": 27,
    "total_chunks": 124,
    "text_content": "parameters, enabling Frontiersin Arti\ufb01cialIntelligence /zero.tnum/seven.tnum frontiersin.org Hartsock and Rasool /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/four.tnum/three.tnum/zero.tnum/nine.tnum/eight.tnum/four.tnum FIGURE/three.tnum Comparison of (A)single-stream and (B)dual-stream VLMs in terms of their advantages, disadvantages , and healthcare applications, to guide the selection of the appropriate architecture for ",
    "full_text_length": 160559,
    "chunk_length": 1860
  },
  {
    "chunk_id": 1149,
    "paper_filename": "hartsock_2024_vision_language-models_for_reports_generation_visual_quesitons_answeriing.pdf",
    "paper_title": "Hartsock 2024 Vision Language-Models For Reports Generation Visual Quesitons Answeriing",
    "chunk_index": 28,
    "total_chunks": 124,
    "text_content": "involves creating continuous vector representations as input hints ( Lester et al., 2021 ), enabling themodeltodynamicallycreatee\ufb00ectivepromptsduringtraining. This iterative process signi\ufb01cantly enhances the model\u2019s ability to generate contextually relevant responses and adapt its behavior based on an evolving task. VLMs like Qwen-VL and InstructBLIP usedprompttuning( Baietal.,2023a ;Daietal.,2023 )./three.tnum./three.tnum./three.tnum Pre\ufb01x token tuning Pre\ufb01x token tuning adds task-speci\ufb01c vecto",
    "full_text_length": 160559,
    "chunk_length": 2122
  },
  {
    "chunk_id": 1150,
    "paper_filename": "hartsock_2024_vision_language-models_for_reports_generation_visual_quesitons_answeriing.pdf",
    "paper_title": "Hartsock 2024 Vision Language-Models For Reports Generation Visual Quesitons Answeriing",
    "chunk_index": 29,
    "total_chunks": 124,
    "text_content": "for a speci\ufb01c image Frontiersin Arti\ufb01cialIntelligence /zero.tnum/eight.tnum frontiersin.org Hartsock and Rasool /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/four.tnum/three.tnum/zero.tnum/nine.tnum/eight.tnum/four.tnum (e.g., RAMM; Pellegrini et al., 2023 ). Prompt engineering can also expose the VLM to interconnected examples or prompts, guiding ittoadesiredoutput.Anotherapproachincorporatesprogressively structured instruc",
    "full_text_length": 160559,
    "chunk_length": 1759
  },
  {
    "chunk_id": 1151,
    "paper_filename": "hartsock_2024_vision_language-models_for_reports_generation_visual_quesitons_answeriing.pdf",
    "paper_title": "Hartsock 2024 Vision Language-Models For Reports Generation Visual Quesitons Answeriing",
    "chunk_index": 30,
    "total_chunks": 124,
    "text_content": "designed for information retrieval. Thisversatilecomponentexcelsinextractingpertinentinformation from extensive datasets, catering to various modalities such as images, text, codes, video, or audio when presented with diverse inputs (Zhao etal.,2023 ).Following theretrieval phase,themodel returns a set of contexts related to the given input. The second component is a generative LLM. This component takes the input and the retrieved context and generates the \ufb01nal output. The generated output is co",
    "full_text_length": 160559,
    "chunk_length": 1582
  },
  {
    "chunk_id": 1152,
    "paper_filename": "hartsock_2024_vision_language-models_for_reports_generation_visual_quesitons_answeriing.pdf",
    "paper_title": "Hartsock 2024 Vision Language-Models For Reports Generation Visual Quesitons Answeriing",
    "chunk_index": 31,
    "total_chunks": 124,
    "text_content": "results and reducing the workload of report writing (Monshi et al., 2020 ;Ting et al., 2023 ;Mohsan et al., 2023). For instance, in radiology, a report generation system could analyze a set of medical images such as X-rays, CT scans, or MRIs and generate a detailed report summarizing the observed abnormalities, their locations, and potential implications for diagnosis or treatment ( Liu et al., 2023b ). A radiology report usually has several sections: (1) Examination (type of exam), (2) Indicati",
    "full_text_length": 160559,
    "chunk_length": 1503
  },
  {
    "chunk_id": 1153,
    "paper_filename": "hartsock_2024_vision_language-models_for_reports_generation_visual_quesitons_answeriing.pdf",
    "paper_title": "Hartsock 2024 Vision Language-Models For Reports Generation Visual Quesitons Answeriing",
    "chunk_index": 32,
    "total_chunks": 124,
    "text_content": "decision-making. Handwriting and telephone dictation are particularly vulnerable to mistakes, as they can su\ufb00er from issues like illegible handwriting and miscommunication, leading to misinterpretations. Structured data entry, although designed to standardize and streamline reporting, often places a signi\ufb01cant cognitive burden on radiologists, who must meticulously input detailed information, potentially leading to fatigue and errors. While technological advancements like electronic health recor",
    "full_text_length": 160559,
    "chunk_length": 1602
  },
  {
    "chunk_id": 1154,
    "paper_filename": "hartsock_2024_vision_language-models_for_reports_generation_visual_quesitons_answeriing.pdf",
    "paper_title": "Hartsock 2024 Vision Language-Models For Reports Generation Visual Quesitons Answeriing",
    "chunk_index": 33,
    "total_chunks": 124,
    "text_content": "The evolution of RG methods parallels the advancements in image captioning. Early methods in image captioning included retrieval-based approaches, where captions were generated by retrieving existing phrases from a database, and template-based approaches, where prede\ufb01ned sentence templates were \ufb01lled with identi\ufb01ed image elements, such as objects, actions, or locations (Bai and An, 2018 ). However, these approaches struggled with generating captions for unseen images. This limitation motivated t",
    "full_text_length": 160559,
    "chunk_length": 1544
  },
  {
    "chunk_id": 1155,
    "paper_filename": "hartsock_2024_vision_language-models_for_reports_generation_visual_quesitons_answeriing.pdf",
    "paper_title": "Hartsock 2024 Vision Language-Models For Reports Generation Visual Quesitons Answeriing",
    "chunk_index": 34,
    "total_chunks": 124,
    "text_content": "are able to generate more comprehensive and coherent reports. They also signi\ufb01cantly reduce cognitive load by automating the creation of comprehensive reports, thereby liberating clinicians from the repetitive and time-consuming task of manual report writing. Furthermore, VLMs provide consistent interpretations of imaging data, which helps minimize the risk of errors associated with clinician fatigue or oversight. Their capability to process large volumes of data e\ufb03ciently streamlines the report",
    "full_text_length": 160559,
    "chunk_length": 1863
  },
  {
    "chunk_id": 1156,
    "paper_filename": "hartsock_2024_vision_language-models_for_reports_generation_visual_quesitons_answeriing.pdf",
    "paper_title": "Hartsock 2024 Vision Language-Models For Reports Generation Visual Quesitons Answeriing",
    "chunk_index": 35,
    "total_chunks": 124,
    "text_content": "objects, their locations, or distinctive properties within the image. Inthemedicalcontext( Linetal.,2023b ),thismayinvolvequestions regardingthepresenceofmedicalconditionsorabnormalities,such as \u201cWhat abnormality is seen in the image?\u201d ( Ionescu et al., 2021 ) or \u201cIs there gastric fullness?\u201d ( Lau et al., 2018 ). Other queries may delve into details like the imaging method used ( Abacha et al., 2019),theorgansysteminvolved( Lauetal.,2018 ),orthepresence ofspeci\ufb01canatomicalstructures( Liuetal.,20",
    "full_text_length": 160559,
    "chunk_length": 1524
  },
  {
    "chunk_id": 1157,
    "paper_filename": "hartsock_2024_vision_language-models_for_reports_generation_visual_quesitons_answeriing.pdf",
    "paper_title": "Hartsock 2024 Vision Language-Models For Reports Generation Visual Quesitons Answeriing",
    "chunk_index": 36,
    "total_chunks": 124,
    "text_content": "the generation task, models produce free-form textual responses unconstrained by prede\ufb01ned options. /three.tnum./five.tnum./three.tnum Other tasks Beyond VQA and RG, a spectrum of VLM tasks exist for the vision-language understanding ( Chen et al., 2023 ). For instance, referring expression comprehension entails a model locating the speci\ufb01careaorobjectinanimagethatthegivenphraseorsentence refers to ( Zhang et al., 2018 ).Visual commonsense reasoning involves answering questions about an image, t",
    "full_text_length": 160559,
    "chunk_length": 1590
  },
  {
    "chunk_id": 1158,
    "paper_filename": "hartsock_2024_vision_language-models_for_reports_generation_visual_quesitons_answeriing.pdf",
    "paper_title": "Hartsock 2024 Vision Language-Models For Reports Generation Visual Quesitons Answeriing",
    "chunk_index": 37,
    "total_chunks": 124,
    "text_content": "Medical datasets for VLMs The adaptation of VLMs to various medical tasks is achieved through their pre-training and \ufb01ne-tuning using specialized task- speci\ufb01c datasets. Below is the list of vision-language datasetsavailable in the public domain that contain medical image-text pairs or question-answer (QA) pairs. Most of them are employed by medical VLMs described in Section 4.3 for pre-training, \ufb01ne- tuning, and evaluating VQA and RG tasks. The comparative analysis of these datasets is presente",
    "full_text_length": 160559,
    "chunk_length": 1642
  },
  {
    "chunk_id": 1159,
    "paper_filename": "hartsock_2024_vision_language-models_for_reports_generation_visual_quesitons_answeriing.pdf",
    "paper_title": "Hartsock 2024 Vision Language-Models For Reports Generation Visual Quesitons Answeriing",
    "chunk_index": 38,
    "total_chunks": 124,
    "text_content": "is strati\ufb01ed into two categories: radiology and out-of-class. The radiology group includes 81,825 radiology images, including CT, ultrasound, x-ray, \ufb02uoroscopy, positron emission tomography (PET), mammography, MRI, angiography, and PET-CT. The out-of-class group has 6,127 images, including synthetic radiology images, clinical photos, portraits, compound radiology images, and digital art. To facilitate model training, the dataset is randomly split into a training set (65,460 radiology and 4,902 o",
    "full_text_length": 160559,
    "chunk_length": 1722
  },
  {
    "chunk_id": 1160,
    "paper_filename": "hartsock_2024_vision_language-models_for_reports_generation_visual_quesitons_answeriing.pdf",
    "paper_title": "Hartsock 2024 Vision Language-Models For Reports Generation Visual Quesitons Answeriing",
    "chunk_index": 39,
    "total_chunks": 124,
    "text_content": "the MIMIC-CXR dataset ( Johnson et al., 2019a ). In this version,theoriginal377,110imagesareconvertedintocompressed JPG format. The 227,827 reports associated with these images are enriched with labels for various common pathologies. The labels are derived from the analysis of the impression, \ufb01ndings, or \ufb01nal sections of the radiology reports, facilitated by the use of NegBio (Peng et al., 2017 ) and CheXpert (Chest eXpert; Irvin et al., 2019 ) tools. /four.tnum./one.tnum./four.tnum MIMIC-NLE MI",
    "full_text_length": 160559,
    "chunk_length": 2060
  },
  {
    "chunk_id": 1161,
    "paper_filename": "hartsock_2024_vision_language-models_for_reports_generation_visual_quesitons_answeriing.pdf",
    "paper_title": "Hartsock 2024 Vision Language-Models For Reports Generation Visual Quesitons Answeriing",
    "chunk_index": 40,
    "total_chunks": 124,
    "text_content": "annotations;inlinereferencestoROCO \ufb01guresGH PMC-OA Linetal.(2023a) 1,650,000 \u2013 \u2013 HF SLAKELiuetal.(2021a) \u2013 14,028 642annotatedimages,5,232medical tripletsWeb VQA-RAD Lauetal.(2018) \u2013 3,515 315radiologyimages Web PathVQA Heetal.(2020) \u2013 32,799 4,998pathologyimages GH VQA-Med2019 Abachaetal.(2019) \u2013 15,292 4,200radiologyimages GH VQA-Med2020 Abachaetal.(2020) \u2013 5,000 5,000radiologyimagesforVQA; imagesandquestionsforVQGGH VQA-Med2021 Ionescuetal.(2021) \u2013 5,500 5,500radiologyimagesforVQA; imagesandq",
    "full_text_length": 160559,
    "chunk_length": 2046
  },
  {
    "chunk_id": 1162,
    "paper_filename": "hartsock_2024_vision_language-models_for_reports_generation_visual_quesitons_answeriing.pdf",
    "paper_title": "Hartsock 2024 Vision Language-Models For Reports Generation Visual Quesitons Answeriing",
    "chunk_index": 41,
    "total_chunks": 124,
    "text_content": "training set with 37,016 images, a test setwith273images,andavalidationsetwith714images. /four.tnum./one.tnum./five.tnum CXR with prior references omitted CXR-PRO dataset is derived from MIMIC-CXR ( Johnson et al., 2019a ). The dataset consists of 374,139 free-text radiology reports containing only the impression sections ( Ramesh et al., 2022). It also incorporates associated chest radiographs; however, the radiology reports and chest X-rays are not paired. This dataset is designed to mitigate ",
    "full_text_length": 160559,
    "chunk_length": 1981
  },
  {
    "chunk_id": 1163,
    "paper_filename": "hartsock_2024_vision_language-models_for_reports_generation_visual_quesitons_answeriing.pdf",
    "paper_title": "Hartsock 2024 Vision Language-Models For Reports Generation Visual Quesitons Answeriing",
    "chunk_index": 42,
    "total_chunks": 124,
    "text_content": "and RadLex (Radiology Lexicon) codes to represent clinical \ufb01ndings and diagnoses.Throughoutthisreview, wewillreferto thedataset interchangeably as IU-XrayandOpen-I, maintaining consistency withthenomenclatureusedinrelatedliterature. /four.tnum./one.tnum./seven.tnum Medical images, captions, and textual references MedICaTdatasetcontains217,060\ufb01guresfrom131,410open- accessPMCpapersfocusedonradiologyimagesandothermedical Frontiersin Arti\ufb01cialIntelligence /one.tnum/one.tnum frontiersin.org Hartsock ",
    "full_text_length": 160559,
    "chunk_length": 2043
  },
  {
    "chunk_id": 1164,
    "paper_filename": "hartsock_2024_vision_language-models_for_reports_generation_visual_quesitons_answeriing.pdf",
    "paper_title": "Hartsock 2024 Vision Language-Models For Reports Generation Visual Quesitons Answeriing",
    "chunk_index": 43,
    "total_chunks": 124,
    "text_content": "spectrum of diseases, with induced cataracts, ear diseases, and low visionbeingamongthemostfrequentlyrepresentedconditions. /four.tnum./one.tnum./nine.tnum MS-CXR MS-CXR dataset contains image bounding box labels paired with radiology \ufb01ndings, annotated and veri\ufb01ed by two board- certi\ufb01edradiologists( Boeckingetal.,2022 ).Thedatasetconsistsof 1,162 image-text pairs of bounding boxes and corresponding text descriptions. The annotations cover 8 di\ufb00erent cardiopulmonary radiological\ufb01ndingsandareextr",
    "full_text_length": 160559,
    "chunk_length": 2089
  },
  {
    "chunk_id": 1165,
    "paper_filename": "hartsock_2024_vision_language-models_for_reports_generation_visual_quesitons_answeriing.pdf",
    "paper_title": "Hartsock 2024 Vision Language-Models For Reports Generation Visual Quesitons Answeriing",
    "chunk_index": 44,
    "total_chunks": 124,
    "text_content": "single-slice CTs or MRIs, 107 chest x-rays, and 104 abdominal axial CTs ( Lau et al., 2018 ). The images are meticulously chosen from MedPix, an open-access online medical image database, ensuring each imagecorrespondstoauniquepatient.Furthermore,everyselected image has an associated caption and is deliberately devoid of any radiology markings. Every caption provides details about theimaging plane, modality, and \ufb01ndings generated and reviewed by expertradiologists.Also,VQA-RADcontains3,515QApair",
    "full_text_length": 160559,
    "chunk_length": 1612
  },
  {
    "chunk_id": 1166,
    "paper_filename": "hartsock_2024_vision_language-models_for_reports_generation_visual_quesitons_answeriing.pdf",
    "paper_title": "Hartsock 2024 Vision Language-Models For Reports Generation Visual Quesitons Answeriing",
    "chunk_index": 45,
    "total_chunks": 124,
    "text_content": "and the digital library \u201cPathology Education Informational Resource\u201d. Out of all QA pairs, 16,465 are of the open-ended type, while the remaining pairs are of the closed-ended yes/no type. On average, each image is associated with 6.6 questions, which cover a broad spectrum of visual contents, encompassing aspects such as color, location,appearance,shape,etc. /four.tnum./one.tnum./one.tnum/three.tnum VQA-Med /two.tnum/zero.tnum/one.tnum/nine.tnum VQA-Med 2019 dataset contains 4,200 radiology ima",
    "full_text_length": 160559,
    "chunk_length": 1547
  },
  {
    "chunk_id": 1167,
    "paper_filename": "hartsock_2024_vision_language-models_for_reports_generation_visual_quesitons_answeriing.pdf",
    "paper_title": "Hartsock 2024 Vision Language-Models For Reports Generation Visual Quesitons Answeriing",
    "chunk_index": 46,
    "total_chunks": 124,
    "text_content": "set comprises 500 images and 500 QA pairs, and the test set includes 500 images and 500 QA pairs. The questions are focused on abnormalities present in the images. Additionally, the dataset contains radiology images and questions for the Visual Question Generation (VQG) task. The training set consists of 780 images and 2,156 associated questions. The validation set comprises 141 imageswith164questions,andthetestsetincludes80images. /four.tnum./one.tnum./one.tnum/five.tnum VQA-Med /two.tnum/zero.",
    "full_text_length": 160559,
    "chunk_length": 1667
  },
  {
    "chunk_id": 1168,
    "paper_filename": "hartsock_2024_vision_language-models_for_reports_generation_visual_quesitons_answeriing.pdf",
    "paper_title": "Hartsock 2024 Vision Language-Models For Reports Generation Visual Quesitons Answeriing",
    "chunk_index": 47,
    "total_chunks": 124,
    "text_content": "and questions for the VQG task. The validation set comprises 85 images with 200 questions,andthetestsetincludes100images. /four.tnum./one.tnum./one.tnum/six.tnum Endoscopic vision /two.tnum/zero.tnum/one.tnum/seven.tnum EndoVis 2017 dataset contains 5 robotic surgery videos (two videos with 8 frames each, one with 18, one with 14, and one with 39 frames) from the MICCAI (Medical Image Computing and Computer Assisted Interventions) Endoscopic Vision 2017 Challenge ( Allan et al., 2019 ). It also ",
    "full_text_length": 160559,
    "chunk_length": 1871
  },
  {
    "chunk_id": 1169,
    "paper_filename": "hartsock_2024_vision_language-models_for_reports_generation_visual_quesitons_answeriing.pdf",
    "paper_title": "Hartsock 2024 Vision Language-Models For Reports Generation Visual Quesitons Answeriing",
    "chunk_index": 48,
    "total_chunks": 124,
    "text_content": "52 regions of interest (ROIs) hand-selected by a board-certi\ufb01ed pathologist from whole slide images (WSIs) in the publicly available The Cancer Genome Atlas (TCGA)repository.Theseimagesrepresentvariousorgansystems: brain, lung, gastrointestinal tract, urinary tract, male reproductive tract, skin/eye/connective tissue, pancreaticohepatobiliary system, endocrine system, head/neck/mediastinum, gynecology, and breast. Per each organ system there are from 4 to 6 images. Each image is paired with a co",
    "full_text_length": 160559,
    "chunk_length": 1988
  },
  {
    "chunk_id": 1170,
    "paper_filename": "hartsock_2024_vision_language-models_for_reports_generation_visual_quesitons_answeriing.pdf",
    "paper_title": "Hartsock 2024 Vision Language-Models For Reports Generation Visual Quesitons Answeriing",
    "chunk_index": 49,
    "total_chunks": 124,
    "text_content": "translation evaluation, but it has been adaptedforRGandevenVQAinamodi\ufb01edform.BLEUprovidesa quantitativemeasureofhowwellthemachine-generatedtextaligns with human-generated reference text ( Papineni et al., 2002 ). First, theprecisionofdi\ufb00erent n-grams,whichareconsecutivesequences ofnwords,iscalculatedusingtheformula: Precision( n)=#overlappingn-grams #alln-gramsinamodel-generatedtext, (1) where \u201coverlapping n-grams\u201d refer to n-grams in the model- generated text that share common elements with at ",
    "full_text_length": 160559,
    "chunk_length": 2008
  },
  {
    "chunk_id": 1171,
    "paper_filename": "hartsock_2024_vision_language-models_for_reports_generation_visual_quesitons_answeriing.pdf",
    "paper_title": "Hartsock 2024 Vision Language-Models For Reports Generation Visual Quesitons Answeriing",
    "chunk_index": 50,
    "total_chunks": 124,
    "text_content": "for Gisting Evaluation (ROUGE) is a set of metrics that evaluate the overlap between the model-generated text and human-generated reference text (Lin, 2004 ). ROUGE-n assesses the overlap of n-grams between model-generatedtextandreferencetext,anditisde\ufb01nedas: ROUGE-n =#overlappingn-grams #alln-gramsinareferencetext. (4) ROUGE-L focuses on measuring the longest common subsequencebetweenmodel-generatedtext Yandreferencetext X, anditiscalculatedusingthefollowingrelationship: ROUGE-L =(1+\u03b22)\u00d7R\u00d7P (R+",
    "full_text_length": 160559,
    "chunk_length": 1988
  },
  {
    "chunk_id": 1172,
    "paper_filename": "hartsock_2024_vision_language-models_for_reports_generation_visual_quesitons_answeriing.pdf",
    "paper_title": "Hartsock 2024 Vision Language-Models For Reports Generation Visual Quesitons Answeriing",
    "chunk_index": 51,
    "total_chunks": 124,
    "text_content": "text ( Banerjee and Lavie, 2005 ). The METEORscoreiscomputedasfollows: METEOR =10\u00d7P\u00d7R R+9\u00d7P(1\u2212Penalty) (6) where R=#overlapping1-grams #1-gramsinareferencetext, (7) P=#overlapping1-grams #1-gramsinamodel-generatedtext, (8) Penalty=1 2\u00d7/parenleftbigg#chunks #overlapping1-grams/parenrightbigg3 , (9) andchunksaregroupsofadjacent1-gramsinthemodel-generated text that overlap with adjacent 1-grams in the reference text. The METEOR score ranges from 0 to 1, with higher scores indicating better alignmen",
    "full_text_length": 160559,
    "chunk_length": 1603
  },
  {
    "chunk_id": 1173,
    "paper_filename": "hartsock_2024_vision_language-models_for_reports_generation_visual_quesitons_answeriing.pdf",
    "paper_title": "Hartsock 2024 Vision Language-Models For Reports Generation Visual Quesitons Answeriing",
    "chunk_index": 52,
    "total_chunks": 124,
    "text_content": "). However, it can also leverage other word embeddings to evaluate the similarity between model-generated and reference text. The BERTScore of a single text pair is calculated according to the relationship: BERTScore =2\u00d7P\u00d7R P+R, (11) wherePrepresents the ratio of the maximum cosine similarity scorebetweentokensinthemodel-generatedtextandthereference text to the numbers of tokens in the model-generated text and Rrepresents the ratio of the maximum cosine similarity score between tokens in the mod",
    "full_text_length": 160559,
    "chunk_length": 1440
  },
  {
    "chunk_id": 1174,
    "paper_filename": "hartsock_2024_vision_language-models_for_reports_generation_visual_quesitons_answeriing.pdf",
    "paper_title": "Hartsock 2024 Vision Language-Models For Reports Generation Visual Quesitons Answeriing",
    "chunk_index": 53,
    "total_chunks": 124,
    "text_content": "Third, the number of edges that match between the two graphs based on their start and end entities and labels (relation type) is calculated. Lastly, the F1 score is separately computed for clinical entities and relations, and then the RadGraph F1 score for a report pair is the average of these two scores. The overall model performance is determined by averaging RadGraph F1 scores across all report pairs. Humanevaluation iscrucialforassessingthequalityofVLMs in medical RG. In Jeong et al. (2023) ",
    "full_text_length": 160559,
    "chunk_length": 1767
  },
  {
    "chunk_id": 1175,
    "paper_filename": "hartsock_2024_vision_language-models_for_reports_generation_visual_quesitons_answeriing.pdf",
    "paper_title": "Hartsock 2024 Vision Language-Models For Reports Generation Visual Quesitons Answeriing",
    "chunk_index": 54,
    "total_chunks": 124,
    "text_content": "negative. The followingmetricsarealsocalledclinicale\ufb03cacymetrics. \u2022Accuracy measures the ratio of all positive predictions to the totalnumberofpredictions. \u2022Precision evaluates the accuracy of positive predictions. It is calculated as the ratio of true positive predictions to the total instancespredictedaspositive,expressedas: Precision =TruePositives TruePositives+FalsePositives. (12) HighPrecisionindicatesalowfalsepositiverate. \u2022Recallassessesthemodel\u2019sabilitytopredictallpositiveclasses. It is",
    "full_text_length": 160559,
    "chunk_length": 1941
  },
  {
    "chunk_id": 1176,
    "paper_filename": "hartsock_2024_vision_language-models_for_reports_generation_visual_quesitons_answeriing.pdf",
    "paper_title": "Hartsock 2024 Vision Language-Models For Reports Generation Visual Quesitons Answeriing",
    "chunk_index": 55,
    "total_chunks": 124,
    "text_content": "and Frontiersin Arti\ufb01cialIntelligence /one.tnum/four.tnum frontiersin.org Hartsock and Rasool /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/four.tnum/three.tnum/zero.tnum/nine.tnum/eight.tnum/four.tnum PathVQA ( He et al., 2020 ). While various metrics are available for VQA evaluation, only a few are highlighted here to avoid redundancywithalreadymentionedmetrics. Accuracy is a fundamental metric for gauging overall model co",
    "full_text_length": 160559,
    "chunk_length": 1758
  },
  {
    "chunk_id": 1177,
    "paper_filename": "hartsock_2024_vision_language-models_for_reports_generation_visual_quesitons_answeriing.pdf",
    "paper_title": "Hartsock 2024 Vision Language-Models For Reports Generation Visual Quesitons Answeriing",
    "chunk_index": 56,
    "total_chunks": 124,
    "text_content": "paper, we provide an overview of existing medical VLMs tailored for VQA and/or RG. The information is organized chronologically based on the \ufb01rst appearance of the model. Our focus is mainly on recently introducedopen-sourceandpubliclyavailablemodels.Asummary oftheseVLMsispresentedin Table2. /four.tnum./three.tnum./one.tnum Medical vision language learner MedViLL can process medical images to generate associated reports (Moon et al., 2022 ). The model employs ResNet-50 ( He etal.,2016 ),trainedo",
    "full_text_length": 160559,
    "chunk_length": 1565
  },
  {
    "chunk_id": 1178,
    "paper_filename": "hartsock_2024_vision_language-models_for_reports_generation_visual_quesitons_answeriing.pdf",
    "paper_title": "Hartsock 2024 Vision Language-Models For Reports Generation Visual Quesitons Answeriing",
    "chunk_index": 57,
    "total_chunks": 124,
    "text_content": "performed on 89,395 image-report pairs from MIMIC-CXR ( Johnson et al., 2019a), with \ufb01ne-tuning on 3,547 pairs from Open-I ( Demner- Fushmanetal.,2015 ).VQAisperformedonVQA-RAD( Lauetal., 2018) (seeTable3), where the output representation of [CLS] is used to predict a one-hot encoded answer. For radiology RG \ufb01ne- tuning,themodelusesasequence-to-sequence(S2S)maskinstead of BAR and generates reports by sequentially recovering MASK tokens. RG is evaluated on MIMIC-CXR ( Johnson et al., 2019a ) and ",
    "full_text_length": 160559,
    "chunk_length": 1357
  },
  {
    "chunk_id": 1179,
    "paper_filename": "hartsock_2024_vision_language-models_for_reports_generation_visual_quesitons_answeriing.pdf",
    "paper_title": "Hartsock 2024 Vision Language-Models For Reports Generation Visual Quesitons Answeriing",
    "chunk_index": 58,
    "total_chunks": 124,
    "text_content": "( Pelka et al., 2018 ) dataset ( Eslami et al., 2023 ). It employs a CLIP text encoder based on the Transformer architecture and three distinct visual encoders: ViT- B/32 (Dosovitskiy et al., 2021 ), ResNet-50, and ResNet-50 \u00d74 (He et al., 2016 ). Following CLIP\u2019s approach, the model generates joint representations by computing cosine similarity between textual and visual features. The pre-training objective involves computing cross-entropy losses for vision and language, which are then averaged",
    "full_text_length": 160559,
    "chunk_length": 1286
  },
  {
    "chunk_id": 1180,
    "paper_filename": "hartsock_2024_vision_language-models_for_reports_generation_visual_quesitons_answeriing.pdf",
    "paper_title": "Hartsock 2024 Vision Language-Models For Reports Generation Visual Quesitons Answeriing",
    "chunk_index": 59,
    "total_chunks": 124,
    "text_content": "resulting representations are classi\ufb01ed using a two-layer feedforward neural network. The VQA loss combines classi\ufb01cation and image reconstruction losses. PubMedCLIP is \ufb01ne-tuned on datasets like SLAKE ( Liu et al., 2021a) and VQA-RAD ( Lau et al., 2018 ). Its performance is compared with existing Medical VQA (MedVQA) methods, such as Mixture of Enhanced Visual Features (MEVF; Zhan et al., 2020 ) and question-conditioned reasoning (QCR; Liu et al., 2023a ). PubMedCLIP, integrated into the QCR fr",
    "full_text_length": 160559,
    "chunk_length": 1496
  },
  {
    "chunk_id": 1181,
    "paper_filename": "hartsock_2024_vision_language-models_for_reports_generation_visual_quesitons_answeriing.pdf",
    "paper_title": "Hartsock 2024 Vision Language-Models For Reports Generation Visual Quesitons Answeriing",
    "chunk_index": 60,
    "total_chunks": 124,
    "text_content": "( Wu et al., 2016 ). Fusion of image and question features is achieved using BAN ( Kim et al., 2018).Toalignimageswithtextualdescriptions,themodelemploys bidirectionalcontrastivelearning( Chenetal.,2020a ).Thelanguage decoder,based onGPT-2, isadapted to incorporate image features and prior context, generating text sequences in an auto-regressive manner until an end-of-sequence token is produced. The overall loss function combines contrastive loss for encoding phase and cross-entropy loss for dec",
    "full_text_length": 160559,
    "chunk_length": 2184
  },
  {
    "chunk_id": 1182,
    "paper_filename": "hartsock_2024_vision_language-models_for_reports_generation_visual_quesitons_answeriing.pdf",
    "paper_title": "Hartsock 2024 Vision Language-Models For Reports Generation Visual Quesitons Answeriing",
    "chunk_index": 61,
    "total_chunks": 124,
    "text_content": "ViT-B/16+PubMedBERT+METER + \u2013 PMC-15,SLAKE,VQA-RAD HF UniXGen Leeetal.(2023)single Yes VQGAN+Transformer \u2013 + MIMIC-CXR GH RAMM Yuanetal.(2023)dual No SwissTransformer+PubMedBERT+ multimodalencoderw/retrieval-atten. module+ \u2013 PMCPM,ROCOMIMIC-CXR, SLAKE,VQA-RAD,VQA-Med2019, VQA-Med2021GH X-REM Jeongetal.(2023)dual No ALBEF(ViT-B/16+BERT+ multimodalencoder)\u2013 + MIMIC-CXR,MedNLI,RadNLI GH VisualMed-Alpaca Shuetal.(2023)single Yes DePlotorMed-GIT+promptmanager +LLaMa-7B+ \u2013 ROCO;MedDialog,MEDIQAQA, MED",
    "full_text_length": 160559,
    "chunk_length": 2273
  },
  {
    "chunk_id": 1183,
    "paper_filename": "hartsock_2024_vision_language-models_for_reports_generation_visual_quesitons_answeriing.pdf",
    "paper_title": "Hartsock 2024 Vision Language-Models For Reports Generation Visual Quesitons Answeriing",
    "chunk_index": 62,
    "total_chunks": 124,
    "text_content": "model adopts the CL approach, and to mitigate memory usage, it utilizes the sharding contrastive loss ( Cherti et al., 2022 ). For adaptation to VQA, the model incorporates the METER ( Dou et al., 2022 ) framework. This involves deploying a Transformer-based co-attention multimodal fusion module that produces cross-modal representations. These representations are then fed into a classi\ufb01er for the \ufb01nal prediction of answers. Themodel is evaluated on VQA-RAD ( Lau et al., 2018 ) and SLAKE (English",
    "full_text_length": 160559,
    "chunk_length": 1697
  },
  {
    "chunk_id": 1184,
    "paper_filename": "hartsock_2024_vision_language-models_for_reports_generation_visual_quesitons_answeriing.pdf",
    "paper_title": "Hartsock 2024 Vision Language-Models For Reports Generation Visual Quesitons Answeriing",
    "chunk_index": 63,
    "total_chunks": 124,
    "text_content": "back into images during Frontiersin Arti\ufb01cialIntelligence /one.tnum/six.tnum frontiersin.org Hartsock and Rasool /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/four.tnum/three.tnum/zero.tnum/nine.tnum/eight.tnum/four.tnum the generation process. For chest X-rays, multiple views from the same study are tokenized into sequences of discrete visual tokens, demarcated by special tokens to distinguish perspectives. In the case of r",
    "full_text_length": 160559,
    "chunk_length": 1692
  },
  {
    "chunk_id": 1185,
    "paper_filename": "hartsock_2024_vision_language-models_for_reports_generation_visual_quesitons_answeriing.pdf",
    "paper_title": "Hartsock 2024 Vision Language-Models For Reports Generation Visual Quesitons Answeriing",
    "chunk_index": 64,
    "total_chunks": 124,
    "text_content": "views. On the MIMIC-CXR dataset, UniXGen achieves a BLEU-4 score of 0.050 and, using the CheXpert labeler ( Irvin et al., 2019 ), attains a precision score of 0.431,arecallvalueof0.410,andanF1scoreof0.420. /four.tnum./three.tnum./six.tnum Retrieval-augmented bioMedical multi-modal pretrain-and-\ufb01netune paradigm RAMM, a retrieval-augmented VLM designed for biomedical VQA, integrates Swin Transformer ( Liu et al., 2021b ) as its image encoder and PubMedBERT ( Gu et al., 2021 ) as its text encoder (",
    "full_text_length": 160559,
    "chunk_length": 1358
  },
  {
    "chunk_id": 1186,
    "paper_filename": "hartsock_2024_vision_language-models_for_reports_generation_visual_quesitons_answeriing.pdf",
    "paper_title": "Hartsock 2024 Vision Language-Models For Reports Generation Visual Quesitons Answeriing",
    "chunk_index": 65,
    "total_chunks": 124,
    "text_content": "ITM, and MLM. Using CL, the model aligns images and texts using the cosine similarity metric. The VQA task is viewed as a classi\ufb01cation problem, and the model is optimized using the cross-entropy loss function.Duringmodel\ufb01ne-tuning,theretrieval-attentionmodule fuses the representations of the image-question input with four representations of the retrieved image-text pairs from the pre- trained datasets. This lets RAMM to focus on relevant parts of the retrieved information when generating answer",
    "full_text_length": 160559,
    "chunk_length": 1352
  },
  {
    "chunk_id": 1187,
    "paper_filename": "hartsock_2024_vision_language-models_for_reports_generation_visual_quesitons_answeriing.pdf",
    "paper_title": "Hartsock 2024 Vision Language-Models For Reports Generation Visual Quesitons Answeriing",
    "chunk_index": 66,
    "total_chunks": 124,
    "text_content": "initializes the text encoder with the \ufb01rst 6 layers of the BERT (Devlin et al., 2019 ) base model. The multimodal encoder in ALBEF, responsible for combining visual and textual features to generate ITM scores, is initialized using the \ufb01nal six layers of the BERT base model. X-REM leverages ALBEF\u2019s pre-trained weightsand performs further pre-training on X-rays paired with extracted impression sections (2,192 pairs), \ufb01ndings sections (1,597 pairs), or both (2,192 pairs) from the MIMIC-CXR ( Johnso",
    "full_text_length": 160559,
    "chunk_length": 1341
  },
  {
    "chunk_id": 1188,
    "paper_filename": "hartsock_2024_vision_language-models_for_reports_generation_visual_quesitons_answeriing.pdf",
    "paper_title": "Hartsock 2024 Vision Language-Models For Reports Generation Visual Quesitons Answeriing",
    "chunk_index": 67,
    "total_chunks": 124,
    "text_content": "unimodal embeddings. This involves narrowing down the candidate reports based on high cosine similarity with the input imagebeforecomputingITMscores.Additionally,thetextencoder undergoes \ufb01ne-tuning on natural language inference (NLI) task, utilizing datasets such as MedNLI ( Romanov and Shivade, 2018 ) and RadNLI Miura et al. (2021) . This step is crucial forpreventing the retrieval of multiple reports with overlapping or con\ufb02icting information. X-REM achieves a BLEU-2 score of 0.186 on the MIMI",
    "full_text_length": 160559,
    "chunk_length": 1553
  },
  {
    "chunk_id": 1189,
    "paper_filename": "hartsock_2024_vision_language-models_for_reports_generation_visual_quesitons_answeriing.pdf",
    "paper_title": "Hartsock 2024 Vision Language-Models For Reports Generation Visual Quesitons Answeriing",
    "chunk_index": 68,
    "total_chunks": 124,
    "text_content": "2023a ) model. However, before generating responses, LLaMa-7B undergoes both standard and LoRA ( Hu et al., 2022 ) \ufb01ne-tuning on a carefully curated set of 54,000 medical QA pairs. The questions within this setarederivedfromquestion-answeringdatasetssuchasMEDIQA QA (Ben Abacha et al., 2019 ), MEDIQA RQE ( Ben Abacha et al., 2019), MedQA ( Jin et al., 2021 ), MedDialog ( Zeng et al., 2020 ), andPubMedQA( Jinetal.,2019 ),withtheircorrespondinganswers synthesized using GPT-3.5-Turbo in the self-ins",
    "full_text_length": 160559,
    "chunk_length": 1743
  },
  {
    "chunk_id": 1190,
    "paper_filename": "hartsock_2024_vision_language-models_for_reports_generation_visual_quesitons_answeriing.pdf",
    "paper_title": "Hartsock 2024 Vision Language-Models For Reports Generation Visual Quesitons Answeriing",
    "chunk_index": 69,
    "total_chunks": 124,
    "text_content": "or sentences corpus with the highest dot-product similarity to the image embedding are retrieved. The CXR-PRO (Ramesh et al., 2022 ) dataset is employed for text retrieval to gather relevant impressions for generating the radiology report. Frontiersin Arti\ufb01cialIntelligence /one.tnum/seven.tnum frontiersin.org Hartsock and Rasool /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/four.tnum/three.tnum/zero.tnum/nine.tnum/eight.tnum",
    "full_text_length": 160559,
    "chunk_length": 1717
  },
  {
    "chunk_id": 1191,
    "paper_filename": "hartsock_2024_vision_language-models_for_reports_generation_visual_quesitons_answeriing.pdf",
    "paper_title": "Hartsock 2024 Vision Language-Models For Reports Generation Visual Quesitons Answeriing",
    "chunk_index": 70,
    "total_chunks": 124,
    "text_content": "2023c ), is customized for the medical domain through training on instruction-following datasets ( Li et al., 2023a ). Visual features are extracted by the pre-trained CLIP visual encoder ViT- L/14 (Dosovitskiy et al., 2021 ), which can be substituted with BiomedCLIP( Zhangetal.,2023a ).Thesefeaturesaremappedinto textual embedding space via linear projection layer and combined with instructions before being input to the LLM LLaMa-7B (Touvronetal.,2023a ),whichcanbereplacedwithVicuna( Chiang et a",
    "full_text_length": 160559,
    "chunk_length": 1454
  },
  {
    "chunk_id": 1192,
    "paper_filename": "hartsock_2024_vision_language-models_for_reports_generation_visual_quesitons_answeriing.pdf",
    "paper_title": "Hartsock 2024 Vision Language-Models For Reports Generation Visual Quesitons Answeriing",
    "chunk_index": 71,
    "total_chunks": 124,
    "text_content": "The visual encoder and language model weights are frozen during this stage, with updates exclusively applied to the linear projection layer. The second stage of training focuses on aligning the model to follow diverse instructions. For this purpose, another instruction- following dataset is generated from PMC-15. Instructions for this dataset are designed to guide the GPT-4 model to generate multi- roundquestionsandanswersfromtheimagecaptionandsentences from the original PMC paper mentioning the",
    "full_text_length": 160559,
    "chunk_length": 1449
  },
  {
    "chunk_id": 1193,
    "paper_filename": "hartsock_2024_vision_language-models_for_reports_generation_visual_quesitons_answeriing.pdf",
    "paper_title": "Hartsock 2024 Vision Language-Models For Reports Generation Visual Quesitons Answeriing",
    "chunk_index": 72,
    "total_chunks": 124,
    "text_content": "XrayGPT XrayGPT is a conversational medical VLM speci\ufb01cally developed for analyzing chest radiographs ( Thawkar et al., 2023 ). The VLM uses MedCLIP ( Wang et al., 2022b ) to generate visualfeatures. These features undergo a meticulous transformation process: initially, they are mapped to a lower-dimensional space through a linear projection head and subsequently translated into tokens via a linear transformation layer. The model incorporates two text queries: an assistant query framing its purp",
    "full_text_length": 160559,
    "chunk_length": 1475
  },
  {
    "chunk_id": 1194,
    "paper_filename": "hartsock_2024_vision_language-models_for_reports_generation_visual_quesitons_answeriing.pdf",
    "paper_title": "Hartsock 2024 Vision Language-Models For Reports Generation Visual Quesitons Answeriing",
    "chunk_index": 73,
    "total_chunks": 124,
    "text_content": "3,000 image-text pairs from Open-I ( Demner-Fushman et al., 2015 ) dataset. XrayGPT achieves ROUGE-1 =0.3213, ROUGE-2 = 0.0912,andROUGE-L =0.1997onMIMIC-CXRdataset. /four.tnum./three.tnum./one.tnum/two.tnum Co-attention gaTed vision-language data-e\ufb03cient image transformer CAT-ViL DeiT is a specialized VLM tailored for VQA within surgical scenarios, focusing on answer localization ( Bai et al., 2023b). It integrates ResNet-18 ( He et al., 2016 ) pre-trained on ImageNet ( Deng et al., 2009 ) to ge",
    "full_text_length": 160559,
    "chunk_length": 1720
  },
  {
    "chunk_id": 1195,
    "paper_filename": "hartsock_2024_vision_language-models_for_reports_generation_visual_quesitons_answeriing.pdf",
    "paper_title": "Hartsock 2024 Vision Language-Models For Reports Generation Visual Quesitons Answeriing",
    "chunk_index": 74,
    "total_chunks": 124,
    "text_content": "the surgical datasets EndoVis 2018 ( Allan et al., 2020 ). The model achieved an accuracyof61.92%ontheremainingdatafromEndoVis2018and 45.55%onEndoVis2017( Allanetal.,2019 )dataset. /four.tnum./three.tnum./one.tnum/three.tnum Masked image and text modeling with unimodal and multimodal contrastive losses MUMC utilizes a ViT-B/12 ( Dosovitskiy et al., 2021 ) as its image encoder, the \ufb01rst 6 layers of BERT ( Devlin et al., 2019 ) as its text encoder, and the last 6 layers of BERT as its multimodal e",
    "full_text_length": 160559,
    "chunk_length": 1589
  },
  {
    "chunk_id": 1196,
    "paper_filename": "hartsock_2024_vision_language-models_for_reports_generation_visual_quesitons_answeriing.pdf",
    "paper_title": "Hartsock 2024 Vision Language-Models For Reports Generation Visual Quesitons Answeriing",
    "chunk_index": 75,
    "total_chunks": 124,
    "text_content": "et al., 2020), and Image Retrieval in Cross-Language Evaluation Forum Frontiersin Arti\ufb01cialIntelligence /one.tnum/eight.tnum frontiersin.org Hartsock and Rasool /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/four.tnum/three.tnum/zero.tnum/nine.tnum/eight.tnum/four.tnum (ImageCLEF) caption ( R\u00fcckert et al., 2022 ) datasets. For VQA tasks, an answering decoder is added to generate answer text tokens. The encoder weights are ini",
    "full_text_length": 160559,
    "chunk_length": 1492
  },
  {
    "chunk_id": 1197,
    "paper_filename": "hartsock_2024_vision_language-models_for_reports_generation_visual_quesitons_answeriing.pdf",
    "paper_title": "Hartsock 2024 Vision Language-Models For Reports Generation Visual Quesitons Answeriing",
    "chunk_index": 76,
    "total_chunks": 124,
    "text_content": "to 10 images, with a speci\ufb01ed maximum length. Also, it is pre-trained on 1.3 M image-caption pairs from the PMC-OA ( Lin et al., 2023a ) dataset. Themodel\u2019sfew-shotcapabilitiesareachievedthroughtrainingon these mixed text and image datasets, enabling it to generalize and perform diverse multimodal tasks with only a few examples. The model utilizes a pre-trained frozen CLIP vision encoder ViT-L/14 forvisualfeaturegeneration.Toconvertthesevisualfeaturesintoa \ufb01xednumberoftokens,themodelemploysamodu",
    "full_text_length": 160559,
    "chunk_length": 1736
  },
  {
    "chunk_id": 1198,
    "paper_filename": "hartsock_2024_vision_language-models_for_reports_generation_visual_quesitons_answeriing.pdf",
    "paper_title": "Hartsock 2024 Vision Language-Models For Reports Generation Visual Quesitons Answeriing",
    "chunk_index": 77,
    "total_chunks": 124,
    "text_content": "USMLE-style questions augmented with images, case vignettes, and tables of laboratory measurements, coveringadiverserangeofmedicalspecialties. /four.tnum./three.tnum./one.tnum/five.tnum RaDialog RaDialog is a VLM that integrates automated radiology RG with conversational assistance ( Pellegrini et al., 2023 ). The model incorporates BioViL-T ( Bannur et al., 2023 ), a hybrid model that fuses the strengths of ResNet-50 ( He et al., 2016 ) and Transformer ( Vaswani et al., 2017 ) architectures. Pr",
    "full_text_length": 160559,
    "chunk_length": 1592
  },
  {
    "chunk_id": 1199,
    "paper_filename": "hartsock_2024_vision_language-models_for_reports_generation_visual_quesitons_answeriing.pdf",
    "paper_title": "Hartsock 2024 Vision Language-Models For Reports Generation Visual Quesitons Answeriing",
    "chunk_index": 78,
    "total_chunks": 124,
    "text_content": "BLEU- 4 score of 0.095, ROUGE-L score of 0.2710, METEOR score of 0.14, and BERTScore of 0.400 on the MIMIC-CXR dataset. To address the challenge of catastrophic forgetting during training andensurethemodel\u2019scapabilityacrossdiversedownstreamtasks, it is speci\ufb01cally trained on the newly created Instruct ( Pellegrini et al., 2023 ) dataset. This dataset is meticulously curated to encompass a spectrum of eight diverse tasks: RG, NLE, complete CheXpert QA, binary CheXpert QA, region QA, summarization",
    "full_text_length": 160559,
    "chunk_length": 1405
  },
  {
    "chunk_id": 1200,
    "paper_filename": "hartsock_2024_vision_language-models_for_reports_generation_visual_quesitons_answeriing.pdf",
    "paper_title": "Hartsock 2024 Vision Language-Models For Reports Generation Visual Quesitons Answeriing",
    "chunk_index": 79,
    "total_chunks": 124,
    "text_content": "pathology ( Lu et al., 2024b ). It employs UNI ( Chen et al., 2024), built on the ViT-L backbone and pre-trained using SSL on over 100 M histology image patches from approximately 100,000 WSIs, to generate visual features. PathChat uses the Llama 2 13B (Touvron et al., 2023b ) LLM for text decoding, which is pre- trained on general text data. The UNI is connected to the LLM through a multimodal projector that maps visual tokens into the LLM\u2019s embedding space. During PathChat\u2019s pre-training phase",
    "full_text_length": 160559,
    "chunk_length": 1307
  },
  {
    "chunk_id": 1201,
    "paper_filename": "hartsock_2024_vision_language-models_for_reports_generation_visual_quesitons_answeriing.pdf",
    "paper_title": "Hartsock 2024 Vision Language-Models For Reports Generation Visual Quesitons Answeriing",
    "chunk_index": 80,
    "total_chunks": 124,
    "text_content": "of 456,916 pathology-speci\ufb01c instructions of 6 di\ufb00erent types and 999,202 QA pairs. The model is evaluated on the newly curated PathQABench dataset, consisting of public and private subparts. On the multiple-choice questions across the entire PathQABench dataset, PathChat achieved an accuracy of 78.1% when only images and questions are provided to the model and 89.5% when clinical data is also included. For open- ended questions, PathChat attained an accuracy of 78.7% on the subset of questions ",
    "full_text_length": 160559,
    "chunk_length": 1753
  },
  {
    "chunk_id": 1202,
    "paper_filename": "hartsock_2024_vision_language-models_for_reports_generation_visual_quesitons_answeriing.pdf",
    "paper_title": "Hartsock 2024 Vision Language-Models For Reports Generation Visual Quesitons Answeriing",
    "chunk_index": 81,
    "total_chunks": 124,
    "text_content": "77.70% \u2013 \u2013 \u2013 \u2013 PubMedCLIP Eslamietal.(2023)78.40% 82.50% 60.10% 80.00% \u2013 \u2013 \u2013 \u2013 RepsNet Tanwanietal.(2022)\u2013 \u2013 \u2013 87.05% \u2013 \u2013 \u2013 \u2013 BioMedCLIP Zhangetal.(2023a)82.50% 89.70% 67.60% 79.80% \u2013 \u2013 \u2013 \u2013 RAMM Yuanetal.(2023)82.48% 91.59% 67.60% 85.29% \u2013 \u2013 82.13% 39.20% LLaVa-Med Lietal.(2023a)\u2013 84.19% \u2013 85.34% \u2013 91.21% \u2013 \u2013 MUMC Lietal.(2023b)\u2013 \u2013 71.50% 84.20% 39.00% 90.4% \u2013 \u2013 Theunderlinedaccuraciesarethehighestforaspeci\ufb01cdataset. /five.tnum./one.tnum Data availability and privacy A signi\ufb01cant challenge in de",
    "full_text_length": 160559,
    "chunk_length": 1538
  },
  {
    "chunk_id": 1203,
    "paper_filename": "hartsock_2024_vision_language-models_for_reports_generation_visual_quesitons_answeriing.pdf",
    "paper_title": "Hartsock 2024 Vision Language-Models For Reports Generation Visual Quesitons Answeriing",
    "chunk_index": 82,
    "total_chunks": 124,
    "text_content": "). ImageDePHI automates the removal of PHI from WSIs ( Clunie et al., 2024 ). Another approach is replacing identifying information with arti\ufb01cial identi\ufb01ers, which keeps data linkable without disclosing personal details.However,theriskofPHIleakagestillremainsaconcern. In the future,addressing this limitation caninvolveemploying innovative approaches such as RAG and federated learning (FL). While RAG usually involves a frozen model during training, exploring the pre-training of VLMs within the R",
    "full_text_length": 160559,
    "chunk_length": 1438
  },
  {
    "chunk_id": 1204,
    "paper_filename": "hartsock_2024_vision_language-models_for_reports_generation_visual_quesitons_answeriing.pdf",
    "paper_title": "Hartsock 2024 Vision Language-Models For Reports Generation Visual Quesitons Answeriing",
    "chunk_index": 83,
    "total_chunks": 124,
    "text_content": "the weights in FL can be protected using techniques such as di\ufb00erential privacy (DP) or homomorphic encryption (HE). In DP, noise is added to the gradients before they are sent to the central server ( Dwork, 2006 ). In contrast, HE encrypts the weights, allowing the central server to perform computations onthemwithoutdecryption( Stripelisetal.,2021 ).Futureresearchcan focusonoptimizingthebalancebetweenprivacyandperformance ofVLMs,andenhancingthee\ufb03ciencyofencryptionmethodsinFL (Koutsoubisetal.,20",
    "full_text_length": 160559,
    "chunk_length": 1906
  },
  {
    "chunk_id": 1205,
    "paper_filename": "hartsock_2024_vision_language-models_for_reports_generation_visual_quesitons_answeriing.pdf",
    "paper_title": "Hartsock 2024 Vision Language-Models For Reports Generation Visual Quesitons Answeriing",
    "chunk_index": 84,
    "total_chunks": 124,
    "text_content": "but they require \ufb01ne-tuning on medical datasets to understand specializedterminologyandrelationships,andmaystillmisssubtle clinicalnuances. In medical VQA, traditional metrics like Accuracy, Precision, and Recall are commonly used to evaluate how well VLMs answer clinical questions, such as identifying medical conditions or anatomical features. While these metrics e\ufb00ectively assess binary outcomes, they fail to account for the varying clinical relevance or signi\ufb01cance of errors made by the model",
    "full_text_length": 160559,
    "chunk_length": 1846
  },
  {
    "chunk_id": 1206,
    "paper_filename": "hartsock_2024_vision_language-models_for_reports_generation_visual_quesitons_answeriing.pdf",
    "paper_title": "Hartsock 2024 Vision Language-Models For Reports Generation Visual Quesitons Answeriing",
    "chunk_index": 85,
    "total_chunks": 124,
    "text_content": "relevant clinical entities but also their correct relationships, which is crucial for the accuracy and integrity of medical conclusions. The development of additional specialized metrics is vital for evaluating VLMs performance and for assessing factors such as generalization, e\ufb03ciency, and robustness in clinical decision- making and diagnostic support. Furthermore, integrating these metrics with other quantitative measures and human assessments can signi\ufb01cantly enhance evaluations and drive con",
    "full_text_length": 160559,
    "chunk_length": 1780
  },
  {
    "chunk_id": 1207,
    "paper_filename": "hartsock_2024_vision_language-models_for_reports_generation_visual_quesitons_answeriing.pdf",
    "paper_title": "Hartsock 2024 Vision Language-Models For Reports Generation Visual Quesitons Answeriing",
    "chunk_index": 86,
    "total_chunks": 124,
    "text_content": "risk of hallucinations. For instance, LLaVA-RLHF (Sunetal.,2023 )achievedhallucinationreductionbyincorporating RLHF to align di\ufb00erent modalities. Further research can focus on integrating RLHF into medical VLMs. Additionally, incorporating RAGcanhelpreducetheriskofgeneratingmisleadingorfabricated outputs by allowing the system to analyze medical images while simultaneouslyaccessingrelevantinformationfromtrustedtextual sources. /five.tnum./four.tnum Catastrophic forgetting Overcoming catastrophic",
    "full_text_length": 160559,
    "chunk_length": 1851
  },
  {
    "chunk_id": 1208,
    "paper_filename": "hartsock_2024_vision_language-models_for_reports_generation_visual_quesitons_answeriing.pdf",
    "paper_title": "Hartsock 2024 Vision Language-Models For Reports Generation Visual Quesitons Answeriing",
    "chunk_index": 87,
    "total_chunks": 124,
    "text_content": "). Also, incorporating adapters within the frameworkofcontinuallearningcanbeavaluabletoolinmitigating catastrophicforgetting( Zhangetal.,2023b ). /five.tnum./five.tnum Integration into hospital systems Integrating VLMs into hospital systems also presents substantial challenges, requiring extensive collaboration between medical professionals and AI/ML researchers. First, medical professionals must maintain rigorous data collection practices to ensure that the data is clean, well-organized, and ac",
    "full_text_length": 160559,
    "chunk_length": 1625
  },
  {
    "chunk_id": 1209,
    "paper_filename": "hartsock_2024_vision_language-models_for_reports_generation_visual_quesitons_answeriing.pdf",
    "paper_title": "Hartsock 2024 Vision Language-Models For Reports Generation Visual Quesitons Answeriing",
    "chunk_index": 88,
    "total_chunks": 124,
    "text_content": "the potential of VLMs to enhance clinical e\ufb00ectiveness. RaDialog demonstrates a solid capability to produce clinically accurate radiology reports. It transforms static reporting into a dynamic tool where clinicians can ask follow-up questions and seamlessly incorporate expert insights. This aligns closely with the interactive processes typical in clinical settings. Meanwhile, PathChat demonstrates promising clinical e\ufb00ectiveness as an AI copilot for pathology. It can assist pathologists in their",
    "full_text_length": 160559,
    "chunk_length": 1731
  },
  {
    "chunk_id": 1210,
    "paper_filename": "hartsock_2024_vision_language-models_for_reports_generation_visual_quesitons_answeriing.pdf",
    "paper_title": "Hartsock 2024 Vision Language-Models For Reports Generation Visual Quesitons Answeriing",
    "chunk_index": 89,
    "total_chunks": 124,
    "text_content": "potential inaccuracies. Additionally, adversarial attacks represent another signi\ufb01cant security issue, as they can exploit vulnerabilities in the model, leading to incorrect predictions. To combat this, incorporating adversarial training by exposing the model to adversarial examples during training can enhance its robustness against such attacks ( He et al., 2023a ). Continuous monitoring and updating of the VLMs are also Frontiersin Arti\ufb01cialIntelligence /two.tnum/one.tnum frontiersin.org Harts",
    "full_text_length": 160559,
    "chunk_length": 1588
  },
  {
    "chunk_id": 1211,
    "paper_filename": "hartsock_2024_vision_language-models_for_reports_generation_visual_quesitons_answeriing.pdf",
    "paper_title": "Hartsock 2024 Vision Language-Models For Reports Generation Visual Quesitons Answeriing",
    "chunk_index": 90,
    "total_chunks": 124,
    "text_content": "produce visual features, which can be used as input to LLMs. The visual features are then combined with tokenized text input and fed into the LLM. This approach simpli\ufb01es model design and leverages the extensive prior knowledge embedded in LLMs. Furthermore, feeding all data features into LLMs enables the generation of human-like text outputs, improving user experience and facilitating more e\ufb00ective communication of medical insights. The review also explores 18 publicly available medical vision-",
    "full_text_length": 160559,
    "chunk_length": 1683
  },
  {
    "chunk_id": 1212,
    "paper_filename": "hartsock_2024_vision_language-models_for_reports_generation_visual_quesitons_answeriing.pdf",
    "paper_title": "Hartsock 2024 Vision Language-Models For Reports Generation Visual Quesitons Answeriing",
    "chunk_index": 91,
    "total_chunks": 124,
    "text_content": "capturing the nuances of clinical language, there is a need to develop specialized metrics tailored to medical RG and VQA. It is likewise crucial to address VLM hallucinations, and incorporating RLHF and RAG are promisingsolutions.Continuallearningmethodscanhelpmitigate catastrophicforgetting,ensuringthatmodelsretaintheknowledge theyhavepreviouslyacquired.Furthermore,collaborationbetweenhealthcare professionals and AI researchers is essential to deploy VLMsinwaysthatgenuinelyimprovepatientcare.L",
    "full_text_length": 160559,
    "chunk_length": 1749
  },
  {
    "chunk_id": 1213,
    "paper_filename": "hartsock_2024_vision_language-models_for_reports_generation_visual_quesitons_answeriing.pdf",
    "paper_title": "Hartsock 2024 Vision Language-Models For Reports Generation Visual Quesitons Answeriing",
    "chunk_index": 92,
    "total_chunks": 124,
    "text_content": "solely those of the authors and do not necessarily represent those of their a\ufb03liated organizations, or those of the publisher, the editors and the reviewers. Any product that may be evaluated in this article, or claim that may be made by its manufacturer, is not guaranteed or endorsedbythepublisher. References Abacha, A. B., Datla, V. V., Hasan, S. A., Demner-Fushman, D., and M\u00fcller, H. (2020). \u201cOverview of the VQA-Med task at ImageCLEF 2020: v isual question answeringandgenerationinthemedicaldo",
    "full_text_length": 160559,
    "chunk_length": 1477
  },
  {
    "chunk_id": 1214,
    "paper_filename": "hartsock_2024_vision_language-models_for_reports_generation_visual_quesitons_answeriing.pdf",
    "paper_title": "Hartsock 2024 Vision Language-Models For Reports Generation Visual Quesitons Answeriing",
    "chunk_index": 93,
    "total_chunks": 124,
    "text_content": "7433\u20137466.doi:10.48550/arXiv.2205.01138 Alayrac, J.-B., Donahue, J., Luc, P., Miech, A., Barr, I., Has son, Y., et al. (2022). Flamingo: a visual language model for few-shot learn ing.Adv. Neural Inform. Process. Syst . 35, 23716\u201323736. doi: 10.48550/arXiv.2204. 14198 Allan, M., Kondo, S., Bodenstedt, S., Leger, S., Kadkhodamoha mmadi, R., Luengo, I., et al. (2020). 2018 robotic scene segmentation ch allenge.arXiv Preprint arXiv:2001.11190 .doi:10.48550/arXiv.2001.11190Allan, M., Shvets, A., Kur",
    "full_text_length": 160559,
    "chunk_length": 1599
  },
  {
    "chunk_id": 1215,
    "paper_filename": "hartsock_2024_vision_language-models_for_reports_generation_visual_quesitons_answeriing.pdf",
    "paper_title": "Hartsock 2024 Vision Language-Models For Reports Generation Visual Quesitons Answeriing",
    "chunk_index": 94,
    "total_chunks": 124,
    "text_content": "H. (2023b). \u201cCAT-ViL: co-attenti on gated vision- language embedding for visual question localized-answering in robotic surgery,\u201d in Medical Image Computing and Computer Assisted Intervention\u2014MICCAI , eds. H. Greenspan,P.Mousavi,S.Salcudean,J.Duncan,T.Syeda-Mahmo od,R.Taylor(Cham: Springer),397\u2013407. Bai, S., and An, S. (2018). A survey on automatic image caption generation. Neurocomputing 311:80.doi:10.1016/j.neucom.2018.05.080 Bajwa, J., Munir, U., Nori, A., and Williams, B. (2021). Arti\ufb01c ial ",
    "full_text_length": 160559,
    "chunk_length": 2019
  },
  {
    "chunk_id": 1216,
    "paper_filename": "hartsock_2024_vision_language-models_for_reports_generation_visual_quesitons_answeriing.pdf",
    "paper_title": "Hartsock 2024 Vision Language-Models For Reports Generation Visual Quesitons Answeriing",
    "chunk_index": 95,
    "total_chunks": 124,
    "text_content": "and Rasool, G. (2023). E\ufb03cient scopeformer: towardscalableandrichfeatureextractionforintracranial hemorrhagedetection. IEEE Access11,81656\u201381671.doi:10.48550/arXiv.2302.00220 Bazi, Y., Rahhal, M. M. A., Bashmal, L., and Zuair, M. (2023). Vi sion\u2014language model for visual question answering in medical imagery. Bioengineering 10:380. doi:10.3390/bioengineering10030380 Beam, A., Kompa, B., Schmaltz, A., Fried, I., Weber, G. M., Palmer , N. P., et al. (2020). Clinical concept embeddings learned from",
    "full_text_length": 160559,
    "chunk_length": 1632
  },
  {
    "chunk_id": 1217,
    "paper_filename": "hartsock_2024_vision_language-models_for_reports_generation_visual_quesitons_answeriing.pdf",
    "paper_title": "Hartsock 2024 Vision Language-Models For Reports Generation Visual Quesitons Answeriing",
    "chunk_index": 96,
    "total_chunks": 124,
    "text_content": "Bannur, S., Castro, D. C., Schwai ghofer, A., Hyland, S., etal.(2022).Makingthemostoftextsemanticstoimprovebiom edicalvision\u2013language processing. Comput.Vis .5,1\u201321.doi:10.1007/978-3-031-20059-51 Bojanowski, P., Grave, E., Joulin, A., and Mikolov, T. (2017). E nriching word vectors with subword information. Trans. Assoc. Comput. Linguist . 5, 135\u2013146. doi:10.1162/tacl_a_00051 Bommasani, R., Hudson, D. A., Adeli, E., Altman, R., Arora, S., von Arx, S., et al. (2022). On the opportunities and risk",
    "full_text_length": 160559,
    "chunk_length": 1595
  },
  {
    "chunk_id": 1218,
    "paper_filename": "hartsock_2024_vision_language-models_for_reports_generation_visual_quesitons_answeriing.pdf",
    "paper_title": "Hartsock 2024 Vision Language-Models For Reports Generation Visual Quesitons Answeriing",
    "chunk_index": 97,
    "total_chunks": 124,
    "text_content": "Chen, X., Shi, J., Xu, S., et al. (2 023). VLP: a survey on vision-language pre-training. Machine Intell. Res . 20, 38\u201356. doi:10.1007/s11633-022-1369-5 Chen, R. J., Ding, T., Lu, M. Y., Williamson, D. F. K., Jaume, G., S ong, A. H., et al.(2024).Towardsageneral-purposefoundationmodelforcompu tationalpathology. Nat.Med.30,850\u2013862.doi:10.1038/s41591-024-02857-3 Chen, T., Kornblith, S., Norouzi, M., and Hinton, G. (2020a). A simple framework for contrastive learning of visual representations. arXi",
    "full_text_length": 160559,
    "chunk_length": 1813
  },
  {
    "chunk_id": 1219,
    "paper_filename": "hartsock_2024_vision_language-models_for_reports_generation_visual_quesitons_answeriing.pdf",
    "paper_title": "Hartsock 2024 Vision Language-Models For Reports Generation Visual Quesitons Answeriing",
    "chunk_index": 98,
    "total_chunks": 124,
    "text_content": "H., et al. (2014). \u201cLearning phrase representations u sing rnn encoder\u2014 decoder for statistical machine translation,\u201d in Conference on Empirical Methods in NaturalLanguageProcessing ,1724\u20131734. Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., R oberts, A., et al. (2022). PaLM: scaling language modeling with pathways. J. Machine Learn. Res . 24, 1\u2013113.doi:10.48550/arXiv.2204.02311 Clunie,D.,Taylor,A.,Bisson,T.,Gutman,D.,Xiao,Y.,Schwar z,C.G.,etal.(2024). Summary of the National Cance",
    "full_text_length": 160559,
    "chunk_length": 1690
  },
  {
    "chunk_id": 1220,
    "paper_filename": "hartsock_2024_vision_language-models_for_reports_generation_visual_quesitons_answeriing.pdf",
    "paper_title": "Hartsock 2024 Vision Language-Models For Reports Generation Visual Quesitons Answeriing",
    "chunk_index": 99,
    "total_chunks": 124,
    "text_content": "radiology examinations for distribution and retrieval. J. Am. Med. Informat. Assoc . 23, 304\u2013310. doi:10.1093/jamia/ocv080 Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei , L. (2009). \u201cImageNet: a large-scale hierarchical image database,\u201d in 2009 IEEE Conference on Computer Vision andPatternRecognition ,248\u2013255. Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. (2019). \u201c BERT: pre-training of deep bidirectional transformers for language understand ing,\u201d inConference of the Nort",
    "full_text_length": 160559,
    "chunk_length": 1584
  },
  {
    "chunk_id": 1221,
    "paper_filename": "hartsock_2024_vision_language-models_for_reports_generation_visual_quesitons_answeriing.pdf",
    "paper_title": "Hartsock 2024 Vision Language-Models For Reports Generation Visual Quesitons Answeriing",
    "chunk_index": 100,
    "total_chunks": 124,
    "text_content": "(Berlin;Heidelberg:Springer),1\u201312. Eslami, S., Meinel, C., and De Melo, G. (2023). PubmedCLIP: how mu ch does clip bene\ufb01t visual question answering in the medical domain? Find. Assoc. Comput. Linguist.88,1181\u20131193.doi:10.18653/v1/2023.\ufb01ndings-eacl.88 Esser, P., Rombach, R., and Ommer, B. (2021). \u201cTaming transf ormers for high- resolution image synthesis,\u201d in 2021 IEEE/CVF Conference on Computer Vision and PatternRecognition(CVPR) (VirtualConference),12868\u201312878. Gan, Z., Li, L., Li, C., Wang, L.",
    "full_text_length": 160559,
    "chunk_length": 1675
  },
  {
    "chunk_id": 1222,
    "paper_filename": "hartsock_2024_vision_language-models_for_reports_generation_visual_quesitons_answeriing.pdf",
    "paper_title": "Hartsock 2024 Vision Language-Models For Reports Generation Visual Quesitons Answeriing",
    "chunk_index": 101,
    "total_chunks": 124,
    "text_content": "et al. (2023). MedAlpaca\u2014an open-source collection of medical conversational AI models and training data. arXiv Preprint arXiv:2304.08247 . doi:10.48550/arXiv.2304.08247 Hao, Y., Mendelsohn, S., Sterneck, R., Martinez, R., and Fran k, R. (2020). Probabilistic predictions of people perusing: evaluating metrics of language model performance for psycholinguistic modeling. arXiv Preprint arXiv:2009.03954 . doi:10.18653/v1/2020.cmcl-1.10 He, B., Jia, X., Liang, S., Lou, T., Liu, Y., and Cao, X. (2023",
    "full_text_length": 160559,
    "chunk_length": 1602
  },
  {
    "chunk_id": 1223,
    "paper_filename": "hartsock_2024_vision_language-models_for_reports_generation_visual_quesitons_answeriing.pdf",
    "paper_title": "Hartsock 2024 Vision Language-Models For Reports Generation Visual Quesitons Answeriing",
    "chunk_index": 102,
    "total_chunks": 124,
    "text_content": "QA: 30000+ questions for medical visual question answering. arXiv Preprint arXiv:2003.10286 . doi:10.48550/arXiv.2003.10286 Hochreiter, S., and Schmidhuber, J. (1997). Long short-ter m memory. Neural Comput.9,1735\u20131780. Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., et al. (2022). \u201cLoRA: low-rankadaptationoflargelanguagemodels,\u201din InternationalConferenceonLearning Representations (VirtualConference). Huang, G., Liu, Z., Pleiss, G., Van Der Maaten, L., and Weinberge r, K. (2022",
    "full_text_length": 160559,
    "chunk_length": 2210
  },
  {
    "chunk_id": 1224,
    "paper_filename": "hartsock_2024_vision_language-models_for_reports_generation_visual_quesitons_answeriing.pdf",
    "paper_title": "Hartsock 2024 Vision Language-Models For Reports Generation Visual Quesitons Answeriing",
    "chunk_index": 103,
    "total_chunks": 124,
    "text_content": "port generation. arXiv PreprintarXiv:2303.17579 .doi:10.48550/arXiv.2303.17579 Ji, Q. (2020). 5\u2014computer vision applications. Comput. Vis. Pat. Recogn . 10, 191\u2013297.doi:10.1016/B978-0-12-803467-5.00010-1 Jia, C., Yang, Y., Xia, Y., Chen, Y., Parekh, Z., Pham, H., et a l. (2021). \u201cScaling up visual and vision-language representation learning with no isy text supervision,\u201d in InternationalConferenceonMachineLearning,Vol.139 ,4904\u20134916. Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C., Chapl",
    "full_text_length": 160559,
    "chunk_length": 1995
  },
  {
    "chunk_id": 1225,
    "paper_filename": "hartsock_2024_vision_language-models_for_reports_generation_visual_quesitons_answeriing.pdf",
    "paper_title": "Hartsock 2024 Vision Language-Models For Reports Generation Visual Quesitons Answeriing",
    "chunk_index": 104,
    "total_chunks": 124,
    "text_content": ".doi:10.48550/arXiv.1901.07042 Kayser, M., Emde, C., Camburu, O., Parsons, G., Papiez, B., and Lukasiewicz, T. (2022). Explaining chest x-ray pathologies in natural language. Int. Conf. Med. Image Comput.Computer-Assist.Interv .13435,701\u2013713.doi:10.1007/978-3-031-16443-9_67 Khan, H., Bouaynaya, N. C., and Rasool, G. (2023). \u201cThe importan ce of robust featuresinmitigatingcatastrophicforgetting,\u201din 2023IEEESymposiumonComputers andCommunications(ISCC) (Gammarth:IEEE),752\u2013757. Khan, H., Bouaynaya, N",
    "full_text_length": 160559,
    "chunk_length": 2140
  },
  {
    "chunk_id": 1226,
    "paper_filename": "hartsock_2024_vision_language-models_for_reports_generation_visual_quesitons_answeriing.pdf",
    "paper_title": "Hartsock 2024 Vision Language-Models For Reports Generation Visual Quesitons Answeriing",
    "chunk_index": 105,
    "total_chunks": 124,
    "text_content": "(2 022).Illustrating Reinforcement Learning From Human Feedback (RLHF) (Hugging Face). Available at: https://huggingface.co/blog/rlhf (accessedFebruary20,2024). Lau, J. J., Gayen, S., Ben Abacha, A., and Demner-Fushman, D. (2018). A dataset of clinically generated visual questions and answers about radi ology images. Sci. Data 5:180251.doi:10.1038/sdata.2018.251 LeCun,Y.,Bengio,Y.,andHinton,G.(2015).Deeplearning. Nature521,436\u2013444. doi:10.1038/nature14539 Lee, H., Lee, D. Y., Kim, W., Kim, J.-H.",
    "full_text_length": 160559,
    "chunk_length": 1635
  },
  {
    "chunk_id": 1227,
    "paper_filename": "hartsock_2024_vision_language-models_for_reports_generation_visual_quesitons_answeriing.pdf",
    "paper_title": "Hartsock 2024 Vision Language-Models For Reports Generation Visual Quesitons Answeriing",
    "chunk_index": 106,
    "total_chunks": 124,
    "text_content": "arXiv Preprint arXiv:2306.00890 . doi: 10.48550/arXiv.2306. 00890 Li, J., Selvaraju, R. R., Gotmare, A. D., Joty, S., Xiong, C., an d Hoi, S. (2021). Align before fuse: vision and language representation learning with momentum distillation. Adv.NeuralInform..Process.Syst .2021:7651.doi:10.48550/arXiv.2107.07651 Li, L. H., Yatskar, M., Yin, D., Hsieh, C.-J., and Chang, K.-W . (2019). VisualBERT:asimpleandperformantbaselineforvisionandlangua ge.arXivPreprint arXiv:1908.03557 .doi:10.48550/arXiv.19",
    "full_text_length": 160559,
    "chunk_length": 2008
  },
  {
    "chunk_id": 1228,
    "paper_filename": "hartsock_2024_vision_language-models_for_reports_generation_visual_quesitons_answeriing.pdf",
    "paper_title": "Hartsock 2024 Vision Language-Models For Reports Generation Visual Quesitons Answeriing",
    "chunk_index": 107,
    "total_chunks": 124,
    "text_content": "contrastive language-image pre-training using biomedical d ocuments. arXiv Preprint arXiv:2303.07240 .doi:10.48550/arXiv.2303.07240 Lin, Z., Zhang, D., Tao, Q., Shi, D., Ha\ufb00ari, G., Wu, Q., et al. (2 023b). Medical visual question answering: a survey. Artif. Intell. Med . 143:102611. doi:10.1016/j.artmed.2023.102611 Liu, B., Zhan, L.-M., Xu, L., Ma, L., Yang, Y. F., and Wu, X.-M. (2021a). SLAKE: a semantically-labeled knowledge-enhanced dataset for medical visual question answering. IEEE 18th In",
    "full_text_length": 160559,
    "chunk_length": 1756
  },
  {
    "chunk_id": 1229,
    "paper_filename": "hartsock_2024_vision_language-models_for_reports_generation_visual_quesitons_answeriing.pdf",
    "paper_title": "Hartsock 2024 Vision Language-Models For Reports Generation Visual Quesitons Answeriing",
    "chunk_index": 108,
    "total_chunks": 124,
    "text_content": "Visual instruc tion tuning. arXiv PreprintarXiv:2304.08485 .doi:10.48550/arXiv.2304.08485 Liu, H., Xue, W., Chen, Y., Chen, D., Zhao, X., Wang, K., et al. ( 2024). A survey on hallucination in large vision-language models. arXiv Preprint arXiv:2402.00253 . doi:10.48550/arXiv.2402.00253 Liu,Z.,Lin,Y.,Cao,Y.,Hu,H.,Wei,Y.,Zhang,Z.,etal.(202 1b).\u201cSwintransformer: hierarchicalvisiontransformerusingshiftedwindows,\u201di nInternationalConferenceon ComputerVision(ICCV) (Montreal,QC),9992\u201310002. Lo, K., Wang",
    "full_text_length": 160559,
    "chunk_length": 1611
  },
  {
    "chunk_id": 1230,
    "paper_filename": "hartsock_2024_vision_language-models_for_reports_generation_visual_quesitons_answeriing.pdf",
    "paper_title": "Hartsock 2024 Vision Language-Models For Reports Generation Visual Quesitons Answeriing",
    "chunk_index": 109,
    "total_chunks": 124,
    "text_content": "generative ai copilot for human patho logy.Nature24:3. doi:10.1038/s41586-024-07618-3 Mabotuwana, T., Hall, C. S., and Cross, N. (2020). Framework fo r extracting critical \ufb01ndings in radiology reports. J. Digit. Imag . 33, 988\u2013995. doi:10.1007/s10278-020-00349-7 Manzari, O. N., Ahmadabadi, H., Kashiani, H., Shokouhi, S. B ., and Ayatollahi, A. (2023). MedViT: a robust vision transformer for generaliz ed medical image classi\ufb01cation. Comput.Biol.Med .157:106791.doi:10.1016/j.compbiomed.2023.106791",
    "full_text_length": 160559,
    "chunk_length": 1995
  },
  {
    "chunk_id": 1231,
    "paper_filename": "hartsock_2024_vision_language-models_for_reports_generation_visual_quesitons_answeriing.pdf",
    "paper_title": "Hartsock 2024 Vision Language-Models For Reports Generation Visual Quesitons Answeriing",
    "chunk_index": 110,
    "total_chunks": 124,
    "text_content": "transformer network for image anomaly detectio n and localization,\u201d in IEEEInternationalSymposiumonIndustrialElectronics(ISIE) (Kyoto),01\u201306. Miura, Y., Zhang, Y., Tsai, E., Langlotz, C., and Jurafsky, D. (2021). \u201cImproving factualcompletenessandconsistencyofimage-to-textradio logyreportgeneration,\u201din NorthAmericanChapteroftheAssociationforComputationalLingui stics,5288\u20135304. Mohsan, M. M., Akram, M. U., Rasool, G., Alghamdi, N. S., Baqai, M. A. A., and Abbas, M. (2023). Vision transformer and l",
    "full_text_length": 160559,
    "chunk_length": 1604
  },
  {
    "chunk_id": 1232,
    "paper_filename": "hartsock_2024_vision_language-models_for_reports_generation_visual_quesitons_answeriing.pdf",
    "paper_title": "Hartsock 2024 Vision Language-Models For Reports Generation Visual Quesitons Answeriing",
    "chunk_index": 111,
    "total_chunks": 124,
    "text_content": "Ohno-Machado, L., and Chapman, W. W. (2011). Natural language processing: an introduction. J. Am. Med. Informat. Assoc . 18, 544\u2013551. doi:10.1136/amiajnl-2011-000464 Norgeot, B., Muenzen, K., Peterson, T. A., Fan, X., Glicksberg , B. S., Schenk, G., et al. (2020). Protected health information \ufb01lter (ph ilter): accurately and securely de-identifying free-text clinical notes. NPJ Digit. Med . 3:258. doi:10.1038/s41746-020-0258-y Ouyang,L.,Wu,J.,Jiang,X.,Almeida,D.,Wainwright,C.,Mi shkin,P.,etal.(2",
    "full_text_length": 160559,
    "chunk_length": 1840
  },
  {
    "chunk_id": 1233,
    "paper_filename": "hartsock_2024_vision_language-models_for_reports_generation_visual_quesitons_answeriing.pdf",
    "paper_title": "Hartsock 2024 Vision Language-Models For Reports Generation Visual Quesitons Answeriing",
    "chunk_index": 112,
    "total_chunks": 124,
    "text_content": "report generation a nd conversational assistance. arXivPreprintarXiv:2311.18681 .doi:10.48550/arXiv.2311.18681 Peng,Y.,Wang,X.,Lu,L.,Bagheri,M.,Summers,R.M.,andLu ,Z.(2017).NegBio: a high-performance tool for negation and uncertainty detect ion in radiology reports. AMIASum.Transl.Sci.Proc .2018,188\u2013196.doi:10.48550/arXiv.1712.05898 Pennington, J., Socher, R., and Manning, C. (2014). Glove: glob al vectors for word representation. Empir. Methods Natur. Lang. Process . 14, 1532\u20131543. doi:10.3115/",
    "full_text_length": 160559,
    "chunk_length": 1904
  },
  {
    "chunk_id": 1234,
    "paper_filename": "hartsock_2024_vision_language-models_for_reports_generation_visual_quesitons_answeriing.pdf",
    "paper_title": "Hartsock 2024 Vision Language-Models For Reports Generation Visual Quesitons Answeriing",
    "chunk_index": 113,
    "total_chunks": 124,
    "text_content": "Ganu, T. (2023). Retr ieval augmented chest X-ray report generation using openAI GPT models .arXiv Preprint arXiv:2305.03660 .doi:10.48550/arXiv.2305.03660 Reddy,S.(2024).GenerativeAIinhealthcare:animplementatio nscienceinformed translational path on application, integration and governance. Implement. Sci . 19:9. doi:10.1186/s13012-024-01357-9 Ren, M., Cao, B., Lin, H., Liu, C., Han, X., Zeng, K., et al. (202 4). Learning or self-aligning? rethinking instruction \ufb01ne-tuning. arXiv Preprint arXiv:",
    "full_text_length": 160559,
    "chunk_length": 1712
  },
  {
    "chunk_id": 1235,
    "paper_filename": "hartsock_2024_vision_language-models_for_reports_generation_visual_quesitons_answeriing.pdf",
    "paper_title": "Hartsock 2024 Vision Language-Models For Reports Generation Visual Quesitons Answeriing",
    "chunk_index": 114,
    "total_chunks": 124,
    "text_content": "L., Br\u00fcngel, R., Idrissi-Yaghir, A., et al. (2022). \u201cOverview of imageclef medical 2022\u2014caption prediction and concept detection,\u201d in CEUR Workshop Proceedings, Vol. 3180 , 1294\u2013 1307. Schmidt, R. M. (2019). Recurrent neural networks (RNNS): a g entle introduction andoverview. arXivPreprintarXiv:1912.05911 .doi:10.48550/arXiv.1912.05911 Seenivasan,L.,Islam,M.,Krishna,A.K.,andRen,H.(2022). \u201cSurgical-VQA:visual questionansweringinsurgicalscenesusingtransformer,\u201di nMedicalImageComputing andComputer",
    "full_text_length": 160559,
    "chunk_length": 1852
  },
  {
    "chunk_id": 1236,
    "paper_filename": "hartsock_2024_vision_language-models_for_reports_generation_visual_quesitons_answeriing.pdf",
    "paper_title": "Hartsock 2024 Vision Language-Models For Reports Generation Visual Quesitons Answeriing",
    "chunk_index": 115,
    "total_chunks": 124,
    "text_content": "Biomedical LLM With Visual Capabili ties.Available at: https://cambridgeltl.github.io/visual-med-alpaca/ (accessedFebruary20,2024). Singhal, K., Azizi, S., Tu, T., Mahdavi, S. S., Wei, J., Chung, H. W., et al. (2023). Large language models encode clinical knowledge. Nature620, 172\u2013180. doi:10.1038/s41586-023-06291-2 Smit, A., Jain, S., Rajpurkar, P., Pareek, A., Ng, A. Y., and Lu ngren, M. P. (2020). CheXbert: combining automatic labelers and expert ann otations for accurate radiology report lab",
    "full_text_length": 160559,
    "chunk_length": 1719
  },
  {
    "chunk_id": 1237,
    "paper_filename": "hartsock_2024_vision_language-models_for_reports_generation_visual_quesitons_answeriing.pdf",
    "paper_title": "Hartsock 2024 Vision Language-Models For Reports Generation Visual Quesitons Answeriing",
    "chunk_index": 116,
    "total_chunks": 124,
    "text_content": "Y., et al. (202 3). Aligning large multimodalmodelswithfactuallyaugmentedRLHF. arXivPreprintarXiv:2309.14525 . doi:10.48550/arXiv.2309.14525 Sutton, R., and Barto, A. (1998). Reinforcement learning: an introduction. IEEE Trans.NeuralNetw .9,1054\u20131054. Tan, M., and Le, Q. V. (2020). E\ufb03cientNet: tethinking model s caling for convolutional neural networks. arXiv Preprint arXiv:1905.11946 . doi:10.48550/arXiv.1905.11946 Tanwani, A. K., Barral, J., and Freedman, D. (2022). \u201cRepsNet: combining vision ",
    "full_text_length": 160559,
    "chunk_length": 1889
  },
  {
    "chunk_id": 1238,
    "paper_filename": "hartsock_2024_vision_language-models_for_reports_generation_visual_quesitons_answeriing.pdf",
    "paper_title": "Hartsock 2024 Vision Language-Models For Reports Generation Visual Quesitons Answeriing",
    "chunk_index": 117,
    "total_chunks": 124,
    "text_content": "H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M .-A., Lacroix, T., et al. (2023a). LLaMA: open and e\ufb03cient foundation language models .arXiv Preprint arXiv:2302.13971 .doi:10.48550/arXiv.2302.13971 Frontiersin Arti\ufb01cialIntelligence /two.tnum/five.tnum frontiersin.org Hartsock and Rasool /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/four.tnum/three.tnum/zero.tnum/nine.tnum/eight.tnum/four.tnum Touvron, H., Martin, L., S",
    "full_text_length": 160559,
    "chunk_length": 1881
  },
  {
    "chunk_id": 1239,
    "paper_filename": "hartsock_2024_vision_language-models_for_reports_generation_visual_quesitons_answeriing.pdf",
    "paper_title": "Hartsock 2024 Vision Language-Models For Reports Generation Visual Quesitons Answeriing",
    "chunk_index": 118,
    "total_chunks": 124,
    "text_content": "N., Uszkoreit, J., Jones, L., Gomez, A. N., et al. (2017). Attention is all you need. Adv. Neural Inform. Process. Syst . 30, 5998\u20136008. doi:10.48550/arXiv.1706.03762 Verspoor, K., and Cohen, K. B. (2013). Encyclopedia of Systems Biology, Chapter NaturalLanguageProcessing .(NewYork,NY:Springer),1495\u20131498. Wang, C., Cho, K., and Gu, J. (2020). \u201cNeural machine translati on with byte-level subwords,\u201din AAAIConferenceonArti\ufb01cialIntelligence ,9154\u20139160. Wang, J., Yang, Z., Hu, X., Li, L., Lin, K., Ga",
    "full_text_length": 160559,
    "chunk_length": 1524
  },
  {
    "chunk_id": 1240,
    "paper_filename": "hartsock_2024_vision_language-models_for_reports_generation_visual_quesitons_answeriing.pdf",
    "paper_title": "Hartsock 2024 Vision Language-Models For Reports Generation Visual Quesitons Answeriing",
    "chunk_index": 119,
    "total_chunks": 124,
    "text_content": "medical images and text. arXiv Preprint arXiv:2210.10163 . doi:10.48550/arXiv.2210.10163 Wang, Z., Yu, J., Yu, A. W., Dai, Z., Tsvetkov, Y., and Cao, Y. ( 2022c). \u201cSimVLM: simple visual language model pretraining with weak supervision, \u201d inInternational ConferenceonLearningRepresentations(ICLR) . Waqas, A., Bui, M. M., Glassy, E. F., El Naqa, I., Borkowski, P., Bo rkowski, A. A., et al. (2023). Revolutionizing digital pathology with the power of generative arti\ufb01cial intelligence and foundation m",
    "full_text_length": 160559,
    "chunk_length": 1889
  },
  {
    "chunk_id": 1241,
    "paper_filename": "hartsock_2024_vision_language-models_for_reports_generation_visual_quesitons_answeriing.pdf",
    "paper_title": "Hartsock 2024 Vision Language-Models For Reports Generation Visual Quesitons Answeriing",
    "chunk_index": 120,
    "total_chunks": 124,
    "text_content": "Tu, Z., and He, K. (2016). \u201cA ggregated residual transformations for deep neural networks,\u201d in IEEE Conference on Computer Vision andPatternRecognition(CVPR) ,5987\u20135995. Xie,Z.,Zhang,Z.,Cao,Y.,Lin,Y.,Bao,J.,Yao,Z.,etal.(20 22).\u201cSimMIM:asimple frameworkformaskedimagemodeling,\u201din IEEE/CVFConferenceonComputerVision andPatternRecognition(CVPR) ,9643\u20139653. Xin, C., Liu, Z., Zhao, K., Miao, L., Ma, Y., Zhu, X., et al. (202 2). An improved transformer network for skin cancer classi\ufb01cation. Comput. Biol",
    "full_text_length": 160559,
    "chunk_length": 1536
  },
  {
    "chunk_id": 1242,
    "paper_filename": "hartsock_2024_vision_language-models_for_reports_generation_visual_quesitons_answeriing.pdf",
    "paper_title": "Hartsock 2024 Vision Language-Models For Reports Generation Visual Quesitons Answeriing",
    "chunk_index": 121,
    "total_chunks": 124,
    "text_content": "Yu, F., Endo, M., Krishnan, R., Pan, I., Tsai, A., Reis, E., et al. (2023). Evaluating progress in automatic chest x-ray radiology report generation .Patterns4:100802. doi:10.1016/j.patter.2023.100802 Yuan, Z., Jin, Q., Tan, C., Zhao, Z., Yuan, H., Huang, F., et al. (2023). RAMM: retrieval-augmented biomedical visual question answe ring with multi- modal pre-training. arXiv Preprint arXiv:2303.00534 . doi: 10.48550/arXiv.2303. 00534 Zellers, R., Bisk, Y., Farhadi, A., and Choi, Y. (2019). \u201cFrom ",
    "full_text_length": 160559,
    "chunk_length": 1839
  },
  {
    "chunk_id": 1243,
    "paper_filename": "hartsock_2024_vision_language-models_for_reports_generation_visual_quesitons_answeriing.pdf",
    "paper_title": "Hartsock 2024 Vision Language-Models For Reports Generation Visual Quesitons Answeriing",
    "chunk_index": 122,
    "total_chunks": 124,
    "text_content": "28th ACM International ConferenceonMultimedia ,2345\u20132354. Zhang,C.,Xie,Y.,Bai,H.,Yu,B.,Li,W.,andGao,Y.(2021). Asurveyonfederated learning. Knowl.BasedSyst .216:106775.doi:10.1016/j.knosys.2021.106775 Zhang, H., Niu, Y., and Chang, S.-F. (2018). \u201cGrounding refer ring expressions in images by variational context,\u201d in IEEE/CVF Conference on Computer Vision and PatternRecognition ,4158\u20134166. Zhang,S.,Xu,Y.,Usuyama,N.,Bagga,J.,Tinn,R.,Preston, S.,etal.(2023a).Large- scale domain-speci\ufb01c pretraining f",
    "full_text_length": 160559,
    "chunk_length": 1798
  },
  {
    "chunk_id": 1244,
    "paper_filename": "hartsock_2024_vision_language-models_for_reports_generation_visual_quesitons_answeriing.pdf",
    "paper_title": "Hartsock 2024 Vision Language-Models For Reports Generation Visual Quesitons Answeriing",
    "chunk_index": 123,
    "total_chunks": 124,
    "text_content": "X., and Peng, D. (2019). \u201cDeep supervis ed cross-modal retrieval,\u201d in IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),10386\u201310395. Zhou, D.-W., Zhang, Y., Ning, J., Ye, H.-J., Zhan, D.-C., and Liu, Z. (2023a). Learning without forgetting for vision-language m odels.arXiv Preprint arXiv:2305.19270 .doi:10.48550/arXiv.2305.19270 Zhou, H., Gu, B., Zou, X., Li, Y., Chen, S. S., Zhou, P., et al. (20 23b). A survey of largelanguagemodelsinmedicine:progress,application,andchalle n",
    "full_text_length": 160559,
    "chunk_length": 841
  },
  {
    "chunk_id": 1245,
    "paper_filename": "Hongmin_2024_an_online_mammography_database_with_biopsy_confirmed_types_cmmd.pdf",
    "paper_title": "Hongmin 2024 An Online Mammography Database With Biopsy Confirmed Types Cmmd",
    "chunk_index": 0,
    "total_chunks": 23,
    "text_content": "1 Scientific Data | (2023) 10:123 | https://doi.org/10.1038/s41597-023-02025-1 www.nature.com/scientificdataan Online Mammography Database with Biopsy Confirmed Types Hongmin Cai 1,5 \u2709, Jinhua Wang2,3,5, Tingting Dan1,5, Jiao Li4, Zhihao Fan1, Weiting Yi1, Chunyan Cui4, Xinhua Jiang4 & Li Li4 \u2709 Breast carcinoma is the second largest cancer in the world among women. Early detection of breast cancer has been shown to increase the survival rate, thereby significantly increasing patients\u2019 lifespan. ",
    "full_text_length": 24981,
    "chunk_length": 1376
  },
  {
    "chunk_id": 1246,
    "paper_filename": "Hongmin_2024_an_online_mammography_database_with_biopsy_confirmed_types_cmmd.pdf",
    "paper_title": "Hongmin 2024 An Online Mammography Database With Biopsy Confirmed Types Cmmd",
    "chunk_index": 1,
    "total_chunks": 23,
    "text_content": "1775 patients, which is divided into two branches. The first dataset CMMD1 contains 1026 cases (2214 mammographies) with biopsy confirmed type of benign or malignant tumors. The second dataset CMMD2 includes 1498 mammographies for 749 patients with known molecular subtypes. Our database is constructed to enrich the diversity of mammography data and promote the development of relevant fields. Background & Summary Breast carcinoma is one of the most commonly diagnosed cancer and the second leading",
    "full_text_length": 24981,
    "chunk_length": 1332
  },
  {
    "chunk_id": 1247,
    "paper_filename": "Hongmin_2024_an_online_mammography_database_with_biopsy_confirmed_types_cmmd.pdf",
    "paper_title": "Hongmin 2024 An Online Mammography Database With Biopsy Confirmed Types Cmmd",
    "chunk_index": 2,
    "total_chunks": 23,
    "text_content": "synergistically lead to a rapid rise of the artificial intelligence (AI) for breast imaging in the following three tasks: (1) Computer-aided detection (CADe)4\u20139 aims at locat - ing suspect lesions such as mass and microcalcification, leaving the classification to the radiologist; and (2) Computer-aided diagnosis (CADx) 10\u201314 aims to characterize the suspicious region of lesion and/or estimate its probability of onset; and (3) Findings of predictive image-based biomarkers15\u201318 by applying the com",
    "full_text_length": 24981,
    "chunk_length": 1408
  },
  {
    "chunk_id": 1248,
    "paper_filename": "Hongmin_2024_an_online_mammography_database_with_biopsy_confirmed_types_cmmd.pdf",
    "paper_title": "Hongmin 2024 An Online Mammography Database With Biopsy Confirmed Types Cmmd",
    "chunk_index": 3,
    "total_chunks": 23,
    "text_content": "authorized investigators. The datasets involve the Digital Database for Screening Mammography (DDSM), the Mammographic Imaging Analysis Society (MIAS) database, the Image Retrieval in Medical Application (IRMA) project, and the Curated Breast Imaging Subset of DDSM (CBIS-DDSM). Notwithstanding these public data - sets are useful, there is still a lack of open access datasets that expand beyond the white population, which will enable researchers to verify previous findings and make the dataset mo",
    "full_text_length": 24981,
    "chunk_length": 1440
  },
  {
    "chunk_id": 1249,
    "paper_filename": "Hongmin_2024_an_online_mammography_database_with_biopsy_confirmed_types_cmmd.pdf",
    "paper_title": "Hongmin 2024 An Online Mammography Database With Biopsy Confirmed Types Cmmd",
    "chunk_index": 4,
    "total_chunks": 23,
    "text_content": "Guangzhou, 510006, China. 2Medical Imaging Center, Shenzhen Hospital, Southern Medical University, Shenzhen, 510515, China. 3the t hird of Clinical Medicine, Southern Medical University, Shenzhen, 510515, China. 4Department of Medical imaging, Collaborative Innovation Center for Cancer Medicine, State Key Laboratory of Oncology in South China, Sun Yat- sen University Cancer Center, Guangzhou, 510060, China. 5these authors contributed equally: Hongmin cai, Jinhua Wang, tingting Dan. \u2709e-mail: hmca",
    "full_text_length": 24981,
    "chunk_length": 1549
  },
  {
    "chunk_id": 1250,
    "paper_filename": "Hongmin_2024_an_online_mammography_database_with_biopsy_confirmed_types_cmmd.pdf",
    "paper_title": "Hongmin 2024 An Online Mammography Database With Biopsy Confirmed Types Cmmd",
    "chunk_index": 5,
    "total_chunks": 23,
    "text_content": "markers than CMMD1. Both datasets involved mammography images and clinical data such as age, and benign or malignant tumor. Currently, it is available for research through the International Data-sharing Initiative. Our free data sharing can hasten the clinical application of radiomics approaches. Table 1 lists the popular and pub - licly available databases in the field of mammography. Methods patient recruitment. Ethical approval was acquired for this retrospective analysis, and the requirement",
    "full_text_length": 24981,
    "chunk_length": 1352
  },
  {
    "chunk_id": 1251,
    "paper_filename": "Hongmin_2024_an_online_mammography_database_with_biopsy_confirmed_types_cmmd.pdf",
    "paper_title": "Hongmin 2024 An Online Mammography Database With Biopsy Confirmed Types Cmmd",
    "chunk_index": 6,
    "total_chunks": 23,
    "text_content": "with the inclusion and exclusion criteria. It is clear that CMMD1 and CMMD2 are the subsets of CMMD, CMMD1 merely distinguishes between benign and malignant patients (see the Exclusion criteria 1 in Fig. 1), while CMMD2 only contains malignant cases with detailed molecular subtypes (see the Exclusion criteria 2 in Fig. 1). image collection and interpretation. Image data were acquired on a GE Senographe DS mammogra- phy system and a Siemens Mammomat Inspiration mammography system in the SunY at-s",
    "full_text_length": 24981,
    "chunk_length": 1309
  },
  {
    "chunk_id": 1252,
    "paper_filename": "Hongmin_2024_an_online_mammography_database_with_biopsy_confirmed_types_cmmd.pdf",
    "paper_title": "Hongmin 2024 An Online Mammography Database With Biopsy Confirmed Types Cmmd",
    "chunk_index": 7,
    "total_chunks": 23,
    "text_content": "resolution of 2294 \u00d7 1914 pixels. Two radiologists with at least five years of experience performed mammography interpretation and guidance before surgery to determine which patients should be treated surgically. It was asked to refer to the standard readings of the breast imaging report and data system, established by the American College of Radiology24. By referring to commonly used X-ray classification methods, the images are divided into three types of masses, calcifications, and both. Note,",
    "full_text_length": 24981,
    "chunk_length": 1412
  },
  {
    "chunk_id": 1253,
    "paper_filename": "Hongmin_2024_an_online_mammography_database_with_biopsy_confirmed_types_cmmd.pdf",
    "paper_title": "Hongmin 2024 An Online Mammography Database With Biopsy Confirmed Types Cmmd",
    "chunk_index": 8,
    "total_chunks": 23,
    "text_content": "analyzed the tissue morphology under the microscope. If necessary, surgery was performed to extract the suspicious lesion specimen. The immu - nohistochemistry test is conducted to determine the pathological result. immunohistochemistry. According to the different expressions for immunohistochemistry including estrogen receptor (ER), progesterone receptor (PR), human epidermal growth factor receptor 2 (HER2), and Ki-67, invasive breast carcinoma is divided into four molecular subtypes, including",
    "full_text_length": 24981,
    "chunk_length": 1284
  },
  {
    "chunk_id": 1254,
    "paper_filename": "Hongmin_2024_an_online_mammography_database_with_biopsy_confirmed_types_cmmd.pdf",
    "paper_title": "Hongmin 2024 An Online Mammography Database With Biopsy Confirmed Types Cmmd",
    "chunk_index": 9,
    "total_chunks": 23,
    "text_content": "greater than or equal to 1% (\u22651%) of tumor cells. In assessing the expression of HER2, the specimen was first graded by IHC and scored by 0 to 3+ , according to the recommendations of the American DatabaseNumber of casesNumber of imagesMolecular subtype Image categories Origin MIAS 23161 322 No benign, malignant, normal UK DDSM302620 10480 No benign, malignant, normal USA LAPIMO22320 1400 No benign, malignant, normal Brazil INBreast21115 410 No benign, malignant, normal Portugal BCDR-DOX, BCDR-N",
    "full_text_length": 24981,
    "chunk_length": 1339
  },
  {
    "chunk_id": 1255,
    "paper_filename": "Hongmin_2024_an_online_mammography_database_with_biopsy_confirmed_types_cmmd.pdf",
    "paper_title": "Hongmin 2024 An Online Mammography Database With Biopsy Confirmed Types Cmmd",
    "chunk_index": 10,
    "total_chunks": 23,
    "text_content": "membrane staining in less than 10% (<10%) of tumor cells, the score was set as 0. If there are greater than or equal to 10% (\u2265 10%) of tumor cell membrane staining or the cell membrane staining faintly/barely noticeable, the score was marked as 1+ . If there is weakly to moderately complete membrane staining observed in more than 10% (>10%) of tumor cells, the score was marked as 2+. In this case, the tissue was further evaluated by fluorescence in situ hybridization (FISH) analysis for HER2 gen",
    "full_text_length": 24981,
    "chunk_length": 1178
  },
  {
    "chunk_id": 1256,
    "paper_filename": "Hongmin_2024_an_online_mammography_database_with_biopsy_confirmed_types_cmmd.pdf",
    "paper_title": "Hongmin 2024 An Online Mammography Database With Biopsy Confirmed Types Cmmd",
    "chunk_index": 11,
    "total_chunks": 23,
    "text_content": "a fully reproducible dataset, as shown in Fig. 2. Data records Subject identifiers. A unique identifier for each subject was identical in all two public datasets in this data- base. Subject IDs were 4-digit numbers in the form of D1-xxxx or D2-xxxx.Fig. 1 Recruitment pathway for patients in our study. ab cd Patien tC linica l DiagnosisPathological DiagnosisIHC marker s Fig. 2 Study design for the construction of mammography data of breast. (a ) Patients with lesions of breast were selected for t",
    "full_text_length": 24981,
    "chunk_length": 1398
  },
  {
    "chunk_id": 1257,
    "paper_filename": "Hongmin_2024_an_online_mammography_database_with_biopsy_confirmed_types_cmmd.pdf",
    "paper_title": "Hongmin 2024 An Online Mammography Database With Biopsy Confirmed Types Cmmd",
    "chunk_index": 12,
    "total_chunks": 23,
    "text_content": "of each case. 4 Scientific Data | (2023) 10:123 | https://doi.org/10.1038/s41597-023-02025-1 www.nature.com/scientificdata www.nature.com/scientificdata/imaging and clinical data. The CMMD collection28 contains breast mammography images and corre - sponding clinical data. Imaging, clinical data for all subjects are stored in The Cancer Imaging Archive https:// www.cancerimagingarchive.net/ under https://doi.org/10.7937/tcia.eqde-4b16. Imaging data for all sub- jects are were store in the folder ",
    "full_text_length": 24981,
    "chunk_length": 1465
  },
  {
    "chunk_id": 1258,
    "paper_filename": "Hongmin_2024_an_online_mammography_database_with_biopsy_confirmed_types_cmmd.pdf",
    "paper_title": "Hongmin 2024 An Online Mammography Database With Biopsy Confirmed Types Cmmd",
    "chunk_index": 13,
    "total_chunks": 23,
    "text_content": "subtypes that are able to assist the doctor for the clinical guidance or the related studies on immunohistochemistry. Limitations of CMMD. Our data has some notable limitations. First, the sample size is not very large. Second, the ROI is not marked. We will add more available information and increase the amount of data in the future. Technical Validation All data were collected by the hospital and used as part of the diagnosis, therefore all quality assurances were performed by the institution ",
    "full_text_length": 24981,
    "chunk_length": 1235
  },
  {
    "chunk_id": 1259,
    "paper_filename": "Hongmin_2024_an_online_mammography_database_with_biopsy_confirmed_types_cmmd.pdf",
    "paper_title": "Hongmin 2024 An Online Mammography Database With Biopsy Confirmed Types Cmmd",
    "chunk_index": 14,
    "total_chunks": 23,
    "text_content": "749 AbnormalityMass {726 417 Calcifications 158 98 Both 223 234 Molecular subtypeLuminal A \u2014 152 Luminal B \u2014 376 HER2-enriched \u2014 135 Triple-negative \u2014 86 Table 2 . Statistics on clinical-demographic of enrolled patients. CDMM2CDMM1 Fig. 3 An example of clinical data for CMMD1 and CMMD2. 5 Scientific Data | (2023) 10:123 | https://doi.org/10.1038/s41597-023-02025-1 www.nature.com/scientificdata www.nature.com/scientificdata/Code availability Code for data cleaning and analysis is provided as part",
    "full_text_length": 24981,
    "chunk_length": 1555
  },
  {
    "chunk_id": 1260,
    "paper_filename": "Hongmin_2024_an_online_mammography_database_with_biopsy_confirmed_types_cmmd.pdf",
    "paper_title": "Hongmin 2024 An Online Mammography Database With Biopsy Confirmed Types Cmmd",
    "chunk_index": 15,
    "total_chunks": 23,
    "text_content": "cancer screening and diagnosis in women with dense breasts\u2013a systematic review and meta-analysis. BMC cancer 18, 1\u20139, https://doi.org/10.1186/s12885- 018-4263-3 (2018). 4. Wang, J. & Y ang, Y . A context-sensitive deep learning approach for microcalcification detection in mammograms. Pattern Recognition 78, 12\u201322, https://doi.org/10.1016/j.patcog.2018.01.009 (2018). 5. Kooi, T. et al . Large scale deep learning for computer aided detection of mammographic lesions. Medical Image Analysis 35, 303\u2013",
    "full_text_length": 24981,
    "chunk_length": 1601
  },
  {
    "chunk_id": 1261,
    "paper_filename": "Hongmin_2024_an_online_mammography_database_with_biopsy_confirmed_types_cmmd.pdf",
    "paper_title": "Hongmin 2024 An Online Mammography Database With Biopsy Confirmed Types Cmmd",
    "chunk_index": 16,
    "total_chunks": 23,
    "text_content": "9. Rodriguez-Ruiz, A. et al . Stand-alone artificial intelligence for breast cancer detection in mammography: comparison with 101 radiologists. JNCI: Journal of the National Cancer Institute 111, 916\u2013922, https://doi.org/10.1093/jnci/djy222 (2019). 10. Li, J. et al. Predicting underestimation of ductal carcinoma in situ : a comparison between radiomics and conventional approaches. International Journal of Computer Assisted Radiology and Surgery 14, 709\u2013721, https://doi.org/10.1007/s11548-018-190",
    "full_text_length": 24981,
    "chunk_length": 1551
  },
  {
    "chunk_id": 1262,
    "paper_filename": "Hongmin_2024_an_online_mammography_database_with_biopsy_confirmed_types_cmmd.pdf",
    "paper_title": "Hongmin 2024 An Online Mammography Database With Biopsy Confirmed Types Cmmd",
    "chunk_index": 17,
    "total_chunks": 23,
    "text_content": "577, 89\u201394, https://doi. org/10.1038/s41586-019-1799-6 (2020). 14. Cai, H. et al . Breast microcalcification diagnosis using deep convolutional neural network from digital mammograms. Computational and Mathematical Methods in Medicine 2019, 2717454, https://doi.org/10.1155/2019/2717454 (2019). 15. Chen, Y . et al . Evaluation of triple-negative breast cancer early detection via mammography screening and outcomes in african american and white american patients. JAMA Surgery https://doi.org/10.100",
    "full_text_length": 24981,
    "chunk_length": 1584
  },
  {
    "chunk_id": 1263,
    "paper_filename": "Hongmin_2024_an_online_mammography_database_with_biopsy_confirmed_types_cmmd.pdf",
    "paper_title": "Hongmin 2024 An Online Mammography Database With Biopsy Confirmed Types Cmmd",
    "chunk_index": 18,
    "total_chunks": 23,
    "text_content": "curated mammography data set for use in computer-aided detection and diagnosis research. Scientific Data 4, 170177\u2013170177, https://doi.org/10.1038/sdata.2017.177 (2017). 20. Lopez, M. G. et al . Bcdr: a breast cancer digital repository. 15th International conference on experimental mechanics 1215, 113\u2013120, https://bcdr.eu/information/about (2012). 21. Moreira, I. C. et al . Inbreast: toward a full-field digital mammographic database. Academic radiology 19, 236\u2013248, https://doi. org/10.1016/j.acr",
    "full_text_length": 24981,
    "chunk_length": 1529
  },
  {
    "chunk_id": 1264,
    "paper_filename": "Hongmin_2024_an_online_mammography_database_with_biopsy_confirmed_types_cmmd.pdf",
    "paper_title": "Hongmin 2024 An Online Mammography Database With Biopsy Confirmed Types Cmmd",
    "chunk_index": 19,
    "total_chunks": 23,
    "text_content": "Yun, S. C., Pawlik, T. M. & Vauthey, J. N. 8th edition of the ajcc cancer staging manual: Pancreas and hepatobiliary cancers. Annals of Surgical Oncology 25, 1\u20133, https://doi.org/10.1245/s10434-017-6025-x (2017). 26. Wolff, A. C. et al. Human Epidermal Growth Factor Receptor 2 Testing in Breast Cancer: American Society of Clinical Oncology/ College of American Pathologists Clinical Practice Guideline Focused Update. Archives of Pathology Laboratory Medicine 142, 1364\u20131382, https://doi.org/10.585",
    "full_text_length": 24981,
    "chunk_length": 1591
  },
  {
    "chunk_id": 1265,
    "paper_filename": "Hongmin_2024_an_online_mammography_database_with_biopsy_confirmed_types_cmmd.pdf",
    "paper_title": "Hongmin 2024 An Online Mammography Database With Biopsy Confirmed Types Cmmd",
    "chunk_index": 20,
    "total_chunks": 23,
    "text_content": "https://doi.org/10.1038/srep27327 (2016). 30. Bowyer, K. et al. The digital database for screening mammography. Third international workshop on digital mammography 58, 27 http://www.eng.usf.edu/cvprg/Mammography/Database.html (1996). 31. Clark, K. et al . The cancer imaging archive (tcia): maintaining and operating a public information repository. Journal of digital imaging 26, 1045\u20131057, https://doi.org/10.1007/s10278-013-9622-7 (2013). 32. Halling-Brown, M. D. et al. Optimam mammography image ",
    "full_text_length": 24981,
    "chunk_length": 1644
  },
  {
    "chunk_id": 1266,
    "paper_filename": "Hongmin_2024_an_online_mammography_database_with_biopsy_confirmed_types_cmmd.pdf",
    "paper_title": "Hongmin 2024 An Online Mammography Database With Biopsy Confirmed Types Cmmd",
    "chunk_index": 21,
    "total_chunks": 23,
    "text_content": "China (U21A20520, 62172112), the Key-Area Research and Development of Guangdong Province (2022A0505050014, 2020B1111190001), the Key-Area Research and Development Program of Guangzhou City (202206030009), the National Key Research and Development Program of China (2022YFE0112200), Shenzhen Science and Technology Program (JCYJ20210324125403011) and Open fund program of national innovation center for advanced medical devices (NMED2021MS-01-003). Author contributions L.L., C.C. and H.C. conceived a",
    "full_text_length": 24981,
    "chunk_length": 1459
  },
  {
    "chunk_id": 1267,
    "paper_filename": "Hongmin_2024_an_online_mammography_database_with_biopsy_confirmed_types_cmmd.pdf",
    "paper_title": "Hongmin 2024 An Online Mammography Database With Biopsy Confirmed Types Cmmd",
    "chunk_index": 22,
    "total_chunks": 23,
    "text_content": "at www.nature.com/reprints.Publisher\u2019s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations. Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Cre-ative Commons license, and indicat",
    "full_text_length": 24981,
    "chunk_length": 1037
  },
  {
    "chunk_id": 1268,
    "paper_filename": "Huan_2024_survey_Multimodal_deep_earning_approaches_for_precision_oncology.pdf",
    "paper_title": "Huan 2024 Survey Multimodal Deep Earning Approaches For Precision Oncology",
    "chunk_index": 0,
    "total_chunks": 79,
    "text_content": "Received: November 12, 2024. Revised: December 2, 2024. Accepted: December 18, 2024 \u00a9 The Author(s) 2025. Published by Oxford University Press. This is an Open Access article distributed under the terms of the Creative Commons Attribution License ( https://creativecommons.org/licenses/by/4.0/ ), which permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited.Briefings in Bioinformatics , 2025, 26(1) , bbae699 https://doi.org/10.1093/bi",
    "full_text_length": 87495,
    "chunk_length": 1611
  },
  {
    "chunk_id": 1269,
    "paper_filename": "Huan_2024_survey_Multimodal_deep_earning_approaches_for_precision_oncology.pdf",
    "paper_title": "Huan 2024 Survey Multimodal Deep Earning Approaches For Precision Oncology",
    "chunk_index": 1,
    "total_chunks": 79,
    "text_content": "University, Jinming Avenue, Longting District, Kaifeng 475001, Henan, China 5Institute of Fundamental and Frontier Sciences, University of Electronic Science and Technology of China, Section2, North Jianshe Road, Chenghua D istrict, Chengdu 610054, Sichuan, China 6Department of Nephrology, Xuanwu Hospital, Capital Medical University, Changchun Street, Xicheng District, Beijing 100053, China *Corresponding authors. Quan Zou, Yangtze Delta Region Institute (Quzhou), University of Electronic Scienc",
    "full_text_length": 87495,
    "chunk_length": 1467
  },
  {
    "chunk_id": 1270,
    "paper_filename": "Huan_2024_survey_Multimodal_deep_earning_approaches_for_precision_oncology.pdf",
    "paper_title": "Huan 2024 Survey Multimodal Deep Earning Approaches For Precision Oncology",
    "chunk_index": 2,
    "total_chunks": 79,
    "text_content": "Abstract The burgeoning accumulation of large-scale biomedical data in oncology, alongside significant strides in deep learning (DL) technolo- gies, has established multimodal DL (MDL) as a cornerstone of precision oncology. This review provides an overview of MDL applications in this field, based on an extensive literature survey. In total, 651 articles published before September 2024 are included. We first outline publicly available multimodal datasets that support cancer research. Then, we di",
    "full_text_length": 87495,
    "chunk_length": 1462
  },
  {
    "chunk_id": 1271,
    "paper_filename": "Huan_2024_survey_Multimodal_deep_earning_approaches_for_precision_oncology.pdf",
    "paper_title": "Huan 2024 Survey Multimodal Deep Earning Approaches For Precision Oncology",
    "chunk_index": 3,
    "total_chunks": 79,
    "text_content": "the foremost causes of mortality glob- ally. The 2020 report from the International Agency for Research on Cancer identified \u223c18.1 million new cancer cases and 9.6 million cancer-related deaths across 185 countries, both figuresrising alarmingly [ 1]. In the USA, the economic burden of can- cer was estimated at \u223c$124.5 billion in 2010, with projections rising to $157.8 billion by 2020 [ 2]. The emergence of novel can- cer therapies, such as targeted therapies and immunotherapies, underscores the",
    "full_text_length": 87495,
    "chunk_length": 1411
  },
  {
    "chunk_id": 1272,
    "paper_filename": "Huan_2024_survey_Multimodal_deep_earning_approaches_for_precision_oncology.pdf",
    "paper_title": "Huan 2024 Survey Multimodal Deep Earning Approaches For Precision Oncology",
    "chunk_index": 4,
    "total_chunks": 79,
    "text_content": "cancer screening have successfully reduced the pool of candidates while maintaining high inclusion rates and positive predictive values [ 5]. Natural language processing (NLP) techniques are increasingly applied to extract valuable insights from electronic health records (EHRs), aiding clinicians in decision-making [ 6]. DL has demonstrated exceptional perfor- mance in tasks such as biological sequence classification [ 7]a n d cancer subtyping [ 8], with artificial intelligence (AI) systems even",
    "full_text_length": 87495,
    "chunk_length": 1475
  },
  {
    "chunk_id": 1273,
    "paper_filename": "Huan_2024_survey_Multimodal_deep_earning_approaches_for_precision_oncology.pdf",
    "paper_title": "Huan 2024 Survey Multimodal Deep Earning Approaches For Precision Oncology",
    "chunk_index": 5,
    "total_chunks": 79,
    "text_content": "rapid advancement of computing and biomedi-cal technologies, along with the explosive growth of clinical data, highlights the urgent need for integrated multimodal data analy- sis to fully harness clinical information and gain deeper insights into cancer mechanisms.Downloaded from https://academic.oup.com/bib/article/26/1/bbae699/7942793 by guest on 03 April 2025 2|Yang et al. Clinical value of multimodal fusion analysis Vast amounts of multimodal data are generated throughout the clinical proce",
    "full_text_length": 87495,
    "chunk_length": 1519
  },
  {
    "chunk_id": 1274,
    "paper_filename": "Huan_2024_survey_Multimodal_deep_earning_approaches_for_precision_oncology.pdf",
    "paper_title": "Huan 2024 Survey Multimodal Deep Earning Approaches For Precision Oncology",
    "chunk_index": 6,
    "total_chunks": 79,
    "text_content": "molecular omics with imaging data further exemplifies this trend [ 14,15]. Cross-modal fusion that encompasses imaging, molecular, and clinical data represents advanced stages of multimodal analysis [16]. Numerous studies have shown that multimodal methods out- performs single-modality approaches in specific tasks [ 17,18]. Nevertheless, designing effective fusion methods presents severalchallenges, including the high-dimensional nature of multimodal data, issues with data incompleteness and mod",
    "full_text_length": 87495,
    "chunk_length": 1488
  },
  {
    "chunk_id": 1275,
    "paper_filename": "Huan_2024_survey_Multimodal_deep_earning_approaches_for_precision_oncology.pdf",
    "paper_title": "Huan 2024 Survey Multimodal Deep Earning Approaches For Precision Oncology",
    "chunk_index": 7,
    "total_chunks": 79,
    "text_content": "a comprehensive overview of cross-scale multimodal data fusion. Consequently, a thorough review of MDL methods across the entire precision oncology continuum is stilllacking. Given the rapid expansion of medical multimodal data and the swift evolution of MDL technologies, this paper aims to surveyvarious modalities involved in precision oncology and the cutting- edge MDL models employed for data integration, thereby estab- lishing a paradigm for the effective utilization of big data in cancerman",
    "full_text_length": 87495,
    "chunk_length": 1431
  },
  {
    "chunk_id": 1276,
    "paper_filename": "Huan_2024_survey_Multimodal_deep_earning_approaches_for_precision_oncology.pdf",
    "paper_title": "Huan 2024 Survey Multimodal Deep Earning Approaches For Precision Oncology",
    "chunk_index": 8,
    "total_chunks": 79,
    "text_content": "methods A systematic search was conducted in September 2024 across PubMed, MEDLINE, and Web of Science Core Collection forpeer-reviewed articles published in English, with no date restrictions. Search terms included medical topics (e.g. cancer, tumors, lesions), methodologies (e.g. deep learning, artificialintelligence, convolutional neural networks, machine learning), and data types (e.g. multimodal, multi-omics, data fusion). T wo independent researchers performed the search to ensureaccuracy;",
    "full_text_length": 87495,
    "chunk_length": 1530
  },
  {
    "chunk_id": 1277,
    "paper_filename": "Huan_2024_survey_Multimodal_deep_earning_approaches_for_precision_oncology.pdf",
    "paper_title": "Huan 2024 Survey Multimodal Deep Earning Approaches For Precision Oncology",
    "chunk_index": 9,
    "total_chunks": 79,
    "text_content": "individuallyor in combination\u2014include radiomics, pathomics, acoustic and endoscopic imaging, genomics, clinical data, dermoscopy, multi- modal data, and emerging real-world data ( Supplementary Mate- rials 1 ). Here, we summarized credible publicly available multi- modal oncology resources and representative MDL studies utiliz- ing these datasets ( Table 1 ) for the readers\u2019 convenience. Notably, The Cancer Genome Atlas (TCGA) ( https:// portal. gdc. cancer. gov/ ) and The Cancer Imaging Archive",
    "full_text_length": 87495,
    "chunk_length": 1424
  },
  {
    "chunk_id": 1278,
    "paper_filename": "Huan_2024_survey_Multimodal_deep_earning_approaches_for_precision_oncology.pdf",
    "paper_title": "Huan 2024 Survey Multimodal Deep Earning Approaches For Precision Oncology",
    "chunk_index": 10,
    "total_chunks": 79,
    "text_content": "responding results [ 33]. DNA Evaluation of Fragments for Early Interception (DELFI) offers cell-free DNA (cfDNA) fragmentation profiles and clinical data for 296 lung cancer patients [ 34]. Adap- tive Support Vector Machine (ASVM) integrates cfDNA fragmen- tome, CNVs, and clinical data for 423 patients across eight cancer types [ 35]. The HAM10000 dataset consists of 10 015 multicenter dermatoscopic images with corresponding clinical data aimed at improving melanoma detection [ 36]. These resou",
    "full_text_length": 87495,
    "chunk_length": 1375
  },
  {
    "chunk_id": 1279,
    "paper_filename": "Huan_2024_survey_Multimodal_deep_earning_approaches_for_precision_oncology.pdf",
    "paper_title": "Huan 2024 Survey Multimodal Deep Earning Approaches For Precision Oncology",
    "chunk_index": 11,
    "total_chunks": 79,
    "text_content": "common in multimodal datasets, which contrasts sharply with DL models\u2019 eagerness for large amounts of labeled data. Fortunately, several techniques have shown promise to reduce the reliance on extensive data labeling while maintaining model performance and data security. Transfer learning Transfer learning (TL) has emerged as a powerful tool in the fieldof DL-based medical data analysis [ 37]. By delivering knowledge from one domain to another, TL facilitates the resolution of analogous tasks. T",
    "full_text_length": 87495,
    "chunk_length": 1618
  },
  {
    "chunk_id": 1280,
    "paper_filename": "Huan_2024_survey_Multimodal_deep_earning_approaches_for_precision_oncology.pdf",
    "paper_title": "Huan 2024 Survey Multimodal Deep Earning Approaches For Precision Oncology",
    "chunk_index": 12,
    "total_chunks": 79,
    "text_content": "Public oncology resources for MDL algorithm development Resource Cancer Modality URL Representative studies TCGA Pan-cancer Histopathology, multi-omics, clinical data https://portal.gdc.cancer.gov/ [14,27\u201330] TCIA Pan-cancer Histopathology, radiology, MR, US, clinical data https://www.cancerimagingarchive.net/ [31,32] Lung-CLiP Lung Clinical data, SNV , CNV https://doi.org/10.1038/s41586-020-2140-0; http:// clip. stanford. edu[33] DELFI Lung Clinical data, cfDNA https://doi.org/10.1016/j.chest.2",
    "full_text_length": 87495,
    "chunk_length": 1656
  },
  {
    "chunk_id": 1281,
    "paper_filename": "Huan_2024_survey_Multimodal_deep_earning_approaches_for_precision_oncology.pdf",
    "paper_title": "Huan 2024 Survey Multimodal Deep Earning Approaches For Precision Oncology",
    "chunk_index": 13,
    "total_chunks": 79,
    "text_content": "At the beginning of each round, the server distributes the current global model to allclients. Each client then trains the model on their local data, updates it, and returns the modified model to the server. The server aggregates these updates to enhance the global model,thus completing one training cycle. Throughout this process, participants\u2019 data remain on their devices, and only encrypted model updates are exchanged with the server, ensuring dataconfidentiality.Supervise or not? The efficacy",
    "full_text_length": 87495,
    "chunk_length": 1486
  },
  {
    "chunk_id": 1282,
    "paper_filename": "Huan_2024_survey_Multimodal_deep_earning_approaches_for_precision_oncology.pdf",
    "paper_title": "Huan 2024 Survey Multimodal Deep Earning Approaches For Precision Oncology",
    "chunk_index": 14,
    "total_chunks": 79,
    "text_content": "the discrepancy between predicted andactual values. SL excels in predictive accuracy, making it widely used for tasks such as classifying tumor subtypes and predicting patient outcomes [ 39,40]. However, SL requires substantial labeled datasets, which can be challenging to obtain in healthcare,Downloaded from https://academic.oup.com/bib/article/26/1/bbae699/7942793 by guest on 03 April 2025 4|Yang et al. and it assumes a specific data distribution, potentially limiting generalization ability to",
    "full_text_length": 87495,
    "chunk_length": 1429
  },
  {
    "chunk_id": 1283,
    "paper_filename": "Huan_2024_survey_Multimodal_deep_earning_approaches_for_precision_oncology.pdf",
    "paper_title": "Huan 2024 Survey Multimodal Deep Earning Approaches For Precision Oncology",
    "chunk_index": 15,
    "total_chunks": 79,
    "text_content": "learning (SSL) allows the model to generate its own labels from the data. Users create a pretext task related to the primary task of interest. By solving this pretext task, pseudo-labels are produced based on specific input attributes, enabling the model to learn representations transferable to the primary task, even with limited labeled data [ 42]. SSL is especially useful when labeled data are scarce or costly to acquire; however, the design of the pretext task is crucial for ensuring the rele",
    "full_text_length": 87495,
    "chunk_length": 1383
  },
  {
    "chunk_id": 1284,
    "paper_filename": "Huan_2024_survey_Multimodal_deep_earning_approaches_for_precision_oncology.pdf",
    "paper_title": "Huan 2024 Survey Multimodal Deep Earning Approaches For Precision Oncology",
    "chunk_index": 16,
    "total_chunks": 79,
    "text_content": "for applications where accuracy is paramount. The choice of learning paradigm in precision oncology depends on the specific application, data availability and quality, and the desired level of accuracy and interpretability. SL is ideal for tasks with ample labeled data and clear target variables, while WSLis beneficial when data are limited or noisy. SSL is effective for pretraining models on large unlabeled datasets, and USL is valu- able for exploratory data analysis. By understanding the stre",
    "full_text_length": 87495,
    "chunk_length": 1519
  },
  {
    "chunk_id": 1285,
    "paper_filename": "Huan_2024_survey_Multimodal_deep_earning_approaches_for_precision_oncology.pdf",
    "paper_title": "Huan 2024 Survey Multimodal Deep Earning Approaches For Precision Oncology",
    "chunk_index": 17,
    "total_chunks": 79,
    "text_content": "involves extracting semantic infor-mation from diverse data forms into real-valued vectors. Medical data encompass structured, semi-structured, and unstructured formats. Effective data representation methods arevital for revealing relational insights, thereby facilitating accurate computer-aided diagnosis and prognosis. This representation can be categorized into unimodal and cross-modal approaches, asdetailed below. Figure 2. Taxonomy of multimodal fusion strategies. Unimodal representation Uni",
    "full_text_length": 87495,
    "chunk_length": 1651
  },
  {
    "chunk_id": 1286,
    "paper_filename": "Huan_2024_survey_Multimodal_deep_earning_approaches_for_precision_oncology.pdf",
    "paper_title": "Huan 2024 Survey Multimodal Deep Earning Approaches For Precision Oncology",
    "chunk_index": 18,
    "total_chunks": 79,
    "text_content": "Cross-modal representation Cross-modal representation, or joint representation, integratesfeatures from multiple modalities, capturing complementary,redundant, or cooperative information. Canonical correlation analysis (CCA) is a traditional method for cross-modal infor- mation representation, mapping multimodal data\u2014such asimages and text\u2014into a shared latent space by identifying linear combinations of multidimensional variables [ 46]. While CCA enhances multimodal model performance, its linear",
    "full_text_length": 87495,
    "chunk_length": 1586
  },
  {
    "chunk_id": 1287,
    "paper_filename": "Huan_2024_survey_Multimodal_deep_earning_approaches_for_precision_oncology.pdf",
    "paper_title": "Huan 2024 Survey Multimodal Deep Earning Approaches For Precision Oncology",
    "chunk_index": 19,
    "total_chunks": 79,
    "text_content": "answering [ 49]. Despite these advancements, current research inadequately addresses interference and adverse effects from modality-specific information irrelevant to target tasks. Additionally, existing encoding methods, often derived from natural language or imageprocessing, may be overly simplistic for the specialized context of medical data, leading to complex, redundant structures and low parameter efficiency in developing multimodal learningframeworks. Multimodal fusion Multimodal feature ",
    "full_text_length": 87495,
    "chunk_length": 1510
  },
  {
    "chunk_id": 1288,
    "paper_filename": "Huan_2024_survey_Multimodal_deep_earning_approaches_for_precision_oncology.pdf",
    "paper_title": "Huan 2024 Survey Multimodal Deep Earning Approaches For Precision Oncology",
    "chunk_index": 20,
    "total_chunks": 79,
    "text_content": "straightforward approach for integrating multimodal data, wherein features from diverse modalities are concatenated and directly input into a DL model ( Fig. 3A ). This technique treats the resulting vector as a unimodal input, preserv- ing the original model architecture. Joint representations of mul- timodal inputs are learned directly, bypassing explicit marginalrepresentations. Early fusion can be further divided into two categories: direct modeling and AutoEncoder (AE) methods. In direct mo",
    "full_text_length": 87495,
    "chunk_length": 1502
  },
  {
    "chunk_id": 1289,
    "paper_filename": "Huan_2024_survey_Multimodal_deep_earning_approaches_for_precision_oncology.pdf",
    "paper_title": "Huan 2024 Survey Multimodal Deep Earning Approaches For Precision Oncology",
    "chunk_index": 21,
    "total_chunks": 79,
    "text_content": "model to integrate multi-omics data, identifying drug\u2013omics asso- ciations across multimodal datasets for type 2 diabetes patients [51]. While early fusion effectively captures low-level cross- modal relationships without requiring marginal representation extraction, it may struggle to discern high-level relationshipsand is sensitive to differences in the sampling rates of various modalities. Intermediate fusion Intermediate fusion involves initially learning each modalityindependently before in",
    "full_text_length": 87495,
    "chunk_length": 1577
  },
  {
    "chunk_id": 1290,
    "paper_filename": "Huan_2024_survey_Multimodal_deep_earning_approaches_for_precision_oncology.pdf",
    "paper_title": "Huan 2024 Survey Multimodal Deep Earning Approaches For Precision Oncology",
    "chunk_index": 22,
    "total_chunks": 79,
    "text_content": "marginal and joint categories based on representation handling. Marginal intermediate fusion concatenates learnedmarginal representations as inputs to fusion layers, while joint intermediate fusion encodes more abstract features from multiple modalities prior to integration. In marginal homogeneous intermediate fusion, identical neu- ral networks learn marginal representations, which are later com- bined for decision-making. For example, Gu et al. employed a 3D U- Net to encode positron emission",
    "full_text_length": 87495,
    "chunk_length": 1519
  },
  {
    "chunk_id": 1291,
    "paper_filename": "Huan_2024_survey_Multimodal_deep_earning_approaches_for_precision_oncology.pdf",
    "paper_title": "Huan 2024 Survey Multimodal Deep Earning Approaches For Precision Oncology",
    "chunk_index": 23,
    "total_chunks": 79,
    "text_content": "followed by joint representation learning fromthis composite. For example, Yuan et al. constructed two identical convolutional\u2013long short-term memory (Conv-LSTM) encoders to extract features from PET and CT, respectively, and these featureswere concatenated and transformed by a LSTM module for the sample [ 53]. Joint heterogeneous intermediate fusion employs different net- works for each modality, subsequently deriving joint represen- tations from concatenated marginal representations. Hu et al ",
    "full_text_length": 87495,
    "chunk_length": 1499
  },
  {
    "chunk_id": 1292,
    "paper_filename": "Huan_2024_survey_Multimodal_deep_earning_approaches_for_precision_oncology.pdf",
    "paper_title": "Huan 2024 Survey Multimodal Deep Earning Approaches For Precision Oncology",
    "chunk_index": 24,
    "total_chunks": 79,
    "text_content": "Late fusion, inspired by ensemble classification, consolidates predictions from individual sub-models trained on distinctdata modalities to make a final decision ( Fig. 3C ). This can be accomplished through various methods, including voting, averaging, or meta-learning. For example, Saikia et al.c o m p a r e d majority voting and weighted voting approaches for predicting human papillomavirus status using PET-CT images [ 31]. Sedghi et al . improved prostate cancer detection by averaging output",
    "full_text_length": 87495,
    "chunk_length": 1448
  },
  {
    "chunk_id": 1293,
    "paper_filename": "Huan_2024_survey_Multimodal_deep_earning_approaches_for_precision_oncology.pdf",
    "paper_title": "Huan 2024 Survey Multimodal Deep Earning Approaches For Precision Oncology",
    "chunk_index": 25,
    "total_chunks": 79,
    "text_content": "including data heterogeneity, researcher intuition, biological implications, thepresence of missing values or noise, experimental evidence, com- putational resources, or a combination of these elements. MDL applications in precision oncology The integration of AI at various stages can correlate clinical lab- oratory tests and examination data with oncological phenotypes.The adaptability of clinical tasks involving multimodal data varies across different contexts. This section delves into cutting",
    "full_text_length": 87495,
    "chunk_length": 1518
  },
  {
    "chunk_id": 1294,
    "paper_filename": "Huan_2024_survey_Multimodal_deep_earning_approaches_for_precision_oncology.pdf",
    "paper_title": "Huan 2024 Survey Multimodal Deep Earning Approaches For Precision Oncology",
    "chunk_index": 26,
    "total_chunks": 79,
    "text_content": "significantly enhancing the accuracy of pulmonary perfu-sion volume quantification compared to methods relying solely on metabolic data [ 52]. The complexity of understanding spa- tial correspondences increases when input modalities exhibitsubstantial discrepancies in appearance. To mitigate this, Song et al. proposed a contrastive learning\u2013based cross-modal attention block that correlates features extracted from transrectal ultra-sound (TRUS) and MRI. These correlations were integrated into a d",
    "full_text_length": 87495,
    "chunk_length": 1545
  },
  {
    "chunk_id": 1295,
    "paper_filename": "Huan_2024_survey_Multimodal_deep_earning_approaches_for_precision_oncology.pdf",
    "paper_title": "Huan 2024 Survey Multimodal Deep Earning Approaches For Precision Oncology",
    "chunk_index": 27,
    "total_chunks": 79,
    "text_content": "integrates fullyconvolutional networks (FCNs) and recurrent neural networks(RNNs) within a unified framework, achieving segmentation results characterized by both appearance and spatial consistency. They trained three segmentation models using 2D MRI patches from axial, coronal, and sagittal views, merging results througha voting-based fusion strategy [ 58]. Beyond tissue or organ segmentation, cell segmentation is fundamental for various downstream biomedical applications, including tumor micro",
    "full_text_length": 87495,
    "chunk_length": 1459
  },
  {
    "chunk_id": 1296,
    "paper_filename": "Huan_2024_survey_Multimodal_deep_earning_approaches_for_precision_oncology.pdf",
    "paper_title": "Huan 2024 Survey Multimodal Deep Earning Approaches For Precision Oncology",
    "chunk_index": 28,
    "total_chunks": 79,
    "text_content": "opportunities to comprehensively assess patients\u2019 tumor status ( Table 3 ). For instance, Li et al .p r o p o s e daV A E - based framework that integrates single-cell multimodal data, utilizing SNV features alongside gene expression characteristics to classify tumor cells [ 15]. Liu et al . introduced AutoCancer, which integrates feature selection, neural architecture search, and hyperparameter optimization, demonstrating strong perfor- mance in cancer detection using heterogeneous liquid biops",
    "full_text_length": 87495,
    "chunk_length": 1278
  },
  {
    "chunk_id": 1297,
    "paper_filename": "Huan_2024_survey_Multimodal_deep_earning_approaches_for_precision_oncology.pdf",
    "paper_title": "Huan 2024 Survey Multimodal Deep Earning Approaches For Precision Oncology",
    "chunk_index": 29,
    "total_chunks": 79,
    "text_content": "[ 64]. Khan et al . combined CT features with pathological features using fully connected layers to classify liver cancer variants [ 12]. Wu et al. developed a clinically aligned platform for grading ductal carcinoma in situ , treating each angle of ultrasound images as a separate modality and deriving final predictions through max pooling across all angles [65]. Wang et al . constructed multiple models for ovarian lesion classification with ultrasound, menopausal status, and serum data. Their t",
    "full_text_length": 87495,
    "chunk_length": 1363
  },
  {
    "chunk_id": 1298,
    "paper_filename": "Huan_2024_survey_Multimodal_deep_earning_approaches_for_precision_oncology.pdf",
    "paper_title": "Huan 2024 Survey Multimodal Deep Earning Approaches For Precision Oncology",
    "chunk_index": 30,
    "total_chunks": 79,
    "text_content": "al.p r e s e n t e d a late fusion model combining histology and RNA-Seq data forlung cancer subtyping, demonstrating that this integrative classi- fication approach outperformed reliance on unimodal data [ 28]. Qiu et al . integrated pathology and genomics data for cancer classification. Their weakly supervised design and hierarchical fusion strategy maximized the utility of WSI labels and facilitate efficient multimodal interactions [ 27]. Wang et al .e m p l o y e da late fusion approach to i",
    "full_text_length": 87495,
    "chunk_length": 1446
  },
  {
    "chunk_id": 1299,
    "paper_filename": "Huan_2024_survey_Multimodal_deep_earning_approaches_for_precision_oncology.pdf",
    "paper_title": "Huan 2024 Survey Multimodal Deep Earning Approaches For Precision Oncology",
    "chunk_index": 31,
    "total_chunks": 79,
    "text_content": "al. (2023) [ 56] Prostate MRI, TRUS CNN, attention Supervised 662 patients Intermediate https:// github. com/ DIAL- RPI/ Attention-RegPartial available Registration Haque et al. (2023) [ 57] Prostate MSI, WSI CNN Supervised 5 patients Early https:// github. com/ inzamam1190/ HEtoMALDINeed request Registration andsegmentationGuet al. (2023) [ 52] Esophagus, lungPET-CT FCN Supervised 53 patients Early No Need request Segmentation Leeet al. (2023) [ 60] Various cancersMultimodalmicroscopyimages Tra",
    "full_text_length": 87495,
    "chunk_length": 1479
  },
  {
    "chunk_id": 1300,
    "paper_filename": "Huan_2024_survey_Multimodal_deep_earning_approaches_for_precision_oncology.pdf",
    "paper_title": "Huan 2024 Survey Multimodal Deep Earning Approaches For Precision Oncology",
    "chunk_index": 32,
    "total_chunks": 79,
    "text_content": "network (GNN)\u2013basedframework, showcasing promise in predicting lymph node metastasis (LNM) in non\u2013small cell lung cancer (NSCLC) patients [ 54]. Zhong et al. developed a PET-CT-based cross-modal biomarker to predict occult nodal metastasis in early-stage NSCLC patients, indicating the superiority of their multimodal model over single- modal approaches [ 70]. Overall, tumor detection, diagnosis, and metastasis prediction involve a diverse array of tumor data modalities, encompass-ing both the fus",
    "full_text_length": 87495,
    "chunk_length": 1513
  },
  {
    "chunk_id": 1301,
    "paper_filename": "Huan_2024_survey_Multimodal_deep_earning_approaches_for_precision_oncology.pdf",
    "paper_title": "Huan 2024 Survey Multimodal Deep Earning Approaches For Precision Oncology",
    "chunk_index": 33,
    "total_chunks": 79,
    "text_content": "particularly in advanced-stage tumors. Enhancing prog- nosis prediction through the integration of multiple early tumor indicators could significantly improve the accuracy of clinicalinterventions, leading to better patient outcomes and reduced waste of medical resources. Recently, MDL has garnered significant attention in tumor prognosis prediction ( Table 4 ). For instance, Li et al . developed a two-stage framework that decouples multimodal feature rep- resentation from the fusion process, de",
    "full_text_length": 87495,
    "chunk_length": 1454
  },
  {
    "chunk_id": 1302,
    "paper_filename": "Huan_2024_survey_Multimodal_deep_earning_approaches_for_precision_oncology.pdf",
    "paper_title": "Huan 2024 Survey Multimodal Deep Earning Approaches For Precision Oncology",
    "chunk_index": 34,
    "total_chunks": 79,
    "text_content": "cancer prognosis; Huang et al. combined non-enhanced CT features with clinical predictors to develop models for assessing nutritionalstatus in gastric cancer, thereby enhancing preoperative survival risk prediction [ 77]. Huang et al. constructed an ensemble model based on EfficientNet-B4, utilizing both PET and CT data to predictprogression in lung malignancies and overall survival (OS). Their findings indicated that this dual-modality model outperformed the PET-only model in accuracy and sensi",
    "full_text_length": 87495,
    "chunk_length": 1440
  },
  {
    "chunk_id": 1303,
    "paper_filename": "Huan_2024_survey_Multimodal_deep_earning_approaches_for_precision_oncology.pdf",
    "paper_title": "Huan 2024 Survey Multimodal Deep Earning Approaches For Precision Oncology",
    "chunk_index": 35,
    "total_chunks": 79,
    "text_content": "multimodal approach based on PET-CT that employs a contrastive hybrid learning strategy to identify primary treatment failure (PTF) in diffuse large B-cell lymphoma (DLBCL), providing a noninvasive tool for assessingPTF risk [ 53]. Distant recurrence significantly contributes to poorDownloaded from https://academic.oup.com/bib/article/26/1/bbae699/7942793 by guest on 03 April 2025 8|Yang et al.Table 3. Representative studies focus on cancer detection and diagnosis Topic Study (year) Cancer Modal",
    "full_text_length": 87495,
    "chunk_length": 1438
  },
  {
    "chunk_id": 1304,
    "paper_filename": "Huan_2024_survey_Multimodal_deep_earning_approaches_for_precision_oncology.pdf",
    "paper_title": "Huan 2024 Survey Multimodal Deep Earning Approaches For Precision Oncology",
    "chunk_index": 36,
    "total_chunks": 79,
    "text_content": "com/ ruitian- olivia/ STIC- modelNeed request Diagnosis Park et al. (2021) [ 64] Lung CT, PET-CT, clinical dataCNN Supervised 359 patients Early No No Diagnosis Khan et al. (2023) [ 12] Liver WSI, CT CNN Supervised 248 patients Early No No Diagnosis Wuet al. (2024) [ 65] Breast Multimodal ultrasound CNN Supervised 733 patients Late Need request Need request Diagnosis Wang et al. (2024) [ 66] Ovarian Ultrasound, serum indicator, clinical dataCNN Supervised 1054patients Intermediate https:// data.",
    "full_text_length": 87495,
    "chunk_length": 1465
  },
  {
    "chunk_id": 1305,
    "paper_filename": "Huan_2024_survey_Multimodal_deep_earning_approaches_for_precision_oncology.pdf",
    "paper_title": "Huan 2024 Survey Multimodal Deep Earning Approaches For Precision Oncology",
    "chunk_index": 37,
    "total_chunks": 79,
    "text_content": "Glioma, lungWSI, CNV GNN, SNN Weaklysupervised 683 gliomapatients;987 lungcancerpatients Intermediate No Yes Metastasisprediction Huet al. (2023) [ 54] Lung CT, clinical data ResNet, Transformer,GNNSupervised 681 patients Intermediate No No Metastasisprediction Zhong et al. (2023) [ 70] Lung PET-CT CNN Supervised 3265 patientsEarly https:// github. com/ zhongthoracic/ DLNMSNeed request Note : SNV, single-nucleotide variation; VAE, variational autoencoder; CNN, convolutional neural network; CNV, ",
    "full_text_length": 87495,
    "chunk_length": 1602
  },
  {
    "chunk_id": 1306,
    "paper_filename": "Huan_2024_survey_Multimodal_deep_earning_approaches_for_precision_oncology.pdf",
    "paper_title": "Huan 2024 Survey Multimodal Deep Earning Approaches For Precision Oncology",
    "chunk_index": 38,
    "total_chunks": 79,
    "text_content": "431 patients Early https:// github. com/ HotHeaven233/ DeepLearning- Radiomics- Research- in-Breast- CancerYes Survival predictionFuet al. (2023) [ 73] Breast IMC, clinical data GNN, CNN Unsupervised 741 patients Intermediate https:// github. com/ xhelenfu/ DMGN_ Survival_ PredictionYes Survival predictionHuang et al. (2024) [ 74] Gastric CT, clinical data CNN Supervised 312 patients Late No No Survival predictionVoet al. (2024) [ 75] NSCLC PET, clinical data CNN Supervised 2898 patientsEarly No",
    "full_text_length": 87495,
    "chunk_length": 1637
  },
  {
    "chunk_id": 1307,
    "paper_filename": "Huan_2024_survey_Multimodal_deep_earning_approaches_for_precision_oncology.pdf",
    "paper_title": "Huan 2024 Survey Multimodal Deep Earning Approaches For Precision Oncology",
    "chunk_index": 39,
    "total_chunks": 79,
    "text_content": ": CRC, colorectal cancer; CNN, convolutional neural network; GNN, graph neural network; IMC, imaging mass cytometry; NSCLC, non\u2013small cell lung can cer; DLBCL, diffuse large B-cell lymphoma; LSTM, long short-term memory; WSI, whole slide image; ViT, vision transformer; CCRCC, clear cell renal cell carcinoma; SNN, self-normalizing networks.Downloaded from https://academic.oup.com/bib/article/26/1/bbae699/7942793 by guest on 03 April 2025 10 |Yang et al. prognosis in cancer patients, yet predictin",
    "full_text_length": 87495,
    "chunk_length": 1558
  },
  {
    "chunk_id": 1308,
    "paper_filename": "Huan_2024_survey_Multimodal_deep_earning_approaches_for_precision_oncology.pdf",
    "paper_title": "Huan 2024 Survey Multimodal Deep Earning Approaches For Precision Oncology",
    "chunk_index": 40,
    "total_chunks": 79,
    "text_content": "or CT with WSI enables a comprehensive analysis of patient prognosis from both macroscopic and microscopic perspectives. For instance, Li et al. presented a weakly supervised framework that employs a hierarchical radiology-guided co-attention mechanism to capture interactions between histopathological characteristics and radiological features, facilitating the identification of prognostic biomarkers withmultimodal interpretability [ 76]. Chen et al .c a l c u l a t e dt h e Kronecker product of ",
    "full_text_length": 87495,
    "chunk_length": 1528
  },
  {
    "chunk_id": 1309,
    "paper_filename": "Huan_2024_survey_Multimodal_deep_earning_approaches_for_precision_oncology.pdf",
    "paper_title": "Huan 2024 Survey Multimodal Deep Earning Approaches For Precision Oncology",
    "chunk_index": 41,
    "total_chunks": 79,
    "text_content": "potential to discover new prognostic biomarkers from cross-scale data. Recent models employing attention mechanisms have enhanced the interpretability of multimodal features, driv-ing advancements in AI applications for clinical use. Treatment decision and response monitoring Neoadjuvant chemotherapy, targeted therapy, and immunother- apy are increasingly integral to cancer management. The modern demand for more effective treatments underscores the need foraccurate, personalized tests over one-s",
    "full_text_length": 87495,
    "chunk_length": 1480
  },
  {
    "chunk_id": 1310,
    "paper_filename": "Huan_2024_survey_Multimodal_deep_earning_approaches_for_precision_oncology.pdf",
    "paper_title": "Huan 2024 Survey Multimodal Deep Earning Approaches For Precision Oncology",
    "chunk_index": 42,
    "total_chunks": 79,
    "text_content": "to segment lesions from multimodal MRI data. They combined extracted radiomic features with clinical baseline data to predict tumor grade and molecular phenotype [ 79]. Tumor mutational burden (TMB) has emerged as a promising indicator of the efficacy and prognosis of ICB therapy in tumors. Huang et al . developed a surrogate method for predicting TMB from WSIs in CRC by training a multimodal model that incorporates WSIs alongside relevant clinical data [ 30]. Esteva et al . created an integrati",
    "full_text_length": 87495,
    "chunk_length": 1408
  },
  {
    "chunk_id": 1311,
    "paper_filename": "Huan_2024_survey_Multimodal_deep_earning_approaches_for_precision_oncology.pdf",
    "paper_title": "Huan 2024 Survey Multimodal Deep Earning Approaches For Precision Oncology",
    "chunk_index": 43,
    "total_chunks": 79,
    "text_content": "Intramedullary gliomaMRI, clinical data Transformer,DNN Supervised 461 patients Early No Need request Treatmentdecision Huang et al. (2022) [ 30] CRC WSI, clinical data CNN, MLP Supervised 509 patients Early https:// github. com/ hkmgeneis/ MMDL/ tree/ masterYes Treatment decisionEsteva et al. (2022) [ 80] Prostate WSI, clinical data CNN Self-supervised 5654 patientsEarly Need request Need request Treatmentdecision Zhou et al. (2023) [ 81] Nasopha- ryngealMRI, CT GAN Supervised 50 patients Inter",
    "full_text_length": 87495,
    "chunk_length": 1500
  },
  {
    "chunk_id": 1312,
    "paper_filename": "Huan_2024_survey_Multimodal_deep_earning_approaches_for_precision_oncology.pdf",
    "paper_title": "Huan 2024 Survey Multimodal Deep Earning Approaches For Precision Oncology",
    "chunk_index": 44,
    "total_chunks": 79,
    "text_content": "neural network; WSI, whole slide image; CNN, convolutional neural network; MLP, multiple layer perceptron; GAN, generative adversarial network; CRC, colorectal cancer; IHC, immunohistochemistry.Downloaded from https://academic.oup.com/bib/article/26/1/bbae699/7942793 by guest on 03 April 2025 Multimodal deep learning approaches for precision oncology |11 generation network for MRI-guided radiation therapy, optimizing time and costs by generating intermediate multimodal sMRI and sCT data, incorpo",
    "full_text_length": 87495,
    "chunk_length": 1551
  },
  {
    "chunk_id": 1313,
    "paper_filename": "Huan_2024_survey_Multimodal_deep_earning_approaches_for_precision_oncology.pdf",
    "paper_title": "Huan 2024 Survey Multimodal Deep Earning Approaches For Precision Oncology",
    "chunk_index": 45,
    "total_chunks": 79,
    "text_content": "model integrating clinical parameters and pretreatment MRI data to predict pCR in breast cancer, outperforming unimodal models [ 82]. Zhou et al . combined PET-CT, clinical variables, and IHC scores within a multimodalframework to predict the efficacy of bevacizumab in advanced CRC patients, utilizing a 2.5D architecture for feature extraction [ 83]. Gu et al. applied a DenseNet-121-based multimodal frame- work to integrate ultrasound and clinicopathological data for stratifying responses to neo",
    "full_text_length": 87495,
    "chunk_length": 1514
  },
  {
    "chunk_id": 1314,
    "paper_filename": "Huan_2024_survey_Multimodal_deep_earning_approaches_for_precision_oncology.pdf",
    "paper_title": "Huan 2024 Survey Multimodal Deep Earning Approaches For Precision Oncology",
    "chunk_index": 46,
    "total_chunks": 79,
    "text_content": "positioning them favorablyto advance clinical decision-making and efficacy evaluation in oncology. Discussion and conclusion Recent advancements in medical imaging and sequencing tech-nologies have led to an exponential increase in biomedical multi- modal data. As the demand for precise tumor diagnosis and per- sonalized treatment continues to rise, effectively harnessing this wealth of data presents a significant challenge in clinical oncol-ogy. The impressive success of DL in domains such as c",
    "full_text_length": 87495,
    "chunk_length": 1440
  },
  {
    "chunk_id": 1315,
    "paper_filename": "Huan_2024_survey_Multimodal_deep_earning_approaches_for_precision_oncology.pdf",
    "paper_title": "Huan 2024 Survey Multimodal Deep Earning Approaches For Precision Oncology",
    "chunk_index": 47,
    "total_chunks": 79,
    "text_content": "We also provide a summary of publicly available multimodal oncology datasets, ranging fromlarge-scale databases like TCGA and TCIA to specialized datasets, such as HAM10000, which focus on specific tumor types or popu- lations. These resources offer valuable data for researchers in thefield. We then outline fundamental DL concepts and common network architectures ( Supplementary Materials 2 ), guiding researchers in selecting appropriate frameworks and methodsfor constructing MDL models. A revie",
    "full_text_length": 87495,
    "chunk_length": 1473
  },
  {
    "chunk_id": 1316,
    "paper_filename": "Huan_2024_survey_Multimodal_deep_earning_approaches_for_precision_oncology.pdf",
    "paper_title": "Huan 2024 Survey Multimodal Deep Earning Approaches For Precision Oncology",
    "chunk_index": 48,
    "total_chunks": 79,
    "text_content": "cutting-edge applications of MDL in oncol- ogy, covering areas such as multimodal data processing, tumordetection and diagnosis, prognosis prediction, treatment selec- tion, and response monitoring. These applications highlight the latest advancements and emerging trends in MDL for precisiononcology. However, challenges remain, as detailed in the follow- ing part. Challenge 1: scarcity of large open-source multimodal datasets and annotated information Stringent ethical reviews constrain the acqu",
    "full_text_length": 87495,
    "chunk_length": 1520
  },
  {
    "chunk_id": 1317,
    "paper_filename": "Huan_2024_survey_Multimodal_deep_earning_approaches_for_precision_oncology.pdf",
    "paper_title": "Huan 2024 Survey Multimodal Deep Earning Approaches For Precision Oncology",
    "chunk_index": 49,
    "total_chunks": 79,
    "text_content": "biomedical multimodal datasets. Another common issue with multimodal datasets is modality incompleteness. For example, full multimodal MRIs typically con- sist of pre-contrast T1, T2, fluid attenuated inversion recovery,and post-contrast T1 scans. Missing sequences due to factors such as acquisition protocols, scanner availability, or patient- specific issues complicate joint modeling. In practice, prioritiz-ing modality completeness or diversity depends on the task; for instance, when crucial m",
    "full_text_length": 87495,
    "chunk_length": 1558
  },
  {
    "chunk_id": 1318,
    "paper_filename": "Huan_2024_survey_Multimodal_deep_earning_approaches_for_precision_oncology.pdf",
    "paper_title": "Huan 2024 Survey Multimodal Deep Earning Approaches For Precision Oncology",
    "chunk_index": 50,
    "total_chunks": 79,
    "text_content": "multimodal datasets remains a bottleneck for MDL model development. Whilevast amounts of unlabeled cross-modal data are available, labeled data are limited and often noisy. Improving annotation reliability through partial label information is critical. Our review identifiestwo weakly supervised annotation approaches: (1) active learning, which selects reliable labels from pseudo-clusters and iterates from \u201ceasy\u201d to \u201chard\u201d annotations, and (2) data- and knowledge-driven annotation, which enhances",
    "full_text_length": 87495,
    "chunk_length": 1579
  },
  {
    "chunk_id": 1319,
    "paper_filename": "Huan_2024_survey_Multimodal_deep_earning_approaches_for_precision_oncology.pdf",
    "paper_title": "Huan 2024 Survey Multimodal Deep Earning Approaches For Precision Oncology",
    "chunk_index": 51,
    "total_chunks": 79,
    "text_content": "and treatment decision support, many areasremain underexplored, and fine-grained models are often lacking. To improve generalization across tasks, it is essential to integrate diverse techniques and domain-specific expertise, enhancing thecapability of pretrained models. The high heterogeneity and cross-scale nature of multimodal medical data pose significant challenges for efficient integration. Fusion strategies are typically categorized into early, intermedi- ate, and late fusion. Early fusio",
    "full_text_length": 87495,
    "chunk_length": 1583
  },
  {
    "chunk_id": 1320,
    "paper_filename": "Huan_2024_survey_Multimodal_deep_earning_approaches_for_precision_oncology.pdf",
    "paper_title": "Huan 2024 Survey Multimodal Deep Earning Approaches For Precision Oncology",
    "chunk_index": 52,
    "total_chunks": 79,
    "text_content": "multimodal architectures, such as multitask models that can train on diverse data types (e.g. images,videos, and audio) simultaneously. Hybrid fusion approaches, which combine the strengths of different fusion strategies, also hold potential. However, the effectiveness of these models,originally designed for natural images or audiovisual data, remains to be fully validated in the context of biomedical data. Challenge 3: poor interpretability of MDL Explainability has become a major concern in me",
    "full_text_length": 87495,
    "chunk_length": 1558
  },
  {
    "chunk_id": 1321,
    "paper_filename": "Huan_2024_survey_Multimodal_deep_earning_approaches_for_precision_oncology.pdf",
    "paper_title": "Huan 2024 Survey Multimodal Deep Earning Approaches For Precision Oncology",
    "chunk_index": 53,
    "total_chunks": 79,
    "text_content": "components and offer moreintuitive explanations [ 93]. Model-agnostic methods, such as Local Interpretable Model-Agnostic Explanations, approximate complex model behaviors with simpler local models to improveinterpretability [ 94]. Debate continues within the academic community about whether AI models should inherently possess explainabilityor rely on post hoc interpretability techniques (e.g. saliency maps or attention mechanisms). Future AI applications should prioritize biologically inspired ",
    "full_text_length": 87495,
    "chunk_length": 1596
  },
  {
    "chunk_id": 1322,
    "paper_filename": "Huan_2024_survey_Multimodal_deep_earning_approaches_for_precision_oncology.pdf",
    "paper_title": "Huan 2024 Survey Multimodal Deep Earning Approaches For Precision Oncology",
    "chunk_index": 54,
    "total_chunks": 79,
    "text_content": "and hindering timely assessments of tumor progres-sion, drug resistance, and toxicity. Future models should incor- porate dynamic medical domain knowledge, establishing real- time MDL frameworks to improve data processing and integration. Additionally, mechanisms to manage modality inconsistency and missing data would broaden the applicability of these models. While most existing models are group based, precision oncol- ogy requires personalized treatment plans tailored to individual patients. T",
    "full_text_length": 87495,
    "chunk_length": 1474
  },
  {
    "chunk_id": 1323,
    "paper_filename": "Huan_2024_survey_Multimodal_deep_earning_approaches_for_precision_oncology.pdf",
    "paper_title": "Huan 2024 Survey Multimodal Deep Earning Approaches For Precision Oncology",
    "chunk_index": 55,
    "total_chunks": 79,
    "text_content": "beyond simply measuring model accu-racy. It must also account for time and data costs, as well as the incremental information gain provided by the inclusion of addi- tional modalities. These factors are critical for optimizing cost-effectiveness and harnessing the synergistic potential of multi- modal data. Consequently, in addition to conventional evaluation metrics used for unimodal models, it is important to incorporateindicators such as modality-specific information gain (e.g. mutual informa",
    "full_text_length": 87495,
    "chunk_length": 1484
  },
  {
    "chunk_id": 1324,
    "paper_filename": "Huan_2024_survey_Multimodal_deep_earning_approaches_for_precision_oncology.pdf",
    "paper_title": "Huan 2024 Survey Multimodal Deep Earning Approaches For Precision Oncology",
    "chunk_index": 56,
    "total_chunks": 79,
    "text_content": "integrat-ing diverse modalities to provide comprehensive and accurate insights. However, the full potential of multimodal data remains underexplored. Key improvements are needed in handling dataheterogeneity, refining fusion strategies, and optimizing network architectures for clinical scenarios. Key Points \u2022This review synthesizes recent advances in MDL for pre- cision oncology, covering applications in image process- ing, diagnosis, prognosis prediction, treatment decisions, and response monit",
    "full_text_length": 87495,
    "chunk_length": 1614
  },
  {
    "chunk_id": 1325,
    "paper_filename": "Huan_2024_survey_Multimodal_deep_earning_approaches_for_precision_oncology.pdf",
    "paper_title": "Huan 2024 Survey Multimodal Deep Earning Approaches For Precision Oncology",
    "chunk_index": 57,
    "total_chunks": 79,
    "text_content": "Effective fusion methods and adaptive MDLframeworks are crucial to overcoming issues like data heterogeneity, incompleteness, and feature redundancy, paving the way for the broader adoption of precisiononcology. Data availability Details about the data discussed in this study have been incor- porated in the article. No additional data were generated for this study. Ethical statement There are no ethical issues. Supplementary data Supplementary data is available at Briefings in Bioinformatics onl",
    "full_text_length": 87495,
    "chunk_length": 1360
  },
  {
    "chunk_id": 1326,
    "paper_filename": "Huan_2024_survey_Multimodal_deep_earning_approaches_for_precision_oncology.pdf",
    "paper_title": "Huan 2024 Survey Multimodal Deep Earning Approaches For Precision Oncology",
    "chunk_index": 58,
    "total_chunks": 79,
    "text_content": "the col-lection, analysis, and interpretation of data, the writing of the manuscript, or in the decision to submit the study for publication. References 1.Sung H, Ferlay J, Siegel RL, et al. Global Cancer Statistics 2020: GLOBOCAN estimates of incidence and mortality worldwide for 36 cancers in 185 countries. CA Cancer J Clin 2021; 71:209\u201349. https:// doi.org/10.3322/ caac.21660 . 2.Yabroff KR, Lund J, Kepka D, et al. Economic burden of cancer in the United States: estimates, projections, and fu",
    "full_text_length": 87495,
    "chunk_length": 1387
  },
  {
    "chunk_id": 1327,
    "paper_filename": "Huan_2024_survey_Multimodal_deep_earning_approaches_for_precision_oncology.pdf",
    "paper_title": "Huan 2024 Survey Multimodal Deep Earning Approaches For Precision Oncology",
    "chunk_index": 59,
    "total_chunks": 79,
    "text_content": "candidate selection for lung cancer CT screening: advancing the 2021 USPSTF recommendations. Radiology 2022; 305:209\u201318. https:// doi.org/10.1148/ radiol.212877 . 6.Sankaranarayanan S, Balan J, Walsh JR, et al. COVID-19 mor- tality prediction from deep learning in a large multistate elec- tronic health record and laboratory information system dataset: algorithm development and validation. J Med Internet Res 2021; 23:e30157. https:// doi.org/10.2196/30157 . 7.Wang Y, Zhai Y, Ding Y, et al. SBSM-p",
    "full_text_length": 87495,
    "chunk_length": 1455
  },
  {
    "chunk_id": 1328,
    "paper_filename": "Huan_2024_survey_Multimodal_deep_earning_approaches_for_precision_oncology.pdf",
    "paper_title": "Huan 2024 Survey Multimodal Deep Earning Approaches For Precision Oncology",
    "chunk_index": 60,
    "total_chunks": 79,
    "text_content": "for pre- diction of colorectal cancer outcome: a discovery and vali- dation study. Lancet 2020; 395:350\u201360. https:// doi.org/10.1016/ S0140-6736 (19)32998-8 . 11. Cui Y, Zhang J, Li Z, et al. A CT-based deep learning radiomics nomogram for predicting the response to neoadjuvant chemotherapy in patients with locally advanced gastric cancer:a multicenter cohort study. EClinicalMedicine 2022; 46:101348. https:// doi.org/10.1016/ j.eclinm.2022.101348 . 12. Khan RA, Fu M, Burbridge B, et al. A multi-",
    "full_text_length": 87495,
    "chunk_length": 1391
  },
  {
    "chunk_id": 1329,
    "paper_filename": "Huan_2024_survey_Multimodal_deep_earning_approaches_for_precision_oncology.pdf",
    "paper_title": "Huan 2024 Survey Multimodal Deep Earning Approaches For Precision Oncology",
    "chunk_index": 61,
    "total_chunks": 79,
    "text_content": "IEEE Trans Med Imaging 2022; 41: 757\u201370. https:// doi.org/10.1109/ TMI.2020.3021387 . 15. Li H, Zhou Y, Zhao N, et al. ISMI-VAE: a deep learning model for classifying disease cells using gene expression and SNV data. Comput Biol Med 2024; 175:108485. https:// doi.org/10.1016/ j.compbiomed.2024.108485 . 16. Y a oY ,L vY ,T o n gL , et al. ICSDA: a multi-modal deep learning model to predict breast cancer recurrence and metastasis riskby integrating pathological, clinical and gene expression data. ",
    "full_text_length": 87495,
    "chunk_length": 1465
  },
  {
    "chunk_id": 1330,
    "paper_filename": "Huan_2024_survey_Multimodal_deep_earning_approaches_for_precision_oncology.pdf",
    "paper_title": "Huan 2024 Survey Multimodal Deep Earning Approaches For Precision Oncology",
    "chunk_index": 62,
    "total_chunks": 79,
    "text_content": "radiographs and clinical param- eters: a case for transformers. Radiology 2023; 309:e230806. https:// doi.org/10.1148/ radiol.230806 . 19. Tran KA, Kondrashova O, Bradley A, et al. Deep learning in cancer diagnosis, prognosis and treatment selection. Genome Med 2021; 13:152. https:// doi.org/10.1186/ s13073-021-00968- x.Downloaded from https://academic.oup.com/bib/article/26/1/bbae699/7942793 by guest on 03 April 2025 14 |Yang et al. 20. Kleppe A, Skrede OJ, De Raedt S, et al. Designing deep lea",
    "full_text_length": 87495,
    "chunk_length": 1497
  },
  {
    "chunk_id": 1331,
    "paper_filename": "Huan_2024_survey_Multimodal_deep_earning_approaches_for_precision_oncology.pdf",
    "paper_title": "Huan 2024 Survey Multimodal Deep Earning Approaches For Precision Oncology",
    "chunk_index": 63,
    "total_chunks": 79,
    "text_content": "https:// doi.org/10.1093/ bib/bbad313 . 24. Khalighi S, Reddy K, Midya A, et al. Artificial intelligence in neuro-oncology: advances and challenges in brain tumor diagnosis, prognosis, and precision treatment. NPJ Precis Oncol 2024; 8:80.https:// doi.org/10.1038/ s41698-024-00575-0 . 25. Stahlschmidt SR, Ulfenborg B, Synnergren J. Multimodal deep learning for biomedical data fusion: a review. Brief Bioinform 2022; 23:bbab569. https:// doi.org/10.1093/ bib/bbab569 . 26. Steyaert S, Pizurica M, Na",
    "full_text_length": 87495,
    "chunk_length": 1484
  },
  {
    "chunk_id": 1332,
    "paper_filename": "Huan_2024_survey_Multimodal_deep_earning_approaches_for_precision_oncology.pdf",
    "paper_title": "Huan 2024 Survey Multimodal Deep Earning Approaches For Precision Oncology",
    "chunk_index": 64,
    "total_chunks": 79,
    "text_content": "BMC Bioinformatics 2021; 22:454. https:// doi.org/10.1186/ s12859-021-04376-1 . 29. Volinsky-Fremond S, Horeweg N, Andani S, et al. Prediction of recurrence risk in endometrial cancer with multimodal deep learning. Nat Med 2024; 30:2092. https:// doi.org/10.1038/ s41591-024-03126- z. 30. Huang K, Lin B, Liu J, et al. Predicting colorectal cancer tumor mutational burden from histopathological images and clin- ical information using multi-modal deep learning. Bioinfor- matics 2022; 38:5108\u201315. htt",
    "full_text_length": 87495,
    "chunk_length": 1441
  },
  {
    "chunk_id": 1333,
    "paper_filename": "Huan_2024_survey_Multimodal_deep_earning_approaches_for_precision_oncology.pdf",
    "paper_title": "Huan 2024 Survey Multimodal Deep Earning Approaches For Precision Oncology",
    "chunk_index": 65,
    "total_chunks": 79,
    "text_content": "lung cancer detection. Nature 2020; 580:245\u201351. https:// doi.org/10.1038/ s41586-020-2140-0 . 34. Leal AIC, Mathios D, Jakubowski D, et al. Cell-free DNA fragmen- tomes in the diagnostic evaluation of patients with symptoms suggestive of lung cancer. Chest 2023; 164:1019\u201327. https:// doi. org/10.1016/ j.chest.2023.04.033 . 35. L i uL ,C h e nX ,W o n gK - C , et al. Early cancer detection from genome-wide cell-free DNA fragmentation via shuffled frog leaping algorithm and support vector machine.",
    "full_text_length": 87495,
    "chunk_length": 1402
  },
  {
    "chunk_id": 1334,
    "paper_filename": "Huan_2024_survey_Multimodal_deep_earning_approaches_for_precision_oncology.pdf",
    "paper_title": "Huan 2024 Survey Multimodal Deep Earning Approaches For Precision Oncology",
    "chunk_index": 66,
    "total_chunks": 79,
    "text_content": "Zhang C, Xie Y, Bai H, et al. A survey on federated learn- ing.Knowl-Based Syst 2021; 216:106775. https:// doi.org/10.1016/ j. knosys.2021.106775 . 39. Wang R, Dai W, Gong J, et al. Development of a novel com- bined nomogram model integrating deep learning-pathomics, radiomics and immunoscore to predict postoperative outcomeof colorectal cancer lung metastasis patients. J Hematol Oncol 2022; 15:11. https:// doi.org/10.1186/ s13045-022-01225-3 . 40. Pan Z, Hu G, Zhu Z, et al. Predicting invasiven",
    "full_text_length": 87495,
    "chunk_length": 1348
  },
  {
    "chunk_id": 1335,
    "paper_filename": "Huan_2024_survey_Multimodal_deep_earning_approaches_for_precision_oncology.pdf",
    "paper_title": "Huan 2024 Survey Multimodal Deep Earning Approaches For Precision Oncology",
    "chunk_index": 67,
    "total_chunks": 79,
    "text_content": "Predicting and characterizing a cancer dependency map of tumors with deep learning. Sci Adv 2021; 7:eabh1275. https:// doi.org/10.1126/ sciadv. abh1275 . 44. C u iJ ,G o n gK ,G u oN , et al. PET image denoising using unsuper- vised deep learning. Eur J Nucl Med Mol Imaging 2019; 46:2780\u20139. https:// doi.org/10.1007/ s00259-019-04468-4 . 45. Chen Z, Li X, Yang M, et al. Optimization of deep learning models for the prediction of gene mutations using unsupervised clustering. J Pathol Clin Res 2023;",
    "full_text_length": 87495,
    "chunk_length": 1336
  },
  {
    "chunk_id": 1336,
    "paper_filename": "Huan_2024_survey_Multimodal_deep_earning_approaches_for_precision_oncology.pdf",
    "paper_title": "Huan 2024 Survey Multimodal Deep Earning Approaches For Precision Oncology",
    "chunk_index": 68,
    "total_chunks": 79,
    "text_content": "doi.org/10.1093/ bioinformatics/ btz342 . 49. Zhao Z, Xiao S, Song Z, et al. Open-ended video question answer- ing via multi-modal conditional adversarial networks. IEEE Trans Image Process 2020; 29:3859\u201370. https:// doi.org/10.1109/ TIP.2020.2963950 . 50. Misra S, Yoon C, Kim KJ, et al. Deep learning-based mul- timodal fusion network for segmentation and classification of breast cancers using B-mode and elastography ultrasound images. Bioeng Transl Med 2023; 8:e10480. https:// doi.org/10.1002/ ",
    "full_text_length": 87495,
    "chunk_length": 1437
  },
  {
    "chunk_id": 1337,
    "paper_filename": "Huan_2024_survey_Multimodal_deep_earning_approaches_for_precision_oncology.pdf",
    "paper_title": "Huan 2024 Survey Multimodal Deep Earning Approaches For Precision Oncology",
    "chunk_index": 69,
    "total_chunks": 79,
    "text_content": "primary treatment failure in diffuse large B-cell lymphoma. Eur Radiol 2023; 33: 77\u201388. https:// doi.org/10.1007/ s00330-022-09031-8 .Downloaded from https://academic.oup.com/bib/article/26/1/bbae699/7942793 by guest on 03 April 2025 Multimodal deep learning approaches for precision oncology |15 54. Hu D, Li S, Wu N, et al. A multi-modal heterogeneous graph forest to predict lymph node metastasis of non-small cell lung cancer. IEEE J Biomed Health Inform 2023; 27:1216\u201324. https:// doi. org/10.11",
    "full_text_length": 87495,
    "chunk_length": 1396
  },
  {
    "chunk_id": 1338,
    "paper_filename": "Huan_2024_survey_Multimodal_deep_earning_approaches_for_precision_oncology.pdf",
    "paper_title": "Huan 2024 Survey Multimodal Deep Earning Approaches For Precision Oncology",
    "chunk_index": 70,
    "total_chunks": 79,
    "text_content": "for predicting prostate cancer directly from tissue images. JA m Soc Mass Spectrom 2023; 34:227\u201335. https://doi.org/10.1021/jasms. 2c00254 . 58. Zhao X, Wu Y, Song G, et al. A deep learning model integrating FCNNs and CRFs for brain tumor segmentation. Med Image Anal 2018; 43:98\u2013111. https:// doi.org/10.1016/ j.media.2017.10.002 . 59. Ma J, Xie R, Ayyadhury S, et al. The multimodality cell segmenta- tion challenge: toward universal solutions. Nat Methods 2024; 21: 1103\u201313. https:// doi.org/10.10",
    "full_text_length": 87495,
    "chunk_length": 1408
  },
  {
    "chunk_id": 1339,
    "paper_filename": "Huan_2024_survey_Multimodal_deep_earning_approaches_for_precision_oncology.pdf",
    "paper_title": "Huan 2024 Survey Multimodal Deep Earning Approaches For Precision Oncology",
    "chunk_index": 71,
    "total_chunks": 79,
    "text_content": "as an automated multimodal framework for early cancer detection. iScience 2024; 27:110183. https:// doi.org/10.1016/ j.isci.2024.110183 . 63. Gao R, Zhao S, Aishanjiang K, et al. Deep learning for differ- ential diagnosis of malignant hepatic tumors based on multi- phase contrast-enhanced CT and clinical data. J Hematol Oncol 2021; 14:154. https:// doi.org/10.1186/ s13045-021-01167-2 . 64. Park YJ, Choi D, Choi JY, et al. Performance evaluation of a deep learning system for differential diagnosi",
    "full_text_length": 87495,
    "chunk_length": 1392
  },
  {
    "chunk_id": 1340,
    "paper_filename": "Huan_2024_survey_Multimodal_deep_earning_approaches_for_precision_oncology.pdf",
    "paper_title": "Huan 2024 Survey Multimodal Deep Earning Approaches For Precision Oncology",
    "chunk_index": 72,
    "total_chunks": 79,
    "text_content": "27:109403. https:// doi.org/10.1016/ j.isci.2024.109403 . 67. X i a n gH ,X i a oY ,L iF , et al. Development and validation of an interpretable model integrating multimodal information forimproving ovarian cancer diagnosis. Nat Commun 2024; 15:2681. https:// doi.org/10.1038/ s41467-024-46700-2 . 68. Du H, Dong Z, Wu L, et al. A deep-learning based system using multi-modal data for diagnosing gastric neoplasms in real-time (with video). Gastric Cancer 2023; 26:275\u201385. https:// doi. org/10.1007/ ",
    "full_text_length": 87495,
    "chunk_length": 1395
  },
  {
    "chunk_id": 1341,
    "paper_filename": "Huan_2024_survey_Multimodal_deep_earning_approaches_for_precision_oncology.pdf",
    "paper_title": "Huan 2024 Survey Multimodal Deep Earning Approaches For Precision Oncology",
    "chunk_index": 73,
    "total_chunks": 79,
    "text_content": "prediction. Comput Biol Med 2023; 156:106715. https://doi.org/10.1016/j.compbiomed.2023. 106715 . 72. Miao S, Jia H, Cheng K, et al. Deep learning radiomics under mul- timodality explore association between muscle/fat and metas- tasis and survival in breast cancer patients. Brief Bioinform 2022; 23:bbac432. https:// doi.org/10.1093/ bib/bbac432 . 73. Fu X, Patrick E, Yang JYH, et al. Deep multimodal graph-based network for survival prediction from highly multiplexed images and patient variables.",
    "full_text_length": 87495,
    "chunk_length": 1476
  },
  {
    "chunk_id": 1342,
    "paper_filename": "Huan_2024_survey_Multimodal_deep_earning_approaches_for_precision_oncology.pdf",
    "paper_title": "Huan 2024 Survey Multimodal Deep Earning Approaches For Precision Oncology",
    "chunk_index": 74,
    "total_chunks": 79,
    "text_content": "j.cmpb.2024.108104 . 76. Li Z, Jiang Y, Lu M, et al. Survival prediction via hierarchical mul- timodal co-attention transformer: a computational histology-radiology solution. IEEE Trans Med Imaging 2023; 42:2678\u201389. https:// doi.org/10.1109/ TMI.2023.3263010 . 77. Zhang H, Zhu X, Li B, et al. Development and validation of a meta-learning-based multi-modal deep learning algorithm for detection of peritoneal metastasis. Int J Comput Assist Radiol Surg 2022; 17:1845\u201353. https://doi.org/10.1007/s115",
    "full_text_length": 87495,
    "chunk_length": 1438
  },
  {
    "chunk_id": 1343,
    "paper_filename": "Huan_2024_survey_Multimodal_deep_earning_approaches_for_precision_oncology.pdf",
    "paper_title": "Huan 2024 Survey Multimodal Deep Earning Approaches For Precision Oncology",
    "chunk_index": 75,
    "total_chunks": 79,
    "text_content": "therapy personalization via multi-modal deep learning on randomized phase III clinical trials. NPJ Digit Med 2022; 5:71. https:// doi. org/10.1038/ s41746-022-00613- w. 81. Zhou X, Cai W, Cai J, et al. Multimodality MRI synchronous construction based deep learning framework for MRI- guided radiotherapy synthetic CT generation. Comput Biol Med 2023; 162:107054. https://doi.org/10.1016/j.compbiomed. 2023.107054 . 82. Joo S, Ko ES, Kwon S, et al. Multimodal deep learning mod- els for the prediction",
    "full_text_length": 87495,
    "chunk_length": 1478
  },
  {
    "chunk_id": 1344,
    "paper_filename": "Huan_2024_survey_Multimodal_deep_earning_approaches_for_precision_oncology.pdf",
    "paper_title": "Huan 2024 Survey Multimodal Deep Earning Approaches For Precision Oncology",
    "chunk_index": 76,
    "total_chunks": 79,
    "text_content": "https://academic.oup.com/bib/article/26/1/bbae699/7942793 by guest on 03 April 2025 16 |Yang et al. in breast cancer before treatment. Oncologist 2024; 29:e187\u201397. https:// doi.org/10.1093/ oncolo/ oyad227 . 85. Rabinovici-Cohen S, Fernandez XM, Grandal Rejo B, et al. Multimodal prediction of five-year breast cancer recurrencein women who receive neoadjuvant chemotherapy. Can- cers (Basel) 2022; 14:3848. https://doi.org/10.3390/cancers1416 3848 . 86. Mahon P, Chatzitheofilou I, Dekker A, et al. ",
    "full_text_length": 87495,
    "chunk_length": 1506
  },
  {
    "chunk_id": 1345,
    "paper_filename": "Huan_2024_survey_Multimodal_deep_earning_approaches_for_precision_oncology.pdf",
    "paper_title": "Huan 2024 Survey Multimodal Deep Earning Approaches For Precision Oncology",
    "chunk_index": 77,
    "total_chunks": 79,
    "text_content": "D. CMOT: cross-modality optimal transport for multimodal inference. Genome Biol 2023; 24:163. https:// doi. org/10.1186/ s13059-023-02989-8 .90. Wang K, Li M. Fusion-based deep learning architecture for detecting drug-target binding affinity using target and drug sequence and structure. IEEE J Biomed Health Inform 2023; 27: 6112\u201320. https:// doi.org/10.1109/ JBHI.2023.3315073 . 91. Bian Z, Zhang J, Chung F-L, et al. Residual sketch learning for a feature-importance-based and linguistically inter",
    "full_text_length": 87495,
    "chunk_length": 1522
  },
  {
    "chunk_id": 1346,
    "paper_filename": "Huan_2024_survey_Multimodal_deep_earning_approaches_for_precision_oncology.pdf",
    "paper_title": "Huan 2024 Survey Multimodal Deep Earning Approaches For Precision Oncology",
    "chunk_index": 78,
    "total_chunks": 79,
    "text_content": "explanations approach for computer-aided diagnosis systems. arXiv preprint arXiv:190610263 2019. https:// doi.org/10.48550/ arXiv.1906.10263 (30 December 2024, date last accessed). \u00a9 The Author(s) 2025. Published by Oxford University Press. This is an Open Access article distributed under the terms of the Creative Commons Attribu tion License ( https:// creativecommons. org/licenses/ by/4.0/ ), which permits unrestricted reuse, distribution, and reproduction in any medium, provided the original ",
    "full_text_length": 87495,
    "chunk_length": 717
  },
  {
    "chunk_id": 1347,
    "paper_filename": "Jailin_2022_lesion_detection_contrast_enhanced_spectral_mammography.pdf",
    "paper_title": "Jailin 2022 Lesion Detection Contrast Enhanced Spectral Mammography",
    "chunk_index": 0,
    "total_chunks": 24,
    "text_content": "Lesion detection in Contrast Enhanced Spectral Mammography Cl\u0013 ement Jailin, Pablo Milioni, Zhijin Li, R\u0015 azvan Iordache, and Serge Muller GE Healthcare, 78530 Buc, France ABSTRACT Background & purpose: The recent emergence of neural networks models for the analysis of breast images has been a breakthrough in computer aided diagnostic. This approach was not yet developed in Contrast Enhanced Spectral Mammography (CESM) where access to large databases is complex. This work proposes a deep-learnin",
    "full_text_length": 22737,
    "chunk_length": 1339
  },
  {
    "chunk_id": 1348,
    "paper_filename": "Jailin_2022_lesion_detection_contrast_enhanced_spectral_mammography.pdf",
    "paper_title": "Jailin 2022 Lesion Detection Contrast Enhanced Spectral Mammography",
    "chunk_index": 1,
    "total_chunks": 24,
    "text_content": "the detection of 1) all lesions, 2) biopsied lesions and 3) malignant lesions. ROC curve was used to evaluate breast cancer classi cation. The metrics were nally compared to clinical results. Results: For the evaluation of the malignant lesion detection, at high sensitivity (Se >0.95), the false positive rate was at 0.61 per image. For the classi cation of malignant cases, the model reached an Area Under the Curve (AUC) in the range of clinical CESM diagnostic results. Conclusion: This CAD is th",
    "full_text_length": 22737,
    "chunk_length": 1308
  },
  {
    "chunk_id": 1349,
    "paper_filename": "Jailin_2022_lesion_detection_contrast_enhanced_spectral_mammography.pdf",
    "paper_title": "Jailin 2022 Lesion Detection Contrast Enhanced Spectral Mammography",
    "chunk_index": 2,
    "total_chunks": 24,
    "text_content": "Aided Detection (CADe) and Computer Aided Diagnostic (CADx) based on Deep Learning (DL) has been a breakthrough in medical imaging1{3and in breast cancer analysis. In breast images, various CAD exist for screening or diagnostic applications and are used on Full Field Digital Mammography4,5or Digital Breast Tomosynthesis6where large databases may be available for the training. Contrast-enhanced spectral mammography (CESM) provides anatomical and functional imaging of breast tissue improving the a",
    "full_text_length": 22737,
    "chunk_length": 1464
  },
  {
    "chunk_id": 1350,
    "paper_filename": "Jailin_2022_lesion_detection_contrast_enhanced_spectral_mammography.pdf",
    "paper_title": "Jailin 2022 Lesion Detection Contrast Enhanced Spectral Mammography",
    "chunk_index": 3,
    "total_chunks": 24,
    "text_content": "than 130 patients), the approaches are often based on the classi cation of manually extracted regions of interests. From those images, handcrafted features ( e.g., radiomics12{15) or deep learning features C.J.: E-mail: clement.jailin@ge.comarXiv:2207.09692v1 [eess.IV] 20 Jul 2022 (e.g., obtained from a convolutional neural network16) are extracted and fed into machine learning algorithms (e.g., support vector machine, multi-layer perceptron, etc.) to perform the classi cation. Those analyses re",
    "full_text_length": 22737,
    "chunk_length": 1369
  },
  {
    "chunk_id": 1351,
    "paper_filename": "Jailin_2022_lesion_detection_contrast_enhanced_spectral_mammography.pdf",
    "paper_title": "Jailin 2022 Lesion Detection Contrast Enhanced Spectral Mammography",
    "chunk_index": 4,
    "total_chunks": 24,
    "text_content": "nally compared with clinical diagnostic results. 2. DATA AND METHOD CESM datasets A CESM dataset consisting of 586 patients with 2510 recombined images of left/right views, mainly cranio-caudal (CC) and mediolateral oblique (MLO), was collected from various hospitals and acquired with di erent systems (Senographe DS, Essential and Pristina from GE Healthcare, Chicago, Illinois, United States ). The lesions in the dataset were all biopsy proven. The number of normal, benign and malignant cases ar",
    "full_text_length": 22737,
    "chunk_length": 1270
  },
  {
    "chunk_id": 1352,
    "paper_filename": "Jailin_2022_lesion_detection_contrast_enhanced_spectral_mammography.pdf",
    "paper_title": "Jailin 2022 Lesion Detection Contrast Enhanced Spectral Mammography",
    "chunk_index": 5,
    "total_chunks": 24,
    "text_content": "may be labeled normal if no ndings are detected. It therefore represents normal cases for the training. The summary of the data is provided in table 1. Hospital Num of patients Acq. sys.Pathology results Normal Benign Malignant Peking University First Hospital & Shanghai First People's Hospital (China)244 (976 images) DS / Essential 35 (14%) 64 (26%) 145 (60%) CBIS - Carolina Breast Imaging Specialists (US)26 (143 images) Pristina 1 (5%) 5 (19%) 20 (77%) Beth Israel Deaconess Medical Center (US)",
    "full_text_length": 22737,
    "chunk_length": 1207
  },
  {
    "chunk_id": 1353,
    "paper_filename": "Jailin_2022_lesion_detection_contrast_enhanced_spectral_mammography.pdf",
    "paper_title": "Jailin 2022 Lesion Detection Contrast Enhanced Spectral Mammography",
    "chunk_index": 6,
    "total_chunks": 24,
    "text_content": "data were split in a training (332 patients - 1501 images) / validation (82 patients - 332 images) / test sets (172 patients - 677 images) strati ed by the pathology results (normal/benign/malignant) and by clinical sites. In addition to the biopsied lesions, all other benign lesions in the breast ( e.g., non-biopsied cysts, broadenoma, nodes, etc.) were annotated with a rectangular bounding box and labeled with a speci c label. All CESM recombined data were processed using the latest GE Healthc",
    "full_text_length": 22737,
    "chunk_length": 1263
  },
  {
    "chunk_id": 1354,
    "paper_filename": "Jailin_2022_lesion_detection_contrast_enhanced_spectral_mammography.pdf",
    "paper_title": "Jailin 2022 Lesion Detection Contrast Enhanced Spectral Mammography",
    "chunk_index": 7,
    "total_chunks": 24,
    "text_content": "the similar Yolo-v4,18a state-of-the-art model for object detection. With a scalable architec- ture, it can be trained with relatively small datasets. From the published Yolo-v4, the only evolution of v5 used in this study is the automatic anchor size estimation based on k-means. The architecture consists in 3 parts: (a) a CSPDarknet backbone to extract features at di erent scales, (b) a PANet neck to perform features combination using a feature pyramidal structure and (c) a Yolo Layer head that",
    "full_text_length": 22737,
    "chunk_length": 1249
  },
  {
    "chunk_id": 1355,
    "paper_filename": "Jailin_2022_lesion_detection_contrast_enhanced_spectral_mammography.pdf",
    "paper_title": "Jailin 2022 Lesion Detection Contrast Enhanced Spectral Mammography",
    "chunk_index": 8,
    "total_chunks": 24,
    "text_content": "IoU is de ned as the ratio between the area of overlap over the area of union between the detected and ground truth bounding boxes. LDIoU = 1\u0000IoU +\u001a2(x;xgt) c2(1) with\u001athe Euclidean distance, xandxgtrespectively the coordinates of the centers for the predicted and ground truth boxes and cthe diagonal length of a rectangle circumscribed to the predicted and ground truth boxes. This loss leads to a better convergence than simple IoU loss. It can be noted that other metrics ( e.g., IoU, Complete Io",
    "full_text_length": 22737,
    "chunk_length": 1335
  },
  {
    "chunk_id": 1356,
    "paper_filename": "Jailin_2022_lesion_detection_contrast_enhanced_spectral_mammography.pdf",
    "paper_title": "Jailin 2022 Lesion Detection Contrast Enhanced Spectral Mammography",
    "chunk_index": 9,
    "total_chunks": 24,
    "text_content": "were used such as image ips, global intensity transforms and dedicated breast geometrical realistic transforms as developed in a previous study.20The weights of the model were initially pre-trained on Image-Net, then re-trained on the lesion detection. Finally, the inference is performed with Test Time Augmentation (TTA) that consists in performing the inference on an image duplicated with simple transforms (scales and ips) and averaging all detections. After the inference, a non-maximum suppres",
    "full_text_length": 22737,
    "chunk_length": 1280
  },
  {
    "chunk_id": 1357,
    "paper_filename": "Jailin_2022_lesion_detection_contrast_enhanced_spectral_mammography.pdf",
    "paper_title": "Jailin 2022 Lesion Detection Contrast Enhanced Spectral Mammography",
    "chunk_index": 10,
    "total_chunks": 24,
    "text_content": "to IoU threshold = 0:3). The results for the lesion detection are evaluated with FROC (Free Receiver Operating Characteristic) curves considering 3 metrics: 1. the sensitivity for the detection of all lesions (biopsied and not) in the breast 2. the sensitivity for the detection of only biopsied lesions (identi ed suspicious by the radiologist). A clinical use of this metric would be suspicious lesion detection in screening CESM. This application may help the radiologist for the analysis of compl",
    "full_text_length": 22737,
    "chunk_length": 1236
  },
  {
    "chunk_id": 1358,
    "paper_filename": "Jailin_2022_lesion_detection_contrast_enhanced_spectral_mammography.pdf",
    "paper_title": "Jailin 2022 Lesion Detection Contrast Enhanced Spectral Mammography",
    "chunk_index": 11,
    "total_chunks": 24,
    "text_content": "cation will be evaluated in term of sensitivity and speci city with a ROC curve and compared with results from radiologists extracted from a CESM diagnostic clinical review paper.21 3. RESULTS The di erent FROC curves corresponding to the 3 de ned metrics are plotted in gure 1(a). With a score threshold at 0.1 corresponding to a sensitivity of 0.95 for the detection of cancers (corresponding to the metric 3.), the false positive rate is 0.61 FP per breast. This metric is presented by the red FRO",
    "full_text_length": 22737,
    "chunk_length": 1138
  },
  {
    "chunk_id": 1359,
    "paper_filename": "Jailin_2022_lesion_detection_contrast_enhanced_spectral_mammography.pdf",
    "paper_title": "Jailin 2022 Lesion Detection Contrast Enhanced Spectral Mammography",
    "chunk_index": 12,
    "total_chunks": 24,
    "text_content": "breast level (blue dots correspond to radiologists' diagnostic results from21). For the detection of cancers, the distribution of the model scores for true positive (TP) or false positive (FP) detections are shown gure 2 with respectively the red and blue bars. When a FP is detected, it is interesting to separate (black) the non-annotated detection, (green) the non-biopsied annotated lesions and (orange) the biopsied benign lesions. With a high detection score ( >0.7), the FP are mainly composed",
    "full_text_length": 22737,
    "chunk_length": 1233
  },
  {
    "chunk_id": 1360,
    "paper_filename": "Jailin_2022_lesion_detection_contrast_enhanced_spectral_mammography.pdf",
    "paper_title": "Jailin 2022 Lesion Detection Contrast Enhanced Spectral Mammography",
    "chunk_index": 13,
    "total_chunks": 24,
    "text_content": "classi cation model is 0.930. The median speci city for the 18 clinical studies is 0.757. At this xed speci city, the sensibility of the AI model is 0.929. The lesion detection results obtained for few patients are illustrated in gure 3 with the red boxes being the detected regions and the green boxes the annotated ground truth. It can be seen that the detections are mainly consistent in the CC and MLO views (similar structure detection when the lesion was visible in the two views). However, the",
    "full_text_length": 22737,
    "chunk_length": 1188
  },
  {
    "chunk_id": 1361,
    "paper_filename": "Jailin_2022_lesion_detection_contrast_enhanced_spectral_mammography.pdf",
    "paper_title": "Jailin 2022 Lesion Detection Contrast Enhanced Spectral Mammography",
    "chunk_index": 14,
    "total_chunks": 24,
    "text_content": "are shown in green and the detected boxes in red. Although the ground truth lesions were correctly identi ed, multiple false positives are also detected. When analyzing the FP detections, three main categories appear: \u2022detection of localized non-lesion patterns ( e.g., vessels, biopsy clips, foreign objects, mole markers, etc.). An example of a non-lesion detection is shown gure 4 (left). Figure 2. Distribution of the TP (red) and FP (blue) scores (in the range [0.1-1]) for all detections on the",
    "full_text_length": 22737,
    "chunk_length": 1268
  },
  {
    "chunk_id": 1362,
    "paper_filename": "Jailin_2022_lesion_detection_contrast_enhanced_spectral_mammography.pdf",
    "paper_title": "Jailin 2022 Lesion Detection Contrast Enhanced Spectral Mammography",
    "chunk_index": 15,
    "total_chunks": 24,
    "text_content": "some cases, all fragmented lesions are detected and supplemented by a broad general annotation. These kinds of inconsistent detections may generate a lot of FP. \u2022detection of CESM artifacts ( e.g., scars, skin folds). For this reason, it is very important to use CESM recombined images with the least artifacts ( e.g., processed with Nira algorithm). 4. CONCLUSION This study presents the development of a deep learning-based detection model in CESM images. A large CESM dataset composed of 586 patie",
    "full_text_length": 22737,
    "chunk_length": 1267
  },
  {
    "chunk_id": 1363,
    "paper_filename": "Jailin_2022_lesion_detection_contrast_enhanced_spectral_mammography.pdf",
    "paper_title": "Jailin 2022 Lesion Detection Contrast Enhanced Spectral Mammography",
    "chunk_index": 16,
    "total_chunks": 24,
    "text_content": "to identify cancers with a sensitivity of 0.95 and 0.61 false positive per image. One perspective of this work is to include multi-view detection22as out current model detects lesions inde- pendently of views. Unifying the detections should improve model performance. Considering both the recombined and the low energy images will be a future investigation. As performed in CESM ROI classi cation papers,15the low energy may contain discriminant information to classify the detected ROIs. The model w",
    "full_text_length": 22737,
    "chunk_length": 1261
  },
  {
    "chunk_id": 1364,
    "paper_filename": "Jailin_2022_lesion_detection_contrast_enhanced_spectral_mammography.pdf",
    "paper_title": "Jailin 2022 Lesion Detection Contrast Enhanced Spectral Mammography",
    "chunk_index": 17,
    "total_chunks": 24,
    "text_content": "for lesion detection and analysis in FFDM and DBT images, they have not been employed in CESM. This work is, to the best of our knowledge, the rst attempt to develop a deep-learning CAD for CESM. Figure 3. Results for lesion detection at Se=0.95. Red box: detected area, green box: ground truth. In addition, a suspicion score is associated to each ROI. Figure 4. Three images containing FP detections (score threshold for cancer detection Se=0.95). (left) detection of a foreign object, (center) two",
    "full_text_length": 22737,
    "chunk_length": 1236
  },
  {
    "chunk_id": 1365,
    "paper_filename": "Jailin_2022_lesion_detection_contrast_enhanced_spectral_mammography.pdf",
    "paper_title": "Jailin 2022 Lesion Detection Contrast Enhanced Spectral Mammography",
    "chunk_index": 18,
    "total_chunks": 24,
    "text_content": "to acknowledge Ruben Sanchez, Ann-Katherine Carton, Jean-Paul Antonini (all working at GE Healthcare, Buc, France) for their help in data collection and insights in mammography and Andrei Petrovskii for useful discussion on deep learning models. We wish to point out the absence of con icts of interest related to our study. 6. COMPLIANCE WITH ETHICAL STANDARDS This research study was conducted retrospectively using anonymized human subject data made available by research partners. Applicable law ",
    "full_text_length": 22737,
    "chunk_length": 1258
  },
  {
    "chunk_id": 1366,
    "paper_filename": "Jailin_2022_lesion_detection_contrast_enhanced_spectral_mammography.pdf",
    "paper_title": "Jailin 2022 Lesion Detection Contrast Enhanced Spectral Mammography",
    "chunk_index": 19,
    "total_chunks": 24,
    "text_content": "[4] L. Shen, L. Margolies, J. Rothstein, E. Fluder, R. McBride, W. Sieh, Deep learning to improve breast cancer detection on screening mammography, Scienti c reports 9 (1) (2019) 1{12. [5] H.-E. Kim, H. H. Kim, B.-K. Han, K. H. Kim, K. Han, H. Nam, E. H. Lee, E.-K. Kim, Changes in cancer detection and false-positive recall in mammography using arti cial intelligence: a retrospective, multireader study, The Lancet Digital Health 2 (3) (2020) e138{e148. [6] M. A. Al-Masni, M. A. Al-Antari, J.-M. P",
    "full_text_length": 22737,
    "chunk_length": 1307
  },
  {
    "chunk_id": 1367,
    "paper_filename": "Jailin_2022_lesion_detection_contrast_enhanced_spectral_mammography.pdf",
    "paper_title": "Jailin 2022 Lesion Detection Contrast Enhanced Spectral Mammography",
    "chunk_index": 20,
    "total_chunks": 24,
    "text_content": "(3) (2011) 565{574. [8] E. Luczy\u0013 nska, S. Heinze-Paluchowska, S. Dyczek, P. Blecharz, J. Rys, M. Reinfuss, Contrast-enhanced spectral mammography: comparison with conventional mammography and histopathology in 152 women, Korean journal of radiology 15 (6) (2014) 689{696. [9] F. Diekmann, S. Diekmann, F. Jeunehomme, S. Muller, B. Hamm, U. Bick, Digital mammography us- ing iodine-based contrast media: initial clinical experience with dynamic contrast medium enhancement, Investigative radiology 40",
    "full_text_length": 22737,
    "chunk_length": 1331
  },
  {
    "chunk_id": 1368,
    "paper_filename": "Jailin_2022_lesion_detection_contrast_enhanced_spectral_mammography.pdf",
    "paper_title": "Jailin 2022 Lesion Detection Contrast Enhanced Spectral Mammography",
    "chunk_index": 21,
    "total_chunks": 24,
    "text_content": "R. Dentamaro, V. Didonna, V. Lorusso, R. Massafra, P. Tamborra, et al., Radiomics analysis on contrast-enhanced spectral mammography images for breast cancer diagnosis: A pilot study, Entropy 21 (11) (2019) 1110. [13] B. K. Patel, S. Ranjbar, T. Wu, B. A. Pockaj, J. Li, N. Zhang, M. Lobbes, B. Zhang, J. R. Mitchell, Computer-aided diagnosis of contrast-enhanced spectral mammography: A feasibility study, European jour- nal of radiology 98 (2018) 207{213. [14] F. Lin, Z. Wang, K. Zhang, P. Yang, H",
    "full_text_length": 22737,
    "chunk_length": 1292
  },
  {
    "chunk_id": 1369,
    "paper_filename": "Jailin_2022_lesion_detection_contrast_enhanced_spectral_mammography.pdf",
    "paper_title": "Jailin 2022 Lesion Detection Contrast Enhanced Spectral Mammography",
    "chunk_index": 22,
    "total_chunks": 24,
    "text_content": "mammography images, Diagnostics 11 (4) (2021) 684. [16] S. Perek, N. Kiryati, G. Zimmerman-Moreno, M. Sklair-Levy, E. Konen, A. Mayer, Classi cation of contrast- enhanced spectral mammography (CESM) images, International journal of computer assisted radiology and surgery 14 (2) (2019) 249{257. [17] M. Caballo, D. R. Pangallo, W. Sanderink, A. M. Hernandez, S. H. Lyu, F. Molinari, J. M. Boone, R. M. Mann, I. Sechopoulos, Multi-marker quantitative radiomics for mass characterization in dedicated b",
    "full_text_length": 22737,
    "chunk_length": 1337
  },
  {
    "chunk_id": 1370,
    "paper_filename": "Jailin_2022_lesion_detection_contrast_enhanced_spectral_mammography.pdf",
    "paper_title": "Jailin 2022 Lesion Detection Contrast Enhanced Spectral Mammography",
    "chunk_index": 23,
    "total_chunks": 24,
    "text_content": "for breast cancer mass segmentation, in: International Conference on Medical Imaging and Computer-Aided Diagnosis, Springer, 2021, pp. 228{237. [21] X. Zhu, J.-M. Huang, K. Zhang, L.-J. Xia, L. Feng, P. Yang, M.-Y. Zhang, W. Xiao, H.-X. Lin, Y.-H. Yu, Diagnostic value of contrast-enhanced spectral mammography for screening breast cancer: systematic review and meta-analysis, Clinical breast cancer 18 (5) (2018) e985{e995. [22] Z. Yang, Z. Cao, Y. Zhang, Y. Tang, X. Lin, R. Ouyang, M. Wu, M. Han, ",
    "full_text_length": 22737,
    "chunk_length": 633
  },
  {
    "chunk_id": 1371,
    "paper_filename": "Jana_2024_artificial_intelligence_for_multimodal_data_integration_in_oncology.pdf",
    "paper_title": "Jana 2024 Artificial Intelligence For Multimodal Data Integration In Oncology",
    "chunk_index": 0,
    "total_chunks": 93,
    "text_content": "Review Arti\ufb01cial intelligence for multimodal data integration in oncology Jana Lipkova,1,2,3,4Richard J. Chen,1,2,3,4,5Bowen Chen,1,2,8Ming Y. Lu,1,2,3,4,7Matteo Barbieri,1Daniel Shao,1,2,6 Anurag J. Vaidya,1,2,6Chengkuan Chen,1,2,3,4Luoting Zhuang,1,3Drew F.K. Williamson,1,2,3,4Muhammad Shaban,1,2,3,4 Tiffany Y. Chen,1,2,3,4and Faisal Mahmood1,2,3,4,9,* 1Department of Pathology, Brigham and Women\u2019s Hospital, Harvard Medical School, Boston, MA, USA 2Department of Pathology, Massachusetts General",
    "full_text_length": 92263,
    "chunk_length": 1707
  },
  {
    "chunk_id": 1372,
    "paper_filename": "Jana_2024_artificial_intelligence_for_multimodal_data_integration_in_oncology.pdf",
    "paper_title": "Jana 2024 Artificial Intelligence For Multimodal Data Integration In Oncology",
    "chunk_index": 1,
    "total_chunks": 93,
    "text_content": "and genomics to electronic health records. Current arti\ufb01cial intelligence (AI) models operate mainly inthe realm of a single modality, neglecting the broader clinical context, which inevitably diminishes their po- tential. Integration of different data modalities provides opportunities to increase robustness and accuracy of diagnostic and prognostic models, bringing AI closer to clinical practice. AI models are also capable ofdiscovering novel patterns within and across modalities suitable for e",
    "full_text_length": 92263,
    "chunk_length": 1475
  },
  {
    "chunk_id": 1373,
    "paper_filename": "Jana_2024_artificial_intelligence_for_multimodal_data_integration_in_oncology.pdf",
    "paper_title": "Jana 2024 Artificial Intelligence For Multimodal Data Integration In Oncology",
    "chunk_index": 2,
    "total_chunks": 93,
    "text_content": "scopic and macroscopic changes with mechanisms and interac- tions that are not yet fully understood. Cancer biomarkers provide insights into the state and course of disease in the form of quan-titative or qualitative measurements, which consequently guide patient management. Based on their primary use, biomarkers can be diagnostic, prognostic or predictive of response and resis-tance to treatment. Diagnostic markers stand at the \ufb01rst line of cancer detection and diagnosis, including examples suc",
    "full_text_length": 92263,
    "chunk_length": 1396
  },
  {
    "chunk_id": 1374,
    "paper_filename": "Jana_2024_artificial_intelligence_for_multimodal_data_integration_in_oncology.pdf",
    "paper_title": "Jana 2024 Artificial Intelligence For Multimodal Data Integration In Oncology",
    "chunk_index": 3,
    "total_chunks": 93,
    "text_content": "tumor grade and stage to genomic and transcriptomic assays such as Oncotype DX and Prosigna (PAM50), often used to estimaterecurrence and survival likelihood ( Paik et al., 2004 ). Despite the vital role of biomarkers, patients with similar pro\ufb01les can exhibitdiverse outcomes, treatment responses ( Shergalis et al., 2018 ), recurrence rates ( Roy et al., 2015 ), or treatment toxicity ( Kennedy and Salama, 2020 ), while the underlying reasons for such dichot- omies largely remain unknown. There i",
    "full_text_length": 92263,
    "chunk_length": 1350
  },
  {
    "chunk_id": 1375,
    "paper_filename": "Jana_2024_artificial_intelligence_for_multimodal_data_integration_in_oncology.pdf",
    "paper_title": "Jana 2024 Artificial Intelligence For Multimodal Data Integration In Oncology",
    "chunk_index": 4,
    "total_chunks": 93,
    "text_content": "the path to the next generation of personalized medicine, as illustrated in Figure 1. An analysis of possible correlation and patterns across diverse data modalities can easily become too complex during subjective analysis, making it an attractive application for AI-methods (Boehm et al., 2022 ). The capacity of AI models to leverage diverse complementary information from multimodal data and identify predictive features within and across modalities allows for automated and objective exploration ",
    "full_text_length": 92263,
    "chunk_length": 1448
  },
  {
    "chunk_id": 1376,
    "paper_filename": "Jana_2024_artificial_intelligence_for_multimodal_data_integration_in_oncology.pdf",
    "paper_title": "Jana 2024 Artificial Intelligence For Multimodal Data Integration In Oncology",
    "chunk_index": 5,
    "total_chunks": 93,
    "text_content": "tissue morphology) or quantitative measurements (such as genomic, transcriptomic alterations) and their associa-tion with clinical endpoints. For instance, standardized morpho- logic assesment pipelines such as the the Nottingham grading system in breast cancer ( Rakha et al., 2008 ) and the Gleason grading in prostate cancers ( Epstein et al., 2016 ) was determined through dedicated examination of thousands of histopathology slides, revealing associations between morphological features and pati",
    "full_text_length": 92263,
    "chunk_length": 1465
  },
  {
    "chunk_id": 1377,
    "paper_filename": "Jana_2024_artificial_intelligence_for_multimodal_data_integration_in_oncology.pdf",
    "paper_title": "Jana 2024 Artificial Intelligence For Multimodal Data Integration In Oncology",
    "chunk_index": 6,
    "total_chunks": 93,
    "text_content": "their clinical potential. For instance, glioma patients withsimilar genetic or histology pro\ufb01les can have diverse outcomes caused by macroscopic factors, such as a tumor location pre- venting full resection and irradiation or disruption of the blood-brain barrier, altering the ef\ufb01cacy of drug delivery ( Miller, 2002 ). Over the past years, arti\ufb01cial intelligence (AI) and in particular representation learning methods have demonstrated great per-formance in many clinically relevant tasks inclusing",
    "full_text_length": 92263,
    "chunk_length": 1325
  },
  {
    "chunk_id": 1378,
    "paper_filename": "Jana_2024_artificial_intelligence_for_multimodal_data_integration_in_oncology.pdf",
    "paper_title": "Jana 2024 Artificial Intelligence For Multimodal Data Integration In Oncology",
    "chunk_index": 7,
    "total_chunks": 93,
    "text_content": "1D). Similarly, AI models can discover associations across multiple modalities, such as relations be- tween certain mutations and speci\ufb01c changes in cellularmorphology ( Coudray et al., 2018 ) or associations between radi- ology \ufb01ndings and histology-speci\ufb01c tumor subtypes ( Ferreira- Junior et al., 2020 ;Hyun et al., 2019 ) or molecular features ( Yan et al., 2021 )(Figure 1 B). Such associations can identify acces- sible or non-invasive alternatives for existing biomarkers to sup- port large-s",
    "full_text_length": 92263,
    "chunk_length": 1358
  },
  {
    "chunk_id": 1379,
    "paper_filename": "Jana_2024_artificial_intelligence_for_multimodal_data_integration_in_oncology.pdf",
    "paper_title": "Jana 2024 Artificial Intelligence For Multimodal Data Integration In Oncology",
    "chunk_index": 8,
    "total_chunks": 93,
    "text_content": "multimodal data integration (A and C\u2013F) (A) AI models can integrate complementary information and clinical context from diverse data sources to provide more accurate outcome pre dictions. The clinical insights identi\ufb01ed by such models can be further elucidated through (C) interpretability methods and (D) quantitative analysis to guid e and accelerate the discovery of new biomarkers or therapeutic targets (E and F). (B) AI can reveal novel multimodal interconnections, such as relations between ce",
    "full_text_length": 92263,
    "chunk_length": 1357
  },
  {
    "chunk_id": 1380,
    "paper_filename": "Jana_2024_artificial_intelligence_for_multimodal_data_integration_in_oncology.pdf",
    "paper_title": "Jana 2024 Artificial Intelligence For Multimodal Data Integration In Oncology",
    "chunk_index": 9,
    "total_chunks": 93,
    "text_content": "supervised methods allow one to train the model with weak, patient-level labels, avoiding the need for manual annotations. (C) Unsupervised methods explore patterns, subgroups, and structures in unlabeled data. For comparison, all methods are illustrated on a binary can cer detection task.ll OPEN ACCESS Cancer Cell 40, October 10, 2022 1097Review each category we present all methods in the framework of com- puter vision as applied to digital pathology ( Figure 2 ). Supervised methods Supervised ",
    "full_text_length": 92263,
    "chunk_length": 1369
  },
  {
    "chunk_id": 1381,
    "paper_filename": "Jana_2024_artificial_intelligence_for_multimodal_data_integration_in_oncology.pdf",
    "paper_title": "Jana 2024 Artificial Intelligence For Multimodal Data Integration In Oncology",
    "chunk_index": 10,
    "total_chunks": 93,
    "text_content": "random forest (RF), sup- port-vector machine (SVM), or multilayer perceptron (MLP) ( Bert- simas and Wiberg, 2020 )(Figure 2 ). Since the feature extraction is not part of the learning process, the models typically have simpler architecture, lower computation cost, and may requireless training data than DL models. An additional bene\ufb01t is a high level of interpretability, since the predictive features can be related to the data. On the other hand, the feature extractionis time consuming and can t",
    "full_text_length": 92263,
    "chunk_length": 1351
  },
  {
    "chunk_id": 1382,
    "paper_filename": "Jana_2024_artificial_intelligence_for_multimodal_data_integration_in_oncology.pdf",
    "paper_title": "Jana 2024 Artificial Intelligence For Multimodal Data Integration In Oncology",
    "chunk_index": 11,
    "total_chunks": 93,
    "text_content": "or malignancies. Despite the popularity of DL methods, in many applications the hand-crafted methods are suf\ufb01cient andpreferred due to their simplicity and ability to learn from smallerdatasets. Representation learning methods Representation learning methods such as deep learning (DL) are capable of learning rich feature representations from the raw data without the need for manual feature engineering. Here we focus on convolutional neural networks (CNNs), the most com-mon DL strategy for image ",
    "full_text_length": 92263,
    "chunk_length": 1395
  },
  {
    "chunk_id": 1383,
    "paper_filename": "Jana_2024_artificial_intelligence_for_multimodal_data_integration_in_oncology.pdf",
    "paper_title": "Jana 2024 Artificial Intelligence For Multimodal Data Integration In Oncology",
    "chunk_index": 12,
    "total_chunks": 93,
    "text_content": "convolutional, pooling, and non-linear activation layers, followed by a smallnumber of fully connected layers. A convolution layer serves as a feature extractor, while the subsequent pooling layer con- denses the features into the most relevant ones. The non-linearactivation function allows the model to explore complex relationsacross features. Fully connected layers then perform the end task, such as classi\ufb01cation. The main strength of CNNs is their ability to extract rich feature representatio",
    "full_text_length": 92263,
    "chunk_length": 1441
  },
  {
    "chunk_id": 1384,
    "paper_filename": "Jana_2024_artificial_intelligence_for_multimodal_data_integration_in_oncology.pdf",
    "paper_title": "Jana 2024 Artificial Intelligence For Multimodal Data Integration In Oncology",
    "chunk_index": 13,
    "total_chunks": 93,
    "text_content": "by the model to make predictive determinations, the overall feature representations remain abstract. Despitethese limitations, CNNs come with impressive performance,contributing to widespread usage in many clinically relevent ap- plications. Weakly supervised methods Weakly supervised learning is a sub-category of supervisedlearning with batch annotations on large clusters of data essen- tially representing a scenario where the supervisory signal is weak compared to the amount of noise in the da",
    "full_text_length": 92263,
    "chunk_length": 1370
  },
  {
    "chunk_id": 1385,
    "paper_filename": "Jana_2024_artificial_intelligence_for_multimodal_data_integration_in_oncology.pdf",
    "paper_title": "Jana 2024 Artificial Intelligence For Multimodal Data Integration In Oncology",
    "chunk_index": 14,
    "total_chunks": 93,
    "text_content": "can be used to explicitly capture structure within data and encode relations between objects making them ideal for analysis of tissue biospy images. A graph is de\ufb01ned by nodesconnected by edges. In histology, a node can represent acell, an image patch, or even a tissue region. Edges encode spatial relations and interactions between nodes ( Zhang et al., 2019 ). The graph, combined with the patient-level labels, is processed by a GCN ( Ahmedt-Aristizabal et al., 2021 ), which can be seen as a gen",
    "full_text_length": 92263,
    "chunk_length": 1271
  },
  {
    "chunk_id": 1386,
    "paper_filename": "Jana_2024_artificial_intelligence_for_multimodal_data_integration_in_oncology.pdf",
    "paper_title": "Jana 2024 Artificial Intelligence For Multimodal Data Integration In Oncology",
    "chunk_index": 15,
    "total_chunks": 93,
    "text_content": "exclusive. This can be bene\ufb01cial in tasks where the spatial context spans beyondthe scope of a single patch (e.g., Gleason score). On the other hand, the interdependence of the nodes in GCNs comes with higher training costs and memory requirements, since the no-des cannot be processed independently. Multiple-instance learning MIL is a type of weakly supervised learning where multiple in-stances of the input are not individually labeled and the supervi- sory signal is only available collectively ",
    "full_text_length": 92263,
    "chunk_length": 1214
  },
  {
    "chunk_id": 1387,
    "paper_filename": "Jana_2024_artificial_intelligence_for_multimodal_data_integration_in_oncology.pdf",
    "paper_title": "Jana 2024 Artificial Intelligence For Multimodal Data Integration In Oncology",
    "chunk_index": 16,
    "total_chunks": 93,
    "text_content": "other higher dimen- tional data into lower-dimensional embeddings this module can be trained on the \ufb02y ( Campanella et al., 2019 ) or a pre-trainedll OPEN ACCESS 1098 Cancer Cell 40, October 10, 2022Review encoder from supervised or self-supervised learning can be used to reduce training time and data-ef\ufb01ciency ( Lu et al., 2021 ). The instance-level embeddings are aggregated to create patient-level representations, which serve as input for the \ufb01nal classi\ufb01cationmodule. A commonly used aggrigati",
    "full_text_length": 92263,
    "chunk_length": 1303
  },
  {
    "chunk_id": 1388,
    "paper_filename": "Jana_2024_artificial_intelligence_for_multimodal_data_integration_in_oncology.pdf",
    "paper_title": "Jana 2024 Artificial Intelligence For Multimodal Data Integration In Oncology",
    "chunk_index": 17,
    "total_chunks": 93,
    "text_content": "datasets \ufb01ne annotations are often not available which makes MIL an ideal approach for training deep models, thereare several recent examples in cancer pathology ( Campanella et al., 2019; Lu et al., 2021a,b ) and genomics ( Sidhom et al., 2021 ). Vision transformersVITs ( Dosovitskiy et al., 2020 ;Vaswani et al., 2017 )a r eat y p eo f attention-based learning which allows for the model to be fully context aware. In contrast to MIL, where patches are assumed in- dependent and identically distri",
    "full_text_length": 92263,
    "chunk_length": 1323
  },
  {
    "chunk_id": 1389,
    "paper_filename": "Jana_2024_artificial_intelligence_for_multimodal_data_integration_in_oncology.pdf",
    "paper_title": "Jana 2024 Artificial Intelligence For Multimodal Data Integration In Oncology",
    "chunk_index": 18,
    "total_chunks": 93,
    "text_content": "different types of interactions between the patches and combines them into a single self-atten-tion output. A typical VIT architecture is shown in Figure 2 . A WSI is converted into a series of patches, each coupled with positional information. Learnable encoders map each patch and its position into a single embedding vector, referred to as a token. An addi-tional tokens is introduced for the classi\ufb01cation task. The class to- ken together with the patch tokens is fed into the transformer encoder",
    "full_text_length": 92263,
    "chunk_length": 1307
  },
  {
    "chunk_id": 1390,
    "paper_filename": "Jana_2024_artificial_intelligence_for_multimodal_data_integration_in_oncology.pdf",
    "paper_title": "Jana 2024 Artificial Intelligence For Multimodal Data Integration In Oncology",
    "chunk_index": 19,
    "total_chunks": 93,
    "text_content": "Li et al., 2022 ;Shamshad et al., 2022 ) of VIT methods over other methods. On the other hand, VITs tend to be more data hungry ( Dosovitskiy et al., 2020 ), a limitation that the machine learning community is actively working to overcome. Weakly supervised methods offer several bene\ufb01ts. The libe- ration from manual annotations reduces the cost of datapreprocessing and mitigates the bias and interrater variability. Consequently, the models can be easily applied to large data- sets, diverse tasks",
    "full_text_length": 92263,
    "chunk_length": 1355
  },
  {
    "chunk_id": 1391,
    "paper_filename": "Jana_2024_artificial_intelligence_for_multimodal_data_integration_in_oncology.pdf",
    "paper_title": "Jana 2024 Artificial Intelligence For Multimodal Data Integration In Oncology",
    "chunk_index": 20,
    "total_chunks": 93,
    "text_content": "any labels. These include self-supervised and fully unsupervised strategies.Self-supervised methods Self-supervised methods aim to learn rich feature representa- tions from within data by posing the learning problem as atask the ground truth for which is de\ufb01ned within the data.Such encoders are often used to obtain high quality lower di- mentional embeddings of complex high dimentional datasets for making downstream tasks more ef\ufb01cient interms of dataand training ef\ufb01ciency. For example in pathol",
    "full_text_length": 92263,
    "chunk_length": 1289
  },
  {
    "chunk_id": 1392,
    "paper_filename": "Jana_2024_artificial_intelligence_for_multimodal_data_integration_in_oncology.pdf",
    "paper_title": "Jana 2024 Artificial Intelligence For Multimodal Data Integration In Oncology",
    "chunk_index": 21,
    "total_chunks": 93,
    "text_content": "the actual patch as a la- bel (Figure 2 ). The patch prediction has no direct clinical rele- vance, but it guides the model to learn general-purpose features of image characteristics, which can be bene\ufb01cial for other prac- tical tasks. The early layers of the network are usually capturegeneral image features, while the later layers pick features rele- vant for the task at hand. The later layers can be excluded, while the early layers serve as feature extractors in for supervisedmodels (i.e., tra",
    "full_text_length": 92263,
    "chunk_length": 1349
  },
  {
    "chunk_id": 1393,
    "paper_filename": "Jana_2024_artificial_intelligence_for_multimodal_data_integration_in_oncology.pdf",
    "paper_title": "Jana 2024 Artificial Intelligence For Multimodal Data Integration In Oncology",
    "chunk_index": 22,
    "total_chunks": 93,
    "text_content": "partition data into subgroups such that the simi- larities within the subgroup and the separation between sub- groups are maximized. Although the output clusters are not task speci\ufb01c, they can reveal different cancer subtypes or pa-tient subgroups. The aim of dimensionality reduction is to obtainlow-dimensional representation capturing the main characteris- tics and correlations in the data. MULTIMODAL DATA FUSION The aim of multimodal data fusion is to extract and combine complementary contextu",
    "full_text_length": 92263,
    "chunk_length": 1347
  },
  {
    "chunk_id": 1394,
    "paper_filename": "Jana_2024_artificial_intelligence_for_multimodal_data_integration_in_oncology.pdf",
    "paper_title": "Jana 2024 Artificial Intelligence For Multimodal Data Integration In Oncology",
    "chunk_index": 23,
    "total_chunks": 93,
    "text_content": "( Louis et al., 2016 ). AI offers an auto- mated and objective way to incorporate complementary infor- mation and clinical context from diverse data for improved predictions. Multimodal data-driven AI models can also utilizell OPEN ACCESS Cancer Cell 40, October 10, 2022 1099Review complementary and supplementary information in modalities; if unimodal data are noisy or incomplete, supplementing redun- dant information from other modalities can improve the robust-ness and accuracy of the predicti",
    "full_text_length": 92263,
    "chunk_length": 1329
  },
  {
    "chunk_id": 1395,
    "paper_filename": "Jana_2024_artificial_intelligence_for_multimodal_data_integration_in_oncology.pdf",
    "paper_title": "Jana 2024 Artificial Intelligence For Multimodal Data Integration In Oncology",
    "chunk_index": 24,
    "total_chunks": 93,
    "text_content": "fusion builds a joint representation from raw data or features at the input level, before feeding it to the model. (B) Late fusion trains a separate model for each modality and aggregates the predictions from individual models at the decision level. (C\u2013E) In intermediate fusion, the prediction loss is propagated back to the feature extraction layer of each modality to iteratively learn improved f eature repre- sentations under the multimodal context. The unimodal data can be fused (C) at a singl",
    "full_text_length": 92263,
    "chunk_length": 1266
  },
  {
    "chunk_id": 1396,
    "paper_filename": "Jana_2024_artificial_intelligence_for_multimodal_data_integration_in_oncology.pdf",
    "paper_title": "Jana 2024 Artificial Intelligence For Multimodal Data Integration In Oncology",
    "chunk_index": 25,
    "total_chunks": 93,
    "text_content": "pro- cess. However, it is assumed that the single model is well suitedto all modalities. Early fusion requires a certain level of alignment or synchronization between the modalities. Although this is more obvious in other domains, such as synchronization of audio andvisual signals in speech recognition, it is also relevant in clinicalsettings. If the modalities come from signi\ufb01cantly different time points, such as pre- and postinterventions, then early fusion might not be an appropriate choice. ",
    "full_text_length": 92263,
    "chunk_length": 1291
  },
  {
    "chunk_id": 1397,
    "paper_filename": "Jana_2024_artificial_intelligence_for_multimodal_data_integration_in_oncology.pdf",
    "paper_title": "Jana 2024 Artificial Intelligence For Multimodal Data Integration In Oncology",
    "chunk_index": 26,
    "total_chunks": 93,
    "text_content": "data with electronic medical records (EMRs), such as integra- tion of dermoscopic images and patient data for skin lesion classi- \ufb01cation ( Yap et al., 2018 ), or fusion of a cervigram and EMRs for cervicaldysplasia diagnosis( Xuetal., 2016 ).Several studies inves- tigate the correlation between changes in gene expression and tis- sue morphology, integrating genomics data with histology and/orradiology images for cancer classi\ufb01cation ( Khosravi et al., 2021 ), survival ( Chen et al., 2020b ,2021",
    "full_text_length": 92263,
    "chunk_length": 1254
  },
  {
    "chunk_id": 1398,
    "paper_filename": "Jana_2024_artificial_intelligence_for_multimodal_data_integration_in_oncology.pdf",
    "paper_title": "Jana 2024 Artificial Intelligence For Multimodal Data Integration In Oncology",
    "chunk_index": 27,
    "total_chunks": 93,
    "text_content": "allows one to use a different model architecture for each modality and does not pose any constraints on data synchroniza-tion, making it suitable for systems with large data heterogeneity or modalities from different time points. In cases of missing or incomplete data, late fusion retains the ability to make predictions,since each model is trained separately, and aggregations, such asmajority voting, can be applied even if a modality is missing. Simi- larly, inclusion of a new modality can be pe",
    "full_text_length": 92263,
    "chunk_length": 1336
  },
  {
    "chunk_id": 1399,
    "paper_filename": "Jana_2024_artificial_intelligence_for_multimodal_data_integration_in_oncology.pdf",
    "paper_title": "Jana 2024 Artificial Intelligence For Multimodal Data Integration In Oncology",
    "chunk_index": 28,
    "total_chunks": 93,
    "text_content": "Furthermore, er-rors from individual models tend to be uncorrelated, resulting in potentially lower bias and variance in late-fusion predictions. In situations when information density varies signi\ufb01cantly acrossmodalities, predictions from shared representations can be heavi-ly in\ufb02uenced by the most dominant modality. In late fusion, the contribution from each modality can be accounted for in a controlled manner by setting equal or diverse weights per modal-ity in the aggregation step. Examples ",
    "full_text_length": 92263,
    "chunk_length": 1304
  },
  {
    "chunk_id": 1400,
    "paper_filename": "Jana_2024_artificial_intelligence_for_multimodal_data_integration_in_oncology.pdf",
    "paper_title": "Jana 2024 Artificial Intelligence For Multimodal Data Integration In Oncology",
    "chunk_index": 29,
    "total_chunks": 93,
    "text_content": "(Joo et al., 2021 ), and survival estimation ( Nie et al., 2016 ). Intermediate fusion This is a strategy wherein the loss from the multimodal modelpropagates back to the feature extraction layer of each mo- dality to iteratively improve feature representations under the multimodal context. For comparison, in early and late fusion,the unimodal embeddings are not affected by the multimodal information. Intermediate fusion can combine individual mo- dalities at different levels of abstractions. Mo",
    "full_text_length": 92263,
    "chunk_length": 1296
  },
  {
    "chunk_id": 1401,
    "paper_filename": "Jana_2024_artificial_intelligence_for_multimodal_data_integration_in_oncology.pdf",
    "paper_title": "Jana 2024 Artificial Intelligence For Multimodal Data Integration In Oncology",
    "chunk_index": 30,
    "total_chunks": 93,
    "text_content": "modalities, followed by fusion with less correlated data in later layers. For instance, in Figure 3 D, genomics and histology data are fused \ufb01rst, to account for the interplay between mutations and changes in the tissue morphology, while the relation with the macroscopic radiology data is considered in the laterlayer. Gradual fusion has shown improved performance oversingle-level fusion in some applications ( Joze et al., 2020 ;Kar- pathy et al., 2014 ). Lastly, guided-fusion allows model to use",
    "full_text_length": 92263,
    "chunk_length": 1330
  },
  {
    "chunk_id": 1402,
    "paper_filename": "Jana_2024_artificial_intelligence_for_multimodal_data_integration_in_oncology.pdf",
    "paper_title": "Jana 2024 Artificial Intelligence For Multimodal Data Integration In Oncology",
    "chunk_index": 31,
    "total_chunks": 93,
    "text_content": "genomics feature and the corresponding genomics-guided histology features are combined for the \ufb01nal model predictions. Examples of intermediate fusion include integration of diverse imaging modalities, such as the fusion of PET and CT scans in lung cancer detection ( Kumar et al., 2019 ), fusion of MRI and ultrasound images in prostate cancer classi\ufb01cation (Sedghi et al., 2020 ), or combination of multimodel MRI scans in glioma segmentation ( Havaei et al., 2016 ). Fusion of diverse multiomics d",
    "full_text_length": 92263,
    "chunk_length": 1072
  },
  {
    "chunk_id": 1403,
    "paper_filename": "Jana_2024_artificial_intelligence_for_multimodal_data_integration_in_oncology.pdf",
    "paper_title": "Jana 2024 Artificial Intelligence For Multimodal Data Integration In Oncology",
    "chunk_index": 32,
    "total_chunks": 93,
    "text_content": "ology modalities was used to improve segmentation of liver le- sions ( Mo et al., 2020 ) and anomalies in breast tissue ( Lei et al., 2020 ). EMRs were used to guide feature extraction from dermoscopic ( Zhou and Luo, 2021 ) and mammography (Vo et al., 2021 ) images to improve detection andll OPEN ACCESS Cancer Cell 40, October 10, 2022 1101Review classi\ufb01cation of lesions. Chen et al. ( Chen et al., 2021b )u s e d genomics information to guide selection of histology features for improved surviva",
    "full_text_length": 92263,
    "chunk_length": 1226
  },
  {
    "chunk_id": 1404,
    "paper_filename": "Jana_2024_artificial_intelligence_for_multimodal_data_integration_in_oncology.pdf",
    "paper_title": "Jana 2024 Artificial Intelligence For Multimodal Data Integration In Oncology",
    "chunk_index": 33,
    "total_chunks": 93,
    "text_content": "models might use spurious shortcuts for Figure 4. Multimodal interpretability and introspection (A and B) Histology: an MIL model was trained to classify subtypes of renal cell carcinoma in WSIs, while CNN was trained to perform the same task in image patches. (A) Attention heatmaps and patches with the lowest and highest attention scores. (B) GradCAM attributions for each class. (C and E) Integrated gradient attributions can be used to analyze (C) genomics or (E) EMRs. The attribution magnitude",
    "full_text_length": 92263,
    "chunk_length": 1203
  },
  {
    "chunk_id": 1405,
    "paper_filename": "Jana_2024_artificial_intelligence_for_multimodal_data_integration_in_oncology.pdf",
    "paper_title": "Jana 2024 Artificial Intelligence For Multimodal Data Integration In Oncology",
    "chunk_index": 34,
    "total_chunks": 93,
    "text_content": "Radiology: an MIL model was trained to predict survival from MRI scans using axial slides as individual instances. (D) Attention heatmaps map ped into the 3D MRI scan and slides with the highest and lowest attention. (F) GradCAM was used to obtain pixel-level interpretability in each MRI slide. A 3D pix el-level interpretability is computed by weighting the slide-level GradCAM maps by the attention score of the respective slide.ll OPEN ACCESS 1102 Cancer Cell 40, October 10, 2022Review predictio",
    "full_text_length": 92263,
    "chunk_length": 1240
  },
  {
    "chunk_id": 1406,
    "paper_filename": "Jana_2024_artificial_intelligence_for_multimodal_data_integration_in_oncology.pdf",
    "paper_title": "Jana 2024 Artificial Intelligence For Multimodal Data Integration In Oncology",
    "chunk_index": 35,
    "total_chunks": 93,
    "text_content": "found in a recent review ( Arrieta et al., 2020 ). It is worth indicating that these methods allow us to introspect parts of the data deemed impor- tant by the model in making predictive determinations yet the feature representation itself remains abstract. Histopathology In histopathology, VITs or MIL can reveal the relative importance of each image patch for the model predictions. Depending on the model architecture attention or probability scores can be map-ped to obtain slide-level attention",
    "full_text_length": 92263,
    "chunk_length": 1273
  },
  {
    "chunk_id": 1407,
    "paper_filename": "Jana_2024_artificial_intelligence_for_multimodal_data_integration_in_oncology.pdf",
    "paper_title": "Jana 2024 Artificial Intelligence For Multimodal Data Integration In Oncology",
    "chunk_index": 36,
    "total_chunks": 93,
    "text_content": "importance of the model inputs (e.g., pixels) by computing how the changes in the inputs affect the model outputs for each prediction class. GradCAM is oftenused in tandem with the guided-backpropagation method, the so-called guided-GradCAM ( Selvaraju et al., 2016 ), where the guided backpropagation determines the pixel-level importanceinside the predictive regions speci\ufb01ed by the GradCAM. This is illustrated in Figure 4 B, where a CNN was trained to classify can- cer subtypes in image patches.",
    "full_text_length": 92263,
    "chunk_length": 1270
  },
  {
    "chunk_id": 1408,
    "paper_filename": "Jana_2024_artificial_intelligence_for_multimodal_data_integration_in_oncology.pdf",
    "paper_title": "Jana 2024 Artificial Intelligence For Multimodal Data Integration In Oncology",
    "chunk_index": 37,
    "total_chunks": 93,
    "text_content": "2022 ). The model considered the 3D MRI scan as a bag, where the axial slides are modeled as individual instances. Even in the absence of manual annotations, the model placedhigh attention to the slides with tumor, while low attention was assigned to healthy tissue. CAM-based methods can be conse- quently deployed to localize the predictive regions within individ-ual slides ( Figure 4 F). Molecular data Molecular data can be analyzed by the integrated gradient method ( Sundararajan et al., 2017 ",
    "full_text_length": 92263,
    "chunk_length": 1285
  },
  {
    "chunk_id": 1409,
    "paper_filename": "Jana_2024_artificial_intelligence_for_multimodal_data_integration_in_oncology.pdf",
    "paper_title": "Jana 2024 Artificial Intelligence For Multimodal Data Integration In Oncology",
    "chunk_index": 38,
    "total_chunks": 93,
    "text_content": "is visualized as a bar plot,where the y axis corresponds to the speci\ufb01c features (ordered by their absolute attribution value) and the x axis shows the cor- responding attribution values. At the population level, the attribu- tion plots depict the distribution of the attribution scores acrossall subjects. Figure 4 C shows the attribution plots for most important genomics features used for survival prediction in gli- oma patients ( Chen et al., 2021c ). Other tabular data, such as hand-crafted fe",
    "full_text_length": 92263,
    "chunk_length": 1326
  },
  {
    "chunk_id": 1410,
    "paper_filename": "Jana_2024_artificial_intelligence_for_multimodal_data_integration_in_oncology.pdf",
    "paper_title": "Jana 2024 Artificial Intelligence For Multimodal Data Integration In Oncology",
    "chunk_index": 39,
    "total_chunks": 93,
    "text_content": "to explore interpretability within each modality. More- over, shifts in feature importance under unimodal and multi- modal settings can be investigated to analyze the impact ofthe multimodal context. The interpretability methods usually come without any accu- racy measures, and thus it is important not to overinterpretthem. While CAM- or attention-based methods can localize the predictive regions, they cannot specify which features are rele- vant, i.e., they can explain where but not why. Moreov",
    "full_text_length": 92263,
    "chunk_length": 1426
  },
  {
    "chunk_id": 1411,
    "paper_filename": "Jana_2024_artificial_intelligence_for_multimodal_data_integration_in_oncology.pdf",
    "paper_title": "Jana 2024 Artificial Intelligence For Multimodal Data Integration In Oncology",
    "chunk_index": 40,
    "total_chunks": 93,
    "text_content": "Figure 5 ). Morphologic associations Malignant changes often propagate across different scales; onco- genic mutations can affectcell behavior, which in turnreshapes tis- sue morphology or the tumor microenvironment visible in histologyimages. Consequently, the microscopic changes might have an impact on tumor metabolic activity and macroscopic appearance detectable by PET or MRI scans. The feasibility of AI methods toidentify associations across modalities was \ufb01rst demonstrated by Coudray et al.",
    "full_text_length": 92263,
    "chunk_length": 1261
  },
  {
    "chunk_id": 1412,
    "paper_filename": "Jana_2024_artificial_intelligence_for_multimodal_data_integration_in_oncology.pdf",
    "paper_title": "Jana 2024 Artificial Intelligence For Multimodal Data Integration In Oncology",
    "chunk_index": 41,
    "total_chunks": 93,
    "text_content": "et al., 2020 ;Kather et al., 2020 ). Additional molec- ular biomarkers, such as gene expression ( Anand et al., 2020 ; Binder et al., 2021 ;Schmauch et al., 2020 ), hormone-receptor sta- tus (Naik et al., 2020 ), tumor mutational burden ( Jain and Massoud, 2020 ), and microsatellite instability ( Cao et al., 2020 ;Echle et al.,ll OPEN ACCESS Cancer Cell 40, October 10, 2022 1103Review 2020 ), have also been inferred from WSIs ( Murchan et al., 2021 ). In radiology, AI models have predicted IDHmu",
    "full_text_length": 92263,
    "chunk_length": 1092
  },
  {
    "chunk_id": 1413,
    "paper_filename": "Jana_2024_artificial_intelligence_for_multimodal_data_integration_in_oncology.pdf",
    "paper_title": "Jana 2024 Artificial Intelligence For Multimodal Data Integration In Oncology",
    "chunk_index": 42,
    "total_chunks": 93,
    "text_content": "been detected from CT scans in lung (Wang et al., 2019 ) and colorectal ( He et al., 2020 )c a n c e r . By discovering the presence of morphological associations across modalities, AI models can enhance exploratory studiesand reduce the search space for possible biomarker candidates. For instance, in Figure 5 A, AI has revealed that one of the studied mutations can be reliably inferred from WSI. Although the predic-tive features used by the model might be unknown, interpret-ability methods can ",
    "full_text_length": 92263,
    "chunk_length": 1361
  },
  {
    "chunk_id": 1414,
    "paper_filename": "Jana_2024_artificial_intelligence_for_multimodal_data_integration_in_oncology.pdf",
    "paper_title": "Jana 2024 Artificial Intelligence For Multimodal Data Integration In Oncology",
    "chunk_index": 43,
    "total_chunks": 93,
    "text_content": "be- tween mutation status and distinct morphological features. The identi\ufb01ed morphological associates can serve as cost-ef\ufb01cientbiomarker surrogates to support screening in low- to middle-in- come settings or reveal new therapeutic targets. Non-invasive alternatives Similarly, AI can discover relationships between non-invasive and invasive modalities. For instance, AI models were used to predicthistology subtypes or grades from radiomics features in lung (Sha et al., 2019 ), brain ( Lasocki et a",
    "full_text_length": 92263,
    "chunk_length": 1374
  },
  {
    "chunk_id": 1415,
    "paper_filename": "Jana_2024_artificial_intelligence_for_multimodal_data_integration_in_oncology.pdf",
    "paper_title": "Jana 2024 Artificial Intelligence For Multimodal Data Integration In Oncology",
    "chunk_index": 44,
    "total_chunks": 93,
    "text_content": "identify associations across modalities, such as (A) the feasibility of inferring certain mutations from histology or radiology imag es or (B) the relation between non-invasive and invasive modalities, such as prediction of histology subtype from radiomics features. (C) The models can uncover associations between clinical data and patient outcome, contributing to the discovery of predictive features within and a cross modalities.(D) Information acquired by EMRs or wearable devices can be analyze",
    "full_text_length": 92263,
    "chunk_length": 1276
  },
  {
    "chunk_id": 1416,
    "paper_filename": "Jana_2024_artificial_intelligence_for_multimodal_data_integration_in_oncology.pdf",
    "paper_title": "Jana 2024 Artificial Intelligence For Multimodal Data Integration In Oncology",
    "chunk_index": 45,
    "total_chunks": 93,
    "text_content": "and multimodal ( Chen et al., 2020b , 2021c ;Joo et al., 2021 ;Mobadersany et al., 2018 ) data. These works imply the feasibility of AI models to discover relevant prog-nostic patterns in data, which might be elucidated by interpret-ability methods. For instance, in Figure 5 C, a model is trained to predict survival from histology and genomics data. Attention heatmaps reveal tissue regions related to low- and high-risk pa-tient groups, while the molecular pro\ufb01les are analyzed through attribution",
    "full_text_length": 92263,
    "chunk_length": 1327
  },
  {
    "chunk_id": 1417,
    "paper_filename": "Jana_2024_artificial_intelligence_for_multimodal_data_integration_in_oncology.pdf",
    "paper_title": "Jana 2024 Artificial Intelligence For Multimodal Data Integration In Oncology",
    "chunk_index": 46,
    "total_chunks": 93,
    "text_content": "new clinical insights. For instance, Geessink et al. ( Geessink et al., 2019 ) showed that the tumor-to-stroma ratio can serve as an indepen- dent prognosticator in rectal cancer, while the ratio of tumor areato metastatic lymph node regions has prognostic value in gastric cancer ( Wang et al., 2021 ). Other morphological features, such as the arrangement of collagen \ufb01bers in breast histology ( Li et al., 2021 ) or spatial tissue organization in colorectal tissue (Qi et al., 2021 ), have been id",
    "full_text_length": 92263,
    "chunk_length": 1283
  },
  {
    "chunk_id": 1418,
    "paper_filename": "Jana_2024_artificial_intelligence_for_multimodal_data_integration_in_oncology.pdf",
    "paper_title": "Jana 2024 Artificial Intelligence For Multimodal Data Integration In Oncology",
    "chunk_index": 47,
    "total_chunks": 93,
    "text_content": "can be correlated with different patient outcomes, while attribu-tion plots can identify the relevance of different factors at the pa-tient and population level. Recently, Placido et al. ( Placido et al., 2021 ) showed the feasibility of AI to identify patients with a higher risk of developing pancreatic cancer by exploration of EMR.Similarly, EMRs were used to predict treatment response ( Chu et al., 2020 ) or length of hospital stay ( Alsinglawi et al., 2022 ). The identi\ufb01ed novel predictive r",
    "full_text_length": 92263,
    "chunk_length": 1383
  },
  {
    "chunk_id": 1419,
    "paper_filename": "Jana_2024_artificial_intelligence_for_multimodal_data_integration_in_oncology.pdf",
    "paper_title": "Jana 2024 Artificial Intelligence For Multimodal Data Integration In Oncology",
    "chunk_index": 48,
    "total_chunks": 93,
    "text_content": "expanding their functionality, including measurements of temperature, stress levels, or blood-oxygen saturation or electrocardiograms. These mea-surements can be analyzed in tandem with clinical data to search for risk factors indicating early stages of increased toxicity or treatment resistance, to allow personalized interventions duringthe course of treatment. Research on personalized monitoring and nanotechnologies is investigating novel directions, such as the detection of patient measuremen",
    "full_text_length": 92263,
    "chunk_length": 1282
  },
  {
    "chunk_id": 1420,
    "paper_filename": "Jana_2024_artificial_intelligence_for_multimodal_data_integration_in_oncology.pdf",
    "paper_title": "Jana 2024 Artificial Intelligence For Multimodal Data Integration In Oncology",
    "chunk_index": 49,
    "total_chunks": 93,
    "text_content": "such as fairness and dataset shifts ( Banerjee et al., 2021 ;Chen et al., 2021a ;Cirillo et al., 2020 ; Howard et al., 2021 ;Mehrabi et al., 2021 ;Zhang et al., 2018 ), limited interpret- ability ( Adebayo et al., 2018 ;Linardatos et al., 2020 ;Reyes et al., 2020 ), or regulatory guidelines ( Cruz Rivera et al., 2020 ;Topol, 2020 ;Wu et al., 2021 ), here we focus on challenges speci\ufb01c to multimodal learning. Missing data The challenge of missing data refers to the absence of part of a modality o",
    "full_text_length": 92263,
    "chunk_length": 1262
  },
  {
    "chunk_id": 1421,
    "paper_filename": "Jana_2024_artificial_intelligence_for_multimodal_data_integration_in_oncology.pdf",
    "paper_title": "Jana 2024 Artificial Intelligence For Multimodal Data Integration In Oncology",
    "chunk_index": 50,
    "total_chunks": 93,
    "text_content": "cancer genome atlas (TCGA) one of the largest publicly available multimodal datasets has signi\ufb01cant missing data points. The incomplete modalitiesstill contain valuable information, and the inability to deploy them poses a signi\ufb01cant limitation. Below we discuss two strate- gies for handling missing data.Synthetic data generation Given the paucity of medical data in general synthetic data is increasingly being used to train, develop and augment AImodels ( Chen et al., 2021 ). If part of an image",
    "full_text_length": 92263,
    "chunk_length": 1261
  },
  {
    "chunk_id": 1422,
    "paper_filename": "Jana_2024_artificial_intelligence_for_multimodal_data_integration_in_oncology.pdf",
    "paper_title": "Jana 2024 Artificial Intelligence For Multimodal Data Integration In Oncology",
    "chunk_index": 51,
    "total_chunks": 93,
    "text_content": "perfectly aligned data obtained through re-staining of the same slides. If paired data are not available, unsupervised methods such as cycle gener-ative adversarial networks (GANs) ( Zhu et al., 2017 )c a nb e used. While synthetic data can improve the performance of detection and classi\ufb01cation methods, they are less suitablefor outcome prediction or biom arker exploration, where the predictive features are not well understood and thus there is no guarantee that the synthetic data contain the re",
    "full_text_length": 92263,
    "chunk_length": 1318
  },
  {
    "chunk_id": 1423,
    "paper_filename": "Jana_2024_artificial_intelligence_for_multimodal_data_integration_in_oncology.pdf",
    "paper_title": "Jana 2024 Artificial Intelligence For Multimodal Data Integration In Oncology",
    "chunk_index": 52,
    "total_chunks": 93,
    "text_content": "incomplete ormissing data during training and deployment. The EmbraceNet model probabilistically selects partial information from each mo- dality and combines it into a single representation vector, whichthen serves as an input for the \ufb01nal decision model. When missingor invalid data are encountered, they are not sampled; instead, other more complete modalities are used to compensate for the missing data. The probabilistic data selection also has a regulari-zation effect, similar to the dropout ",
    "full_text_length": 92263,
    "chunk_length": 1385
  },
  {
    "chunk_id": 1424,
    "paper_filename": "Jana_2024_artificial_intelligence_for_multimodal_data_integration_in_oncology.pdf",
    "paper_title": "Jana 2024 Artificial Intelligence For Multimodal Data Integration In Oncology",
    "chunk_index": 53,
    "total_chunks": 93,
    "text_content": "of MRI and PET brain scans is usually achieved with high accuracy, even with simple af\ufb01ne registration, thanks to the rigid skull. The situation is more com-plex in the presence of motion and deformations, e.g., breathing in lung imaging or changes in the body posture between scan- ning sessions. Alignment of such data usually requires deform-able registrations using natural or manually placed landmarksfor guidance. A particularly challenging situation is the registra- tion of scans between inte",
    "full_text_length": 92263,
    "chunk_length": 1314
  },
  {
    "chunk_id": 1425,
    "paper_filename": "Jana_2024_artificial_intelligence_for_multimodal_data_integration_in_oncology.pdf",
    "paper_title": "Jana 2024 Artificial Intelligence For Multimodal Data Integration In Oncology",
    "chunk_index": 54,
    "total_chunks": 93,
    "text_content": "), which all complicate data alignment. Robust and automated registration of histology im- ages can be challenging ( Borovec et al., 2020 ), and thus many studies deploy non-algorithmic strategies such as clearing andre-staining of the tissue slides ( Hinton et al., 2019 ). A newly emerging direction is stainless imaging, including approaches such as ultraviolet microscopy ( Fereidouni et al., 2017 ), stimu- lated Raman histology ( Hollon et al., 2020 ), or colorimetric imag- ing (Balaur et al.,",
    "full_text_length": 92263,
    "chunk_length": 1365
  },
  {
    "chunk_id": 1426,
    "paper_filename": "Jana_2024_artificial_intelligence_for_multimodal_data_integration_in_oncology.pdf",
    "paper_title": "Jana 2024 Artificial Intelligence For Multimodal Data Integration In Oncology",
    "chunk_index": 55,
    "total_chunks": 93,
    "text_content": "and translation between arbitrary modalities. Cross-modal autoencoders ( Dai Yang et al., 2021 ) build a pair of encoder-decoder networks for each modality, where the encoder maps each modality into a lower-dimensional latent space, while the decoder maps it back into the original space. A discriminative objective function is used to match the different modalities inthe common latent space. With the shared latent space in place, one can combine an encoder of one modality with the decoder of anot",
    "full_text_length": 92263,
    "chunk_length": 1315
  },
  {
    "chunk_id": 1427,
    "paper_filename": "Jana_2024_artificial_intelligence_for_multimodal_data_integration_in_oncology.pdf",
    "paper_title": "Jana 2024 Artificial Intelligence For Multimodal Data Integration In Oncology",
    "chunk_index": 56,
    "total_chunks": 93,
    "text_content": "representation learning-based modern AImethods and the fact that they use abstract feature representa- tions, it is possible that their mechanisms will not be fully under- stood in the near future. However, one may argue that many as-pects in medicine are not fully understood, either ( Kirkpatrick, 2005 ). Some of the interpretability methods discussed earlier are capable of indicating regions within data used to make predic-tion determination yet the actual feature representation remainsabstrac",
    "full_text_length": 92263,
    "chunk_length": 1312
  },
  {
    "chunk_id": 1428,
    "paper_filename": "Jana_2024_artificial_intelligence_for_multimodal_data_integration_in_oncology.pdf",
    "paper_title": "Jana 2024 Artificial Intelligence For Multimodal Data Integration In Oncology",
    "chunk_index": 57,
    "total_chunks": 93,
    "text_content": "models can enhance, rather than disturb, the clinical work\ufb02ow. In the case of biomarker surrogatesdiscovered by AI methods, regulation paths similar to \u2018\u2018me-too\u2019\u2019drugs and devices ( Aronson and Green, 2020 ) could be used to ensure comparable levels of performance. Transparency about study design and the data used are necessary to determine the in-tended use and conditions under which the model performance has been veri\ufb01ed and evaluated ( Haibe-Kains et al., 2020 ). Pro- spective clinical trials",
    "full_text_length": 92263,
    "chunk_length": 1364
  },
  {
    "chunk_id": 1429,
    "paper_filename": "Jana_2024_artificial_intelligence_for_multimodal_data_integration_in_oncology.pdf",
    "paper_title": "Jana 2024 Artificial Intelligence For Multimodal Data Integration In Oncology",
    "chunk_index": 58,
    "total_chunks": 93,
    "text_content": "reveal associa- tions across modalities to help identify diagnostic or prognostic biomarkers from easily accessible data to improve patient risk strati\ufb01cation or selection for clinical trials. In a similar way, themodels can identify non-invasive alternatives to existing bio-markers to minimize invasive procedures. Prognostic models can predict risk factors or adverse treatment outcomes prior to in- terventions to guide patient management. Information acquiredfrom personal wearable devices or na",
    "full_text_length": 92263,
    "chunk_length": 1354
  },
  {
    "chunk_id": 1430,
    "paper_filename": "Jana_2024_artificial_intelligence_for_multimodal_data_integration_in_oncology.pdf",
    "paper_title": "Jana 2024 Artificial Intelligence For Multimodal Data Integration In Oncology",
    "chunk_index": 59,
    "total_chunks": 93,
    "text_content": "that will drive further progress. AI models come with limitations and chal- lenges; however, these should not intimidate but rather inspireus. With increasing incidence rates of cancer, it is our obligationto capitalize on bene\ufb01ts offered by AI methods to accelerate dis- covery and translation of advances into clinical practice to serve patients and health care providers. ACKNOWLEDGMENTS This work was supported in part by the BWH President\u2019s Fund, National Insti- tute of General Medical Sciences",
    "full_text_length": 92263,
    "chunk_length": 1323
  },
  {
    "chunk_id": 1431,
    "paper_filename": "Jana_2024_artificial_intelligence_for_multimodal_data_integration_in_oncology.pdf",
    "paper_title": "Jana 2024 Artificial Intelligence For Multimodal Data Integration In Oncology",
    "chunk_index": 60,
    "total_chunks": 93,
    "text_content": "not re\ufb02ect the of\ufb01cial views of the NIH, NSF, NCI or NIGMS. DECLARATION OF INTERESTS F.M. and R.J.C. are inventors on a patent related to multimodal learning. REFERENCES Adebayo, J., Gilmer, J., Muelly, M., Goodfellow, I., Hardt, M., and Kim, B. (2018). Sanity checks for saliency maps. In Advances in Neural Information Processing Systems, 31, S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, eds. (Curran Associates, Inc) . Ahmedt-Aristizabal, D., Armin, M.A., Den",
    "full_text_length": 92263,
    "chunk_length": 1319
  },
  {
    "chunk_id": 1432,
    "paper_filename": "Jana_2024_artificial_intelligence_for_multimodal_data_integration_in_oncology.pdf",
    "paper_title": "Jana 2024 Artificial Intelligence For Multimodal Data Integration In Oncology",
    "chunk_index": 61,
    "total_chunks": 93,
    "text_content": "and Sethi, A. (2020). Deep learning to estimate human epidermal growth factor receptor 2 status from hematoxylin and eosin-stained breast tissue images. J. Pathol. Inform. 11,1 9. Aronson, J.K., and Green, A.R. (2020). Me-too pharmaceutical products: his- tory, de\ufb01nitions, examples, and relevance to drug shortages and essential medicines lists. Br. J. Clin. Pharmacol. 86, 2114\u20132122 . Arrieta, A.B., D \u0131az-Rod /C19r\u0131guez, N., Del Ser, J., Bennetot, A., Tabik, S., Barbado, A., Gar /C19c\u0131a, S., Gil-",
    "full_text_length": 92263,
    "chunk_length": 1350
  },
  {
    "chunk_id": 1433,
    "paper_filename": "Jana_2024_artificial_intelligence_for_multimodal_data_integration_in_oncology.pdf",
    "paper_title": "Jana 2024 Artificial Intelligence For Multimodal Data Integration In Oncology",
    "chunk_index": 62,
    "total_chunks": 93,
    "text_content": "machine learning: a survey and taxonomy. IEEE Trans. Pattern Anal. Mach. Intell. 41, 423\u2013443 . Banerjee, I., Bhimireddy, A.R., Burns, J.L., Celi, L.A., Chen, L.-C., Correa, R., Dullerud, N., Ghassemi, M., Huang, S.-C., Kuo, P.-C., et al. (2021). Reading race: ai recognises patient\u2019s racial identity in medical images. Preprint at arXiv, 2107.10356 . Bangalore Yogananda, C.G., Shah, B.R., Vejdani-Jahromi, M., Nalawade, S.S., Murugesan, G.K., Yu, F.F., Bangalore Yogananda, C.G., Shah, B.R., Vej- da",
    "full_text_length": 92263,
    "chunk_length": 1343
  },
  {
    "chunk_id": 1434,
    "paper_filename": "Jana_2024_artificial_intelligence_for_multimodal_data_integration_in_oncology.pdf",
    "paper_title": "Jana 2024 Artificial Intelligence For Multimodal Data Integration In Oncology",
    "chunk_index": 63,
    "total_chunks": 93,
    "text_content": ", 885\u2013894 . Binder, A., Bockmayr, M., H \u20acagele, M., Wienert, S., Heim, D., Hellweg, K., Ishii, M., Stenzinger, A., Hocke, A., Denkert, C., et al. (2021). Morphological and mo- lecular breast cancer pro\ufb01ling through explainable machine learning. Nat. Mach. Intell. 3, 355\u2013366 . Bl\u20acuthgen, C., Patella, M., Euler, A., Baessler, B., Martini, K., von Spiczak, J., Schneiter, D., Opitz, I., and Frauen- felder, T. (2021). Computed tomography radiomics for the prediction of thymic epithelial tumor histolo",
    "full_text_length": 92263,
    "chunk_length": 1362
  },
  {
    "chunk_id": 1435,
    "paper_filename": "Jana_2024_artificial_intelligence_for_multimodal_data_integration_in_oncology.pdf",
    "paper_title": "Jana 2024 Artificial Intelligence For Multimodal Data Integration In Oncology",
    "chunk_index": 64,
    "total_chunks": 93,
    "text_content": "Brancato, V., Garbino, N., Salvatore, M., and Cavaliere, C. (2022). Mri-based radiomic features help identify lesions and predict histopathological grade of hepatocellular carcinoma. Diagnostics 12, 1085 . Campanella, G., Hanna, M.G., Geneslaw, L., Mira\ufb02or, A., Werneck Krauss Silva, V., Busam, K.J., Brogi, E., Reuter, V.E., Klimstra, D.S., and Fuchs, T.J. (2019). Clinical-grade computational pathology using weakly supervised deep learning on whole slide images. Nat Med 25, 1301\u20131309 . Cao, R., Y",
    "full_text_length": 92263,
    "chunk_length": 1374
  },
  {
    "chunk_id": 1436,
    "paper_filename": "Jana_2024_artificial_intelligence_for_multimodal_data_integration_in_oncology.pdf",
    "paper_title": "Jana 2024 Artificial Intelligence For Multimodal Data Integration In Oncology",
    "chunk_index": 65,
    "total_chunks": 93,
    "text_content": "gradient- based visual explanations for deep convolutional networks. In In 2018 IEEE Winter Conference on Applica- tions of Computer Vision (WACV) (IEEE), pp. 839\u2013847 . Chen, M., Zhang, B., Topatana, W., Cao, J., Zhu, H., Juengpanich, S., Mao, Q., Yu, H., and Cai, X. (2020a). Classi\ufb01cation and mutation prediction based on histopathology h&e images in liver cancer using deep learning. NPJ Precis. Oncol. 4, 14\u201317 . Chen, R.J., Lu, M.Y., Chen, T.Y., Williamson, D.F., and Mahmood, F. (2021). Synthet",
    "full_text_length": 92263,
    "chunk_length": 1327
  },
  {
    "chunk_id": 1437,
    "paper_filename": "Jana_2024_artificial_intelligence_for_multimodal_data_integration_in_oncology.pdf",
    "paper_title": "Jana 2024 Artificial Intelligence For Multimodal Data Integration In Oncology",
    "chunk_index": 66,
    "total_chunks": 93,
    "text_content": "fairness in ai for medicine and healthcare. Preprint at arXiv, 2110.00603 . Chen, R.J., Lu, M.Y., Weng, W.-H., Chen, T.Y., Williamson, D.F., Manz, T., Shady, M., and Mahmood, F. (2021b). Multimodal co-attention transformer for survival prediction in gigapixel whole slide images. In In Proceedings ofthe IEEE/CVF International Conference on Computer Vision, pp. 4015\u20134025 . Chen, R.J., Lu, M.Y., Williamson, D.F., Chen, T.Y., Lipkova, J., Shaban, M., Shady, M., Williams, M., Joo, B., Noor, Z., et al",
    "full_text_length": 92263,
    "chunk_length": 1352
  },
  {
    "chunk_id": 1438,
    "paper_filename": "Jana_2024_artificial_intelligence_for_multimodal_data_integration_in_oncology.pdf",
    "paper_title": "Jana 2024 Artificial Intelligence For Multimodal Data Integration In Oncology",
    "chunk_index": 67,
    "total_chunks": 93,
    "text_content": "Chu, J., Dong, W., Wang, J., He, K., and Huang, Z. (2020). Treatment effect prediction with adversarial deep learning using electronic health records. BMC Med. Inform. Decis. Mak. 20, 139\u2013214 . Cirillo, D., Catuara-Solarz, S., Morey, C., Guney, E., Subirats, L., Mellino, S., Gigante, A., Valencia, A., Re- menteria, M.J., Chadha, A.S., and Mavridis, N. (2020). Sex and gender differences and biases in arti\ufb01cial intelligence forbiomedicine and healthcare. NPJ Digit. Med. 3, 1\u201311 . Cohen, J.P., Luck",
    "full_text_length": 92263,
    "chunk_length": 1409
  },
  {
    "chunk_id": 1439,
    "paper_filename": "Jana_2024_artificial_intelligence_for_multimodal_data_integration_in_oncology.pdf",
    "paper_title": "Jana 2024 Artificial Intelligence For Multimodal Data Integration In Oncology",
    "chunk_index": 68,
    "total_chunks": 93,
    "text_content": "S., Liu, X., Chan, A.-W., Denniston, A.K., and Calvert, M.J.; SPIRIT-AI and CONSORT-AI Working Group; SPIRIT-AI and CONSORT-AI Steering Group; SPIRIT-AI and CONSORT-AI Consensus Group (2020).Guidelines for clinical trial protocols for interventions involving arti\ufb01cial intelli- gence: the spirit-ai extension. Nat. Med. 26, 1351\u20131363 . Dai Yang, K., Belyaeva, A., Venkatachalapathy, S., Damodaran, K., Katcoff, A., Radhakrishnan, A., Shiv- ashankar, G., and Uhler, C. (2021). Multi-domain translation",
    "full_text_length": 92263,
    "chunk_length": 1375
  },
  {
    "chunk_id": 1440,
    "paper_filename": "Jana_2024_artificial_intelligence_for_multimodal_data_integration_in_oncology.pdf",
    "paper_title": "Jana 2024 Artificial Intelligence For Multimodal Data Integration In Oncology",
    "chunk_index": 69,
    "total_chunks": 93,
    "text_content": "transformers for image recognition at scale. Preprint at arXiv, 2010.11929 . Echle, A., Grabsch, H.I., Quirke, P., van den Brandt, P.A., West, N.P., Hutchins, G.G.A., Heij, L.R., Tan, X., Richman, S.D., Krause, J., et al. (2020). Clinical- grade detection of microsatellite instability in colorectal tumors by deep learning. Gastroenterology 159, 1406\u20131416.e11 . Epstein, J.I., Zelefsky, M.J., Sjoberg, D.D., Nelson, J.B., Egevad, L., Magi-Gal- luzzi, C., Vickers, A.J., Parwani, A.V., Reuter, V.E., ",
    "full_text_length": 92263,
    "chunk_length": 1353
  },
  {
    "chunk_id": 1441,
    "paper_filename": "Jana_2024_artificial_intelligence_for_multimodal_data_integration_in_oncology.pdf",
    "paper_title": "Jana 2024 Artificial Intelligence For Multimodal Data Integration In Oncology",
    "chunk_index": 70,
    "total_chunks": 93,
    "text_content": "Fereidouni, F., Harmany, Z.T., Tian, M., Todd, A., Kintner, J.A., McPherson, J.D., Borowsky, A.D., Bishop, J., Lechpammer, M., Demos, S.G., and Leven- son, R. (2017). Microscopy with ultraviolet surface excitation for rapid slide- free histology. Nat. Biomed. Eng. 1, 957\u2013966 . Ferreira-Junior, J.R., Koenigkam-Santos, M., Magalha \u02dces Teno \u00b4rio, A.P., Fa- leiros, M.C., Garcia Cipriano, F.E., Fabro, A.T., N \u20acappi, J., Yoshida, H., and de Azevedo-Marques, P.M. (2020). Ct-based radiomics for predicti",
    "full_text_length": 92263,
    "chunk_length": 1399
  },
  {
    "chunk_id": 1442,
    "paper_filename": "Jana_2024_artificial_intelligence_for_multimodal_data_integration_in_oncology.pdf",
    "paper_title": "Jana 2024 Artificial Intelligence For Multimodal Data Integration In Oncology",
    "chunk_index": 71,
    "total_chunks": 93,
    "text_content": "F., and van der Laak, J.A.W.M. (2019). Computer aided quanti\ufb01cation of intratumoral stroma yields an independent prognosticator in rectal cancer. Cell. Oncol. 42, 331\u2013341 .Ghassemi, M., Oakden-Rayner, L., and Beam, A.L. (2021). The false hope of current approaches to explainable arti\ufb01cial intelligence in health care. Lancet Digit Health 3, e745\u2013e750 . Ha, S.M., Chae, E.Y., Cha, J.H., Kim, H.H., Shin, H.J., and Choi, W.J. (2017). Association of brca mutation types, imaging features, and pathologi",
    "full_text_length": 92263,
    "chunk_length": 1331
  },
  {
    "chunk_id": 1443,
    "paper_filename": "Jana_2024_artificial_intelligence_for_multimodal_data_integration_in_oncology.pdf",
    "paper_title": "Jana 2024 Artificial Intelligence For Multimodal Data Integration In Oncology",
    "chunk_index": 72,
    "total_chunks": 93,
    "text_content": "Guizard, N., Chapados, N., and Hemis, Y.B. (2016). Hetero-modal image segmentation. In In Inter- national Conference on Medical Image Computing and Computer-Assisted Intervention (Springer), pp. 469\u2013477 . He, K., Liu, X., Li, M., Li, X., Yang, H., and Zhang, H. (2020). Noninvasive kras mutation estimation in colorectal cancer using a deep learning method basedon ct imaging. BMC Med. Imaging 20, 1\u20139 . Hinton, J.P., Dvorak, K., Roberts, E., French, W.J., Grubbs, J.C., Cress, A.E., Tiwari, H.A., an",
    "full_text_length": 92263,
    "chunk_length": 1305
  },
  {
    "chunk_id": 1444,
    "paper_filename": "Jana_2024_artificial_intelligence_for_multimodal_data_integration_in_oncology.pdf",
    "paper_title": "Jana 2024 Artificial Intelligence For Multimodal Data Integration In Oncology",
    "chunk_index": 73,
    "total_chunks": 93,
    "text_content": "and Aerts, H.J. (2018). Arti\ufb01cial intelligence in radiology. Nat Rev Cancer 18, 500\u2013510 . Howard, F.M., Dolezal, J., Kochanny, S., Schulte, J., Chen, H., Heij, L., Huo, D., Nanda, R., Olopade, O.I., Kather, J.N., et al. (2021). The impact of site-speci\ufb01c digital histology signatures on deep learning model accuracy and bias. Nat. Commun. 12, 4423\u20134513 . Huang, S.-C., Pareek, A., Seyyedi, S., Banerjee, I., and Lungren, M.P. (2020). Fusion of medical imaging and elec- tronic health records using de",
    "full_text_length": 92263,
    "chunk_length": 1332
  },
  {
    "chunk_id": 1445,
    "paper_filename": "Jana_2024_artificial_intelligence_for_multimodal_data_integration_in_oncology.pdf",
    "paper_title": "Jana 2024 Artificial Intelligence For Multimodal Data Integration In Oncology",
    "chunk_index": 74,
    "total_chunks": 93,
    "text_content": "and Ghosh, P. (2021). Multimodal Classi\ufb01cation: Cur- rent Landscape, Taxonomy and Future Directions (ACM Computing Sur-veys (CSUR)) . Jain, M.S., and Massoud, T.F. (2020). Predicting tumour mutational burden from histopathological images using multiscale deep learning. Nat. Mach. In- tell.2, 356\u2013362 . Jang, H.-J., Lee, A., Kang, J., Song, I.H., and Lee, S.H. (2020). Prediction of clinically actionable genetic alterations from colorectal cancer histopathology images using deep learning. World J. ",
    "full_text_length": 92263,
    "chunk_length": 1376
  },
  {
    "chunk_id": 1446,
    "paper_filename": "Jana_2024_artificial_intelligence_for_multimodal_data_integration_in_oncology.pdf",
    "paper_title": "Jana 2024 Artificial Intelligence For Multimodal Data Integration In Oncology",
    "chunk_index": 75,
    "total_chunks": 93,
    "text_content": "In In Proceedings of the IEEE/CVF Con- ference on Computer Vision and Pattern Recognition, pp. 13289\u201313299 . Karpathy, A., Toderici, G., Shetty, S., Leung, T., Sukthankar, R., and Fei-Fei, L. (2014). Large-scale video classi\ufb01cation with convolutional neural networks. Inll OPEN ACCESS 1108 Cancer Cell 40, October 10, 2022Review In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition, pp. 1725\u20131732 . Kather, J.N., Heij, L.R., Grabsch, H.I., Loef\ufb02er, C., Echle, A., Muti, H.",
    "full_text_length": 92263,
    "chunk_length": 1310
  },
  {
    "chunk_id": 1447,
    "paper_filename": "Jana_2024_artificial_intelligence_for_multimodal_data_integration_in_oncology.pdf",
    "paper_title": "Jana 2024 Artificial Intelligence For Multimodal Data Integration In Oncology",
    "chunk_index": 76,
    "total_chunks": 93,
    "text_content": "of prostate cancer using pathology\u2013 radi-ology fusion. J. Magn. Reson. Imaging 54, 462\u2013471 . Kirkpatrick, P. (2005). New clues in the acetaminophen mystery. Nat. Rev. Drug Discov. 4, 883 . Kumar, A., Fulham, M., Feng, D., and Kim, J. (2020). Co-learning feature fusion maps from pet-ct images of lung cancer. IEEE Trans. Med. Imaging 39, 204\u2013217 . Lai, Y.-H., Chen, W.-N., Hsu, T.-C., Lin, C., Tsao, Y., and Wu, S. (2020). Overall survival prediction of non-small cell lung cancer by integrating micr",
    "full_text_length": 92263,
    "chunk_length": 1252
  },
  {
    "chunk_id": 1448,
    "paper_filename": "Jana_2024_artificial_intelligence_for_multimodal_data_integration_in_oncology.pdf",
    "paper_title": "Jana 2024 Artificial Intelligence For Multimodal Data Integration In Oncology",
    "chunk_index": 77,
    "total_chunks": 93,
    "text_content": "multi-parametric mri based on multimodal convolutional neural networks. Phys. Med. Biol. 62, 6497\u20136514 . Lei, B., Huang, S., Li, H., Li, R., Bian, C., Chou, Y.-H., Qin, J., Zhou, P., Gong, X., and Cheng, J.-Z. (2020). Self-co- attention neural network for anatomy seg- mentation in whole breast ultrasound. Med. Image Anal. 64, 101753 . Li, H., Bera, K., Toro, P., Fu, P., Zhang, Z., Lu, C., Feldman, M., Ganesan, S., Goldstein, L.J., Davidson, N.E., et al. (2021). Collagen \ufb01ber orientation disorder",
    "full_text_length": 92263,
    "chunk_length": 1283
  },
  {
    "chunk_id": 1449,
    "paper_filename": "Jana_2024_artificial_intelligence_for_multimodal_data_integration_in_oncology.pdf",
    "paper_title": "Jana 2024 Artificial Intelligence For Multimodal Data Integration In Oncology",
    "chunk_index": 78,
    "total_chunks": 93,
    "text_content": "cancer data with a multimodal deep learning approach. IEEE/ ACM Trans. Comput. Biol. Bioinform. 12, 928\u2013937 . Linardatos, P., Papastefanopoulos, V., and Kotsiantis, S. (2020). A review of machine learning inter- pretability methods. Entropy 23,1 8. Lipkova \u00b4, J., Angelikopoulos, P., Wu, S., Alberts, E., Wiestler, B., Diehl, C., Pre- ibisch, C., Pyka, T., Combs, S.E., Hadjidoukas, P., et al. (2019). Personalized radiotherapy design for glioblastoma: integrating mathematical tumor models, multimod",
    "full_text_length": 92263,
    "chunk_length": 1405
  },
  {
    "chunk_id": 1450,
    "paper_filename": "Jana_2024_artificial_intelligence_for_multimodal_data_integration_in_oncology.pdf",
    "paper_title": "Jana 2024 Artificial Intelligence For Multimodal Data Integration In Oncology",
    "chunk_index": 79,
    "total_chunks": 93,
    "text_content": "D.W. (2016). The 2016 world health organization classi\ufb01cation of tumors of the central nervous system: a summary. Acta Neuropathol. 131, 803\u2013820 . Low, C.A. (2020). Harnessing consumer smartphone and wearable sensors for clinical cancer research. NPJ Digit. Med. 3, 140\u2013147 . Lu, M.Y., Chen, T.Y., Williamson, D.F.K., Zhao, M., Shady, M., Lipkova, J., and Mahmood, F. (2021). Ai-based pathology predicts origins for cancers of un- known primary. Nature 594, 106\u2013110 . Lu, M.Y., Williamson, D.F., Chen",
    "full_text_length": 92263,
    "chunk_length": 1288
  },
  {
    "chunk_id": 1451,
    "paper_filename": "Jana_2024_artificial_intelligence_for_multimodal_data_integration_in_oncology.pdf",
    "paper_title": "Jana 2024 Artificial Intelligence For Multimodal Data Integration In Oncology",
    "chunk_index": 80,
    "total_chunks": 93,
    "text_content": "Surv. 54, 1\u201335 . Men, K., Geng, H., Zhong, H., Fan, Y., Lin, A., and Xiao, Y. (2019). A deep learning model for predicting xerostomia due to radiation therapy for head and neck squamous cell carcinoma in the rtog 0522 clinical trial. Int. J. Radiat. Oncol. Biol. Phys. 105, 440\u2013447 . Miller, G. (2002). Breaking Down Barriers . Mo, S., Cai, M., Lin, L., Tong, R., Chen, Q., Wang, F., Hu, H., Iwamoto, Y., Han, X.-H., and Chen, Y.-W. (2020). Multi- modal priors guided segmentation of liverlesions in ",
    "full_text_length": 92263,
    "chunk_length": 1279
  },
  {
    "chunk_id": 1452,
    "paper_filename": "Jana_2024_artificial_intelligence_for_multimodal_data_integration_in_oncology.pdf",
    "paper_title": "Jana 2024 Artificial Intelligence For Multimodal Data Integration In Oncology",
    "chunk_index": 81,
    "total_chunks": 93,
    "text_content": "Murchan, P., O \u00b4\u2019Brien, C., O\u2019Connell, S., McNevin, C.S., Baird, A.-M., Sheils, O., O\u00b4Broin, P., and Finn, S.P. (2021). Deep learning of histopathological fea- tures for the prediction of tumour molecular genetics. Diagnostics 11, 1406 . Naik, N., Madani, A., Esteva, A., Keskar, N.S., Press, M.F., Ruderman, D., Agus, D.B., and Socher, R. (2020). Deep learning-enabled breast cancer hor- monal receptor status determination from base-level h&e stains. Nat. Com-mun. 11, 5727\u20135728 . Nie, D., Zhang, H",
    "full_text_length": 92263,
    "chunk_length": 1288
  },
  {
    "chunk_id": 1453,
    "paper_filename": "Jana_2024_artificial_intelligence_for_multimodal_data_integration_in_oncology.pdf",
    "paper_title": "Jana 2024 Artificial Intelligence For Multimodal Data Integration In Oncology",
    "chunk_index": 82,
    "total_chunks": 93,
    "text_content": "multi-modal neuroimages. Sci. Rep.9, 1103\u20131114 . Paik, S., Shak, S., Tang, G., Kim, C., Baker, J., Cronin, M., Baehner, F.L., Walker, M.G., Watson, D., Park, T., et al. (2004). A multigene assay to predict recurrence of tamoxifen-treated, node-negative breast cancer. N. Engl. J. Med. 351, 2817\u20132826 . Placido, D., Yuan, B., Hjaltelin, J.X., Haue, A.D., Yuan, C., Kim, J., Umeton, R., Antell, G., Chowdhury, A., Franz, A., et al. (2021). Pancreatic cancer risk pre- dicted from disease trajectories u",
    "full_text_length": 92263,
    "chunk_length": 1273
  },
  {
    "chunk_id": 1454,
    "paper_filename": "Jana_2024_artificial_intelligence_for_multimodal_data_integration_in_oncology.pdf",
    "paper_title": "Jana 2024 Artificial Intelligence For Multimodal Data Integration In Oncology",
    "chunk_index": 83,
    "total_chunks": 93,
    "text_content": "breast cancer risk from multimodal multiview ultrasound images via clinically applicable deep learning. Nat. Biomed. Eng. 5, 522\u2013532 . Rakha, E.A., El-Sayed, M.E., Lee, A.H., Elston, C.W., Grainge, M.J., Hodi, Z., Blamey, R.W., and Ellis, I.O. (2008). Prognostic signi\ufb01cance of Nottingham his- tologic grade in invasive breast carcinoma. J Clin Oncol 26, 3153\u20133158 . Ramachandram, D., and Taylor, G.W. (2017). Deep multimodal learning: a sur- vey on recent advances and trends. IEEE Signal Process. M",
    "full_text_length": 92263,
    "chunk_length": 1314
  },
  {
    "chunk_id": 1455,
    "paper_filename": "Jana_2024_artificial_intelligence_for_multimodal_data_integration_in_oncology.pdf",
    "paper_title": "Jana 2024 Artificial Intelligence For Multimodal Data Integration In Oncology",
    "chunk_index": 84,
    "total_chunks": 93,
    "text_content": "Silva, C.A., Dahlweid, F.-M., von Tengg-Ko- bligk, H., Summers, R.M., and Wiest, R. (2020). On the interpretability of arti\ufb01- cial intelligence in radiology: challenges and opportunities. Radiol. Artif. Intell. 2, e190043 .ll OPEN ACCESS Cancer Cell 40, October 10, 2022 1109Review Rokach, L., and Maimon, O. (2005). Clustering methods. In In Data mining and knowledge discovery handbook (Springer), pp. 321\u2013352 . Roy, S., Lahiri, D., Maji, T., and Biswas, J. (2015). Recurrent glioblastoma: where we",
    "full_text_length": 92263,
    "chunk_length": 1297
  },
  {
    "chunk_id": 1456,
    "paper_filename": "Jana_2024_artificial_intelligence_for_multimodal_data_integration_in_oncology.pdf",
    "paper_title": "Jana 2024 Artificial Intelligence For Multimodal Data Integration In Oncology",
    "chunk_index": 85,
    "total_chunks": 93,
    "text_content": "of tumours from whole slide im- ages. Nat. Commun. 11, 3877\u20133915 . Sedghi, A., Mehrtash, A., Jamzad, A., Amalou, A., Wells, W.M., Kapur, T., Kwak, J.T., Turkbey, B., Choyke, P., Pinto, P., et al. (2020). Improving detec- tion of prostate cancer foci via information fusion of mri and temporal enhanced ultrasound. Int. J. Comput. Assist. Radiol. Surg. 15, 1215\u20131223 . Selvaraju, R.R., Das, A., Vedantam, R., Cogswell, M., Parikh, D., and Grad- cam, D.B. (2016). Why did you say that?. Preprint at arX",
    "full_text_length": 92263,
    "chunk_length": 1261
  },
  {
    "chunk_id": 1457,
    "paper_filename": "Jana_2024_artificial_intelligence_for_multimodal_data_integration_in_oncology.pdf",
    "paper_title": "Jana 2024 Artificial Intelligence For Multimodal Data Integration In Oncology",
    "chunk_index": 86,
    "total_chunks": 93,
    "text_content": "of 18f-\ufb02uorodeoxyglucose positron emission computed tomography. Transl. Cancer Res. 8, 1741\u20131749 . Shamshad, F., Khan, S., Zamir, S.W., Khan, M.H., Hayat, M., Khan, F.S., and Fu, H. (2022). Transformers in medical imaging: a survey. Preprint at arXiv, 2201.09873 . Shao, W., Han, Z., Cheng, J., Cheng, L., Wang, T., Sun, L., Lu, Z., Zhang, J., Zhang, D., and Huang, K. (2019). Integrative analysis of pathological images and multi-dimensional genomic data for early-stage cancer prognosis. IEEE Trans",
    "full_text_length": 92263,
    "chunk_length": 1310
  },
  {
    "chunk_id": 1458,
    "paper_filename": "Jana_2024_artificial_intelligence_for_multimodal_data_integration_in_oncology.pdf",
    "paper_title": "Jana 2024 Artificial Intelligence For Multimodal Data Integration In Oncology",
    "chunk_index": 87,
    "total_chunks": 93,
    "text_content": "on machine learning (PMLR), pp. 3319\u20133328 . Taqi, S.A., Sami, S.A., Sami, L.B., and Zaki, S.A. (2018). A review of artifacts in histopathology. J. Oral Maxillofac. Pathol. 22, 279 . Topol, E.J. (2020). Welcoming new guidelines for ai clinical research. Nat. Med. 26, 1318\u20131320 . Tsou, P., and Wu, C.-J. (2019). Mapping driver mutations to histopathological subtypes in papillary thyroid carcinoma: applying a deep convolutional neural network. J. Clin. Med. 8, 1675 . Vale-Silva, L.A., and Rohr, K. (",
    "full_text_length": 92263,
    "chunk_length": 1250
  },
  {
    "chunk_id": 1459,
    "paper_filename": "Jana_2024_artificial_intelligence_for_multimodal_data_integration_in_oncology.pdf",
    "paper_title": "Jana 2024 Artificial Intelligence For Multimodal Data Integration In Oncology",
    "chunk_index": 88,
    "total_chunks": 93,
    "text_content": "Deep learning in histopa- thology: the path to the clinic. Nature medicine 27, 775\u2013784 . Vasileiou, G., Costa, M.J., Long, C., Wetzler, I.R., Hoyer, J., Kraus, C., Popp, B., Emons, J., Wunderle, M., Wenkel, E., et al. (2020). Breast mri texture anal- ysis for prediction of brca-associated genetic risk. BMC Med. Imaging 20,8 6. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, q., and Polosukhin, I. (2017). Atten- tion is all you need. Adv. Neural Inf. Process. ",
    "full_text_length": 92263,
    "chunk_length": 1208
  },
  {
    "chunk_id": 1460,
    "paper_filename": "Jana_2024_artificial_intelligence_for_multimodal_data_integration_in_oncology.pdf",
    "paper_title": "Jana 2024 Artificial Intelligence For Multimodal Data Integration In Oncology",
    "chunk_index": 89,
    "total_chunks": 93,
    "text_content": "adenocarci- noma on computed tomography image using deep learning. Eur. Respir. J. 53, 1800986 . Wang, X., Chen, Y., Gao, Y., Zhang, H., Guan, Z., Dong, Z., Zheng, Y., Jiang, J., Yang, H., Wang, L., et al. (2021). Predict- ing gastric cancer outcome from re- sected lymph node histopathology images using deep learning. Nat. Commun. 12, 1637\u20131713 . Weeks, W.A., Dua, A., Hutchison, J., Joshi, R., Li, R., Szejer, J., and Azevedo, R.G. (2018). A low-power, low-cost in- gestible and wearable sensing p",
    "full_text_length": 92263,
    "chunk_length": 1272
  },
  {
    "chunk_id": 1461,
    "paper_filename": "Jana_2024_artificial_intelligence_for_multimodal_data_integration_in_oncology.pdf",
    "paper_title": "Jana 2024 Artificial Intelligence For Multimodal Data Integration In Oncology",
    "chunk_index": 90,
    "total_chunks": 93,
    "text_content": "J.A. (2019). Skin Sensors Are the Future of Health Care . Xu, T., Zhang, H., Huang, X., Zhang, S., and Metaxas, D.N. (2016). Multimodal Deep Learning for Cervical Dysplasia Diagnosis. In In International conference on medical image computing and computer-assisted intervention (Springer), pp. 115\u2013123 . Yala, A., Lehman, C., Schuster, T., Portnoi, T., and Barzilay, R. (2019). A deep learning mammography-based model for improved breast cancer risk predic- tion. Radiology 292, 60\u201366 . Yamamoto, Y., ",
    "full_text_length": 92263,
    "chunk_length": 1304
  },
  {
    "chunk_id": 1462,
    "paper_filename": "Jana_2024_artificial_intelligence_for_multimodal_data_integration_in_oncology.pdf",
    "paper_title": "Jana 2024 Artificial Intelligence For Multimodal Data Integration In Oncology",
    "chunk_index": 91,
    "total_chunks": 93,
    "text_content": "Oncol. 5, 72\u201379 . Yap, J., Yolland, W., and Tschandl, P. (2018). Multimodal skin lesion classi\ufb01ca- tion using deep learning. Exp. Dermatol. 27, 1261\u20131267 . Yogananda, C.G.B., Shah, B.R., Yu, F.F., Pinho, M.C., Nalawade, S.S., Muru- gesan, G.K., Wagner, B.C., Mickey, B., Patel, T.R., Fei, B., et al. (2020). A novel fully automated mri-based deep-learning method for classi\ufb01cation of 1p/19q co-deletion status in brain gliomas. Neurooncol. Adv. 2(Supplement 4 ), iv42\u2013iv48 . Zhang, B.H., Lemoine, B.,",
    "full_text_length": 92263,
    "chunk_length": 1325
  },
  {
    "chunk_id": 1463,
    "paper_filename": "Jana_2024_artificial_intelligence_for_multimodal_data_integration_in_oncology.pdf",
    "paper_title": "Jana 2024 Artificial Intelligence For Multimodal Data Integration In Oncology",
    "chunk_index": 92,
    "total_chunks": 93,
    "text_content": "(IEEE), pp. 3797\u20133801 . Zhu, J.-Y., Park, T., Isola, P., and Efros, A.A. (2017). Unpaired image-to- image translation using cycle-consistent adversarial networks. In In Pro- ceedings of the IEEE international conference on computer vision, pp. 2223\u20132232 . Zhuang, L., Lipkova, J., Chen, R., and Mahmood, F. (2022). Deep learning- based integration of histology, radiology, and genomics for improved survival prediction in glioma patients. In In Medical Imaging 2022: Digital and Compu- tational Patho",
    "full_text_length": 92263,
    "chunk_length": 807
  },
  {
    "chunk_id": 1464,
    "paper_filename": "Jeffers_2017_breast_cancer_rsik_and_mammography_density_assested_with_BIRADS.pdf",
    "paper_title": "Jeffers 2017 Breast Cancer Rsik And Mammography Density Assested With Birads",
    "chunk_index": 0,
    "total_chunks": 41,
    "text_content": "Original research n Breast Imag Ing 348 radiology.rsna.org n Radiology: Volume 282: Number 2\u2014February 2017 1 From the Departments of Management Science and Engineering (A.M.J.) and Medicine (Biomedical Infor - matics Research) (D.L.R.), Stanford University, Stanford, Calif; and Departments of Health Research and Policy (W.S., J.H.R., V.M., A.S.W.) and Radiology (J.A.L., D.L.R.), Stanford University School of Medicine, 1201 Welch Rd, Office P285, Stanford, CA 94305. Received September 17, 2015; r",
    "full_text_length": 39823,
    "chunk_length": 1473
  },
  {
    "chunk_id": 1465,
    "paper_filename": "Jeffers_2017_breast_cancer_rsik_and_mammography_density_assested_with_BIRADS.pdf",
    "paper_title": "Jeffers 2017 Breast Cancer Rsik And Mammography Density Assested With Birads",
    "chunk_index": 1,
    "total_chunks": 41,
    "text_content": "control subjects who underwent screening FFDM during 2004\u20132013 and provided informed consent. The percentage of density and dense area were assessed semiautomatically with software (Cumulus 4.0; University of Toronto, Toronto, Canada), and volumetric percentage of density and dense volume were assessed automatically with software (Volpara; Volpara Solutions, Wellington, New Zealand). Clinical Breast Imaging Reporting and Data System (BI-RADS) classifications of breast density were extracted from",
    "full_text_length": 39823,
    "chunk_length": 1354
  },
  {
    "chunk_id": 1466,
    "paper_filename": "Jeffers_2017_breast_cancer_rsik_and_mammography_density_assested_with_BIRADS.pdf",
    "paper_title": "Jeffers 2017 Breast Cancer Rsik And Mammography Density Assested With Birads",
    "chunk_index": 2,
    "total_chunks": 41,
    "text_content": "(95% CI: 1.12, 2.10), and 1.41 (95% CI: 1.11, 1.80), respectively. Odds ratios for women with extremely dense breasts compared with those with scattered areas of fi - broglandular density were 2.06 (95% CI: 0.85, 4.97) and 2.05 (95% CI: 0.90, 4.64) for BI-RADS and Volpara den - sity classifications, respectively. Clinical BI-RADS was more accurate (AUC, 0.68; 95% CI: 0.63, 0.74) than Volpara (AUC, 0.64; 95% CI: 0.58, 0.70) and continuous measures of percentage of density (AUC, 0.66; 95% CI: 0.60",
    "full_text_length": 39823,
    "chunk_length": 1286
  },
  {
    "chunk_id": 1467,
    "paper_filename": "Jeffers_2017_breast_cancer_rsik_and_mammography_density_assested_with_BIRADS.pdf",
    "paper_title": "Jeffers 2017 Breast Cancer Rsik And Mammography Density Assested With Birads",
    "chunk_index": 3,
    "total_chunks": 41,
    "text_content": "crimination of patients from control subjects. q RSNA, 2016Abra M. Jeffers, PhD, MPhil Weiva Sieh, PhD, MD Jafi A. Lipson, MD Joseph H. Rothstein, MS Valerie McGuire, PhD Alice S. Whittemore, PhD Daniel L. Rubin, MD, MSBreast cancer risk and Mammographic Density assessed with semiautomated and Fully automated Methods and B i-raDs1 This copy is for personal use only. To order printed copies, contact reprints@rsna.org Radiology: Volume 282: Number 2\u2014February 2017 n radiology.rsna.org 349 BREAST IM",
    "full_text_length": 39823,
    "chunk_length": 1317
  },
  {
    "chunk_id": 1468,
    "paper_filename": "Jeffers_2017_breast_cancer_rsik_and_mammography_density_assested_with_BIRADS.pdf",
    "paper_title": "Jeffers 2017 Breast Cancer Rsik And Mammography Density Assested With Birads",
    "chunk_index": 4,
    "total_chunks": 41,
    "text_content": "have been performed to compare the association between density estimated by using different methods for process - ing FFDM images and breast cancer risk (10,13,14). The objective of this study was to compare the association of density measurements obtained by us - ing three different approaches (Cumu - lus, Volpara, and radiologist-assigned BI-RADS classification) with breast cancer risk. Materials and Methods Study data were collected under a pro - tocol approved by our institutional re - view ",
    "full_text_length": 39823,
    "chunk_length": 1308
  },
  {
    "chunk_id": 1469,
    "paper_filename": "Jeffers_2017_breast_cancer_rsik_and_mammography_density_assested_with_BIRADS.pdf",
    "paper_title": "Jeffers 2017 Breast Cancer Rsik And Mammography Density Assested With Birads",
    "chunk_index": 5,
    "total_chunks": 41,
    "text_content": "qualitative breast density categories: almost entirely fatty, scattered areas of fibroglandu - lar density, heterogeneously dense, and extremely dense (7,8). A BI- RADS density classification is as - signed by the radiologist on the basis of visual inspection of the image. The most common quantitative measure - ment software, Cumulus (University of Toronto, Toronto, Canada) (9,10), uses semiautomated, user-interactive thresholding of the image to estimate the percentage of breast area that is de",
    "full_text_length": 39823,
    "chunk_length": 1318
  },
  {
    "chunk_id": 1470,
    "paper_filename": "Jeffers_2017_breast_cancer_rsik_and_mammography_density_assested_with_BIRADS.pdf",
    "paper_title": "Jeffers 2017 Breast Cancer Rsik And Mammography Density Assested With Birads",
    "chunk_index": 6,
    "total_chunks": 41,
    "text_content": "is limited because breast density is three-dimensional and potentially vari - able in appearance on two-dimensional mammograms due to differences in compression and projection angle (12). A measure that allows estimation of the volume of fibroglandular tissue relative to the total breast is expected to en - hance the association between breast cancer and breast density. Volpara (Volpara Solutions, Wellington, New Published online before print 10.1148/radiol.2016152062 Content code: Radiology 201",
    "full_text_length": 39823,
    "chunk_length": 1460
  },
  {
    "chunk_id": 1471,
    "paper_filename": "Jeffers_2017_breast_cancer_rsik_and_mammography_density_assested_with_BIRADS.pdf",
    "paper_title": "Jeffers 2017 Breast Cancer Rsik And Mammography Density Assested With Birads",
    "chunk_index": 7,
    "total_chunks": 41,
    "text_content": "any questions related to the work are appropriately resolved, all authors; literature research, A.M.J., J.A.L.; clinical studies, A.M.J., J.A.L.; experimental studies, D.L.R.; statistical analysis, A.M.J., W.S., J.H.R., V.M., A.S.W.; and manuscript editing, all authors Conflicts of interest are listed at the end of this article.Advances in Knowledge nMammographic density measured at full-field digital mammography was significantly associated with breast cancer risk when assessed by radiologists ",
    "full_text_length": 39823,
    "chunk_length": 1404
  },
  {
    "chunk_id": 1472,
    "paper_filename": "Jeffers_2017_breast_cancer_rsik_and_mammography_density_assested_with_BIRADS.pdf",
    "paper_title": "Jeffers 2017 Breast Cancer Rsik And Mammography Density Assested With Birads",
    "chunk_index": 8,
    "total_chunks": 41,
    "text_content": "- mographic density for estimation of patient risk of breast cancer and clinical decision making.Mammographic density refers to the opacity or white areas on a mammogram that represent parts of the breast containing large amounts of epithelial and stromal tis - sue relative to adipose tissue (1,2). The association between mammographic density and breast cancer risk was es - tablished in studies (3,4) in which in - vestigators used film mammography. Estimated odds ratios ranged from 2.9 to 6.0, w",
    "full_text_length": 39823,
    "chunk_length": 1202
  },
  {
    "chunk_id": 1473,
    "paper_filename": "Jeffers_2017_breast_cancer_rsik_and_mammography_density_assested_with_BIRADS.pdf",
    "paper_title": "Jeffers 2017 Breast Cancer Rsik And Mammography Density Assested With Birads",
    "chunk_index": 9,
    "total_chunks": 41,
    "text_content": "on film or as digital data, respectively. In FFDM, there are two types of images: raw and processed. Raw images directly reflect x-ray absorption by the imaging detec - tors, which is processed by using the manufacturers\u2019 proprietary algorithms to improve the aesthetics and conspicu - ity of cancer on the processed images 350 radiology.rsna.org n Radiology: Volume 282: Number 2\u2014February 2017 BREAST IMAGING: Breast Cancer Risk and Mammographic Density Jeffers et alfraction of their 12-bit dynamic",
    "full_text_length": 39823,
    "chunk_length": 1192
  },
  {
    "chunk_id": 1474,
    "paper_filename": "Jeffers_2017_breast_cancer_rsik_and_mammography_density_assested_with_BIRADS.pdf",
    "paper_title": "Jeffers 2017 Breast Cancer Rsik And Mammography Density Assested With Birads",
    "chunk_index": 10,
    "total_chunks": 41,
    "text_content": "subset of 10% of the study images ( n = 48) were read a sec - ond time to assess intrareader repro - ducibility. A single reader (A.J., with 2 years of experience), who was blinded to whether the images were for patients or control subjects, performed all Cu - mulus measurements. The reader was trained by the providers of the Cumulus software (9). We previously found that noise reduction of processed FFDM im - ages makes them appear more like film mammograms, and this can significant- ly improve",
    "full_text_length": 39823,
    "chunk_length": 1204
  },
  {
    "chunk_id": 1475,
    "paper_filename": "Jeffers_2017_breast_cancer_rsik_and_mammography_density_assested_with_BIRADS.pdf",
    "paper_title": "Jeffers 2017 Breast Cancer Rsik And Mammography Density Assested With Birads",
    "chunk_index": 11,
    "total_chunks": 41,
    "text_content": "twice in a different random order three days apart to reduce recall bias and an interreader correlation of 0.81, with the Cumulus instructor reading the same images. Reproducibility between Cumulus operators is generally lower than within the same operator (18). The proficiency of the study reader was independently confirmed in a blinded test of 74 study images that were read 5 months apart, in which she attained an intrareader correlation coefficient of 0.92. To perform three-dimensional breast",
    "full_text_length": 39823,
    "chunk_length": 1276
  },
  {
    "chunk_id": 1476,
    "paper_filename": "Jeffers_2017_breast_cancer_rsik_and_mammography_density_assested_with_BIRADS.pdf",
    "paper_title": "Jeffers 2017 Breast Cancer Rsik And Mammography Density Assested With Birads",
    "chunk_index": 12,
    "total_chunks": 41,
    "text_content": "selected the craniocaudal view of the noncan - cerous breast for patients and the left craniocaudal view for the matched control subjects. All mammograms were acquired with FFDM mam - mography units (Senograph Essen - tial or Senograph 2000D; GE Medical Systems, Milwaukee, Wis). The pa - tients were not stratified according to machine type, because this informa - tion (namely, the Digital Imaging and Communications in Medicine header for the \u201cManufacturerModelName\u201d data element) was missing for ",
    "full_text_length": 39823,
    "chunk_length": 1250
  },
  {
    "chunk_id": 1477,
    "paper_filename": "Jeffers_2017_breast_cancer_rsik_and_mammography_density_assested_with_BIRADS.pdf",
    "paper_title": "Jeffers 2017 Breast Cancer Rsik And Mammography Density Assested With Birads",
    "chunk_index": 13,
    "total_chunks": 41,
    "text_content": "image re - gions appear bright (high pixel values) and fibroglandular regions appear dark (low pixel values). The processed im - ages have a reversed intensity scale and a reduced dynamic range and resemble film mammograms. To perform two-dimensional breast density assessment, the percentage of density measurements were estimated on the basis of the processed FFDM images in Digital Imaging and Commu - nications in Medicine format by using Cumulus 6 software (9). To perform the Cumulus assessment",
    "full_text_length": 39823,
    "chunk_length": 1245
  },
  {
    "chunk_id": 1478,
    "paper_filename": "Jeffers_2017_breast_cancer_rsik_and_mammography_density_assested_with_BIRADS.pdf",
    "paper_title": "Jeffers 2017 Breast Cancer Rsik And Mammography Density Assested With Birads",
    "chunk_index": 14,
    "total_chunks": 41,
    "text_content": "data and the information submitted for publication. Study Design and Population We performed a matched case-control study to investigate the association of the percentage of density and absolute density with future risk of developing breast cancer on the basis of FFDM images assessed by using three density measurement approaches: Cumulus two-dimensional quantitative assess - ment, Volpara three-dimensional quan - titative and qualitative assessment, and qualitative assessment by a radiologist ac",
    "full_text_length": 39823,
    "chunk_length": 1310
  },
  {
    "chunk_id": 1479,
    "paper_filename": "Jeffers_2017_breast_cancer_rsik_and_mammography_density_assested_with_BIRADS.pdf",
    "paper_title": "Jeffers 2017 Breast Cancer Rsik And Mammography Density Assested With Birads",
    "chunk_index": 15,
    "total_chunks": 41,
    "text_content": "noncancer - ous breast contralateral to the affected breast to assess breast density, while avoiding the effect of the presence of cancer on the density measurement. Control subjects comprised 274 women without a history of breast can - cer who underwent screening mam - mography at our institution between 2004 and 2013. We ascertained the breast cancer\u2013free status of control subjects by using the following inclu - sion criteria: (a) at least 10 years of fol - low-up for women aged 50 years or ol",
    "full_text_length": 39823,
    "chunk_length": 1234
  },
  {
    "chunk_id": 1480,
    "paper_filename": "Jeffers_2017_breast_cancer_rsik_and_mammography_density_assested_with_BIRADS.pdf",
    "paper_title": "Jeffers 2017 Breast Cancer Rsik And Mammography Density Assested With Birads",
    "chunk_index": 16,
    "total_chunks": 41,
    "text_content": "in mammography reports, or (d) breast cancer or breast implants noted in pathologic reports. Control Radiology: Volume 282: Number 2\u2014February 2017 n radiology.rsna.org 351 BREAST IMAGING: Breast Cancer Risk and Mammographic Density Jeffers et alsubjects (Table 1). Compared with con - trol subjects, a larger proportion of the patients were postmenopausal and had a BMI greater than or equal to 30 kg/ m2, and a smaller proportion of the pa - tients were nulliparous. Approximately 55% of patients an",
    "full_text_length": 39823,
    "chunk_length": 1299
  },
  {
    "chunk_id": 1481,
    "paper_filename": "Jeffers_2017_breast_cancer_rsik_and_mammography_density_assested_with_BIRADS.pdf",
    "paper_title": "Jeffers 2017 Breast Cancer Rsik And Mammography Density Assested With Birads",
    "chunk_index": 17,
    "total_chunks": 41,
    "text_content": "0.71. The agreement of clinical BI- RADS and Volpara density categoriza - tions was fair, with a weighted k sta- tistic of 0.47. Association of Breast Density with Breast Cancer Risk Table 2 shows the associations of quan - titative Cumulus and Volpara density measures and breast cancer risk, con - trolling for age, race, BMI, parity, and menopausal status. Compared with women in the second quartile, women with Cumulus percentage of density in the highest quartile had an odds ratio of 2.00 (95% ",
    "full_text_length": 39823,
    "chunk_length": 1139
  },
  {
    "chunk_id": 1482,
    "paper_filename": "Jeffers_2017_breast_cancer_rsik_and_mammography_density_assested_with_BIRADS.pdf",
    "paper_title": "Jeffers 2017 Breast Cancer Rsik And Mammography Density Assested With Birads",
    "chunk_index": 18,
    "total_chunks": 41,
    "text_content": "for both the Cumulus and Volpara methods. The odds ratio for each standard deviation increment was 1.61 (95% CI: 1.19, 2.19; P = .002) and 1.49 (95% CI: 1.15, 1.92; P = .002) for Cumulus percentage of den - sity and dense area, and 1.54 (95% CI: 1.12, 2.10; P = .007) and 1.41 (95% CI: 1.11, 1.80; P = .005) for Volpara volumetric percentage of density and dense volume.in the control women. BMI (in kilo - grams per square meter) was catego - rized as less than 25, 25\u201329, and 30 plus. Menopausal st",
    "full_text_length": 39823,
    "chunk_length": 1138
  },
  {
    "chunk_id": 1483,
    "paper_filename": "Jeffers_2017_breast_cancer_rsik_and_mammography_density_assested_with_BIRADS.pdf",
    "paper_title": "Jeffers 2017 Breast Cancer Rsik And Mammography Density Assested With Birads",
    "chunk_index": 19,
    "total_chunks": 41,
    "text_content": "plausible values by using the Markov chain Monte Carlo method. Each imputed dataset ( n = 10) was then analyzed by using conditional logistic regression matched according to age and race and adjusted for meno - pausal status, parity, and BMI. The pa - rameter estimates were then combined to produce a single risk estimate with a 95% CI. A receiver operating characteristic curve for each density measure was cre - ated by plotting the true-positive rate versus the false-positive rate at varying den",
    "full_text_length": 39823,
    "chunk_length": 1199
  },
  {
    "chunk_id": 1484,
    "paper_filename": "Jeffers_2017_breast_cancer_rsik_and_mammography_density_assested_with_BIRADS.pdf",
    "paper_title": "Jeffers 2017 Breast Cancer Rsik And Mammography Density Assested With Birads",
    "chunk_index": 20,
    "total_chunks": 41,
    "text_content": "dicated that the model was no better than chance at making a prediction of membership in a group, and a value of 1.0 indicated that the model perfectly predicted membership in a group. We compared the AUCs for different den - sity estimation methods by using the DeLong test (21). All analyses were performed with software (SAS version 9.4; SAS Institute, Cary, NC). A P value of less than or equal to .05 was consid - ered to indicate a significant difference. Results Study Population The study inc",
    "full_text_length": 39823,
    "chunk_length": 1212
  },
  {
    "chunk_id": 1485,
    "paper_filename": "Jeffers_2017_breast_cancer_rsik_and_mammography_density_assested_with_BIRADS.pdf",
    "paper_title": "Jeffers 2017 Breast Cancer Rsik And Mammography Density Assested With Birads",
    "chunk_index": 21,
    "total_chunks": 41,
    "text_content": "- ric density. The software also assigns classifications that are similar to the BI- RADS density categories 1\u20134, defined as less than 4.5%, 4.5%\u20137.49%, 7.5%\u2013 15.49%, and greater than or equal to 15.5% volumetric percentage of density (14). To obtain the clinical assessment of breast density, we recorded the BI- RADS category assigned by the radiol - ogist in the mammographic report for each case. Statistical Analysis We used conditional logistic regression stratified by age ( ,40, 40\u201344, 45\u201349,",
    "full_text_length": 39823,
    "chunk_length": 1314
  },
  {
    "chunk_id": 1486,
    "paper_filename": "Jeffers_2017_breast_cancer_rsik_and_mammography_density_assested_with_BIRADS.pdf",
    "paper_title": "Jeffers 2017 Breast Cancer Rsik And Mammography Density Assested With Birads",
    "chunk_index": 22,
    "total_chunks": 41,
    "text_content": "stepwise conditional logistic regression with the Akaike information criterion (19). Quantitative density measurements were categorized into quartiles on the basis of the density distribution in the control women. We applied square-root and cube-root transformations to Cu - mulus percentage of density and dense area, respectively, and log-transforma - tions to Volpara volumetric percentage of density and dense volumes, to ob - tain normal distributions. Quantitative density measurements were als",
    "full_text_length": 39823,
    "chunk_length": 1368
  },
  {
    "chunk_id": 1487,
    "paper_filename": "Jeffers_2017_breast_cancer_rsik_and_mammography_density_assested_with_BIRADS.pdf",
    "paper_title": "Jeffers 2017 Breast Cancer Rsik And Mammography Density Assested With Birads",
    "chunk_index": 23,
    "total_chunks": 41,
    "text_content": "BMI, parity, and menopausal status. Compared with women with areas of scattered fibroglandular den - sity (BI-RADS B), women with fatty breasts had reduced risk (odds ratio, 0.38; 95% CI: 0.17, 0.84; P = .017) and women with heterogeneously dense (odds ratio, 2.35; 95% CI: 1.34, 4.12; P = .003) or extremely dense (odds ra - tio, 2.06; 95% CI: 0.85, 4.97; P = .107) breasts had elevated risk. Associations were generally weaker for Volpara classifications than for clinical BI-RADS classifications, ",
    "full_text_length": 39823,
    "chunk_length": 1276
  },
  {
    "chunk_id": 1488,
    "paper_filename": "Jeffers_2017_breast_cancer_rsik_and_mammography_density_assested_with_BIRADS.pdf",
    "paper_title": "Jeffers 2017 Breast Cancer Rsik And Mammography Density Assested With Birads",
    "chunk_index": 24,
    "total_chunks": 41,
    "text_content": "the highest cat - egory had more than twofold increased risks of breast cancer for Cumulus per - centage of density and dense area, and Volpara volumetric percentage of den - sity and dense volume measurements. The positive trend toward increasing risks with increasing density was most statistically significant for clinical BI- RADS assessments. Discrimination of Patients and Control Subjects The Figure shows the receiver oper - ating characteristic curves for clinical BI-RADS, Cumulus, and Volp",
    "full_text_length": 39823,
    "chunk_length": 1253
  },
  {
    "chunk_id": 1489,
    "paper_filename": "Jeffers_2017_breast_cancer_rsik_and_mammography_density_assested_with_BIRADS.pdf",
    "paper_title": "Jeffers 2017 Breast Cancer Rsik And Mammography Density Assested With Birads",
    "chunk_index": 25,
    "total_chunks": 41,
    "text_content": "n = 125) Control Subjects ( n = 274) P Value Age at mammography (y) .95 ,50 52 (41.6) 113 (41.2) 50+ 73 (58.4) 161 (58.8) Menopausal status .35 Premenopausal 37 (29.6) 94 (34.3) Postmenopausal 88 (70.4) 180 (65.7) Parity .27 0 20 (21.5) 66 (26.7) 1 18 (19.4) 42 (17.0) 2 28 (30.1) 89 (36.0) 3+ 27 (29.0) 50 (20.3) Missing data 32 27 BMI (kg/m2) .13 ,25 58 (47.5) 135 (57.7) 25\u201329 37 (30.4) 64 (27.3) 30+ 27 (22.1) 35 (15.0) Missing data 3 40 BI-RADS classification .007 A, Amost entirely fatty 10 (8.",
    "full_text_length": 39823,
    "chunk_length": 1124
  },
  {
    "chunk_id": 1490,
    "paper_filename": "Jeffers_2017_breast_cancer_rsik_and_mammography_density_assested_with_BIRADS.pdf",
    "paper_title": "Jeffers 2017 Breast Cancer Rsik And Mammography Density Assested With Birads",
    "chunk_index": 26,
    "total_chunks": 41,
    "text_content": "(36.1) 4, Extremely dense 32 (25.6) 67 (24.5) Cumulus percentage of density quartile .36 1, ,12.12% 22 (17.6) 68 (24.8) 2, 12.12%\u201325.19% 30 (24.0) 69 (25.2) 3, 25.20%\u201336.34% 35 (28.0) 68 (25.2) 4, \ue02436.35% 38 (30.4) 69 (24.8) Mean* 28.1 6 15.1 25.1 6 14.9 .06 Cumulus dense area quartile .13 1, ,146.74 cm219 (15.2) 68 (24.8) 2, 146.74\u2013269.64 cm234 (27.2) 69 (25.2) 3, 269.65\u2013400.29 cm231 (24.8) 68 (25.2) 4, \ue024400.30 cm241 (32.8) 69 (24.8) Mean* 341.8 6 208.5 288.1 6 190.0 .01 Volpara volumetric perc",
    "full_text_length": 39823,
    "chunk_length": 1203
  },
  {
    "chunk_id": 1491,
    "paper_filename": "Jeffers_2017_breast_cancer_rsik_and_mammography_density_assested_with_BIRADS.pdf",
    "paper_title": "Jeffers 2017 Breast Cancer Rsik And Mammography Density Assested With Birads",
    "chunk_index": 27,
    "total_chunks": 41,
    "text_content": "(24.8) Mean* 67.4 6 42.3 57.0 6 32.6 .02 Note.\u2014Unless otherwise indicated, data are number of patients, with percentage in parentheses. *Data are means 6 standard deviation. Radiology: Volume 282: Number 2\u2014February 2017 n radiology.rsna.org 353 BREAST IMAGING: Breast Cancer Risk and Mammographic Density Jeffers et alTable 2 Breast Cancer Risk Associated with Quantitative Mammographic Density Measured with Cumulus and Volpara Methods Density Quartile or Variable Cumulus Volpara Percentage of Dens",
    "full_text_length": 39823,
    "chunk_length": 1299
  },
  {
    "chunk_id": 1492,
    "paper_filename": "Jeffers_2017_breast_cancer_rsik_and_mammography_density_assested_with_BIRADS.pdf",
    "paper_title": "Jeffers 2017 Breast Cancer Rsik And Mammography Density Assested With Birads",
    "chunk_index": 28,
    "total_chunks": 41,
    "text_content": "(1.15, 1.92)\u20201.54 (1.12, 2.10)* 1.41 (1.11, 1.80)* Note.\u2014Unless otherwise indicated, data are odds ratios, with 95% CIs in parentheses. Odds ratios and CIs were estimated with conditional logistic regression, with age and race matched, and were adjusted for menopausal status, parity, and BMI. * P , .05. \u2020 P , .005. Table 3 Breast Cancer Risk Associated with BI-RADS and Volpara Density Categories Density Category BI-RADS Volpara Almost entirely fatty 0.38 (0.17, 0.84)* 0.77 (0.35, 1.67) Scattered",
    "full_text_length": 39823,
    "chunk_length": 1271
  },
  {
    "chunk_id": 1493,
    "paper_filename": "Jeffers_2017_breast_cancer_rsik_and_mammography_density_assested_with_BIRADS.pdf",
    "paper_title": "Jeffers 2017 Breast Cancer Rsik And Mammography Density Assested With Birads",
    "chunk_index": 29,
    "total_chunks": 41,
    "text_content": "(AUC, 0.64; 95% CI: 0.58, 0.70) and dense volume (AUC, 0.65; 95% CI: 0.59, 0.71). However, the dif - ferences between the AUCs for clinical BI-RADS and other density measures were not statistically significant. Discussion In this case-control study of breast density assessed with Cumulus, Vol - para, and BI-RADS from processed FFDM images, we found that all den - sity measures were positively associ - ated with breast cancer risk. Clinical assessment with BI-RADS allowed the best discrimination ",
    "full_text_length": 39823,
    "chunk_length": 1286
  },
  {
    "chunk_id": 1494,
    "paper_filename": "Jeffers_2017_breast_cancer_rsik_and_mammography_density_assested_with_BIRADS.pdf",
    "paper_title": "Jeffers 2017 Breast Cancer Rsik And Mammography Density Assested With Birads",
    "chunk_index": 30,
    "total_chunks": 41,
    "text_content": "density measurements based on FFDM images were all pos - itively associated with breast cancer risk; although in their study, the AUCs were similar for Volpara and Cumu - lus percentage of density measures and were not reported for BI-RADS assessments. Clinical BI-RADS scores are quali - tative determinations of breast density made by radiologists. These assessments may be influenced by imaging charac - teristics introduced into the processed mammograms that are not present on the raw images. Th",
    "full_text_length": 39823,
    "chunk_length": 1236
  },
  {
    "chunk_id": 1495,
    "paper_filename": "Jeffers_2017_breast_cancer_rsik_and_mammography_density_assested_with_BIRADS.pdf",
    "paper_title": "Jeffers 2017 Breast Cancer Rsik And Mammography Density Assested With Birads",
    "chunk_index": 31,
    "total_chunks": 41,
    "text_content": "very strong risk factor (25). In addition, it is difficult to determine the reference standard for breast density estimation on FFDM images. However, our results are in - teresting in that, if visual assessment is the standard, as is now legislated in more than half of all states in the United States, then Cumulus and Vol - para do not appear to provide much additional value for prediction of risk beyond that of routine clinical BI-RADS assessment. Additional studies would be helpful to determin",
    "full_text_length": 39823,
    "chunk_length": 1317
  },
  {
    "chunk_id": 1496,
    "paper_filename": "Jeffers_2017_breast_cancer_rsik_and_mammography_density_assested_with_BIRADS.pdf",
    "paper_title": "Jeffers 2017 Breast Cancer Rsik And Mammography Density Assested With Birads",
    "chunk_index": 32,
    "total_chunks": 41,
    "text_content": "both the raw and 354 radiology.rsna.org n Radiology: Volume 282: Number 2\u2014February 2017 BREAST IMAGING: Breast Cancer Risk and Mammographic Density Jeffers et alscreening from the electronic medical record. There were several limitations to our study. First, the available sample size limited the ability to detect subtle differences in discrimination among the density assessment methods. Sec - ond, clinical BI-RADS density assess - ment was made by a single reader, as is the standard clinical pra",
    "full_text_length": 39823,
    "chunk_length": 1298
  },
  {
    "chunk_id": 1497,
    "paper_filename": "Jeffers_2017_breast_cancer_rsik_and_mammography_density_assested_with_BIRADS.pdf",
    "paper_title": "Jeffers 2017 Breast Cancer Rsik And Mammography Density Assested With Birads",
    "chunk_index": 33,
    "total_chunks": 41,
    "text_content": "of practice for using Cumulus software is to require the reader to undergo spe - cialized training and attain high levels of intrareader reproducibility with test images before reading the study im - ages. The extensive training and time required to perform Cumulus mea - surements made it impractical to have more than one Cumulus reader for this study, although we acknowledge that having multiple readers could have strengthened the results. Mammographic density assessed on the basis of FFDM imag",
    "full_text_length": 39823,
    "chunk_length": 1261
  },
  {
    "chunk_id": 1498,
    "paper_filename": "Jeffers_2017_breast_cancer_rsik_and_mammography_density_assested_with_BIRADS.pdf",
    "paper_title": "Jeffers 2017 Breast Cancer Rsik And Mammography Density Assested With Birads",
    "chunk_index": 34,
    "total_chunks": 41,
    "text_content": "compare Cumu - lus, Volpara, and BI-RADS density as - sessments on all study images. Most institutions store only the processed images, and not the raw images, which prohibits retrospective Volpara assess - ments. A second important strength Table 4 Breast Cancer Risk Associated with Cumulus fand Volpara Density Categories Defined by Using the Percentile Cutpoints of Radiologist BI-RADS Assessments in Control Subjects Density Category BI-RADSCumulus Volpara Percentage of Density Dense Area Volum",
    "full_text_length": 39823,
    "chunk_length": 1310
  },
  {
    "chunk_id": 1499,
    "paper_filename": "Jeffers_2017_breast_cancer_rsik_and_mammography_density_assested_with_BIRADS.pdf",
    "paper_title": "Jeffers 2017 Breast Cancer Rsik And Mammography Density Assested With Birads",
    "chunk_index": 35,
    "total_chunks": 41,
    "text_content": ".03 Note.\u2014Unless otherwise indicated, data are odds ratios, with 95% Cls in parentheses. Odds ratios and CIs were estimated with conditional logistic regression, with age and race matched, and were adjusted for menopausal status, parity, and BMI. * P , .05. \u2020 P , .005. Graph shows AUCs for mammographic density measures. PD = percentage of density, DA = dense area, VPD = volumetric percentage of density, DV = dense volume. Radiology: Volume 282: Number 2\u2014February 2017 n radiology.rsna.org 355 BRE",
    "full_text_length": 39823,
    "chunk_length": 1380
  },
  {
    "chunk_id": 1500,
    "paper_filename": "Jeffers_2017_breast_cancer_rsik_and_mammography_density_assested_with_BIRADS.pdf",
    "paper_title": "Jeffers 2017 Breast Cancer Rsik And Mammography Density Assested With Birads",
    "chunk_index": 36,
    "total_chunks": 41,
    "text_content": "relationships. W.S. disclosed no relevant relationships. J.A.L. disclosed no relevant relationships. J.H.R. disclosed no rel - evant relationships. V.M. disclosed no relevant relationships. A.S.W. disclosed no relevant relationships. D.L.R. disclosed no relevant relationships. References 1. Li T, Sun L, Miller N, et al. The association of measured breast tissue characteristics with mammographic density and other risk factors for breast cancer. Cancer Epidemiol Biomarkers Prev 2005;14(2):343\u2013349.",
    "full_text_length": 39823,
    "chunk_length": 1351
  },
  {
    "chunk_id": 1501,
    "paper_filename": "Jeffers_2017_breast_cancer_rsik_and_mammography_density_assested_with_BIRADS.pdf",
    "paper_title": "Jeffers 2017 Breast Cancer Rsik And Mammography Density Assested With Birads",
    "chunk_index": 37,
    "total_chunks": 41,
    "text_content": "meta-analysis. J Natl Cancer Inst 2014;106(5):dju078. 6. Pisano ED, Zuley M, Baum JK, Marques HS. Issues to consider in converting to dig - ital mammography. Radiol Clin North Am 2007;45(5):813\u2013830, vi. 7. Nicholson BT, LoRusso AP, Smolkin M, Bovbjerg VE, Petroni GR, Harvey JA. Accu - racy of assigned BI-RADS breast density cat - egory definitions. Acad Radiol 2006;13(9): 1143\u20131149. 8. Boyd NF, Martin LJ, Yaffe MJ, Minkin S. Mammographic density and breast cancer risk: current understanding and ",
    "full_text_length": 39823,
    "chunk_length": 1307
  },
  {
    "chunk_id": 1502,
    "paper_filename": "Jeffers_2017_breast_cancer_rsik_and_mammography_density_assested_with_BIRADS.pdf",
    "paper_title": "Jeffers 2017 Breast Cancer Rsik And Mammography Density Assested With Birads",
    "chunk_index": 38,
    "total_chunks": 41,
    "text_content": "patterns as markers of breast cancer risk: a meta-analy - sis. Cancer Epidemiol Biomarkers Prev 2006; 15(6):1159\u20131169. 12. Yaffe MJ. Mammographic density. Mea - surement of mammographic density. Breast Cancer Res 2008;10(3):209. 13. Fowler EE, Vachon CM, Scott CG, Sellers TA, Heine JJ. Automated percentage of breast density measurements for full-field digital mammography applications. Acad Radiol 2014;21(8):958\u2013970. 14. Brandt KR, Scott CG, Ma L, et al. Compari - son of clinical and automated br",
    "full_text_length": 39823,
    "chunk_length": 1328
  },
  {
    "chunk_id": 1503,
    "paper_filename": "Jeffers_2017_breast_cancer_rsik_and_mammography_density_assested_with_BIRADS.pdf",
    "paper_title": "Jeffers 2017 Breast Cancer Rsik And Mammography Density Assested With Birads",
    "chunk_index": 39,
    "total_chunks": 41,
    "text_content": "interme - diate phenotype for breast cancer. Lancet Oncol 2005;6(10):798\u2013808. 18. Keller BM, Nathan DL, Gavenonis SC, Chen J, Conant EF, Kontos D. Reader var - iability in breast density estimation from full-field digital mammograms: the effect of image postprocessing on relative and ab - solute measures. Acad Radiol 2013;20(5): 560\u2013568. 19. Draper N, Smith H. Applied regression analysis. 2nd ed. New York, NY: Wiley, 1976. 20. Berglund P, Heeringa S. SAS Institute. Mul - tiple imputation of miss",
    "full_text_length": 39823,
    "chunk_length": 1294
  },
  {
    "chunk_id": 1504,
    "paper_filename": "Jeffers_2017_breast_cancer_rsik_and_mammography_density_assested_with_BIRADS.pdf",
    "paper_title": "Jeffers 2017 Breast Cancer Rsik And Mammography Density Assested With Birads",
    "chunk_index": 40,
    "total_chunks": 41,
    "text_content": "Comparison of fully and semi-automated area-based methods for measuring mammographic density and predicting breast cancer risk. Br J Cancer 2014;110(7):1908\u20131916. 24. Darabi H, Czene K, Zhao W, Liu J, Hall P, Humphreys K. Breast cancer risk pre - diction and individualised screening based on common genetic variation and breast density measurement. Breast Cancer Res 2012;14(1):R25. 25. Vickers AJ, Cronin AM, Begg CB. One sta - tistical test is sufficient for assessing new predictive markers. BMC ",
    "full_text_length": 39823,
    "chunk_length": 531
  },
  {
    "chunk_id": 1505,
    "paper_filename": "Jingqi_2021_mulitview_multimodal_network_for_breast_cancer_dignosisnis_in_spectral_mammogram_images.pdf",
    "paper_title": "Jingqi 2021 Mulitview Multimodal Network For Breast Cancer Dignosisnis In Spectral Mammogram Images",
    "chunk_index": 0,
    "total_chunks": 34,
    "text_content": "International Journal of Computer Assisted Radiology and Surgery (2021) 16:979\u2013988 https://doi.org/10.1007/s11548-021-02391-4 ORIGINAL ARTICLE Multiview multimodal network for breast cancer diagnosis in contrast-enhanced spectral mammography images Jingqi Song1\u00b7Yuanjie Zheng1\u00b7Muhammad Zakir Ullah1\u00b7Junxia Wang1\u00b7Yanyun Jiang1\u00b7Chenxi Xu1\u00b7 Zhenxing Zou2\u00b7Guocheng Ding2 Received: 28 December 2020 / Accepted: 27 April 2021 / Published online: 8 May 2021 \u00a9 CARS 2021 Abstract Purpose CESM (contrast-enhan",
    "full_text_length": 32947,
    "chunk_length": 1462
  },
  {
    "chunk_id": 1506,
    "paper_filename": "Jingqi_2021_mulitview_multimodal_network_for_breast_cancer_dignosisnis_in_spectral_mammogram_images.pdf",
    "paper_title": "Jingqi 2021 Mulitview Multimodal Network For Breast Cancer Dignosisnis In Spectral Mammogram Images",
    "chunk_index": 1,
    "total_chunks": 34,
    "text_content": "of the model, image feature extraction, and image classi\ufb01cation. The \ufb01rst stage is to preprocess the CESM to utilize its multiview and multimodal features effectively. In the feature extraction stage, a deep learning-based network is used to extract CESM images features. The last stage is tointegrate different features for classi\ufb01cation using the MVMM-Net model. Results According to the experimental results, the proposed method based on the Res2Net50 framework achieves an accuracy of 96.591%, se",
    "full_text_length": 32947,
    "chunk_length": 1375
  },
  {
    "chunk_id": 1507,
    "paper_filename": "Jingqi_2021_mulitview_multimodal_network_for_breast_cancer_dignosisnis_in_spectral_mammogram_images.pdf",
    "paper_title": "Jingqi 2021 Mulitview Multimodal Network For Breast Cancer Dignosisnis In Spectral Mammogram Images",
    "chunk_index": 2,
    "total_chunks": 34,
    "text_content": "Contrast-enhanced spectral mammography \u00b7Classi\ufb01cation \u00b7Breast cancer \u00b7Multiview \u00b7Multimodal Introduction Breast cancer is one of the most frequent female malignan- cies, which seriously affects their health [ 1]. According to Global Cancer Statistics [ 2] among all the new cases of can- cers, 30% will be diagnosed as breast cancer and 15% ofdeaths will be caused due to breast cancer in 2021. Due tothe unclear causes and effective prevention of breast can- cer, breast cancer cases have not been s",
    "full_text_length": 32947,
    "chunk_length": 1376
  },
  {
    "chunk_id": 1508,
    "paper_filename": "Jingqi_2021_mulitview_multimodal_network_for_breast_cancer_dignosisnis_in_spectral_mammogram_images.pdf",
    "paper_title": "Jingqi 2021 Mulitview Multimodal Network For Breast Cancer Dignosisnis In Spectral Mammogram Images",
    "chunk_index": 3,
    "total_chunks": 34,
    "text_content": "to detect breast cancer[4]. The CESM images are acquired through a digital dual- energy CESM device which is derived from a standard Digital Senographe Essential Mammography [ 5]. The prototype of CESM device is a full-\ufb01eld digital mammography systemthat is modi\ufb01ed to allow dual-energy exposures. It com- prises a \ufb02at panel detector with a cesium iodide absorber, dedicated software, and hardware. The standard CESM pro-cedure consists of the following steps: an iodinated contrast agent (300mg iodi",
    "full_text_length": 32947,
    "chunk_length": 1347
  },
  {
    "chunk_id": 1509,
    "paper_filename": "Jingqi_2021_mulitview_multimodal_network_for_breast_cancer_dignosisnis_in_spectral_mammogram_images.pdf",
    "paper_title": "Jingqi 2021 Mulitview Multimodal Network For Breast Cancer Dignosisnis In Spectral Mammogram Images",
    "chunk_index": 4,
    "total_chunks": 34,
    "text_content": "and high-energy (45\u201349 kVp) X-ray spec-trum [ 6]. Then, the low-energy (LE) and high-energy (HE) images are reconstructed by a speci\ufb01c image reconstruction algorithm to obtain the dual-energy subtracted (DES) image that can suppress background texture and highlight contrastuptake. HE images are rarely studied images due to their low clinical value. Therefore, CESM consists of two completely different modalities of image: LE image and DES image.The typical CESM images are shown in Fig. 1. Because",
    "full_text_length": 32947,
    "chunk_length": 1339
  },
  {
    "chunk_id": 1510,
    "paper_filename": "Jingqi_2021_mulitview_multimodal_network_for_breast_cancer_dignosisnis_in_spectral_mammogram_images.pdf",
    "paper_title": "Jingqi 2021 Mulitview Multimodal Network For Breast Cancer Dignosisnis In Spectral Mammogram Images",
    "chunk_index": 5,
    "total_chunks": 34,
    "text_content": "and provides tumor informationwith high image resolution similar to magnetic resonance imaging (MRI), which is a promising imaging technique [ 13]. In clinical practice, the detection of breast images by experts is usually a manual process, which is time-consumingand costly [ 14]. In addition, the radiologist may miss the primary lesion due to the overlapping of dense tissue or changes in the shape, size, and borders of the mass [ 15]. Machine learning (ML)-based image analysis methods can impro",
    "full_text_length": 32947,
    "chunk_length": 1322
  },
  {
    "chunk_id": 1511,
    "paper_filename": "Jingqi_2021_mulitview_multimodal_network_for_breast_cancer_dignosisnis_in_spectral_mammogram_images.pdf",
    "paper_title": "Jingqi 2021 Mulitview Multimodal Network For Breast Cancer Dignosisnis In Spectral Mammogram Images",
    "chunk_index": 6,
    "total_chunks": 34,
    "text_content": "because they are relatively new inimaging technology. Mateos et al. [ 20] analyzed seventeen textural descriptors from gray-level co-occurrence matrix (GLCM) and summarized the potential of CESM imagesto improve the diagnostic accuracy of breast cancer. Perek et al. [ 21] compared two analysis approaches, \ufb01ne-tuned a pretrained network and fully trained convolutional neuralnetwork, to classify breast masses as benign or malignantin CESM images. They also used breast imaging report- ing and data ",
    "full_text_length": 32947,
    "chunk_length": 1317
  },
  {
    "chunk_id": 1512,
    "paper_filename": "Jingqi_2021_mulitview_multimodal_network_for_breast_cancer_dignosisnis_in_spectral_mammogram_images.pdf",
    "paper_title": "Jingqi 2021 Mulitview Multimodal Network For Breast Cancer Dignosisnis In Spectral Mammogram Images",
    "chunk_index": 7,
    "total_chunks": 34,
    "text_content": "Another study [ 9] focused on the lesion region of interest (ROI) and extracted 236 textural features from the CESM.Then, the most noteworthy features were classi\ufb01ed using thesupport vector machines (SVM). Recently, Fanizzi et al. [ 23] proposed a fully automated support system for diagnosis of breast cancer in CESM Images. Then, different techniqueswere used to extract the ROI features set of low-energy and recombined images. Through the sequential feature selec- tion algorithm, the random fore",
    "full_text_length": 32947,
    "chunk_length": 1350
  },
  {
    "chunk_id": 1513,
    "paper_filename": "Jingqi_2021_mulitview_multimodal_network_for_breast_cancer_dignosisnis_in_spectral_mammogram_images.pdf",
    "paper_title": "Jingqi 2021 Mulitview Multimodal Network For Breast Cancer Dignosisnis In Spectral Mammogram Images",
    "chunk_index": 8,
    "total_chunks": 34,
    "text_content": "existing methods replace the natural input image with medical image without effectively considering the speci\ufb01c characteristics of CESM. For instance, CESM images have multiple views (CC and MLO view) to provide different information aboutlesions, which can help in improving the performance of clas-si\ufb01cation. In addition, CESM images are characterized by multimodality (LE and DES modes), which has rarely been considered simultaneously in previous studies. Based on theabove considerations, we pro",
    "full_text_length": 32947,
    "chunk_length": 1353
  },
  {
    "chunk_id": 1514,
    "paper_filename": "Jingqi_2021_mulitview_multimodal_network_for_breast_cancer_dignosisnis_in_spectral_mammogram_images.pdf",
    "paper_title": "Jingqi 2021 Mulitview Multimodal Network For Breast Cancer Dignosisnis In Spectral Mammogram Images",
    "chunk_index": 9,
    "total_chunks": 34,
    "text_content": "patient. Each patient has eight images, four on each sideof the left and right breast. Since the condition of the left and right breast may be different in each patient, we treat the images of a patient as two cases (one case for each breast).Our dataset includes 95 patients, of which 58 are malignant and 132 benign cases. Each case has 4 images, with a total of 760 images. The typical size of CESM images in ourexperiment is 2394*3062, and these images are divided into 123 International Journal ",
    "full_text_length": 32947,
    "chunk_length": 1139
  },
  {
    "chunk_id": 1515,
    "paper_filename": "Jingqi_2021_mulitview_multimodal_network_for_breast_cancer_dignosisnis_in_spectral_mammogram_images.pdf",
    "paper_title": "Jingqi 2021 Mulitview Multimodal Network For Breast Cancer Dignosisnis In Spectral Mammogram Images",
    "chunk_index": 10,
    "total_chunks": 34,
    "text_content": "classes (benign/malignant). The proportion of images i ss h o w ni nF i g . 2a. As can be seen from the \ufb01gure, the dataset is highly imbalanced. We randomly discard half ofthe benign image data to produce a more balanced dataset to minimize the potential classi\ufb01cation bias towards minority (malignant) cases. By utilizing the random discard strategy,the number of benign cases is narrowed from 132 to 66. The dataset becomes more balanced with 66 (53.2%) benign and 58 (46.8%) malignant cases (Fig. ",
    "full_text_length": 32947,
    "chunk_length": 1207
  },
  {
    "chunk_id": 1516,
    "paper_filename": "Jingqi_2021_mulitview_multimodal_network_for_breast_cancer_dignosisnis_in_spectral_mammogram_images.pdf",
    "paper_title": "Jingqi 2021 Mulitview Multimodal Network For Breast Cancer Dignosisnis In Spectral Mammogram Images",
    "chunk_index": 11,
    "total_chunks": 34,
    "text_content": "the image content. However, these backgrounds have no positive impact on our classi\ufb01cation. To maximize the removal of theblack background while ensuring that the image area of the breast is not removed, we segment the image into the size of 1350*3062. In addition, the classi\ufb01cation model alwaysperforms well when the dataset is large enough. Then, we randomly select half of the images to \ufb02ip horizontally or ver- tically for meeting the model\u2019s needs. Besides, we randomlyselect 20% of the images ",
    "full_text_length": 32947,
    "chunk_length": 1227
  },
  {
    "chunk_id": 1517,
    "paper_filename": "Jingqi_2021_mulitview_multimodal_network_for_breast_cancer_dignosisnis_in_spectral_mammogram_images.pdf",
    "paper_title": "Jingqi 2021 Mulitview Multimodal Network For Breast Cancer Dignosisnis In Spectral Mammogram Images",
    "chunk_index": 12,
    "total_chunks": 34,
    "text_content": "number of the image in our dataset is increased from 496 to 17360. We randomly select 80% of the image for training, 15% for validation, and5% for \ufb01nal testing. After augmenting, we resize the wholeimage to a uniform size before feeding it into the network. The structure of MVMM-Net As mentioned above, the discrimination performance can beimproved by using multiview and multimodal CESM images.Inspired by this motivation, we propose a novel network toclassify the CESM images. We propose a new mul",
    "full_text_length": 32947,
    "chunk_length": 1216
  },
  {
    "chunk_id": 1518,
    "paper_filename": "Jingqi_2021_mulitview_multimodal_network_for_breast_cancer_dignosisnis_in_spectral_mammogram_images.pdf",
    "paper_title": "Jingqi 2021 Mulitview Multimodal Network For Breast Cancer Dignosisnis In Spectral Mammogram Images",
    "chunk_index": 13,
    "total_chunks": 34,
    "text_content": "In our experiments, we \ufb01nd that the \u2019Res2Net50\u2019 model have the highest accuracy on the test set 123 982 International Journal of Computer Assisted Radiology and Surgery (2021) 16:979\u2013988 Fig. 3 Main \ufb02owchart of the proposed MVMM-Net for the benign/malignant classi\ufb01cation task (shown in Table 2). Unless we explicitly specify, we will report the results of this model. In the MVMM-Net, the model shares the same network, while holding independent parameters. In detail, the network consisting of a ca",
    "full_text_length": 32947,
    "chunk_length": 1288
  },
  {
    "chunk_id": 1519,
    "paper_filename": "Jingqi_2021_mulitview_multimodal_network_for_breast_cancer_dignosisnis_in_spectral_mammogram_images.pdf",
    "paper_title": "Jingqi 2021 Mulitview Multimodal Network For Breast Cancer Dignosisnis In Spectral Mammogram Images",
    "chunk_index": 14,
    "total_chunks": 34,
    "text_content": "used to make the \ufb01nal decision of the classi\ufb01cation task. We denote the training dataset of our MVMM-Net by {(x i,yi), i = 1,2,...,N}, where x idenotes the ith set of input CESM image, y i\u2208{0,1} denotes the ground truth label (proved by biopsy) assigned to ith input set. We denote the output of clas- si\ufb01cation by \u02dcyi\u2208{0,1} . The loss function of classi\ufb01cation is de\ufb01ned asLoss=1 NN/summationdisplay i=1H(\u02dcyi,yi) (1) where H() is the cross-entropy function for measuring the dissimilarity between \u02dcy",
    "full_text_length": 32947,
    "chunk_length": 1188
  },
  {
    "chunk_id": 1520,
    "paper_filename": "Jingqi_2021_mulitview_multimodal_network_for_breast_cancer_dignosisnis_in_spectral_mammogram_images.pdf",
    "paper_title": "Jingqi 2021 Mulitview Multimodal Network For Breast Cancer Dignosisnis In Spectral Mammogram Images",
    "chunk_index": 15,
    "total_chunks": 34,
    "text_content": "The software environment is Ubuntu 16.04 and Pytorch 3.7. The hardware environment is Intel(R) Xeon(R) CPU E5-2630 v4 @ 2.20GHz and NVIDIAGV100GL Tesla V100 32GB GPU. 123 International Journal of Computer Assisted Radiology and Surgery (2021) 16:979\u2013988 983 Fig. 4 The loss and accuracy curves on training and validation process and the ROC curve in the test phase. aThe loss curves. bThe accuracy curves. cThe ROC curve Performance evaluation To evaluate the performance of the proposed method in cl",
    "full_text_length": 32947,
    "chunk_length": 1337
  },
  {
    "chunk_id": 1521,
    "paper_filename": "Jingqi_2021_mulitview_multimodal_network_for_breast_cancer_dignosisnis_in_spectral_mammogram_images.pdf",
    "paper_title": "Jingqi 2021 Mulitview Multimodal Network For Breast Cancer Dignosisnis In Spectral Mammogram Images",
    "chunk_index": 16,
    "total_chunks": 34,
    "text_content": "the system. Among these metrics, sensitivity is particularly important. And it is de\ufb01ned as the proportion of samplesthat are predicted to be positive cases out of all samples thatare positive cases. In the medical \ufb01eld, high-risk categories such as diseases are often used as positive categories, and the cost of missing a positive category is very high. In additionto the above \ufb01ve criteria, the ROC curve and AUC are also used to evaluate the performance. The ROC curve near the upper left corner ",
    "full_text_length": 32947,
    "chunk_length": 1246
  },
  {
    "chunk_id": 1522,
    "paper_filename": "Jingqi_2021_mulitview_multimodal_network_for_breast_cancer_dignosisnis_in_spectral_mammogram_images.pdf",
    "paper_title": "Jingqi 2021 Mulitview Multimodal Network For Breast Cancer Dignosisnis In Spectral Mammogram Images",
    "chunk_index": 17,
    "total_chunks": 34,
    "text_content": "proceeds, the accuracy of classi\ufb01cationgradually increases and then reaches the maximum value. Atthe same time, the loss curve gradually decreases until it tends to be smooth. The training accuracy increases to almost 100% and the loss no longer changes, which indicates the successof our approach. This shows that our method has achieved good results in the training process. Figure 4ci st h eR O C curve of our model on the test set. The accuracy, sensitivity,speci\ufb01city, and AUC of MVMM-Res2Net50 ",
    "full_text_length": 32947,
    "chunk_length": 1268
  },
  {
    "chunk_id": 1523,
    "paper_filename": "Jingqi_2021_mulitview_multimodal_network_for_breast_cancer_dignosisnis_in_spectral_mammogram_images.pdf",
    "paper_title": "Jingqi 2021 Mulitview Multimodal Network For Breast Cancer Dignosisnis In Spectral Mammogram Images",
    "chunk_index": 18,
    "total_chunks": 34,
    "text_content": "proposed method. The highlights in the \ufb01rst row focus on the tumor area, indicat- ing that the model can accurately distinguish the tumor areafrom the normal area. The lighter areas of normal images are mainly in the whole breast region. As can be seen from the \ufb01gure, the proposed method identi\ufb01es the regions that playan important role in the classi\ufb01cation. Based on the radiol- ogist\u2019s opinion, the location of the CAM thermogram of the lesion obtained by the model also matches the physician\u2019sreg",
    "full_text_length": 32947,
    "chunk_length": 1204
  },
  {
    "chunk_id": 1524,
    "paper_filename": "Jingqi_2021_mulitview_multimodal_network_for_breast_cancer_dignosisnis_in_spectral_mammogram_images.pdf",
    "paper_title": "Jingqi 2021 Mulitview Multimodal Network For Breast Cancer Dignosisnis In Spectral Mammogram Images",
    "chunk_index": 19,
    "total_chunks": 34,
    "text_content": "al. [ 10] 78.4 \u2013 \u2013 0.848 Fanizzi et al. [ 23] 87.5 87.5 91.7 0.931 MVMM-Res2Net50 (ours) 96.6 96.4 96.4 0.966 Bold indicates the results that surpass all competing methods Fig. 5 Visualization of class activation mapping for CESM images. a\u2013dThe class activation mapping on the cancer breast. e\u2013hThe class activation mapping on the normal breast breast regions demonstrates the potential value of our net- work in CESM image classi\ufb01cation. Performance of the proposed method compared with existing met",
    "full_text_length": 32947,
    "chunk_length": 1246
  },
  {
    "chunk_id": 1525,
    "paper_filename": "Jingqi_2021_mulitview_multimodal_network_for_breast_cancer_dignosisnis_in_spectral_mammogram_images.pdf",
    "paper_title": "Jingqi 2021 Mulitview Multimodal Network For Breast Cancer Dignosisnis In Spectral Mammogram Images",
    "chunk_index": 20,
    "total_chunks": 34,
    "text_content": "the sensitivity of Perek [ 21] is 100%, the speci\ufb01city index of this method is only 66%, which is the lowest among all methods. Presum-ably the method in [ 21] trained on a relatively small dataset. We hypothesize that the superior performance of the pro- posed method is related to its ability to ef\ufb01ciently integratemultiview and multimodal features. In Sect. 4,w ee m p i r - ically investigate this hypothesis through multiple ablation studies. Discussion In this paper, we present a method that ",
    "full_text_length": 32947,
    "chunk_length": 1392
  },
  {
    "chunk_id": 1526,
    "paper_filename": "Jingqi_2021_mulitview_multimodal_network_for_breast_cancer_dignosisnis_in_spectral_mammogram_images.pdf",
    "paper_title": "Jingqi 2021 Mulitview Multimodal Network For Breast Cancer Dignosisnis In Spectral Mammogram Images",
    "chunk_index": 21,
    "total_chunks": 34,
    "text_content": "of classi\ufb01cation methods under different networks CNN Architecture Precision (%) Speci\ufb01city (%) Sensitivity (%) AUC MVMM-VGGNet19 82.74 83.64 84.23 0.832 MVMM-ResNet18 95.11 96.28 96.37 0.957MVMM-ResNet50 93.95 91.11 90.99 0.925MVMM-SE-ResNet50 94.50 92.66 92.79 0.936MVMM-ResNeXt50 92.66 90.99 90.99 0.918MVMM-WRN-50 92.83 93.09 93.24 0.930MVMM-Res2Net18 94.60 94.50 94.60 0.946MVMM-Res2Net50 96.83 96.35 96.40 0.966 Bold indicates the results that exceed all competing methods in this column Fig. 6",
    "full_text_length": 32947,
    "chunk_length": 1458
  },
  {
    "chunk_id": 1527,
    "paper_filename": "Jingqi_2021_mulitview_multimodal_network_for_breast_cancer_dignosisnis_in_spectral_mammogram_images.pdf",
    "paper_title": "Jingqi 2021 Mulitview Multimodal Network For Breast Cancer Dignosisnis In Spectral Mammogram Images",
    "chunk_index": 22,
    "total_chunks": 34,
    "text_content": "their precision, speci\ufb01city, sensitivity, and AUC.Overall, the improved classi\ufb01cation network achieved high results, with almost all metrics above 90%. The precision, speci\ufb01city, sensitivity, and AUC of MVMM-Res2Net50 are the highest among the mentioned networks. From the aboveexperiments, we con\ufb01rm that our proposed structure is suit-able for CESM image classi\ufb01cation. The performance of classification under different CESM views In this part, we design an experiment to evaluate whether the multi",
    "full_text_length": 32947,
    "chunk_length": 1289
  },
  {
    "chunk_id": 1528,
    "paper_filename": "Jingqi_2021_mulitview_multimodal_network_for_breast_cancer_dignosisnis_in_spectral_mammogram_images.pdf",
    "paper_title": "Jingqi 2021 Mulitview Multimodal Network For Breast Cancer Dignosisnis In Spectral Mammogram Images",
    "chunk_index": 23,
    "total_chunks": 34,
    "text_content": "classi\ufb01cation, we input both views into the network for analysis. We compare the F1_score of these twotypes of networks. The results are shown in Fig. 6. As can be seen from the \ufb01gure, the results of the F1_score with multi- ple views are better than those of the single view, regardlessof the type of network. It can be seen that a multiview-basednetwork can improve the performance of CESM image clas- si\ufb01cation. The comparison of different CESM image modalities on classification performance In th",
    "full_text_length": 32947,
    "chunk_length": 1249
  },
  {
    "chunk_id": 1529,
    "paper_filename": "Jingqi_2021_mulitview_multimodal_network_for_breast_cancer_dignosisnis_in_spectral_mammogram_images.pdf",
    "paper_title": "Jingqi 2021 Mulitview Multimodal Network For Breast Cancer Dignosisnis In Spectral Mammogram Images",
    "chunk_index": 24,
    "total_chunks": 34,
    "text_content": "analysis. The \ufb01nal evaluation results obtained are shown in Fig. 7.A si ti s seen from the \ufb01gure, the classi\ufb01cation accuracy of the mul- timodal network is higher than that of the single modalnetwork. In other words, the network performance is indeed improved when multimodal images are utilized. In addition, the network using DES images for classi\ufb01cation obtained sig-ni\ufb01cantly better results than the network using LE images, proving that DES images do help to improve the accuracy of diagnosis. O",
    "full_text_length": 32947,
    "chunk_length": 1344
  },
  {
    "chunk_id": 1530,
    "paper_filename": "Jingqi_2021_mulitview_multimodal_network_for_breast_cancer_dignosisnis_in_spectral_mammogram_images.pdf",
    "paper_title": "Jingqi 2021 Mulitview Multimodal Network For Breast Cancer Dignosisnis In Spectral Mammogram Images",
    "chunk_index": 25,
    "total_chunks": 34,
    "text_content": "the classi\ufb01cation results more accurate. Our experimental resultshave demonstrated that it is essential to classify CESM 123 International Journal of Computer Assisted Radiology and Surgery (2021) 16:979\u2013988 987 images using images in multiple views. We also show exper- imentally that using CESM images of multiple modalitiessimultaneously can also improve the classi\ufb01cation perfor- mance of the network. Our algorithm can provide physicians with more recommendations when diagnosing breast can-cer ",
    "full_text_length": 32947,
    "chunk_length": 1409
  },
  {
    "chunk_id": 1531,
    "paper_filename": "Jingqi_2021_mulitview_multimodal_network_for_breast_cancer_dignosisnis_in_spectral_mammogram_images.pdf",
    "paper_title": "Jingqi 2021 Mulitview Multimodal Network For Breast Cancer Dignosisnis In Spectral Mammogram Images",
    "chunk_index": 26,
    "total_chunks": 34,
    "text_content": "Shandong Province Natural Science Foundation Grant Number ZR2018ZB0419, and theTaishan Scholar Program of Shandong Province of China grant numberTSHW201502038. Declarations Con\ufb02icts of Interest The authors declare that they have no con\ufb02ict of interest. Ethical approval All procedures performed in studies involving human participants were in accordance with the ethical standards of the insti-tutional and/or national research committee and with the 1964 HelsinkiDeclaration and its later amendments",
    "full_text_length": 32947,
    "chunk_length": 1370
  },
  {
    "chunk_id": 1532,
    "paper_filename": "Jingqi_2021_mulitview_multimodal_network_for_breast_cancer_dignosisnis_in_spectral_mammogram_images.pdf",
    "paper_title": "Jingqi 2021 Mulitview Multimodal Network For Breast Cancer Dignosisnis In Spectral Mammogram Images",
    "chunk_index": 27,
    "total_chunks": 34,
    "text_content": "cancer. Neurocomputing 392:168\u2013180 4. Martin D, Tobias DZ, Wolfram S, Birgit A, Florian K, Werner J, Clarisse D, Willi O, Michael H, Christian M (2015) Dual-energy contrast-enhanced spectral mammography (CESM). Arch Gynecol Obstet 292(4):739\u2013747 5. Lisa H, Martin D, Christoph J, Elena E, Julia H (2019) Contrast- enhanced spectral mammography with a compact synchrotronsource. PLoS ONE 14(10): 6. James JJ, Tennant SL (2018) Contrast-enhanced spectral mammog- raphy (CESM). Clin Radiol 73(8):715\u2013723",
    "full_text_length": 32947,
    "chunk_length": 1399
  },
  {
    "chunk_id": 1533,
    "paper_filename": "Jingqi_2021_mulitview_multimodal_network_for_breast_cancer_dignosisnis_in_spectral_mammogram_images.pdf",
    "paper_title": "Jingqi 2021 Mulitview Multimodal Network For Breast Cancer Dignosisnis In Spectral Mammogram Images",
    "chunk_index": 28,
    "total_chunks": 34,
    "text_content": "contrast-enhanced spectral mammography: a feasibility study. Eur J Radiol 98(3):207\u2013213 10. Gopichandh D, Bhavika P , Faranak A, Morteza H, Jing L, Teresa W, Bin Z (2018) Classi\ufb01cation of breast masses using a computer-aided diagnosis scheme of contrast enhanced digital mammograms.Ann Biomed Eng 46(9):1419\u20131431 11. Dromain C, Thibault F, Diekmann F, Fallenberg EM, Jong RA, Koomen M, Hendrick RE, Tardivon A, Toledano A (2012) Dual-energy contrast-enhanced digital mammography: initial clinicalresu",
    "full_text_length": 32947,
    "chunk_length": 1352
  },
  {
    "chunk_id": 1534,
    "paper_filename": "Jingqi_2021_mulitview_multimodal_network_for_breast_cancer_dignosisnis_in_spectral_mammogram_images.pdf",
    "paper_title": "Jingqi 2021 Mulitview Multimodal Network For Breast Cancer Dignosisnis In Spectral Mammogram Images",
    "chunk_index": 29,
    "total_chunks": 34,
    "text_content": "H, Po HCC, Rakha Emad A (2020) Arti\ufb01cial intelligencein digital breast pathology: techniques and applications. Breast49:267\u2013273 15. Arnau O, Jordi F, Joan M, Elsa P , Josep P , Denton Erika RE, Reyer Z (2010) A review of automatic mass detection and segmentationin mammographic images. Med Image Anal 14(2):87\u2013110 16. Stephanie R, Hossein A, Kevin S, Johan H (2018) Digital image analysis in breast pathology from image processing techniques toarti\ufb01cial intelligence. Transl Res 194:19\u201335 17. Le EPV ",
    "full_text_length": 32947,
    "chunk_length": 1333
  },
  {
    "chunk_id": 1535,
    "paper_filename": "Jingqi_2021_mulitview_multimodal_network_for_breast_cancer_dignosisnis_in_spectral_mammogram_images.pdf",
    "paper_title": "Jingqi 2021 Mulitview Multimodal Network For Breast Cancer Dignosisnis In Spectral Mammogram Images",
    "chunk_index": 30,
    "total_chunks": 34,
    "text_content": "conjunction with MICCAI 2020, Lima, Peru, October 4\u20138, 2020,Proceedings, vol 12445, pp 125\u2013135. Springer 19. Shaikh TA, Rashid A, Sufyan Beg MM (2020) Transfer learning privileged information fuels cad diagnosis of breast cancer. MachVis Appl 31(1):1\u201323 20. Mateos MJ, Gastelum A, M\u00e1rquez J, Brandan ME (2016). Tex- ture analysis of contrast-enhanced digital mammography (CEDM)images. In: Lecture Notes in Computer Science (including sub-series lecture notes in arti\ufb01cial intelligence and lecture not",
    "full_text_length": 32947,
    "chunk_length": 1343
  },
  {
    "chunk_id": 1536,
    "paper_filename": "Jingqi_2021_mulitview_multimodal_network_for_breast_cancer_dignosisnis_in_spectral_mammogram_images.pdf",
    "paper_title": "Jingqi 2021 Mulitview Multimodal Network For Breast Cancer Dignosisnis In Spectral Mammogram Images",
    "chunk_index": 31,
    "total_chunks": 34,
    "text_content": ",Massafra R, Tangaro S, Forgia DL (2019) Fully automated supportsystem for diagnosis of breast cancer in contrast-enhanced spectralmammography images. J Clin Med 8(6):891 24. Liliana L, Annarita F, Basile Teresa Maria A, Roberto B, La Forgia D (2019) Radiomics analysis on contrast-enhanced spectral mam-mography images for breast cancer diagnosis: a pilot study. Entropy21(11):1110 25. Daniele LF, Annarita F, Francesco C, Roberto B, Vittorio D, Vito L, Marco M, Raffaella M, Pasquale T, Sabina T, M",
    "full_text_length": 32947,
    "chunk_length": 1403
  },
  {
    "chunk_id": 1537,
    "paper_filename": "Jingqi_2021_mulitview_multimodal_network_for_breast_cancer_dignosisnis_in_spectral_mammogram_images.pdf",
    "paper_title": "Jingqi 2021 Mulitview Multimodal Network For Breast Cancer Dignosisnis In Spectral Mammogram Images",
    "chunk_index": 32,
    "total_chunks": 34,
    "text_content": "pp 770\u2013778 28. Jie H, Li S, Gang S (2018). Squeeze-and-excitation networks. In: Proceedings of the IEEE conference on computer vision and patternrecognition, pp 7132\u20137141 29. Saining X, Ross G, Piotr D, Zhuowen T, Kaiming H ( 2017). Aggregated residual transformations for deep neural networks. In:Proceedings - 30th IEEE conference on computer vision and pat-tern recognition, CVPR 2017, vol 2017, pp 5987\u20135995 30. Zagoruyko S, Komodakis N (2016). Wide residual networks. In: British machine vision ",
    "full_text_length": 32947,
    "chunk_length": 1228
  },
  {
    "chunk_id": 1538,
    "paper_filename": "Jingqi_2021_mulitview_multimodal_network_for_breast_cancer_dignosisnis_in_spectral_mammogram_images.pdf",
    "paper_title": "Jingqi 2021 Mulitview Multimodal Network For Breast Cancer Dignosisnis In Spectral Mammogram Images",
    "chunk_index": 33,
    "total_chunks": 34,
    "text_content": "explanations from deep networks viagradient-based localization. In: Proceedings of the IEEE interna-tional conference on computer vision, pp 618\u2013626 Publisher\u2019s Note Springer Nature remains neutral with regard to juris- dictional claims in published maps and institutional af\ufb01liations. 123",
    "full_text_length": 32947,
    "chunk_length": 289
  },
  {
    "chunk_id": 1539,
    "paper_filename": "Julie_2021_contrast_enhance_mammography_past_present_and_future.pdf",
    "paper_title": "Julie 2021 Contrast Enhance Mammography Past Present And Future",
    "chunk_index": 0,
    "total_chunks": 59,
    "text_content": "Contrast-Enhanced Mammography: Past, Present, and Future Julie Sogania, Victoria L. Mangoa, Delia Keatinga, Janice S. Sunga, Maxine S. Jochelsona aMemorial Sloan Kettering Cancer Center, 300 East 66th Street, New York, NY 10065, USA. Abstract Contrast-enhanced mammography (CEM) combines conventional mammography with iodinated contrast material to improve cancer detection. CEM has comparable performance to breast MRI without the added cost or time of conventional MRI protocols. Thus, this techniq",
    "full_text_length": 60061,
    "chunk_length": 1482
  },
  {
    "chunk_id": 1540,
    "paper_filename": "Julie_2021_contrast_enhance_mammography_past_present_and_future.pdf",
    "paper_title": "Julie 2021 Contrast Enhance Mammography Past Present And Future",
    "chunk_index": 1,
    "total_chunks": 59,
    "text_content": "imaging 1. Introduction Contrast-enhanced digital mammography (CEM) is a promising technique that uses iodinated contrast to detect tumor vascularity. The United States Food and Drug Administration (FDA) approved CEM for clinical use as an adjunct to mammography in 2011. The success of mammography, which is the most frequently used breast imaging modality for breast cancer detection, arises from its ability to detect morphologic abnormalities. However, in women with dense breasts who comprise ne",
    "full_text_length": 60061,
    "chunk_length": 1364
  },
  {
    "chunk_id": 1541,
    "paper_filename": "Julie_2021_contrast_enhance_mammography_past_present_and_future.pdf",
    "paper_title": "Julie 2021 Contrast Enhance Mammography Past Present And Future",
    "chunk_index": 2,
    "total_chunks": 59,
    "text_content": "be the most sensitive imaging technique in cancer detection with sensitivity of 96\u201399% [ 3\u20135]. Enhancement on MRI is related to tumor neovascularity, which may be apparent prior to morphologic changes on mammography or Corresponding Author: Victoria L. Mango, mangov@mskcc.org, Postal address: Memorial Sloan Kettering Cancer Center, 300 East 66th Street, New York, NY 10065, USA. HHS Public Access Author manuscript Clin Imaging . Author manuscript; available in PMC 2022 January 01. Published in fi",
    "full_text_length": 60061,
    "chunk_length": 1361
  },
  {
    "chunk_id": 1542,
    "paper_filename": "Julie_2021_contrast_enhance_mammography_past_present_and_future.pdf",
    "paper_title": "Julie 2021 Contrast Enhance Mammography Past Present And Future",
    "chunk_index": 3,
    "total_chunks": 59,
    "text_content": "over time. It was first introduced in 1985 with digital subtraction angiography of the breast [ 7,8]. The goal was to differentiate benign from malignant breast lesions to avoid surgical biopsy. This technique was performed by placing a catheter through an antecubital vein into the superior vena cava and obtaining multiple image exposures as contrast entered the vessels in the breast. However, given the invasive nature of this procedure and initial suboptimal results, its continued use was not p",
    "full_text_length": 60061,
    "chunk_length": 1340
  },
  {
    "chunk_id": 1543,
    "paper_filename": "Julie_2021_contrast_enhance_mammography_past_present_and_future.pdf",
    "paper_title": "Julie 2021 Contrast Enhance Mammography Past Present And Future",
    "chunk_index": 4,
    "total_chunks": 59,
    "text_content": "motion due to the long acquisition time. In addition, only one breast can be examined in one view limiting the ability to localize lesions that are detected. Any additional views of the ipsilateral or contralateral breast requires another dose of contrast. Finally, the breast is in compression during contrast administration, which may restrict blood flow and result in suboptimal tissue enhancement. 2.2 Current Technique CEM using dual-energy energy technique, first described in 2003 by Lewin et.",
    "full_text_length": 60061,
    "chunk_length": 1304
  },
  {
    "chunk_id": 1544,
    "paper_filename": "Julie_2021_contrast_enhance_mammography_past_present_and_future.pdf",
    "paper_title": "Julie 2021 Contrast Enhance Mammography Past Present And Future",
    "chunk_index": 5,
    "total_chunks": 59,
    "text_content": "150 ml) at a rate of 3 ml/sec. Two minutes after completion of injection, the breast is placed into compression and paired low-energy and high-energy images are obtained in the standard craniocaudal and mediolateral oblique views (Fig. 1). Low-energy images are acquired below the K-edge of iodine at a kVp of 28\u201333 and demonstrate only breast tissue, similar to full field digital mammography (FFDM). Studies have shown low-energy images are diagnostically equivalent to FFDM despite presence of int",
    "full_text_length": 60061,
    "chunk_length": 1305
  },
  {
    "chunk_id": 1545,
    "paper_filename": "Julie_2021_contrast_enhance_mammography_past_present_and_future.pdf",
    "paper_title": "Julie 2021 Contrast Enhance Mammography Past Present And Future",
    "chunk_index": 6,
    "total_chunks": 59,
    "text_content": "intensity projection (MIP) images on MRI (Fig. 2). Iodine uptake in the breast is a depiction of tumor vascularity on CEM. The optimal imaging window is 2\u20138 minutes after injection [ 13]. The images do not need to be obtained in a specific order with institutions varying in their practice [ 3]. Contrast remains present for up to 10 minutes, which allows for additional contrast enhanced views to be obtained. Diagnostic views, such as spot compression or magnification, can subsequently be performe",
    "full_text_length": 60061,
    "chunk_length": 1323
  },
  {
    "chunk_id": 1546,
    "paper_filename": "Julie_2021_contrast_enhance_mammography_past_present_and_future.pdf",
    "paper_title": "Julie 2021 Contrast Enhance Mammography Past Present And Future",
    "chunk_index": 7,
    "total_chunks": 59,
    "text_content": "also include an assessment of breast density based on low-energy images and background parenchymal enhancement (BPE) using recombined images. A \u201cmalignancy potential score\u201d (MPS) has been proposed by Tsigginou et al. [ 15] for final assessment reporting. This aggregate score combines the standard mammography BI-RADS score and a CEM score based on a 4-level scale of lesion enhancement ranging from negative to intense enhancement. They found the MPS score had improved diagnostic accuracy in breast",
    "full_text_length": 60061,
    "chunk_length": 1348
  },
  {
    "chunk_id": 1547,
    "paper_filename": "Julie_2021_contrast_enhance_mammography_past_present_and_future.pdf",
    "paper_title": "Julie 2021 Contrast Enhance Mammography Past Present And Future",
    "chunk_index": 8,
    "total_chunks": 59,
    "text_content": "lack of enhancement does not always exclude malignancy. It has been shown that enhancement may be decreased in invasive lobular carcinoma [ 17] and mucinous carcinoma [ 18]. Calcifications may be a manifestation of DCIS without associated enhancement, particularly if low-grade or small. CEM has the advantage of calcification detection on low-energy images, which is not possible with MRI. While the NPV of CEM has been reported to be as high as 93% in the evaluation of suspicious calcifications [ ",
    "full_text_length": 60061,
    "chunk_length": 1297
  },
  {
    "chunk_id": 1548,
    "paper_filename": "Julie_2021_contrast_enhance_mammography_past_present_and_future.pdf",
    "paper_title": "Julie 2021 Contrast Enhance Mammography Past Present And Future",
    "chunk_index": 9,
    "total_chunks": 59,
    "text_content": "biopsy if warranted as there is no CEM-guided biopsy system commercially available in the United States. CEM reports should include a management plan in case a correlate is not found on supplemental imaging. Six-month follow-up CEM can be considered for a low suspicion CEM finding without US or MRI correlate. Alternatively, to guide biopsy, a clip or seed is placed at the Sogani et al. Page 3 Clin Imaging . Author manuscript; available in PMC 2022 January 01. Author Manuscript Author Manuscript ",
    "full_text_length": 60061,
    "chunk_length": 1260
  },
  {
    "chunk_id": 1549,
    "paper_filename": "Julie_2021_contrast_enhance_mammography_past_present_and_future.pdf",
    "paper_title": "Julie 2021 Contrast Enhance Mammography Past Present And Future",
    "chunk_index": 10,
    "total_chunks": 59,
    "text_content": "of normal BPE is essential (Fig. 3). BPE on CEM is similar to MRI, and may lower sensitivity due to obscuration of enhancing lesions. Scheduling of CEM need not be based on timing of the menstrual cycle which appears to have minimal effect on BPE [ 21,22]. Similar to MRI, increased BPE levels on CEM may be an indication of breast cancer risk [ 23] and thus is important to report. Not all enhancing lesions on CEM are malignant, resulting in false-positive findings. Benign enhancing findings may i",
    "full_text_length": 60061,
    "chunk_length": 1313
  },
  {
    "chunk_id": 1550,
    "paper_filename": "Julie_2021_contrast_enhance_mammography_past_present_and_future.pdf",
    "paper_title": "Julie 2021 Contrast Enhance Mammography Past Present And Future",
    "chunk_index": 11,
    "total_chunks": 59,
    "text_content": "of low-energy and high-energy images. Skin contamination from contrast material may occur, which presents as avid enhancement greater than seen with suspicious findings. Skin contamination may be visible even on low-energy images unlike contrast injected intravenously, which may be due to a higher concentration of contrast on skin leading to greater photon attenuation [ 28]. Additional artifacts include axillary line artifact due to use of small compression paddles and air artifact at sites of s",
    "full_text_length": 60061,
    "chunk_length": 1275
  },
  {
    "chunk_id": 1551,
    "paper_filename": "Julie_2021_contrast_enhance_mammography_past_present_and_future.pdf",
    "paper_title": "Julie 2021 Contrast Enhance Mammography Past Present And Future",
    "chunk_index": 12,
    "total_chunks": 59,
    "text_content": "symptoms. They reported presence of enhancement in 75/80 (82%) of cancers. They concluded CEM had better diagnostic accuracy than FFDM alone or FFDM and US. In 2014, Jochelson et al. [ 3] evaluated bilateral CEM in 52 women with known cancer. They found that CEM detected known primary tumors at a rate comparable to MRI, each detecting 50/52 (96%) of the index lesions, which was significantly better than mammography, which detected 42/52 (81%). Comparing CEM and US \u00dffor breast cancer detection, K",
    "full_text_length": 60061,
    "chunk_length": 1291
  },
  {
    "chunk_id": 1552,
    "paper_filename": "Julie_2021_contrast_enhance_mammography_past_present_and_future.pdf",
    "paper_title": "Julie 2021 Contrast Enhance Mammography Past Present And Future",
    "chunk_index": 13,
    "total_chunks": 59,
    "text_content": "A recent meta-analysis by Zhu et al. [ 36] reported a pooled sensitivity and specificity of 89% (95% CI, 88\u201391) and 84% (95% CI, 82\u201385), respectively. The negative predictive value (NPV) of CEM is also high, ranging from 92\u2013100% [ 18,19,32,37,38] with a positive predictive value (PPV) greater than MRI from 93\u201397% [ 3,5]. The available data support CEM as a high-quality alternative examination for patients unable to undergo MRI or for whom MRI is not recommended. With the widespread adoption of d",
    "full_text_length": 60061,
    "chunk_length": 1228
  },
  {
    "chunk_id": 1553,
    "paper_filename": "Julie_2021_contrast_enhance_mammography_past_present_and_future.pdf",
    "paper_title": "Julie 2021 Contrast Enhance Mammography Past Present And Future",
    "chunk_index": 14,
    "total_chunks": 59,
    "text_content": "in performance. Further studies will be necessary to evaluate the utility of CET. 5. Clinical Applications of CEM Clinical applications of CEM include those that are currently accepted for MRI as both techniques are based on tumor enhancement (Table 1). A summary of studies evaluating the performance of CEM in the diagnostic and screening settings are provided in Tables 2 and 3. 5.1 Problem Solving for Inconclusive Findings CEM can be a useful problem-solving tool in the diagnostic setting for p",
    "full_text_length": 60061,
    "chunk_length": 1263
  },
  {
    "chunk_id": 1554,
    "paper_filename": "Julie_2021_contrast_enhance_mammography_past_present_and_future.pdf",
    "paper_title": "Julie 2021 Contrast Enhance Mammography Past Present And Future",
    "chunk_index": 15,
    "total_chunks": 59,
    "text_content": "These results were observed for ten readers in the study regardless of their level of CEM experience, suggesting that CEM interpretation does not have a steep learning curve. Zuley et al. [ 40] showed that CEM may also be valuable by reducing the number of biopsies of low suspicion lesions that do not enhance without compromising cancer detection. They found that the addition of CEM improved PPV of biopsy for low-intermediate suspicion lesions to 20.5% compared to 13.2% on FFDM [ 41]. Therefore,",
    "full_text_length": 60061,
    "chunk_length": 1263
  },
  {
    "chunk_id": 1555,
    "paper_filename": "Julie_2021_contrast_enhance_mammography_past_present_and_future.pdf",
    "paper_title": "Julie 2021 Contrast Enhance Mammography Past Present And Future",
    "chunk_index": 16,
    "total_chunks": 59,
    "text_content": "with area under curve (AUC), 0.93 versus 0.83, respectively. CEM was Sogani et al. Page 5 Clin Imaging . Author manuscript; available in PMC 2022 January 01. Author Manuscript Author Manuscript Author Manuscript Author Manuscript helpful for diagnosis in 75% of the symptomatic cases. Luczynska et al. [ 32] compared performance of CEM, FFDM, and US in 116 symptomatic patients. CEM sensitivity was 100%, significantly higher than FFDM and US. CEM accuracy was also higher at 78% compared to FFDM (69",
    "full_text_length": 60061,
    "chunk_length": 1260
  },
  {
    "chunk_id": 1556,
    "paper_filename": "Julie_2021_contrast_enhance_mammography_past_present_and_future.pdf",
    "paper_title": "Julie 2021 Contrast Enhance Mammography Past Present And Future",
    "chunk_index": 17,
    "total_chunks": 59,
    "text_content": "of disease extent In patients with newly diagnosed breast cancer, additional imaging may be performed to determine disease extent or to identify disease in the contralateral breast (Fig. 6). While the value of supplemental imaging for staging, particularly MRI, remains controversial, CEM has proven to be superior to FFDM with or without US and comparable to or even better than MRI [ 3\u20135,24,42]. Jochelson et al. [ 3] found CEM was less sensitive than MRI for detection of additional ipsilateral di",
    "full_text_length": 60061,
    "chunk_length": 1249
  },
  {
    "chunk_id": 1557,
    "paper_filename": "Julie_2021_contrast_enhance_mammography_past_present_and_future.pdf",
    "paper_title": "Julie 2021 Contrast Enhance Mammography Past Present And Future",
    "chunk_index": 18,
    "total_chunks": 59,
    "text_content": "cancer staging not only consists of identifying multifocal, multicentric, or contralateral disease but also accurate tumor sizing for preoperative planning. Multiple studies have demonstrated CEM provides comparable or better tumor size estimation than MRI [ 4,24,43]. More recently, Patel et al. [ 44] compared tumor size correlation for CEM and FFDM in women with dense and nondense breasts. They found that CEM improved size estimation for both dense and non-dense breasts but the benefit was grea",
    "full_text_length": 60061,
    "chunk_length": 1334
  },
  {
    "chunk_id": 1558,
    "paper_filename": "Julie_2021_contrast_enhance_mammography_past_present_and_future.pdf",
    "paper_title": "Julie 2021 Contrast Enhance Mammography Past Present And Future",
    "chunk_index": 19,
    "total_chunks": 59,
    "text_content": "surgical planning. While MRI is currently the most accurate imaging modality to evaluate response to neoadjuvant therapy in breast cancer patients [ 45,46], initial small studies show CEM may be comparable to MRI for this purpose [47\u201349]. ElSaid et al. [ 50] studied 21 women undergoing neoadjuvant chemotherapy and found CEM had 100% sensitivity and 83% specificity for detecting complete pathologic response. In a larger cohort of 46 women, Iotti et al. [ 47] prospectively compared CEM and MRI, wh",
    "full_text_length": 60061,
    "chunk_length": 1337
  },
  {
    "chunk_id": 1559,
    "paper_filename": "Julie_2021_contrast_enhance_mammography_past_present_and_future.pdf",
    "paper_title": "Julie 2021 Contrast Enhance Mammography Past Present And Future",
    "chunk_index": 20,
    "total_chunks": 59,
    "text_content": "therapy response (Fig. 7). 5.5 Evaluation of the Posttreatment Breast Evaluation of patients after breast cancer treatment can be challenging due to overlap in appearances of benign post-treatment changes and cancer. Posttreatment changes, including parenchymal distortion and asymmetry, calcification, and seroma, may not be easily differentiated from cancer on FFDM, leading to missed cancers, additional imaging, and biopsies. CEM may confirm benign findings with the absence of enhancement in the",
    "full_text_length": 60061,
    "chunk_length": 1351
  },
  {
    "chunk_id": 1560,
    "paper_filename": "Julie_2021_contrast_enhance_mammography_past_present_and_future.pdf",
    "paper_title": "Julie 2021 Contrast Enhance Mammography Past Present And Future",
    "chunk_index": 21,
    "total_chunks": 59,
    "text_content": ", and NPV . False-positive findings on CEM were mostly due to enhancing fat necrosis. False-negative cases on CEM depicted minimal enhancement possibly due to extensive postoperative fibrosis limiting vascular perfusion and visualization of cancers. 5.6 Screening CEM has primarily been used in the diagnostic setting and its role in screening remains to be determined [ 52\u201354]. The use of contrast material and its potential side effects deter the widespread acceptance of CEM in the general populat",
    "full_text_length": 60061,
    "chunk_length": 1316
  },
  {
    "chunk_id": 1561,
    "paper_filename": "Julie_2021_contrast_enhance_mammography_past_present_and_future.pdf",
    "paper_title": "Julie 2021 Contrast Enhance Mammography Past Present And Future",
    "chunk_index": 22,
    "total_chunks": 59,
    "text_content": "contrast deposition in neuronal tissues [ 58]. While the clinical significance is unknown, it raises questions about using contrast-enhanced MRI annually for high-risk screening. Like any screening test, the potential risks of CEM will need to be weighed in relation to the benefits. Several studies have evaluated the use of CEM in screening. A pilot study by Jochelson et al. [ 52] compared MRI to CEM in women at increased risk for breast cancer, including intermediate- and high-risk patients. Th",
    "full_text_length": 60061,
    "chunk_length": 1144
  },
  {
    "chunk_id": 1562,
    "paper_filename": "Julie_2021_contrast_enhance_mammography_past_present_and_future.pdf",
    "paper_title": "Julie 2021 Contrast Enhance Mammography Past Present And Future",
    "chunk_index": 23,
    "total_chunks": 59,
    "text_content": "at 14.9 of 1000 [ 57]. The incremental CDR was 6.7 of 1000. Sorin et al. [ 53] performed a study in 611 intermediate-risk women and observed a CDR of 31 of 1000 women screened with an incremental CDR of 13.1 of 1000.Sogani et al. Page 7 Clin Imaging . Author manuscript; available in PMC 2022 January 01. Author Manuscript Author Manuscript Author Manuscript Author Manuscript As breast density notification laws continue to be adopted in more states, there is growing interest in supplemental screen",
    "full_text_length": 60061,
    "chunk_length": 1210
  },
  {
    "chunk_id": 1563,
    "paper_filename": "Julie_2021_contrast_enhance_mammography_past_present_and_future.pdf",
    "paper_title": "Julie 2021 Contrast Enhance Mammography Past Present And Future",
    "chunk_index": 24,
    "total_chunks": 59,
    "text_content": "leading to a more personalized approach to screening. 6. Advantages of CEM Studies have consistently shown CEM to be superior to FFDM and comparable to MRI in terms of diagnostic accuracy. Thus, CEM may serve as an alternate modality for patients who are unable to undergo MRI due to contraindication or inaccessibility. Common contraindications to MRI, include metallic implants, claustrophobia, and weight limitations. Other reasons to consider adopting CEM into clinical practice include patient c",
    "full_text_length": 60061,
    "chunk_length": 1286
  },
  {
    "chunk_id": 1564,
    "paper_filename": "Julie_2021_contrast_enhance_mammography_past_present_and_future.pdf",
    "paper_title": "Julie 2021 Contrast Enhance Mammography Past Present And Future",
    "chunk_index": 25,
    "total_chunks": 59,
    "text_content": "it also leads to increased throughput and less radiologist interpretation time. Even as abbreviated MRI protocols are becoming common, CEM can provide similar information more efficiently when performed at the time of the patient\u2019s initial mammogram. CEM is less costly than a full or abbreviated MRI protocol with the total cost approximately four times less than a full screening MRI [ 6]. The costs and space and safety requirements of an MRI magnet limit its widespread availability. On the other",
    "full_text_length": 60061,
    "chunk_length": 1261
  },
  {
    "chunk_id": 1565,
    "paper_filename": "Julie_2021_contrast_enhance_mammography_past_present_and_future.pdf",
    "paper_title": "Julie 2021 Contrast Enhance Mammography Past Present And Future",
    "chunk_index": 26,
    "total_chunks": 59,
    "text_content": "to be the result of contrast collecting inside ducts by diffusion [ 61]. The rate of diffusion is time-dependent, so the longer time delay between contrast injection and CEM exposure compared to MRI may result in stronger enhancement and better visualization of DCIS on CEM [ 4]. The differences in structure and mechanism of gadolinium and iodinated contrast agents may also contribute to variability in visibility of lesions on MRI and CEM [ 4]. 7. Disadvantages of CEM The greatest disadvantage of",
    "full_text_length": 60061,
    "chunk_length": 1318
  },
  {
    "chunk_id": 1566,
    "paper_filename": "Julie_2021_contrast_enhance_mammography_past_present_and_future.pdf",
    "paper_title": "Julie 2021 Contrast Enhance Mammography Past Present And Future",
    "chunk_index": 27,
    "total_chunks": 59,
    "text_content": "Manuscript that contrast agents are relatively safe. Nonetheless, staff members must have adequate training on appropriate contrast administration and management of reactions. The radiation dose from CEM is higher than FFDM and DBT. Two separate exposures are performed per imaging position. The reported dose estimates for CEM vary from 20\u2013 80% higher than that of FFDM [ 24,64] and 20\u201342% higher than that of DBT [ 65,66]. Nonetheless, the radiation dose from CEM remains within the regulatory rang",
    "full_text_length": 60061,
    "chunk_length": 1355
  },
  {
    "chunk_id": 1567,
    "paper_filename": "Julie_2021_contrast_enhance_mammography_past_present_and_future.pdf",
    "paper_title": "Julie 2021 Contrast Enhance Mammography Past Present And Future",
    "chunk_index": 28,
    "total_chunks": 59,
    "text_content": "technologist has improved the efficiency of performing CEM in our practice. A CEM-guided biopsy device is not yet commercially available in the United States, limiting efficiency of biopsy of CEM-only findings. As discussed previously, biopsy must currently be performed using an alternative imaging modality or two-procedure method with CEM-guided clip or seed placement followed by stereotactic-guided or surgical biopsy, respectively. False-positive and false-negative findings may occur on CEM si",
    "full_text_length": 60061,
    "chunk_length": 1326
  },
  {
    "chunk_id": 1568,
    "paper_filename": "Julie_2021_contrast_enhance_mammography_past_present_and_future.pdf",
    "paper_title": "Julie 2021 Contrast Enhance Mammography Past Present And Future",
    "chunk_index": 29,
    "total_chunks": 59,
    "text_content": "Research Foundation, and GE Healthcare announced their plan for the Contrast Enhanced Mammography Imaging Screening Trial (CMIST). This large multicenter trial will compare the performance of CEM to a combination of DBT and whole breast US in screening women with dense breasts. As the use of CEM is increasing in clinical practice, the need for a CEM-guided biopsy device has become apparent. In 2019, GE Healthcare revealed its CEM-guided biopsy system called Serena Bright. While FDA clearance in ",
    "full_text_length": 60061,
    "chunk_length": 1263
  },
  {
    "chunk_id": 1569,
    "paper_filename": "Julie_2021_contrast_enhance_mammography_past_present_and_future.pdf",
    "paper_title": "Julie 2021 Contrast Enhance Mammography Past Present And Future",
    "chunk_index": 30,
    "total_chunks": 59,
    "text_content": "used to characterize tumors and predict outcomes in a non-invasive manner. While MRI has been the primary modality studied Sogani et al. Page 9 Clin Imaging . Author manuscript; available in PMC 2022 January 01. Author Manuscript Author Manuscript Author Manuscript Author Manuscript in radiomics research in breast imaging, CEM may also have a role. An initial study in 2019 by Marino et al. [ 70] demonstrated radiomics analysis using CEM has the potential for differentiation of cancers based on l",
    "full_text_length": 60061,
    "chunk_length": 1319
  },
  {
    "chunk_id": 1570,
    "paper_filename": "Julie_2021_contrast_enhance_mammography_past_present_and_future.pdf",
    "paper_title": "Julie 2021 Contrast Enhance Mammography Past Present And Future",
    "chunk_index": 31,
    "total_chunks": 59,
    "text_content": "is superior to conventional mammography and similar to MRI with evidence of fewer false-positive findings. Compared to MRI, CEM has advantages of low cost, speed, and accessibility and is better tolerated by patients. As the popularity of CEM continues to grow, additional larger studies will be needed to validate its role as an acceptable contrast-enhanced alternative to MRI in the diagnostic and screening settings. References [1]. Sprague BL, Gangnon RE, Burt V , Trentham-Dietz A, Hampton JM, W",
    "full_text_length": 60061,
    "chunk_length": 1427
  },
  {
    "chunk_id": 1571,
    "paper_filename": "Julie_2021_contrast_enhance_mammography_past_present_and_future.pdf",
    "paper_title": "Julie 2021 Contrast Enhance Mammography Past Present And Future",
    "chunk_index": 32,
    "total_chunks": 59,
    "text_content": "et al. Bilateral contrast-enhanced dual-energy digital mammography: feasibility and comparison with conventional digital mammography and MR imaging in women with known breast carcinoma. Radiology 2013;266:743\u201351. 10.1148/radiol.12121084. [PubMed: 23220903] [4]. Fallenberg EM, Dromain C, Diekmann F, Engelken F, Krohn M, Singh JM, et al. Contrast\u00ad enhanced spectral mammography versus MRI: Initial results in the detection of breast cancer and assessment of tumour size. Eur Radiol 2014;24:256\u201364. 10",
    "full_text_length": 60061,
    "chunk_length": 1513
  },
  {
    "chunk_id": 1572,
    "paper_filename": "Julie_2021_contrast_enhance_mammography_past_present_and_future.pdf",
    "paper_title": "Julie 2021 Contrast Enhance Mammography Past Present And Future",
    "chunk_index": 33,
    "total_chunks": 59,
    "text_content": "digital angiography. Work in progress. Radiology 1985;155:65\u201368. 10.1097/00004424-198708000-00017. [PubMed: 3883425] [8]. Watt AC, Ackerman LV , Shetty PC, Burke M, Flynn M, Grodsinsky C, Fine G, Wilderman S. Differentiation between benign and malignant disease of the breast using digital subtraction angiography of the breast. Cancer 1985;56:1287\u20131292. [PubMed: 3896454] [9]. Jong RA, Yaffe MJ, Skarpathiotakis M, Shumak RS, Danjoux NM, Gunesekara A, et al. Contrast-enhanced digital mammography: i",
    "full_text_length": 60061,
    "chunk_length": 1548
  },
  {
    "chunk_id": 1573,
    "paper_filename": "Julie_2021_contrast_enhance_mammography_past_present_and_future.pdf",
    "paper_title": "Julie 2021 Contrast Enhance Mammography Past Present And Future",
    "chunk_index": 34,
    "total_chunks": 59,
    "text_content": "MA, Jochelson MS, Dershaw DD, Sung JS, Hughes MC, Zheng J, et al. Low energy mammogram obtained in contrast-enhanced digital mammography (CEDM) is comparable to routine full-field digital mammography (FFDM). Eur J Radiol 2014;83:1350\u20135. 10.1016/ j.ejrad.2014.05.015. [PubMed: 24932846] [13]. Smith A, Ph D. The Principles of Contrast Mammography 2014:8. [14]. Kamal RM, Helal MH, Mansour SM, Haggag MA, Nada OM, Farahat IG, et al. Can we apply the MRI BI-RADS lexicon morphology descriptors on contra",
    "full_text_length": 60061,
    "chunk_length": 1380
  },
  {
    "chunk_id": 1574,
    "paper_filename": "Julie_2021_contrast_enhance_mammography_past_present_and_future.pdf",
    "paper_title": "Julie 2021 Contrast Enhance Mammography Past Present And Future",
    "chunk_index": 35,
    "total_chunks": 59,
    "text_content": "on histological results. Med Sci Monit 2016;22:3886\u2013 93. 10.12659/MSM.900371. [PubMed: 27768681] [17]. van Nijnatten TJ, Jochelson MS, Pinker K, Keating DM, Sung JS, Morrow M, et al. Differences in degree of lesion enhancement on CEM between ILC and IDC. BJR|Open 2019;1:20180046. 10.1259/bjro.20180046. [PubMed: 33178931] [18]. Lalji UC, Houben IPL, Prevos R, Gommers S, van Goethem M, Vanwetswinkel S, et al. Contrast\u00ad enhanced spectral mammography in recalls from the Dutch breast cancer screening",
    "full_text_length": 60061,
    "chunk_length": 1520
  },
  {
    "chunk_id": 1575,
    "paper_filename": "Julie_2021_contrast_enhance_mammography_past_present_and_future.pdf",
    "paper_title": "Julie 2021 Contrast Enhance Mammography Past Present And Future",
    "chunk_index": 36,
    "total_chunks": 59,
    "text_content": "Morris EA, Kaplan JB, D\u2019Alessio D, Goldman D, Moskowitz CS, et al. Comparison of background parenchymal enhancement at contrast-enhanced spectral mammography and breast MR imaging. Radiology 2017;282:63\u201373. 10.1148/radiol.2016160284. [PubMed: 27379544] [22]. Savaridas SL, Taylor DB, Gunawardana D, Phillips M. Could parenchymal enhancement on contrast-enhanced spectral mammography (CESM) represent a new breast cancer risk factor? Correlation with known radiology risk factors. Clin Radiol 2017;72:",
    "full_text_length": 60061,
    "chunk_length": 1586
  },
  {
    "chunk_id": 1576,
    "paper_filename": "Julie_2021_contrast_enhance_mammography_past_present_and_future.pdf",
    "paper_title": "Julie 2021 Contrast Enhance Mammography Past Present And Future",
    "chunk_index": 37,
    "total_chunks": 59,
    "text_content": "spectral mammography interpretation: artefacts lexicon. Clin Radiol 2016;71:450\u20137. 10.1016/j.crad.2016.01.012. [PubMed: 26897335] [26]. Bhimani C, Li L, Liao L, Roth RG, Tinney E, Germaine P. Contrast-enhanced spectral mammography: modality-specific artifacts and other factors which may interfere with image quality. Acad Radiol 2017;24:89\u201394. 10.1016/j.acra.2016.08.024. [PubMed: 27765597] [27]. Nori J, Gill MK, Vignoli C, Bicchierai G, De Benedetto D, Di Naro F, et al. Artefacts in contrast enha",
    "full_text_length": 60061,
    "chunk_length": 1537
  },
  {
    "chunk_id": 1577,
    "paper_filename": "Julie_2021_contrast_enhance_mammography_past_present_and_future.pdf",
    "paper_title": "Julie 2021 Contrast Enhance Mammography Past Present And Future",
    "chunk_index": 38,
    "total_chunks": 59,
    "text_content": "et al. Utility of routine use of breast ultrasound following contrast-enhanced spectral mammography. Clin Radiol 2018;73:908.e11\u2013908.e16. 10.1016/j.crad.2018.05.031. [30]. Mori M, Akashi-Tanaka S, Suzuki S, Daniels MI, Watanabe C, Hirose M, et al. Diagnostic accuracy of contrast-enhanced spectral mammography in comparison to conventional full-field digital mammography in a population of women with dense breasts. Breast Cancer 2017;24:104\u2013 10. 10.1007/s12282-016-0681-8. [PubMed: 26942415] [31]. C",
    "full_text_length": 60061,
    "chunk_length": 1584
  },
  {
    "chunk_id": 1578,
    "paper_filename": "Julie_2021_contrast_enhance_mammography_past_present_and_future.pdf",
    "paper_title": "Julie 2021 Contrast Enhance Mammography Past Present And Future",
    "chunk_index": 39,
    "total_chunks": 59,
    "text_content": "referred from the breast cancer screening programme. Eur Radiol 2014;24:1668\u201376. 10.1007/s00330-014-3154-5. [PubMed: 24696228] [34]. Tennant SL, James JJ, Cornford EJ, Chen Y , Burrell HC, Hamilton LJ, et al. Contrast-enhanced spectral mammography improves diagnostic accuracy in the symptomatic setting. Clin Radiol 2016;71:1148\u201355. 10.1016/j.crad.2016.05.009. [PubMed: 27296475] [35]. Luczy\u0144ska E, Heinze-Paluchowska S, Dyczek S, Blecharz P, Rys J, Reinfuss M. Contrast-enhanced spectral mammograph",
    "full_text_length": 60061,
    "chunk_length": 1578
  },
  {
    "chunk_id": 1579,
    "paper_filename": "Julie_2021_contrast_enhance_mammography_past_present_and_future.pdf",
    "paper_title": "Julie 2021 Contrast Enhance Mammography Past Present And Future",
    "chunk_index": 40,
    "total_chunks": 59,
    "text_content": ", Faermann R, Yagil Y , Shalmon A, Gotlieb M, Halshtok-Neiman O, et al. Contrast\u00ad enhanced spectral mammography (CESM) in women presenting with palpable breast findings. Clin Imaging 2020;61:99\u2013105. 10.1016/j.clinimag.2020.01.019. [PubMed: 32014818] [39]. Chou CP, Lewin JM, Chiang CL, Hung BH, Yang TL, Huang JS, et al. Clinical evaluation of contrast-enhanced digital mammography and contrast enhanced tomosynthesis - comparison to contrast-enhanced breast MRI. Eur J Radiol 2015;84:2501\u20138. 10.1016",
    "full_text_length": 60061,
    "chunk_length": 1515
  },
  {
    "chunk_id": 1580,
    "paper_filename": "Julie_2021_contrast_enhance_mammography_past_present_and_future.pdf",
    "paper_title": "Julie 2021 Contrast Enhance Mammography Past Present And Future",
    "chunk_index": 41,
    "total_chunks": 59,
    "text_content": "F, Renz DM, Amer H, Ingold-Heppner B, et al. Contrast\u00ad enhanced spectral mammography: does mammography provide additional clinical benefits or can some radiation exposure be avoided? Breast Cancer Res Treat 2014;146:371\u201381. 10.1007/ s10549-014-3023-6. [PubMed: 24986697] Sogani et al. Page 12 Clin Imaging . Author manuscript; available in PMC 2022 January 01. Author Manuscript Author Manuscript Author Manuscript Author Manuscript [43]. Lobbes MBI, Lalji UC, Nelemans PJ, Houben I, Smidt ML, Heuts ",
    "full_text_length": 60061,
    "chunk_length": 1456
  },
  {
    "chunk_id": 1581,
    "paper_filename": "Julie_2021_contrast_enhance_mammography_past_present_and_future.pdf",
    "paper_title": "Julie 2021 Contrast Enhance Mammography Past Present And Future",
    "chunk_index": 42,
    "total_chunks": 59,
    "text_content": "detection of residual breast cancer after neoadjuvant chemotherapy. Am J Roentgenol 2003;181:1275\u201382. 10.2214/ajr.181.5.1811275. [PubMed: 14573420] [46]. Marinovich ML, Macaskill P, Irwig L, Sardanelli F, Mamounas E, von Minckwitz G, et al. Agreement between MRI and pathologic breast tumor size after neoadjuvant chemotherapy, and comparison with alternative tests: Individual patient data meta-analysis. BMC Cancer 2015;15:1\u2013 12. 10.1186/s12885-015-1664-4. [PubMed: 25971837] [47]. Iotti V , Ravaio",
    "full_text_length": 60061,
    "chunk_length": 1537
  },
  {
    "chunk_id": 1582,
    "paper_filename": "Julie_2021_contrast_enhance_mammography_past_present_and_future.pdf",
    "paper_title": "Julie 2021 Contrast Enhance Mammography Past Present And Future",
    "chunk_index": 43,
    "total_chunks": 59,
    "text_content": "spectral mammography is comparable to MRI in the assessment of residual breast cancer following neoadjuvant systemic therapy. Ann Surg Oncol 2018;25:1350\u20136. 10.1245/ s10434-018-6413-x. [PubMed: 29516362] [50]. ElSaid NAE, Mahmoud HGM, Salama A, Nabil M, ElDesouky ED. Role of contrast enhanced spectral mammography in predicting pathological response of locally advanced breast cancer post neo-adjuvant chemotherapy. Egypt J Radiol Nucl Med 2017;48:519\u201327. 10.1016/ j.ejrnm.2017.03.022. [51]. Helal M",
    "full_text_length": 60061,
    "chunk_length": 1437
  },
  {
    "chunk_id": 1583,
    "paper_filename": "Julie_2021_contrast_enhance_mammography_past_present_and_future.pdf",
    "paper_title": "Julie 2021 Contrast Enhance Mammography Past Present And Future",
    "chunk_index": 44,
    "total_chunks": 59,
    "text_content": "et al. Contrast-enhanced spectral mammography in women with intermediate breast cancer risk and dense breasts 2018:1\u2013 8. 10.2214/AJR.17.19355. [54]. Sung JS, Lebron L, Keating D, D\u2019Alessio D, Comstock CE, Lee CH, et al. Performance of dual-energy contrast-enhanced digital mammography for screening women at increased risk of breast cancer. Radiology 2019;293:81\u20138. 10.1148/radiol.2019182660. [PubMed: 31453765] [55]. Kuhl CK, Schmutzler RK, Leutner CC, Kempe A, Wardelmann E, Hocke A, et al. Breast ",
    "full_text_length": 60061,
    "chunk_length": 1412
  },
  {
    "chunk_id": 1584,
    "paper_filename": "Julie_2021_contrast_enhance_mammography_past_present_and_future.pdf",
    "paper_title": "Julie 2021 Contrast Enhance Mammography Past Present And Future",
    "chunk_index": 45,
    "total_chunks": 59,
    "text_content": "H, K\u00f6nig R, et al. Prospective multicenter cohort study to refine management recommendations for women at elevated familial risk of breast cancer: the EV A trial. J Clin Oncol 2010;28:1450\u20137. 10.1200/JCO.2009.23.0839. [PubMed: 20177029] Sogani et al. Page 13 Clin Imaging . Author manuscript; available in PMC 2022 January 01. Author Manuscript Author Manuscript Author Manuscript Author Manuscript [58]. McDonald RJ, McDonald JS, Kallmes DF, Jentoft ME, Murray DL, Thielen KR, et al. Gadolinium depo",
    "full_text_length": 60061,
    "chunk_length": 1490
  },
  {
    "chunk_id": 1585,
    "paper_filename": "Julie_2021_contrast_enhance_mammography_past_present_and_future.pdf",
    "paper_title": "Julie 2021 Contrast Enhance Mammography Past Present And Future",
    "chunk_index": 46,
    "total_chunks": 59,
    "text_content": "[PubMed: 28107737] [61]. Fan X, V ogt S, Krausz T, Newstead GM, Karczmar GS. Ductal carcinoma in situ: x-ray fluorescence microscopy and dynamic contrast-enhanced MR imaging reveal gadolinium uptake within neoplastic mammary ducts in a murine model 2009;253:399\u2013406. 10.1148/ radiol.2533082026/-/DC1. [62]. Houben IPL, Van de V oorde P, Jeukens CRLPN, Wildberger JE, Kooreman LF, Smidt ML, et al. Contrast-enhanced spectral mammography as work-up tool in patients recalled from breast cancer screenin",
    "full_text_length": 60061,
    "chunk_length": 1479
  },
  {
    "chunk_id": 1586,
    "paper_filename": "Julie_2021_contrast_enhance_mammography_past_present_and_future.pdf",
    "paper_title": "Julie 2021 Contrast Enhance Mammography Past Present And Future",
    "chunk_index": 47,
    "total_chunks": 59,
    "text_content": "[65]. James JR, Pavlicek W, Hanson JA, Boltz TF, Patel BK. Breast radiation dose with CESM compared with 2D FFDM and 3D tomosynthesis mammography. Am J Roentgenol 2017;208:362\u201372. 10.2214/AJR.16.16743. [PubMed: 28112559] [66]. Phillips J, Mihai G, Hassonjee SE, Raj SD, Palmer MR, Brook A, et al. Comparative dose of contrast-enhanced spectral mammography (CESM), digital mammography, and digital breast tomosynthesis. Am J Roentgenol 2018;211:839\u201346. 10.2214/AJR.17.19036. [PubMed: 30063367] [67]. P",
    "full_text_length": 60061,
    "chunk_length": 1589
  },
  {
    "chunk_id": 1587,
    "paper_filename": "Julie_2021_contrast_enhance_mammography_past_present_and_future.pdf",
    "paper_title": "Julie 2021 Contrast Enhance Mammography Past Present And Future",
    "chunk_index": 48,
    "total_chunks": 59,
    "text_content": "D, Morris EA, et al. Contrast-enhanced mammography and radiomics analysis for noninvasive breast cancer characterization: initial results. Mol Imaging Biol 2019. 10.1007/s11307-019-01423-5.Sogani et al. Page 14 Clin Imaging . Author manuscript; available in PMC 2022 January 01. Author Manuscript Author Manuscript Author Manuscript Author Manuscript Highlights: \u2022 CEM is an emerging imaging technique for breast cancer detection. \u2022 CEM has primarily been used in the diagnostic setting. \u2022 CEM may ha",
    "full_text_length": 60061,
    "chunk_length": 1415
  },
  {
    "chunk_id": 1588,
    "paper_filename": "Julie_2021_contrast_enhance_mammography_past_present_and_future.pdf",
    "paper_title": "Julie 2021 Contrast Enhance Mammography Past Present And Future",
    "chunk_index": 49,
    "total_chunks": 59,
    "text_content": "the four standard mammographic views within the next six minutes.Sogani et al. Page 16 Clin Imaging . Author manuscript; available in PMC 2022 January 01. Author Manuscript Author Manuscript Author Manuscript Author Manuscript Fig. 2. Paired low-energy (A) and high-energy (B) craniocaudal images are acquired after intravenous contrast agent administration. Post-processing generates a recombined (C) image, which highlights areas of iodine uptake. No suspicious findings are present on the recombin",
    "full_text_length": 60061,
    "chunk_length": 1379
  },
  {
    "chunk_id": 1589,
    "paper_filename": "Julie_2021_contrast_enhance_mammography_past_present_and_future.pdf",
    "paper_title": "Julie 2021 Contrast Enhance Mammography Past Present And Future",
    "chunk_index": 50,
    "total_chunks": 59,
    "text_content": "4. Common artifacts seen on CEM. Rim artifact (A) and skin line artifact (B) from non\u00ad uniform scatter radiation in the breast and skin. Ripple artifact (C) from motion. Skin contamination artifact (D) from contrast spillage.Sogani et al. Page 19 Clin Imaging . Author manuscript; available in PMC 2022 January 01. Author Manuscript Author Manuscript Author Manuscript Author Manuscript Fig. 5. 49-year-old with remote history of phyllodes tumor with new palpable left breast mass. MLO low-energy ima",
    "full_text_length": 60061,
    "chunk_length": 1336
  },
  {
    "chunk_id": 1590,
    "paper_filename": "Julie_2021_contrast_enhance_mammography_past_present_and_future.pdf",
    "paper_title": "Julie 2021 Contrast Enhance Mammography Past Present And Future",
    "chunk_index": 51,
    "total_chunks": 59,
    "text_content": "areas of dense parenchymal breast tissue.Sogani et al. Page 20 Clin Imaging . Author manuscript; available in PMC 2022 January 01. Author Manuscript Author Manuscript Author Manuscript Author Manuscript Fig. 6. 59-year-old women with history of LCIS with suspicious left breast calcifications on screening CEM. MLO low-energy image (A) and 2D ML magnification view (B) shows segmental fine pleomorphic calcifications in the lower breast (circle). MLO recombined image (C) demonstrates two enhancing m",
    "full_text_length": 60061,
    "chunk_length": 1435
  },
  {
    "chunk_id": 1591,
    "paper_filename": "Julie_2021_contrast_enhance_mammography_past_present_and_future.pdf",
    "paper_title": "Julie 2021 Contrast Enhance Mammography Past Present And Future",
    "chunk_index": 52,
    "total_chunks": 59,
    "text_content": "breast mass status post ultrasound-guided biopsy yielding invasive ductal carcinoma and subsequently treated with neoadjuvant chemotherapy. Pre\u00ad treatment digital mammogram (A) demonstrates a high density irregular mass in the posterior breast corresponding to the palpable finding and known cancer. Post-treatment low-energy image (B) performed six months later shows decreased mass surrounding the biopsy marker suggestive of residual disease. Post-treatment recombined image (C) shows no abnormal ",
    "full_text_length": 60061,
    "chunk_length": 1462
  },
  {
    "chunk_id": 1592,
    "paper_filename": "Julie_2021_contrast_enhance_mammography_past_present_and_future.pdf",
    "paper_title": "Julie 2021 Contrast Enhance Mammography Past Present And Future",
    "chunk_index": 53,
    "total_chunks": 59,
    "text_content": "to the lumpectomy site (arrow). Targeted ultrasound was negative (not shown). Sagittal MRI postcontrast subtraction image (C) shows an enhancing mass, which correlates to the CEM finding. MRI-guided biopsy yielded invasive lobular carcinoma recurrence.Sogani et al. Page 23 Clin Imaging . Author manuscript; available in PMC 2022 January 01. Author Manuscript Author Manuscript Author Manuscript Author Manuscript Fig. 9. 47-year-old woman with family history of breast cancer presents for screening ",
    "full_text_length": 60061,
    "chunk_length": 1394
  },
  {
    "chunk_id": 1593,
    "paper_filename": "Julie_2021_contrast_enhance_mammography_past_present_and_future.pdf",
    "paper_title": "Julie 2021 Contrast Enhance Mammography Past Present And Future",
    "chunk_index": 54,
    "total_chunks": 59,
    "text_content": "palpable mass with false-negative finding on CEM. MLO low-energy (A) and recombined (B) images demonstrate no suspicious finding. Ultrasound targeted to site of palpable concern was also negative (not shown). Given persistent palpable concern, MRI was performed. Sagittal MRI postcontrast subtraction image (C) shows an enhancing focus at the site of palpable concern in the retroareolar region. MRI-guided biopsy yielded ductal carcinoma in situ. CEM may not detect all cancers, particularly if they",
    "full_text_length": 60061,
    "chunk_length": 1452
  },
  {
    "chunk_id": 1594,
    "paper_filename": "Julie_2021_contrast_enhance_mammography_past_present_and_future.pdf",
    "paper_title": "Julie 2021 Contrast Enhance Mammography Past Present And Future",
    "chunk_index": 55,
    "total_chunks": 59,
    "text_content": "high-risk patients Clin Imaging . Author manuscript; available in PMC 2022 January 01. Author Manuscript Author Manuscript Author Manuscript Author ManuscriptSogani et al. Page 27 Table 2: Evaluation of CEM in the Diagnostic Setting Clinical IndicationStudy and Year No. of WomenModalities ComparedSensitivity (%)Specificity (%)PPV (%)NPV (%) Problem Solving Lobbes et al, 2014 ( 33) 113 CEM vs FFDM 100 88 76 100 97 42 40 97 Lalji et al, 2016 ( 18) 199 CEM vs LE 97 70 58 98 93 36 39 93 Symptomatic ",
    "full_text_length": 60061,
    "chunk_length": 954
  },
  {
    "chunk_id": 1595,
    "paper_filename": "Julie_2021_contrast_enhance_mammography_past_present_and_future.pdf",
    "paper_title": "Julie 2021 Contrast Enhance Mammography Past Present And Future",
    "chunk_index": 56,
    "total_chunks": 59,
    "text_content": "et al, 2011 (24)120 CEM vs FFDM vs FFDM + US93 56 73 85 80 50 67 66 94 39 67 82 Jochelson et al, 2013 ( 3)52 CEM vs FFDM vs MRI96 97 81 - - - 96 85 Fallenberg et al, 2014 (4)80 CEM vs FFDM vs MRI100 83 - - - 97 Lee-Felker et al, 2017 (5)52 CEM vs MRI 94 17 93 20 99 4 60 67 Neoadjuvant therapy responseElSaid et al, 2017 ( 50) 21 CEM 100 83 - - Iotti et al, 2017 ( 47) 46 CEM vs MRI 100 84 57 100 87 60 32 96 Barra et al, 2018 ( 48) 33 CEM vs FFDM vs MRI76 88 95 54 76 63 86 45 92 75 92 75 Patel et a",
    "full_text_length": 60061,
    "chunk_length": 897
  },
  {
    "chunk_id": 1596,
    "paper_filename": "Julie_2021_contrast_enhance_mammography_past_present_and_future.pdf",
    "paper_title": "Julie 2021 Contrast Enhance Mammography Past Present And Future",
    "chunk_index": 57,
    "total_chunks": 59,
    "text_content": "51) 76 CEM vs FFDM 91 75 78 90 50 22 38 32 Note.\u2013CEM = contrast-enhanced mammography, FFDM = full-field digital mammography, LE = low-energy imaging, MRI = magnetic resonance imaging, NPV = negative predictive value, PPV = positive predictive value. Clin Imaging . Author manuscript; available in PMC 2022 January 01. Author Manuscript Author Manuscript Author Manuscript Author ManuscriptSogani et al. Page 28 Table 3: Evaluation of CEM in the Screening Setting Study and Year No. of Women Modalitie",
    "full_text_length": 60061,
    "chunk_length": 1041
  },
  {
    "chunk_id": 1597,
    "paper_filename": "Julie_2021_contrast_enhance_mammography_past_present_and_future.pdf",
    "paper_title": "Julie 2021 Contrast Enhance Mammography Past Present And Future",
    "chunk_index": 58,
    "total_chunks": 59,
    "text_content": "25 99 Note.\u2013CEM = contrast-enhanced mammography, LE = low-energy imaging, MRI = magnetic resonance imaging, NPV = negative predictive value, PPV = positive predictive value. US = ultrasound. Clin Imaging . Author manuscript; available in PMC 2022 January 01.",
    "full_text_length": 60061,
    "chunk_length": 258
  },
  {
    "chunk_id": 1598,
    "paper_filename": "juntongdu_2024_MultiBCD_ mammography_chat.pdf",
    "paper_title": "Juntongdu 2024 Multibcd  Mammography Chat",
    "chunk_index": 0,
    "total_chunks": 42,
    "text_content": "Highlights MultiBCD: A Multimodal model that simulates the human diagnostic process for automated Breast Cancer Detection JuntongDu,ZihanZhang,WeiyangTao \u2022Introducing MultiBCD, a multimodal model that emulates the human diagnostic process for the automated detection of breast cancer. \u2022Integrating an image classification framework with the capabilities of GPT-4 to enhance diagnostic decision-making. \u2022Demonstrating that MultiBCD achieves superior diagnostic accuracy and efficiency. \u2022Highlighting i",
    "full_text_length": 51340,
    "chunk_length": 1830
  },
  {
    "chunk_id": 1599,
    "paper_filename": "juntongdu_2024_MultiBCD_ mammography_chat.pdf",
    "paper_title": "Juntongdu 2024 Multibcd  Mammography Chat",
    "chunk_index": 1,
    "total_chunks": 42,
    "text_content": "cancer detection, our study introduces MultiBCD, a multimodal model that emulates the human diagnostic process for breast cancer detection. Integrating an image classifierwithGPT-4,itevaluatesmammographicimagesalongsidepatientcomplaints.Themodel\u2019s dual-head autoencoder efficiently processes image data, eliminating the need for manual lesion delineation, while GPT-4 extracts critical information from patient narratives. MultiBCD demonstrates superior diagnostic accuracy and efficiency, achieving ",
    "full_text_length": 51340,
    "chunk_length": 1609
  },
  {
    "chunk_id": 1600,
    "paper_filename": "juntongdu_2024_MultiBCD_ mammography_chat.pdf",
    "paper_title": "Juntongdu 2024 Multibcd  Mammography Chat",
    "chunk_index": 2,
    "total_chunks": 42,
    "text_content": "new cases has been increasing year after year, and at the same time, breast cancer is one of the mostcommoncancersleadingtowomen\u2019smortalities[1,2]. There are many high-risk elements for breast cancer [3, 4], including family and genetic history of breast cancer, age factor,earlymenarcheorlatemenopause,earlychildbearing age or first delivery age greater than 30 years old, obesity, unhealthydietaryhabitssuchasalcoholismandhigh-fat,and so forth [5, 6, 7]. Breast cancer is a heterogeneous disease wi",
    "full_text_length": 51340,
    "chunk_length": 1623
  },
  {
    "chunk_id": 1601,
    "paper_filename": "juntongdu_2024_MultiBCD_ mammography_chat.pdf",
    "paper_title": "Juntongdu 2024 Multibcd  Mammography Chat",
    "chunk_index": 3,
    "total_chunks": 42,
    "text_content": "and need to be diagnosed with the support of imaging, \u2217Corresponding author 13936610448@163.com ( JuntongDu); zihanzhang@ir.hit.edu.cn ( ZihanZhang); twysci@163.com ( WeiyangTao) https://github.com/DU0122 ( JuntongDu); https://github.com/zhangzihan-is-good ( ZihanZhang) ORCID(s):0009-0001-4381-5280 ( JuntongDu); 0009-0006-7530-9127 ( ZihanZhang); 0000-0003-1107-5975 ( WeiyangTao)such as mammography (MG), ultrasound and magnetic res- onance imaging (MRI) [12, 13]. Among them, mammog- raphy has be",
    "full_text_length": 51340,
    "chunk_length": 1468
  },
  {
    "chunk_id": 1602,
    "paper_filename": "juntongdu_2024_MultiBCD_ mammography_chat.pdf",
    "paper_title": "Juntongdu 2024 Multibcd  Mammography Chat",
    "chunk_index": 4,
    "total_chunks": 42,
    "text_content": "[21, 19, 22]. The diagnostic sensitivity for breast cancer is 82% - 89% and the specificity is over 87% [23].Butwhileensuringtheaccuracyofthemachine,human interpretation and judgment of the mammogram has also become an influential factor in the breast cancer screening rate. Mammograms usually need to be read by two imaging physicians to ensure the accuracy of the judgments and to improve sensitivity [24, 25], but this ensures that the numberofdoctorsissufficient.Thus,theadventofartificial intell",
    "full_text_length": 51340,
    "chunk_length": 1603
  },
  {
    "chunk_id": 1603,
    "paper_filename": "juntongdu_2024_MultiBCD_ mammography_chat.pdf",
    "paper_title": "Juntongdu 2024 Multibcd  Mammography Chat",
    "chunk_index": 5,
    "total_chunks": 42,
    "text_content": "not been peer reviewed. Electronic copy available at: https://ssrn.com/abstract=4950797Preprint not peer reviewed MultiBCD: A Multimodal model that simulates the human diagnostic process for automated Breast Cancer Detection MutiBCDThe decision -making process of human physicians Key image information (such as lesion location, characteristics, etc.)Textual information (such as the absence of pain, etc.)Physician's diagnosis of benign/malignant Complaint Mammographic images MutiBCD's diagnosis of",
    "full_text_length": 51340,
    "chunk_length": 1809
  },
  {
    "chunk_id": 1604,
    "paper_filename": "juntongdu_2024_MultiBCD_ mammography_chat.pdf",
    "paper_title": "Juntongdu 2024 Multibcd  Mammography Chat",
    "chunk_index": 6,
    "total_chunks": 42,
    "text_content": "Regardingthecomputervisionmodelsthathandlemam- mographic images, the better-performing models tradition- ally require manual demarcation of lesion locations, fol- lowed by benign-malignant classification based on radio- graphic features, or involved image segmentation using an- notated data [37, 38, 39, 40]. This approach reduces the difficulty of learning lesion features from mammographic images but increases the workload for imaging physicians. Our study addresses this issue with a modified au",
    "full_text_length": 51340,
    "chunk_length": 1565
  },
  {
    "chunk_id": 1605,
    "paper_filename": "juntongdu_2024_MultiBCD_ mammography_chat.pdf",
    "paper_title": "Juntongdu 2024 Multibcd  Mammography Chat",
    "chunk_index": 7,
    "total_chunks": 42,
    "text_content": "a model based on autoencoders with varying sizes of encoders and decoders, effectively extracting key features and pinpointing lesion locations. 3. Our model not only demonstrates superior perfor- mance, achieving an F1 score of 86.49% and a recall rate of 94.12%, approaching the performance of hu- manimagingphysicians,butalsosimulatesthehuman diagnostic process, aligning with patients\u2019 intuitive experience of medical consultations and offering en- hanced interpretability. 4. All data adheres to",
    "full_text_length": 51340,
    "chunk_length": 1490
  },
  {
    "chunk_id": 1606,
    "paper_filename": "juntongdu_2024_MultiBCD_ mammography_chat.pdf",
    "paper_title": "Juntongdu 2024 Multibcd  Mammography Chat",
    "chunk_index": 8,
    "total_chunks": 42,
    "text_content": "for automated Breast Cancer Detection Table 1 CMMD Dataset Benign Malignant Total Training Set 2336 6864 9203 Validation Set 288 864 1152 Test Set 240 912 1152 Total 2864 8640 11504 Table 2 Chinese Breast Disease Clinical Imaging Database Dataset Benign Malignant Total Training Set 24 37 61 Test Set 10 17 27 Total 34 54 88 2. Materials and methods 2.1. Datasets In our study, we employ two datasets, each serving a distinct purpose: The Chinese Mammography Database (CMMD) is used to train image cl",
    "full_text_length": 51340,
    "chunk_length": 1308
  },
  {
    "chunk_id": 1607,
    "paper_filename": "juntongdu_2024_MultiBCD_ mammography_chat.pdf",
    "paper_title": "Juntongdu 2024 Multibcd  Mammography Chat",
    "chunk_index": 9,
    "total_chunks": 42,
    "text_content": "benign or malignant tu- mors. For 749 of these patients (1,498 mammogra- phies), the database also includes patients\u2019 molec- ular subtypes. Image data were acquired on a GE Senographe DS mammography system. The data can be obtained from this link.2We performed data aug- mentation and dataset partitioning on the original dataset,theprocesseddataset\u2019sstatisticalinformation is presented in Table 1. \u2022ChineseBreastDiseaseClinicalImagingDatabase: This database includes 176 mammographic images and 84 c",
    "full_text_length": 51340,
    "chunk_length": 1590
  },
  {
    "chunk_id": 1608,
    "paper_filename": "juntongdu_2024_MultiBCD_ mammography_chat.pdf",
    "paper_title": "Juntongdu 2024 Multibcd  Mammography Chat",
    "chunk_index": 10,
    "total_chunks": 42,
    "text_content": "aims to optimize the function: max\ud835\udc41\u2211 \ud835\udc56=1[ (1 \u2212\ud835\udc66\ud835\udc56)\u22c5\ud835\udc43(\ud835\udc66\u2032 \ud835\udc56= 0 \u2223\ud835\udc50\ud835\udc56,\ud835\udc5a\ud835\udc56,\ud835\udc61\ud835\udc56)+ \ud835\udc66\ud835\udc56\u22c5\ud835\udc43(\ud835\udc66\u2032 \ud835\udc56= 1 \u2223\ud835\udc50\ud835\udc56,\ud835\udc5a\ud835\udc56,\ud835\udc61\ud835\udc56)](1) where\ud835\udc66\ud835\udc56and\ud835\udc66\u2032 \ud835\udc56are Boolean values indicating the pres- ence or absence of cancer in the patient. 2.3. Overview of MultiBCD The diagnostic workflow of the MultiBCD model effec- tively replicates the process of human clinical consultation. The structural design of the MultiBCD model and its com- parison to the human diagnostic process are illustrated in Figure 1. TheMultiBCDframeworkiscomprisedofthre",
    "full_text_length": 51340,
    "chunk_length": 1674
  },
  {
    "chunk_id": 1609,
    "paper_filename": "juntongdu_2024_MultiBCD_ mammography_chat.pdf",
    "paper_title": "Juntongdu 2024 Multibcd  Mammography Chat",
    "chunk_index": 11,
    "total_chunks": 42,
    "text_content": "the high performance characteristic of LLMs but also the interpretability essential for practical application in medical AI. 2.4. Image classifier module The image classifier within the MultiBCD framework is bifurcated into two key components: a dual-head autoen- coder with shared parameters and a dual-head latent feature classifier.Notably,thetrainingphaseoftheautoencoderalso servesasthepretrainingstageforthelatentfeatureclassifier. 2.4.1. Dual-head autoencoder The architecture of the dual-head",
    "full_text_length": 51340,
    "chunk_length": 1682
  },
  {
    "chunk_id": 1610,
    "paper_filename": "juntongdu_2024_MultiBCD_ mammography_chat.pdf",
    "paper_title": "Juntongdu 2024 Multibcd  Mammography Chat",
    "chunk_index": 12,
    "total_chunks": 42,
    "text_content": "model that simulates the human diagnostic process for automated Breast Cancer Detection ComplaintGPT -4Decision moduleBenign/Malignant Latent featureLatent feature classifier Encoder 1GradientCC position latent featuresMLO position latent featuresConcatenate CC position MLO positionDecoder 1 Auxiliary Training Encoder 2Decoder 2 Auxiliary TrainingPrompts In-Context LearningProbabilityKey information (a)(b)(c)(d) Figure 2: Overview of MultiBCD. (a) Dual-head autoencoder, a part of Image classifie",
    "full_text_length": 51340,
    "chunk_length": 1672
  },
  {
    "chunk_id": 1611,
    "paper_filename": "juntongdu_2024_MultiBCD_ mammography_chat.pdf",
    "paper_title": "Juntongdu 2024 Multibcd  Mammography Chat",
    "chunk_index": 13,
    "total_chunks": 42,
    "text_content": "(\ud835\udc50\ud835\udc56,\ud835\udc50\u2032 \ud835\udc56))(6) Lossm=\ud835\udc5b\u2211 \ud835\udc56=1(\ud835\udefc\u00d7MSE (\ud835\udc5a\ud835\udc56,\ud835\udc5a\u2032 \ud835\udc56))+ (\ud835\udefd\u00d7SSIM (\ud835\udc5a\ud835\udc56,\ud835\udc5a\u2032 \ud835\udc56))(7) Consideringthesimilarityinfeatureextractionmethods for mammographic images at different positions, addition- ally, to mitigate overfitting, both encoders and decoders in our model employed a uniform parameter update strategy, as opposed to other methods that rely on weighted moving averages. Furthermore, this encoder-decoder architecture capital- izes on the principles of compressed sensing. The dimen- sionalityofthelatentfeature",
    "full_text_length": 51340,
    "chunk_length": 1853
  },
  {
    "chunk_id": 1612,
    "paper_filename": "juntongdu_2024_MultiBCD_ mammography_chat.pdf",
    "paper_title": "Juntongdu 2024 Multibcd  Mammography Chat",
    "chunk_index": 14,
    "total_chunks": 42,
    "text_content": "image information. The patient\u2019s biopsy pathology results are used as the gold standard \ud835\udc66\ud835\udc56for this assessment. The classifier\u2019slossfunctionisthecross-entropyloss,calculated as: JuntongDu and ZihanZhang et al.: Preprint submitted to Elsevier Page 4 of 11 This preprint research paper has not been peer reviewed. Electronic copy available at: https://ssrn.com/abstract=4950797Preprint not peer reviewed MultiBCD: A Multimodal model that simulates the human diagnostic process for automated Breast Cance",
    "full_text_length": 51340,
    "chunk_length": 1401
  },
  {
    "chunk_id": 1613,
    "paper_filename": "juntongdu_2024_MultiBCD_ mammography_chat.pdf",
    "paper_title": "Juntongdu 2024 Multibcd  Mammography Chat",
    "chunk_index": 15,
    "total_chunks": 42,
    "text_content": "three years would be '010' (distending pain). A patient who found a hard, approximately 4cm lump with poorly defined edges and limited mobility, accompanied by mild tenderness, but without nipple discharge, nipple retraction, or skin redness and ulceration, would be '100' (tenderness). ComplaintGPT -4 GPT -4 GPT -4 GPT -4+ Prompt 1 + Prompt 2 + Prompt 3 + Prompt 4Pain sensation Tumor texture Skin changes Nipple discharge In-Context LearningKey attributes in complaint (a) (b) Figure 3: Examples o",
    "full_text_length": 51340,
    "chunk_length": 1523
  },
  {
    "chunk_id": 1614,
    "paper_filename": "juntongdu_2024_MultiBCD_ mammography_chat.pdf",
    "paper_title": "Juntongdu 2024 Multibcd  Mammography Chat",
    "chunk_index": 16,
    "total_chunks": 42,
    "text_content": "optimum compared to those of an untrained model. This alignment significantlyfacilitatesconvergencetosuperioroutcomesin classification tasks. 2.5. Complaint key information extraction module Theefficacyoftheinformationextractionmodulehinges on the strategic application of well-crafted prompts and In-Context Learning (ICL) techniques to guide GPT-4 in extractingkeyinformationfrompatients\u2019narratives.Specif- ically, multiple ingenious prompts were meticulously de- signed to leverage GPT-4\u2019s formida",
    "full_text_length": 51340,
    "chunk_length": 1810
  },
  {
    "chunk_id": 1615,
    "paper_filename": "juntongdu_2024_MultiBCD_ mammography_chat.pdf",
    "paper_title": "Juntongdu 2024 Multibcd  Mammography Chat",
    "chunk_index": 17,
    "total_chunks": 42,
    "text_content": "Employing such prompts and ICL techniques ensures that the extraction of crucial information from patient nar- ratives does not rely solely on the model\u2019s intrinsic med- ical knowledge. Instead, it transforms the understanding of patient complaints into a logical reasoning task. This strategyallowstheeffortlessapplicationofthemodel\u2019slogic reasoning skills, acquired in other domains, to this study without the necessity for extensive medical-related training data. 2.6. Final decision module Human ",
    "full_text_length": 51340,
    "chunk_length": 1406
  },
  {
    "chunk_id": 1616,
    "paper_filename": "juntongdu_2024_MultiBCD_ mammography_chat.pdf",
    "paper_title": "Juntongdu 2024 Multibcd  Mammography Chat",
    "chunk_index": 18,
    "total_chunks": 42,
    "text_content": "(a) (b) (c) Figure 4: Three examples of mammographic images. (a) and (b) contain malignant tumors, while (c) represents a normal breast. The leftmost images are the original mammograms, the center images are the outputs from Stage 4, and the rightmost images are the outputs from Stage 6. In our study, we have integrated the EXtreme Gradient Boosting tree (XGBoost) model. XGBoost synergizes the probabilities of benign/malignant conditions derived from images with useful diagnostic information ext",
    "full_text_length": 51340,
    "chunk_length": 1640
  },
  {
    "chunk_id": 1617,
    "paper_filename": "juntongdu_2024_MultiBCD_ mammography_chat.pdf",
    "paper_title": "Juntongdu 2024 Multibcd  Mammography Chat",
    "chunk_index": 19,
    "total_chunks": 42,
    "text_content": "decision models, in comparison to the currently popular multi-modal alignment models like CLIP, are more adept at effectively leveraging complementary information across different modalities.3. Results 3.1. Experiment settings and Performance of MultiBCD 3.1.1. Image classifier module Inthedual-headautoencodermodule,eachencoderhasa parametercountof0.02M,whilethedecoderhasaparameter count of 0.003M. This asymmetric parameter configuration endowstheencoderswithenhancedfeatureextractioncapa- biliti",
    "full_text_length": 51340,
    "chunk_length": 1557
  },
  {
    "chunk_id": 1618,
    "paper_filename": "juntongdu_2024_MultiBCD_ mammography_chat.pdf",
    "paper_title": "Juntongdu 2024 Multibcd  Mammography Chat",
    "chunk_index": 20,
    "total_chunks": 42,
    "text_content": "facilitate the extraction of features from grayscale images, thereby reducing the model\u2019s complexity in feature discernment. It emphasizes the detection of lesion locations. This approach mirrorsthemethodusedbyhumanimagingphysicians,who identify anomalies in density and morphology against the JuntongDu and ZihanZhang et al.: Preprint submitted to Elsevier Page 6 of 11 This preprint research paper has not been peer reviewed. Electronic copy available at: https://ssrn.com/abstract=4950797Preprint ",
    "full_text_length": 51340,
    "chunk_length": 1528
  },
  {
    "chunk_id": 1619,
    "paper_filename": "juntongdu_2024_MultiBCD_ mammography_chat.pdf",
    "paper_title": "Juntongdu 2024 Multibcd  Mammography Chat",
    "chunk_index": 21,
    "total_chunks": 42,
    "text_content": "grayscale image of 1144 \u00d71144. Stage 2:Apply 8x data augmentation (rotations at 0, 90, 180, and 270 degrees, along with their respective mirror images). Stage 3: Perform CLAHE (Contrast Limited Adaptive Histogram Equalization) [42] for contrast enhancement. Stage 4: Normalize the image data using the mean and variance calculated from the entire dataset, which were 22.05 and 42.99, respectively. Stage 5:Encode the image into a latent vector represen- tation using Encoder. Stage 6:Decode the image",
    "full_text_length": 51340,
    "chunk_length": 1425
  },
  {
    "chunk_id": 1620,
    "paper_filename": "juntongdu_2024_MultiBCD_ mammography_chat.pdf",
    "paper_title": "Juntongdu 2024 Multibcd  Mammography Chat",
    "chunk_index": 22,
    "total_chunks": 42,
    "text_content": "exhibit any notable abnormal features. For the Latent Feature Classifier module, the overall architecture employs a modified ResNet34 model. This re- search adapted the first layer of ResNet34 to accommodate the output of the pretrained encoder.InthesecondtrainingphaseoftheImageClassifiermod- ule. The hyperparameters of the model are set as follows: The initial learning rate for the encoder part of the Adam optimizer is 5e-4, and for the classifier part, it is 1e-3. This specific setting is chos",
    "full_text_length": 51340,
    "chunk_length": 1827
  },
  {
    "chunk_id": 1621,
    "paper_filename": "juntongdu_2024_MultiBCD_ mammography_chat.pdf",
    "paper_title": "Juntongdu 2024 Multibcd  Mammography Chat",
    "chunk_index": 23,
    "total_chunks": 42,
    "text_content": "thepenultimatehiddenlayerintheLatentFeatureClassifier. The method for calculating similarity is as follows: \ud835\udc65\ud835\udc56 and\ud835\udc65\ud835\udc57representdifferentvectors,and \ud835\udc58denotesthevarious dimensions. \ud835\udc51(\ud835\udc65\ud835\udc56,\ud835\udc65\ud835\udc57) =\u221a\u2211 \ud835\udc58(\ud835\udc65\ud835\udc56\ud835\udc58\u2212\ud835\udc65\ud835\udc57\ud835\udc58)2 (12) \ud835\udc37(\ud835\udc65\ud835\udc56,\ud835\udc65\ud835\udc57) =\ud835\udc51(\ud835\udc65\ud835\udc56,\ud835\udc65\ud835\udc57) max\ud835\udc56,\ud835\udc57\ud835\udc51(\ud835\udc65\ud835\udc56,\ud835\udc65\ud835\udc57)(13) \ud835\udc46imilarity = 1 \u2212\ud835\udc37(\ud835\udc65\ud835\udc56,\ud835\udc65\ud835\udc57) (14) From the heatmap, it is evident that in Figures a, b, andc,thedemarcationbetweenbenignandmalignantcases is quite distinct. The similarity is higher among different benign cases and among different malignant cases, while it is",
    "full_text_length": 51340,
    "chunk_length": 1622
  },
  {
    "chunk_id": 1622,
    "paper_filename": "juntongdu_2024_MultiBCD_ mammography_chat.pdf",
    "paper_title": "Juntongdu 2024 Multibcd  Mammography Chat",
    "chunk_index": 24,
    "total_chunks": 42,
    "text_content": "of features beyond the benign-malignant distinction, aiding in imagereconstruction,whichresultedinasomewhatblurred demarcation between these classifications. After the second stage of training, this boundary became more pronounced. In thepenultimate layerof the LatentFeature Classifier, the features distinguishing benign from malignant cases were further differentiated. 3.1.2. Complaint key information extraction module Utilizing various prompts, GPT-4 is capable of extract- ingfeaturesfrompatie",
    "full_text_length": 51340,
    "chunk_length": 1918
  },
  {
    "chunk_id": 1623,
    "paper_filename": "juntongdu_2024_MultiBCD_ mammography_chat.pdf",
    "paper_title": "Juntongdu 2024 Multibcd  Mammography Chat",
    "chunk_index": 25,
    "total_chunks": 42,
    "text_content": "cautious adjustment of weights, thereby enhancing generalization. Furthermore, the con- structionof200treeswasdecidedupontofacilitatethorough learning and pattern recognition. To further suppress over- fitting, a random sampling technique was adopted, utilizing only80%ofthedatasamplesineachiteration;additionally, only 20% of features were considered for splitting in each tree, increasing the model\u2019s randomness and robustness. Lastly, the minimum loss reduction required to split a nodewassettozer",
    "full_text_length": 51340,
    "chunk_length": 1503
  },
  {
    "chunk_id": 1624,
    "paper_filename": "juntongdu_2024_MultiBCD_ mammography_chat.pdf",
    "paper_title": "Juntongdu 2024 Multibcd  Mammography Chat",
    "chunk_index": 26,
    "total_chunks": 42,
    "text_content": "on imaging The training of the Image Classifier module is divided intotwostages.Inthefirststage,theEncoderistrainedtoau- tonomously learn image features through self-supervision. The second stage focuses on training the module to discern features pertinent to the benign or malignant nature of the tissue and to predict a probability associated with these characteristics. Diagnoses based solely on this probability have shown comparatively favorable classification results. To substantiate this conc",
    "full_text_length": 51340,
    "chunk_length": 1745
  },
  {
    "chunk_id": 1625,
    "paper_filename": "juntongdu_2024_MultiBCD_ mammography_chat.pdf",
    "paper_title": "Juntongdu 2024 Multibcd  Mammography Chat",
    "chunk_index": 27,
    "total_chunks": 42,
    "text_content": "Image Classifier module exhibited superior performance in all aspects of classification. DuringthesecondtrainingphaseoftheImageClassifier module,differentlearningrateswereassignedtotheencoder and classifier. This decision was based on the fact that the encoder, having been trained in the first phase, already possessedsomeknowledgeoffeatureextractionfrommam- mographic images. However, a too high learning rate might lead to the forgetting of this pre-trained knowledge, while a too low rate could c",
    "full_text_length": 51340,
    "chunk_length": 1458
  },
  {
    "chunk_id": 1626,
    "paper_filename": "juntongdu_2024_MultiBCD_ mammography_chat.pdf",
    "paper_title": "Juntongdu 2024 Multibcd  Mammography Chat",
    "chunk_index": 28,
    "total_chunks": 42,
    "text_content": "irrelevant features. To validate this hypothesis, various learning rates were applied to the encoder during the sec- ond phase of training. Additionally, the impact of using a randomly initialized encoder (without a pretraining step) on image classification effectiveness is also presented, as detailed in the table 5. Figure 6: Receiver Operating Characteristic(ROC) curves. Consequently,itisevidentthatanappropriatelybalanced learning rate leads to improved outcomes. 3.2.2. Performance of the vari",
    "full_text_length": 51340,
    "chunk_length": 1503
  },
  {
    "chunk_id": 1627,
    "paper_filename": "juntongdu_2024_MultiBCD_ mammography_chat.pdf",
    "paper_title": "Juntongdu 2024 Multibcd  Mammography Chat",
    "chunk_index": 29,
    "total_chunks": 42,
    "text_content": "preference of the Image Classifier module ensures a higher recall rate. The powerful inference capabilities of GPT-4, along with inge- nious prompt design and the effective design of the Com- plaint Key Information Extraction module, enable the effi- cient extraction of diagnostically relevant information from complaints. The final dual-modality decision module effec- tively leverages the strengths of both elements.Table 6 Performance of Different Components of the MultiBCD and Human Module P R ",
    "full_text_length": 51340,
    "chunk_length": 1387
  },
  {
    "chunk_id": 1628,
    "paper_filename": "juntongdu_2024_MultiBCD_ mammography_chat.pdf",
    "paper_title": "Juntongdu 2024 Multibcd  Mammography Chat",
    "chunk_index": 30,
    "total_chunks": 42,
    "text_content": "the cost of misclassifying a malignant case as benign is significantly higher than misclassifying a benign case as malignant, and quantifying this cost is challenging. This scenario can lead to models erring on the side of caution by overly predicting malignancy, an outcome that is undesirable. Insuchtasks,recallandprecisionaretypicallythemost critical metrics. The final model that integrates both textual and image information, MultiBCD, achieved a commend- able recall rate (94.12%) and precisio",
    "full_text_length": 51340,
    "chunk_length": 1653
  },
  {
    "chunk_id": 1629,
    "paper_filename": "juntongdu_2024_MultiBCD_ mammography_chat.pdf",
    "paper_title": "Juntongdu 2024 Multibcd  Mammography Chat",
    "chunk_index": 31,
    "total_chunks": 42,
    "text_content": "for the image classifier to locate and diagnose lesions from raw mammographic images. Additionally, the design of the MultiBCD model simulates the diagnostic process of human physicians, enhancing the model\u2019s in- terpretability. Experimental results indicate that MultiBCD excels in the automatic diagnosis of breast cancer. JuntongDu and ZihanZhang et al.: Preprint submitted to Elsevier Page 9 of 11 This preprint research paper has not been peer reviewed. Electronic copy available at: https://ssr",
    "full_text_length": 51340,
    "chunk_length": 1890
  },
  {
    "chunk_id": 1630,
    "paper_filename": "juntongdu_2024_MultiBCD_ mammography_chat.pdf",
    "paper_title": "Juntongdu 2024 Multibcd  Mammography Chat",
    "chunk_index": 32,
    "total_chunks": 42,
    "text_content": "information: mammographicimagesandpatientcomplaints.Futurework could involve incorporating more modalities, such as breast ultrasound,andexploringtheapplicabilityofthismethodto othertypesofcancerdiagnosis,aswellasfurtherenhancing diagnostic accuracy and efficiency. CRediT authorship contribution statement JuntongDu: Conceptualization, Investigation, Method- ology, Writing-Original draft preparation. ZihanZhang: Software,Writing-Originaldraftpreparation. WeiyangTao: Writing-Reviewing; Validation;",
    "full_text_length": 51340,
    "chunk_length": 1998
  },
  {
    "chunk_id": 1631,
    "paper_filename": "juntongdu_2024_MultiBCD_ mammography_chat.pdf",
    "paper_title": "Juntongdu 2024 Multibcd  Mammography Chat",
    "chunk_index": 33,
    "total_chunks": 42,
    "text_content": ", 14(3):196\u2013202, 2015. [4] Xiaohui Sun, Anne S Reiner, Anh Phong Tran, Gordon P Watt, Jung Hun Oh, Lene Mellemkj\u00e6r, Charles F Lynch, Julia A Knight, EstherMJohn,KathleenEMalone,etal. Agenome-wideassociation study of contralateral breast cancer in the women\u2019s environmental cancer and radiation epidemiology study. Breast Cancer Research , 26(1):1\u20137, 2024. [5] Zohre Momenimovahed and Hamid Salehiniya. Epidemiological characteristicsofandriskfactorsforbreastcancerintheworld. Breast Cancer: Targets a",
    "full_text_length": 51340,
    "chunk_length": 1762
  },
  {
    "chunk_id": 1632,
    "paper_filename": "juntongdu_2024_MultiBCD_ mammography_chat.pdf",
    "paper_title": "Juntongdu 2024 Multibcd  Mammography Chat",
    "chunk_index": 34,
    "total_chunks": 42,
    "text_content": "changes over time. Pathologica , 112(1):25, 2020. [10] BiniyamGDemissei,RebeccaAHubbard,LiyongZhang,AmandaM Smith, Karyn Sheline, Caitlin McDonald, Vivek Narayan, Susan M Domchek, Angela DeMichele, Payal Shah, et al. Changes in cardio- vascularbiomarkerswithbreastcancertherapyandassociationswith cardiac dysfunction. Journal of the American Heart Association , 9 (2):e014708, 2020. [11] Xingli Dong, Xupeng Bai, Jie Ni, Hao Zhang, Wei Duan, Peter Graham, and Yong Li. Exosomes and breast cancer drug",
    "full_text_length": 51340,
    "chunk_length": 1822
  },
  {
    "chunk_id": 1633,
    "paper_filename": "juntongdu_2024_MultiBCD_ mammography_chat.pdf",
    "paper_title": "Juntongdu 2024 Multibcd  Mammography Chat",
    "chunk_index": 35,
    "total_chunks": 42,
    "text_content": "126(13):2971\u20132979, 2020. [15] CaixingYuan,GuolinXu,XiangmeiZhan,MinXie,MingcongLuo, Lilan She, and Yunjing Xue. Molybdenum target mammography- basedpredictionmodelformetastasisofaxillarysentinellymphnode in early-stage breast cancer. Medicine, 102(42):e35672, 2023. [16] Takayoshi Uematsu, Ayumi Izumori, and Woo Kyung Moon. Over- coming the limitations of screening mammography in japan and korea:aparadigmshifttopersonalizedbreastcancerscreeningbased on ultrasonography. Ultrasonography , 42(4):508",
    "full_text_length": 51340,
    "chunk_length": 1573
  },
  {
    "chunk_id": 1634,
    "paper_filename": "juntongdu_2024_MultiBCD_ mammography_chat.pdf",
    "paper_title": "Juntongdu 2024 Multibcd  Mammography Chat",
    "chunk_index": 36,
    "total_chunks": 42,
    "text_content": "clinics. Postgraduate medical journal , 99(1169):153\u2013 158, 2023. [20] Mindy L Yang, Chandni Bhimani, Robyn Roth, and Pauline Ger- maine. Contrast enhanced mammography: focus on frequently en- countered benign and malignant diagnoses. Cancer Imaging , 23(1): 10, 2023. [21] Muayad Sadik Croock, Saja Dhyaa Khuder, Ayad Esho Korial, and Sahar Salman Mahmood. Early detection of breast cancer using mammography images and software engineering process. Telkom- nika(TelecommunicationComputingElectronicsa",
    "full_text_length": 51340,
    "chunk_length": 1840
  },
  {
    "chunk_id": 1635,
    "paper_filename": "juntongdu_2024_MultiBCD_ mammography_chat.pdf",
    "paper_title": "Juntongdu 2024 Multibcd  Mammography Chat",
    "chunk_index": 37,
    "total_chunks": 42,
    "text_content": "review and meta-analysis. BMC cancer , 20:1\u201315, 2020. [24] NClerkin,CFSki,PCBrennan,andRuthStrudwick.Identificationof factorsassociatedwithdiagnosticperformancevariationinreporting of mammograms: a review. Radiography , 29(2):340\u2013346, 2023. [25] Rebecca Oudsema, Esther Hwang, Sharon Steinberger, Rowena Yip, andLaurieRMargolies. Screeningmammography:guidelinesversus clinical practice. Journal of Breast Imaging , 2(3):217\u2013224, 2020. [26] Jung Hyun Yoon, Kyungwha Han, Hee Jung Suh, Ji Hyun Youk, Si",
    "full_text_length": 51340,
    "chunk_length": 1937
  },
  {
    "chunk_id": 1636,
    "paper_filename": "juntongdu_2024_MultiBCD_ mammography_chat.pdf",
    "paper_title": "Juntongdu 2024 Multibcd  Mammography Chat",
    "chunk_index": 38,
    "total_chunks": 42,
    "text_content": ",2023. URL https://link.springer.com/article/10.1007/s00330-022-09263-8 . [30] I Sechopoulos, J Teuwen, and R Mann. Artificial intelligence for breastcancerdetectioninmammographyanddigitalbreasttomosyn- thesis:Stateoftheart. SeminarsinCancerBiology ,2021.URL https: //www.sciencedirect.com/science/article/pii/S1044579X20301358 . [31] RADar,MRasool,andAAssad. Breastcancerdetectionusingdeep learning: Datasets, methods, and challenges ahead. Computers in Biology and Medicine , 2022. URL https://www.",
    "full_text_length": 51340,
    "chunk_length": 2040
  },
  {
    "chunk_id": 1637,
    "paper_filename": "juntongdu_2024_MultiBCD_ mammography_chat.pdf",
    "paper_title": "Juntongdu 2024 Multibcd  Mammography Chat",
    "chunk_index": 39,
    "total_chunks": 42,
    "text_content": "s41586-019-1799-6 . [36] Brady Lund. A brief review of chatgpt: Its value and the underlying gpt technology. University of North Texas. Project: ChatGPT and Its Impact on ... , 2023. URL https://www.researchgate.net/publication/366809571_A_Brief_ Review_of_ChatGPT_Its_Value_and_the_Underlying_GPT_Technology . [37] WMa,YZhao,YJi,XGuo,XJian,PLiu,andSWu. Breastcancer molecular subtype prediction by mammographic radiomic features. AcademicRadiology ,2019. URL https://www.ncbi.nlm.nih.gov/pmc/ articl",
    "full_text_length": 51340,
    "chunk_length": 1877
  },
  {
    "chunk_id": 1638,
    "paper_filename": "juntongdu_2024_MultiBCD_ mammography_chat.pdf",
    "paper_title": "Juntongdu 2024 Multibcd  Mammography Chat",
    "chunk_index": 40,
    "total_chunks": 42,
    "text_content": "and R. De Ryke. Feature enhancement in medical ultrasound videos using contrast-limited adaptive histogram equalization. Journal of Digital Imaging , 33(1):213\u2013230, 2020. doi: 10.1007/s10278-019-00211-5. URL https://www.ncbi.nlm.nih.gov/ pmc/articles/PMC7064707/ . [43] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. Proceedings of the IEEE conferenceoncomputervisionandpatternrecognition(CVPR) ,2016. [44] Christian Szegedy, Wei Liu, Yangqing Ji",
    "full_text_length": 51340,
    "chunk_length": 1638
  },
  {
    "chunk_id": 1639,
    "paper_filename": "juntongdu_2024_MultiBCD_ mammography_chat.pdf",
    "paper_title": "Juntongdu 2024 Multibcd  Mammography Chat",
    "chunk_index": 41,
    "total_chunks": 42,
    "text_content": "is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929 , 2020. JuntongDu and ZihanZhang et al.: Preprint submitted to Elsevier Page 11 of 11 This preprint research paper has not been peer reviewed. Electronic copy available at: https://ssrn.com/abstract=4950797Preprint not peer reviewed",
    "full_text_length": 51340,
    "chunk_length": 331
  },
  {
    "chunk_id": 1640,
    "paper_filename": "kaixiang_2024_artificial_intelligence_and_breast_cancer_management_from_data_to_clinic.pdf",
    "paper_title": "Kaixiang 2024 Artificial Intelligence And Breast Cancer Management From Data To Clinic",
    "chunk_index": 0,
    "total_chunks": 75,
    "text_content": "Cancer Innovation REVIEW Artificial Intelligence and Breast Cancer Management: From Data to the Clinic Kaixiang Feng1,2| Zongbi Yi2| Binghe Xu3 1Department of Breast and Thyroid Surgery, Hubei Key Laboratory of Tumor Biological Behaviors, Hubei Cancer Clinical Study Center, Zhongnan Hospita lo f Wuhan University, Wuhan, Hubei, China |2Department of Radiation and Medical Oncology, Hubei Key Laboratory of Tumor Biological Behaviors, Hubei Cancer Clinical Study Center, Zhongnan Hospital of Wuhan Un",
    "full_text_length": 81068,
    "chunk_length": 1449
  },
  {
    "chunk_id": 1641,
    "paper_filename": "kaixiang_2024_artificial_intelligence_and_breast_cancer_management_from_data_to_clinic.pdf",
    "paper_title": "Kaixiang 2024 Artificial Intelligence And Breast Cancer Management From Data To Clinic",
    "chunk_index": 1,
    "total_chunks": 75,
    "text_content": "breast cancer | clinical practice | deep learning | machine learning ABSTRACT Breast cancer (BC) remains a significant threat to women's health worldwide. The oncology field had an exponential growth in the abundance of medical images, clinical information, and genomic data. With its continuous advancement and refinement,artificial intelligence (AI) has demonstrated exceptional capabilities in processing intricate multidimensional BC \u2010related data. AI has proven advantageous in various facets of",
    "full_text_length": 81068,
    "chunk_length": 1527
  },
  {
    "chunk_id": 1642,
    "paper_filename": "kaixiang_2024_artificial_intelligence_and_breast_cancer_management_from_data_to_clinic.pdf",
    "paper_title": "Kaixiang 2024 Artificial Intelligence And Breast Cancer Management From Data To Clinic",
    "chunk_index": 2,
    "total_chunks": 75,
    "text_content": "AI algorithms. Additionally, we explore the opportunities and potential research directions within this burgeoning field. 1 | Introduction Seventy years after its inception, artificial intelligence (AI) is being developed at an unprecedented pace. Predominantly re-lying on machine learning (ML) and deep learning (DL) methods, AI has demonstrated remarkable superiority in advancing contemporary medicine, particularly, within theoncology field, through continuous innovation [ 1, 2]. ML en- compass",
    "full_text_length": 81068,
    "chunk_length": 1441
  },
  {
    "chunk_id": 1643,
    "paper_filename": "kaixiang_2024_artificial_intelligence_and_breast_cancer_management_from_data_to_clinic.pdf",
    "paper_title": "Kaixiang 2024 Artificial Intelligence And Breast Cancer Management From Data To Clinic",
    "chunk_index": 3,
    "total_chunks": 75,
    "text_content": "en-compass recurrent neural networks, convolutional neural net-works, transformers, and others. DL primarily focuses on This is an open access article under the terms of the Creative Commons Attribution \u2010NonCommercial \u2010NoDerivs License, which permits use and distribution in any medium, provided the original work is properly cited, the use is non \u2010commercial and no modifications or adaptations are made. \u00a9 2025 The Author(s). Cancer Innovation published by John Wiley & Sons Ltd on behalf of Tsingh",
    "full_text_length": 81068,
    "chunk_length": 1457
  },
  {
    "chunk_id": 1644,
    "paper_filename": "kaixiang_2024_artificial_intelligence_and_breast_cancer_management_from_data_to_clinic.pdf",
    "paper_title": "Kaixiang 2024 Artificial Intelligence And Breast Cancer Management From Data To Clinic",
    "chunk_index": 4,
    "total_chunks": 75,
    "text_content": "convolutional neural networks to learn rules and representa-tions, while iteratively updating the specific parameters to es-tablish accurate and reliable models [ 1]. AI applications have become widely used for the comprehen- sive management of all steps of the cancer process, thus es- tablishing new possibilities for cancer research. By leveraginginnovative self \u2010supervised learning methods, developing fun- damental models capable of encoding intricate medical data hasbecome feasible [ 6]. The ",
    "full_text_length": 81068,
    "chunk_length": 1513
  },
  {
    "chunk_id": 1645,
    "paper_filename": "kaixiang_2024_artificial_intelligence_and_breast_cancer_management_from_data_to_clinic.pdf",
    "paper_title": "Kaixiang 2024 Artificial Intelligence And Breast Cancer Management From Data To Clinic",
    "chunk_index": 5,
    "total_chunks": 75,
    "text_content": "AI and medicaltechnology will propel the implementation of technologicaladvancements in precision oncology [ 2]. Breast cancer (BC) remains a significant global health challenge because of its high incidence and mortality rates [ 7]. Fortu- nately, advancements in screening, diagnosis, systematic treat-ment, and personalized therapy have led to improved survivalrates for BC patients [ 8]. Correspondingly, these patients also face longer treatment and follow \u2010up times, posing challengesfor integr",
    "full_text_length": 81068,
    "chunk_length": 1478
  },
  {
    "chunk_id": 1646,
    "paper_filename": "kaixiang_2024_artificial_intelligence_and_breast_cancer_management_from_data_to_clinic.pdf",
    "paper_title": "Kaixiang 2024 Artificial Intelligence And Breast Cancer Management From Data To Clinic",
    "chunk_index": 6,
    "total_chunks": 75,
    "text_content": "numerous challenges remain in diagnosing and treating BC, particularly, triple \u2010negative breast cancer (TNBC) and advanced \u2010stage disease, which lack effective treatments and have higher associated mortality rates [ 10]. Early detection and accurate treatment strategies are crucial forimproving patient prognosis and overall quality of life [ 11]. In the future, further research, enhanced practices, and appropri-ate regulation will facilitate the broader clinical translation ofpatient \u2010centered A",
    "full_text_length": 81068,
    "chunk_length": 1366
  },
  {
    "chunk_id": 1647,
    "paper_filename": "kaixiang_2024_artificial_intelligence_and_breast_cancer_management_from_data_to_clinic.pdf",
    "paper_title": "Kaixiang 2024 Artificial Intelligence And Breast Cancer Management From Data To Clinic",
    "chunk_index": 7,
    "total_chunks": 75,
    "text_content": "byeliminating repetitive or unrelated articles, as well as reviews FIGURE 1 | Overview of the whole \u2010cycle involvement of AI in BC management. The application of AI has become prevalent in BC diagnosis, prognosis, and treatment methods. By processing and analyzing clinical information, medical images, and other patient \u2010related data, AI aids healthcare professionals in implementing personalized health management for patients. Furthermore, AI encounters numerous challenges in itsclinical applicat",
    "full_text_length": 81068,
    "chunk_length": 1498
  },
  {
    "chunk_id": 1648,
    "paper_filename": "kaixiang_2024_artificial_intelligence_and_breast_cancer_management_from_data_to_clinic.pdf",
    "paper_title": "Kaixiang 2024 Artificial Intelligence And Breast Cancer Management From Data To Clinic",
    "chunk_index": 8,
    "total_chunks": 75,
    "text_content": "To ensure comprehensive coverage, we included approximately 20 articles under each subheading using factors, such as patient population size, training set size, and test set size. The objective of this review is to provide a comprehensive overview of the latest advancements and challenges in BC AIapplications, as these emerging technologies gradually transi-tion from behind \u2010the\u2010scenes operations to prominent roles, facilitating data translation into clinical applications. This review primarily ",
    "full_text_length": 81068,
    "chunk_length": 1413
  },
  {
    "chunk_id": 1649,
    "paper_filename": "kaixiang_2024_artificial_intelligence_and_breast_cancer_management_from_data_to_clinic.pdf",
    "paper_title": "Kaixiang 2024 Artificial Intelligence And Breast Cancer Management From Data To Clinic",
    "chunk_index": 9,
    "total_chunks": 75,
    "text_content": "following [ 12, 13 ]. However, because of limitations, such as false positives and increased recall rates, CAD cannot meet the increased demand for mammography performance [ 14]. Early detection and diagnosis are key for improving BC patient survival rates. In general, the BC mor-tality rate has been significantly reduced with mammography \u2010 based screening and more effective treatment methods. The5\u2010year survival rate of early \u2010stage BC is more than 90%. How- ever, the decline in BC mortality has",
    "full_text_length": 81068,
    "chunk_length": 1369
  },
  {
    "chunk_id": 1650,
    "paper_filename": "kaixiang_2024_artificial_intelligence_and_breast_cancer_management_from_data_to_clinic.pdf",
    "paper_title": "Kaixiang 2024 Artificial Intelligence And Breast Cancer Management From Data To Clinic",
    "chunk_index": 10,
    "total_chunks": 75,
    "text_content": "retrospective study, radiologistsfrom multiple institutions found excellent performance whenevaluating more than 30,000 pathology \u2010proven cancer \u2010positive mammograms by adding AI. This demonstrates that AI canimprove the diagnostic performance of radiologists by over-coming the problems associated with traditional CAD [ 17]. Another study noted that a system consisting of pretrained black \u2010box predictive AI and learning \u2010delay AI reduced the false positive rate by 25% with the same rate of false",
    "full_text_length": 81068,
    "chunk_length": 1440
  },
  {
    "chunk_id": 1651,
    "paper_filename": "kaixiang_2024_artificial_intelligence_and_breast_cancer_management_from_data_to_clinic.pdf",
    "paper_title": "Kaixiang 2024 Artificial Intelligence And Breast Cancer Management From Data To Clinic",
    "chunk_index": 11,
    "total_chunks": 75,
    "text_content": "Online Library for rules of use; OA articles are governed by the applicable Creative Commons License reducing the clinician workload by 66%, compared with double \u2010 read arbitration in a screening program [ 18]. In addition to mammography, unique features and rich imaging data, such as ultrasound, magnetic resonance imaging (MRI),and positron emission tomography/computed tomography (PET/CT) scans, can provide opportunities for clinically meaningful AI. The preliminary results of applying AI in th",
    "full_text_length": 81068,
    "chunk_length": 1376
  },
  {
    "chunk_id": 1652,
    "paper_filename": "kaixiang_2024_artificial_intelligence_and_breast_cancer_management_from_data_to_clinic.pdf",
    "paper_title": "Kaixiang 2024 Artificial Intelligence And Breast Cancer Management From Data To Clinic",
    "chunk_index": 12,
    "total_chunks": 75,
    "text_content": "23]. Moreover, using ensemble DL models can identify subtle elements on breastlesion images, further improving the performance and read time of ultrasound images [ 19, 24 ]. DL can also combine ultrasound and elastography to predict axillary lymph node(LN) metastasis, which may reduce false positive diagnoses andunnecessary biopsies [ 25]. There is great potential in applying AI \u2010based risk models derived from medical imaging data to analyze unstructured information, which can enable an accurate",
    "full_text_length": 81068,
    "chunk_length": 1279
  },
  {
    "chunk_id": 1653,
    "paper_filename": "kaixiang_2024_artificial_intelligence_and_breast_cancer_management_from_data_to_clinic.pdf",
    "paper_title": "Kaixiang 2024 Artificial Intelligence And Breast Cancer Management From Data To Clinic",
    "chunk_index": 13,
    "total_chunks": 75,
    "text_content": "multigenerational family his-tory, can effectively predict cancer risk [ 28]. According to a study with a 10 \u2010year follow \u2010up period, AI \u2010based risk model images for mammography showed that the AI model was superior to the Tyrer \u2013Cuzick model for both short \u2010term and long \u2010term assessments [ 29]. An image \u2010based DL method dem- onstrated superior discrimination of individual risk in a 5 \u2010year cancer risk prediction study using breast MRI scans. The areaunder the receiver operating characteristic ",
    "full_text_length": 81068,
    "chunk_length": 1289
  },
  {
    "chunk_id": 1654,
    "paper_filename": "kaixiang_2024_artificial_intelligence_and_breast_cancer_management_from_data_to_clinic.pdf",
    "paper_title": "Kaixiang 2024 Artificial Intelligence And Breast Cancer Management From Data To Clinic",
    "chunk_index": 14,
    "total_chunks": 75,
    "text_content": "patients. Vachon and colleagues evaluated mammograms captured within a period of 2 years preceding BC onset, with a maximum of 5.5 years. They showed that Trans-para AI algorithms combined with breast density can contributeto improved detection and long \u2010term risk prediction for invasive BC [ 32]. AI algorithms can identify suspicious areas that may develop into advanced BC, providing screening opportunitiesfor people at high risk of cancer to support early intervention or prevention. This is, p",
    "full_text_length": 81068,
    "chunk_length": 1362
  },
  {
    "chunk_id": 1655,
    "paper_filename": "kaixiang_2024_artificial_intelligence_and_breast_cancer_management_from_data_to_clinic.pdf",
    "paper_title": "Kaixiang 2024 Artificial Intelligence And Breast Cancer Management From Data To Clinic",
    "chunk_index": 15,
    "total_chunks": 75,
    "text_content": "still required before its widespread implementation in clinical practice. In the future,integrating AI with comprehensi ve clinical data and multiple radiomics may potentially surpa ss the constraints of the current analytical methods for BC screenin g and risk assessment. This will aid in reducing false positive results, effectively detecting interval cancers, and assisting physicians in achieving accurate diagnoses. 2.2 | Pathology Pathology has long played a central role in tumor diagnosis. A",
    "full_text_length": 81068,
    "chunk_length": 1385
  },
  {
    "chunk_id": 1656,
    "paper_filename": "kaixiang_2024_artificial_intelligence_and_breast_cancer_management_from_data_to_clinic.pdf",
    "paper_title": "Kaixiang 2024 Artificial Intelligence And Breast Cancer Management From Data To Clinic",
    "chunk_index": 16,
    "total_chunks": 75,
    "text_content": "thereby effectively reducing their workload. A notable example is thesignificant improvement in detection sensitivity for micro-metastatic BC from 83% to 91%, accompanied by a remarkablereduction in the average detection time by pathologists from116 s to 61 s through the assistance of DL algorithms [ 36]. The DL field offers precise and efficient tools to assist pathol- ogists with tasks, such as tumor diagnosis, molecular typing, LN metastasis identification, immune infiltration analysis, andpr",
    "full_text_length": 81068,
    "chunk_length": 1362
  },
  {
    "chunk_id": 1657,
    "paper_filename": "kaixiang_2024_artificial_intelligence_and_breast_cancer_management_from_data_to_clinic.pdf",
    "paper_title": "Kaixiang 2024 Artificial Intelligence And Breast Cancer Management From Data To Clinic",
    "chunk_index": 17,
    "total_chunks": 75,
    "text_content": "of AI assistance on the interobserverreliability was investigated, with Krippendorff's \u03b1slightly increasing from 0.69 (95% CI: 0.65 \u20130.73) to 0.72 (95% CI: 0.68 \u20130.76) [ 38]. Correctly interpreting the HER2 immuno- histochemistry staining results is crucial for the personalizedtreatment of patients [ 39]. It can be challenging to distinguish between HER2 0 and 1+ cases. Wu et al. showed that theaccuracy of AI \u2010assisted interpretation increased by 13%, with the AI algorithm improving the overall ",
    "full_text_length": 81068,
    "chunk_length": 1454
  },
  {
    "chunk_id": 1658,
    "paper_filename": "kaixiang_2024_artificial_intelligence_and_breast_cancer_management_from_data_to_clinic.pdf",
    "paper_title": "Kaixiang 2024 Artificial Intelligence And Breast Cancer Management From Data To Clinic",
    "chunk_index": 18,
    "total_chunks": 75,
    "text_content": "represent the entire lesion. 4o f1 4 Cancer Innovation , 2025 27709183, 2025, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/cai2.159 by INASP/HINARI - PAKISTAN, Wiley Online Library on [12/03/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License Currently, AI methods are mainly used to analyze two \u2010 dimensional (2D) pathology images. ",
    "full_text_length": 81068,
    "chunk_length": 1421
  },
  {
    "chunk_id": 1659,
    "paper_filename": "kaixiang_2024_artificial_intelligence_and_breast_cancer_management_from_data_to_clinic.pdf",
    "paper_title": "Kaixiang 2024 Artificial Intelligence And Breast Cancer Management From Data To Clinic",
    "chunk_index": 19,
    "total_chunks": 75,
    "text_content": "be processed and interpreted at the genetic and clinical levels. This must also be done within a reasonable time frame to assist in clinical decision \u2010making [42, 43 ]. In addition, analysis of other omics data containing complex and important information, such as proteomics andmethylome data, will be inseparable from AI use in the future. 2.3 | Liquid Biopsy In recent years, using liquid biopsy of body fluid samples, whichencompass a diverse range of tumor derivatives, has gained sig-nificant t",
    "full_text_length": 81068,
    "chunk_length": 1387
  },
  {
    "chunk_id": 1660,
    "paper_filename": "kaixiang_2024_artificial_intelligence_and_breast_cancer_management_from_data_to_clinic.pdf",
    "paper_title": "Kaixiang 2024 Artificial Intelligence And Breast Cancer Management From Data To Clinic",
    "chunk_index": 20,
    "total_chunks": 75,
    "text_content": "cancerdetection [ 45]. Methylation sequencing has the potential to improve current cfDNA testing methods. Indeed, abnormal methylation of CpG islands is often widespread during cancerinitiation, reflecting early tumor changes. The methylation pat-tern of cfDNA is consistent with the cell or tissue of origin, butthe blood concentration is usually very low. Although the currentdetection and analysis methods have broad clinical application prospects, there are still many challenges and difficulties",
    "full_text_length": 81068,
    "chunk_length": 1479
  },
  {
    "chunk_id": 1661,
    "paper_filename": "kaixiang_2024_artificial_intelligence_and_breast_cancer_management_from_data_to_clinic.pdf",
    "paper_title": "Kaixiang 2024 Artificial Intelligence And Breast Cancer Management From Data To Clinic",
    "chunk_index": 21,
    "total_chunks": 75,
    "text_content": "cfDNAsequencing panel to distinguish multiple cancer types, includingBC, and to discriminate cancer from noncancer at very lowctDNA fractions [ 48]. Predictably, incorporating the cfDNA sequence, methylation status, and fragmentation patterns into the classifier can potentially augment ML cancer detectionmodels, enhance classifier performance, elevate the diagnosticand predictive capabilities of liquid biopsy, and facilitate real \u2010 time monitoring of a cancer patient's health status. Accurate an",
    "full_text_length": 81068,
    "chunk_length": 1573
  },
  {
    "chunk_id": 1662,
    "paper_filename": "kaixiang_2024_artificial_intelligence_and_breast_cancer_management_from_data_to_clinic.pdf",
    "paper_title": "Kaixiang 2024 Artificial Intelligence And Breast Cancer Management From Data To Clinic",
    "chunk_index": 22,
    "total_chunks": 75,
    "text_content": "allrequired. 3 | BC Prognosis Prediction BC patient prognosis is influenced by a multitude of intricatefactors, including age, family history, lifestyle, and pathologicalcharacteristics. Promptly predicting cancer prognosis poses amultifaceted challenge [ 50]. By integrating various data sources to construct survival prediction models, AI can assist clinicianswith more accurately forecasting patient outcomes. A studyconducted by Xiao et al. revealed that the random survival forest model exhibite",
    "full_text_length": 81068,
    "chunk_length": 1425
  },
  {
    "chunk_id": 1663,
    "paper_filename": "kaixiang_2024_artificial_intelligence_and_breast_cancer_management_from_data_to_clinic.pdf",
    "paper_title": "Kaixiang 2024 Artificial Intelligence And Breast Cancer Management From Data To Clinic",
    "chunk_index": 23,
    "total_chunks": 75,
    "text_content": "brain metastases.This model exhibited excellent performance in external inde-pendent data sets, with a survival AUC value exceeding 0.8from 6 months to 3 years [ 54]. Of note, the widespread appli- cation of these models remains to be carefully considered. A recent large cohort study in the United Kingdom reported thatstatistical regression models performed similarly to or betterthan ML models in predicting the 10 \u2010year risk of BC \u2010related death among women at any stage. Compared with Cox pro-po",
    "full_text_length": 81068,
    "chunk_length": 1409
  },
  {
    "chunk_id": 1664,
    "paper_filename": "kaixiang_2024_artificial_intelligence_and_breast_cancer_management_from_data_to_clinic.pdf",
    "paper_title": "Kaixiang 2024 Artificial Intelligence And Breast Cancer Management From Data To Clinic",
    "chunk_index": 24,
    "total_chunks": 75,
    "text_content": "inability to fully supplant traditional prognostic predictionmethods. Early prediction of recurrence in BC patients ishelpful for developing personalized treatment plans and post-operative follow \u2010up strategies, as well as for improving survival rates [ 52]. TNBC patients are more likely to experience meta- static recurrence and death than patients with other BC sub- types [ 52, 56 ]. An advanced DL \u2010based image analysis model has enabled the objective and highly reproducible assessment oftumor ",
    "full_text_length": 81068,
    "chunk_length": 1479
  },
  {
    "chunk_id": 1665,
    "paper_filename": "kaixiang_2024_artificial_intelligence_and_breast_cancer_management_from_data_to_clinic.pdf",
    "paper_title": "Kaixiang 2024 Artificial Intelligence And Breast Cancer Management From Data To Clinic",
    "chunk_index": 25,
    "total_chunks": 75,
    "text_content": "Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License in prognosticating recurrence and evaluating TNBC patient prognosis. These methods hold significant value in predictingrecurrence and evaluating prognosis for these individuals[57, 58 ]. Exploring potential survival predictors through large \u2010scale data analysis is also an important reflection of the involvement of AI in personalized prognosis prediction [ 59]. Traditional patho- logical fac",
    "full_text_length": 81068,
    "chunk_length": 1394
  },
  {
    "chunk_id": 1666,
    "paper_filename": "kaixiang_2024_artificial_intelligence_and_breast_cancer_management_from_data_to_clinic.pdf",
    "paper_title": "Kaixiang 2024 Artificial Intelligence And Breast Cancer Management From Data To Clinic",
    "chunk_index": 26,
    "total_chunks": 75,
    "text_content": "biomarkers (CCNE2, NUSAP1, TPX2, S100P,ITM2A, LIFR, TNXA, and ZBTB16) [ 61]. Zheng et al. combined clinical features and transcriptome analysis with ML screening of prognostic biomarkers and applied the LASSO \u2010Cox regres- sion coefficient to construct a risk model [ 62]. Prognostic models with more patient information and clinical trials canhelp achieve better clinical data practicability and more efficientand accurate treatment planning and clinical decision \u2010making. 4 | BC Treatment 4.1 | Drug",
    "full_text_length": 81068,
    "chunk_length": 1415
  },
  {
    "chunk_id": 1667,
    "paper_filename": "kaixiang_2024_artificial_intelligence_and_breast_cancer_management_from_data_to_clinic.pdf",
    "paper_title": "Kaixiang 2024 Artificial Intelligence And Breast Cancer Management From Data To Clinic",
    "chunk_index": 27,
    "total_chunks": 75,
    "text_content": "63, 64 ]. The objective tumor response rate of first \u2010line treatment with CDK4/6 inhibitors is less than 50%, and conducting a comprehensive analysis with the construction of interpretable DL for the tumorgene profile can predict the effectiveness and resistance of palbociclib for BC [ 65]. Sammut et al. used ML approaches that integrated clinical, molecular, and digital pathology data topredict treatment responses [ 63]. These models were externally validated and showed excellent discrimination",
    "full_text_length": 81068,
    "chunk_length": 1452
  },
  {
    "chunk_id": 1668,
    "paper_filename": "kaixiang_2024_artificial_intelligence_and_breast_cancer_management_from_data_to_clinic.pdf",
    "paper_title": "Kaixiang 2024 Artificial Intelligence And Breast Cancer Management From Data To Clinic",
    "chunk_index": 28,
    "total_chunks": 75,
    "text_content": "regimen adjustments, whichwould benefit patients with poor response to NAC. Multiple DLmodels have demonstrated superior overall performance inevaluating the effects of NAC compared with some conven- tional response prediction methods (Table 1). In a retrospective study, one model focused on MRI \u2010based quantitative in- tratumor heterogeneity measures, combined with clinico-pathological variables and conventional radiomics. This modeldisplayed excellent performance in predicting the pathologicalc",
    "full_text_length": 81068,
    "chunk_length": 1444
  },
  {
    "chunk_id": 1669,
    "paper_filename": "kaixiang_2024_artificial_intelligence_and_breast_cancer_management_from_data_to_clinic.pdf",
    "paper_title": "Kaixiang 2024 Artificial Intelligence And Breast Cancer Management From Data To Clinic",
    "chunk_index": 29,
    "total_chunks": 75,
    "text_content": "AI has emerged as an invaluable tool across various stages of the drugdiscovery pipeline, encompassing novel drug design, analysis ofdrug reactions and reverse synthesis, and molecular optimi- zation and screening (Figure 3)[76]. By analyzing and inter- preting vast amounts of biological, chemical, and clinical data,AI can be trained to accurately identify compound hits anddrug molecular structures. It can also rapidly verify drug tar-gets and optimize drug structure designs. A recent study hasd",
    "full_text_length": 81068,
    "chunk_length": 1230
  },
  {
    "chunk_id": 1670,
    "paper_filename": "kaixiang_2024_artificial_intelligence_and_breast_cancer_management_from_data_to_clinic.pdf",
    "paper_title": "Kaixiang 2024 Artificial Intelligence And Breast Cancer Management From Data To Clinic",
    "chunk_index": 30,
    "total_chunks": 75,
    "text_content": "different deep CNNs 420 183 0.94 [ 71] MRI Imaging \u2010based decision tree models 335 1254 0.87 [ 72] MRI Multilayer perception, and so on 409 343/170/340 0.93 [ 73] Pathology Federated learning 449 237 0.78 [ 74] Note: The selected AUC refers to the optimal value obtained from the validation set mentioned in the citation. Please refer to the original for the exact valu e. Abbreviations: AUC, area under the receiver operating characteristic curve; CNN, convolutional neural network; DL, deep learnin",
    "full_text_length": 81068,
    "chunk_length": 1459
  },
  {
    "chunk_id": 1671,
    "paper_filename": "kaixiang_2024_artificial_intelligence_and_breast_cancer_management_from_data_to_clinic.pdf",
    "paper_title": "Kaixiang 2024 Artificial Intelligence And Breast Cancer Management From Data To Clinic",
    "chunk_index": 31,
    "total_chunks": 75,
    "text_content": "RNA (mRNA) vaccines and drugs generated using this AI tool exhibited ex-ceptional accessibility, optimized stability, and encoded thenecessary epitopes. Consequently, they can provide valuable support for further advancements in mRNA \u2010based anticancer drug research [ 77]. Additionally, the continuous advancement of DL offers enhanced prospects for protein structure predic-tion and drug design [ 78, 79 ]. For example, the AlphaFold tool has successfully elucidated the structures of approximately2",
    "full_text_length": 81068,
    "chunk_length": 1413
  },
  {
    "chunk_id": 1672,
    "paper_filename": "kaixiang_2024_artificial_intelligence_and_breast_cancer_management_from_data_to_clinic.pdf",
    "paper_title": "Kaixiang 2024 Artificial Intelligence And Breast Cancer Management From Data To Clinic",
    "chunk_index": 32,
    "total_chunks": 75,
    "text_content": "characteristicsand toxic side effects [ 82]. Applying AI in ADC development and innovation is worth exploring further. The use of AI inlarge \u2010molecule drug discovery is also rapidly increasing. In oncology, large \u2010molecule drugs are expected to account for about 50% of market revenue by 2030, suggesting that AI may establish additional opportunities for developing BC ther-apeutics [ 87]. With the advent of the AI \u2010driven drug discovery era, AI \u2010derived drugs are being increasingly implemented in",
    "full_text_length": 81068,
    "chunk_length": 1378
  },
  {
    "chunk_id": 1673,
    "paper_filename": "kaixiang_2024_artificial_intelligence_and_breast_cancer_management_from_data_to_clinic.pdf",
    "paper_title": "Kaixiang 2024 Artificial Intelligence And Breast Cancer Management From Data To Clinic",
    "chunk_index": 33,
    "total_chunks": 75,
    "text_content": "of radiomics, pathology, and other data sets has enabledmore precise, safe, efficient, and satisfactory surgical decision \u2010 making [ 88]. One study used ML algorithms to predict breast satisfaction during follow \u2010up for women considering mastec- tomy and reconstruction as part of their BC treatment plan [ 26], which provided a personalized reference. Furthermore, patterns and associations are employed for anatomical visualization andsurgical navigation to assist surgeons in achieving accuratepro",
    "full_text_length": 81068,
    "chunk_length": 1471
  },
  {
    "chunk_id": 1674,
    "paper_filename": "kaixiang_2024_artificial_intelligence_and_breast_cancer_management_from_data_to_clinic.pdf",
    "paper_title": "Kaixiang 2024 Artificial Intelligence And Breast Cancer Management From Data To Clinic",
    "chunk_index": 34,
    "total_chunks": 75,
    "text_content": "experience of young surgeons and increase their understanding and depth perception [ 93, 94 ]. When applied to surgical implant materials, AI \u2010driven 3D printing technology has accelerated the construction of new engineered tissue structures and promotedpersonalized organ substitutes for patients. 3D/4D printed im-plants loaded with chemotherapeutic drugs, such as paclitaxeland fluorouracil, have shown high customization ability and good anticancer activity [ 95, 96 ]. We infer that in the futur",
    "full_text_length": 81068,
    "chunk_length": 1512
  },
  {
    "chunk_id": 1675,
    "paper_filename": "kaixiang_2024_artificial_intelligence_and_breast_cancer_management_from_data_to_clinic.pdf",
    "paper_title": "Kaixiang 2024 Artificial Intelligence And Breast Cancer Management From Data To Clinic",
    "chunk_index": 35,
    "total_chunks": 75,
    "text_content": "Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License better fit the natural shape of the patient's breast and person- alize their treatment plan. There is currently a gradual increasein robotic surgery research for BC, especially for breast \u2010 conserving surgery and minimally invasive surgery [ 97]. The integration of ML, machine vision, and haptic control intosurgi",
    "full_text_length": 81068,
    "chunk_length": 1447
  },
  {
    "chunk_id": 1676,
    "paper_filename": "kaixiang_2024_artificial_intelligence_and_breast_cancer_management_from_data_to_clinic.pdf",
    "paper_title": "Kaixiang 2024 Artificial Intelligence And Breast Cancer Management From Data To Clinic",
    "chunk_index": 36,
    "total_chunks": 75,
    "text_content": "trend of precise dose distribution and minimize the risk of locoregional recurrence, as well as side effects. However, various limitations haveresulted in a high reliance on clinicians for breast clinical targetvolume segmentation [ 99]. DL algorithms for regions, bound- aries, dosimetry, and other indicators have been empiricallydemonstrated to be effective, consistent, and time \u2010saving in the context of automatic segmentation tasks [ 101, 102 ]. In general, AI\u2010based automation has emerged as a",
    "full_text_length": 81068,
    "chunk_length": 1489
  },
  {
    "chunk_id": 1677,
    "paper_filename": "kaixiang_2024_artificial_intelligence_and_breast_cancer_management_from_data_to_clinic.pdf",
    "paper_title": "Kaixiang 2024 Artificial Intelligence And Breast Cancer Management From Data To Clinic",
    "chunk_index": 37,
    "total_chunks": 75,
    "text_content": "treatment.However, the response rates vary among individual patients[105]. The KEYNOTE \u2010355 trial revealed that the combination of pembrolizumab with chemotherapy resulted in longer overallsurvival compared with chemotherapy alone, especially for patients with a programmed death \u2010ligand 1 (PD \u2010L1) combined positive score of 10 or higher. Notably, within the intention \u2010to\u2010 treat population, 40.8% of patients receiving the combinationtherapy achieved a confirmed objective response [ 106]. The afor",
    "full_text_length": 81068,
    "chunk_length": 1496
  },
  {
    "chunk_id": 1678,
    "paper_filename": "kaixiang_2024_artificial_intelligence_and_breast_cancer_management_from_data_to_clinic.pdf",
    "paper_title": "Kaixiang 2024 Artificial Intelligence And Breast Cancer Management From Data To Clinic",
    "chunk_index": 38,
    "total_chunks": 75,
    "text_content": "integrated with high \u2010dimensional data, such as single \u2010celltranscriptomics and spatial transcriptomics, has been employed for predicting the composition and spatial distribution of cellswithin the tumor microenvironment (Table 2), thereby aiding in antibody design and immunotherapy prediction [ 64]. 4.5 | Clinical Decisions The increasing availability of therapeutic options and the pro- liferation of patient clinical imaging methods and pathologicalinformation have posed a challenge in selectin",
    "full_text_length": 81068,
    "chunk_length": 1531
  },
  {
    "chunk_id": 1679,
    "paper_filename": "kaixiang_2024_artificial_intelligence_and_breast_cancer_management_from_data_to_clinic.pdf",
    "paper_title": "Kaixiang 2024 Artificial Intelligence And Breast Cancer Management From Data To Clinic",
    "chunk_index": 39,
    "total_chunks": 75,
    "text_content": "the comprehensive medical information of relevantpatients. The algorithm integrates cutting \u2010edge medical knowledge and uses input digital variables, namely patientmedical information, to calculate the probability of patientoutcomes [ 115], primarily simulating the clinical thinking process. The DL \u2010based CDS that facilitates physician decision \u2010 making in an interactive manner between humans and com-puters can provide reliable evidence \u2010based recommendations and real \u2010time capabilities. This ca",
    "full_text_length": 81068,
    "chunk_length": 1467
  },
  {
    "chunk_id": 1680,
    "paper_filename": "kaixiang_2024_artificial_intelligence_and_breast_cancer_management_from_data_to_clinic.pdf",
    "paper_title": "Kaixiang 2024 Artificial Intelligence And Breast Cancer Management From Data To Clinic",
    "chunk_index": 40,
    "total_chunks": 75,
    "text_content": "in the development of smart hospitals, encompassing various aspects of enhancing care quality, such as medication monitoring,medical record quality control, clinical warning alerts, andpatient experience surveys [ 119\u2013122]. CDS is gradually being integrated into clinical practice and expanding its reach toprimary healthcare institutions. To effectively adapt to theintricate realities of medical environments, CDS strives to en- hance physician autonomy, safeguard privacy, improve trans- parency, ",
    "full_text_length": 81068,
    "chunk_length": 1519
  },
  {
    "chunk_id": 1681,
    "paper_filename": "kaixiang_2024_artificial_intelligence_and_breast_cancer_management_from_data_to_clinic.pdf",
    "paper_title": "Kaixiang 2024 Artificial Intelligence And Breast Cancer Management From Data To Clinic",
    "chunk_index": 41,
    "total_chunks": 75,
    "text_content": "better serve patients both pre \u2010and posttreatment, it is also necessary to improve the clinical fit and integrate 8o f1 4 Cancer Innovation , 2025 27709183, 2025, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/cai2.159 by INASP/HINARI - PAKISTAN, Wiley Online Library on [12/03/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License TABLE",
    "full_text_length": 81068,
    "chunk_length": 1430
  },
  {
    "chunk_id": 1682,
    "paper_filename": "kaixiang_2024_artificial_intelligence_and_breast_cancer_management_from_data_to_clinic.pdf",
    "paper_title": "Kaixiang 2024 Artificial Intelligence And Breast Cancer Management From Data To Clinic",
    "chunk_index": 42,
    "total_chunks": 75,
    "text_content": "\u2010L1 The assistance of AI eliminated any notable disparity in PD \u2010L1 CPS interpretation results among pathologists, while also boosting the intra \u2010group correlation coefficient from 0.62 to 0.93, further enhancing result accuracy.[111] Pathology DL TIL The TME's immunophenotype can be accurately predicted for different tumor types using a DL model based on TIL analysis, enabling precise determination of the response to ICI treatment.[112] Genomics ML Macrophage By integrating scRNA \u2010seq data, the",
    "full_text_length": 81068,
    "chunk_length": 1542
  },
  {
    "chunk_id": 1683,
    "paper_filename": "kaixiang_2024_artificial_intelligence_and_breast_cancer_management_from_data_to_clinic.pdf",
    "paper_title": "Kaixiang 2024 Artificial Intelligence And Breast Cancer Management From Data To Clinic",
    "chunk_index": 43,
    "total_chunks": 75,
    "text_content": "machine learning; PD \u2010L1, programmed death \u2010ligand 1; scRNA, small conditional RNA; TIL, tumor \u2010infiltrating lymphocyte; TME, tumor microenvironment. 9o f1 4 27709183, 2025, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/cai2.159 by INASP/HINARI - PAKISTAN, Wiley Online Library on [12/03/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons Li",
    "full_text_length": 81068,
    "chunk_length": 1559
  },
  {
    "chunk_id": 1684,
    "paper_filename": "kaixiang_2024_artificial_intelligence_and_breast_cancer_management_from_data_to_clinic.pdf",
    "paper_title": "Kaixiang 2024 Artificial Intelligence And Breast Cancer Management From Data To Clinic",
    "chunk_index": 44,
    "total_chunks": 75,
    "text_content": "rates and quality of life. To optimize thebenefits for BC patients, it is crucial to develop and implement AI technologies in a manner that ensures safety, trustworthi- ness, and patient \u2010centricity. Author Contributions Kaixiang Feng: writing \u2013original draft (lead), writing \u2013review and editing (equal). Zongbi Yi: conceptualization (equal), funding acquisi- tion (lead), supervision (equal), writing \u2013review and editing (lead). Binghe Xu: conceptualization (equal), funding acquisition (equal), sup",
    "full_text_length": 81068,
    "chunk_length": 1402
  },
  {
    "chunk_id": 1685,
    "paper_filename": "kaixiang_2024_artificial_intelligence_and_breast_cancer_management_from_data_to_clinic.pdf",
    "paper_title": "Kaixiang 2024 Artificial Intelligence And Breast Cancer Management From Data To Clinic",
    "chunk_index": 45,
    "total_chunks": 75,
    "text_content": "Swanson, E. Wu, A. Zhang, A. A. Alizadeh, and J. Zou, \u201cFrom Patterns to Patients: Advances in Clinical Machine Learning for Cancer Diagnosis, Prognosis, and Treatment, \u201dCell186, no. 8 (2023): 1772 \u20131791, https://doi.org/10.1016/j.cell.2023.01.035 . 2. P. Rajpurkar and M. P. Lungren, \u201cThe Current and Future State of AI Interpretation of Medical Images, \u201dNew England Journal of Medicine 388, no. 21 (2023): 1981 \u20131990, https://doi.org/10.1056/NEJMra2301725 . 3. K. Bera, K. A. Schalper, D. L. Rimm, V",
    "full_text_length": 81068,
    "chunk_length": 1409
  },
  {
    "chunk_id": 1686,
    "paper_filename": "kaixiang_2024_artificial_intelligence_and_breast_cancer_management_from_data_to_clinic.pdf",
    "paper_title": "Kaixiang 2024 Artificial Intelligence And Breast Cancer Management From Data To Clinic",
    "chunk_index": 46,
    "total_chunks": 75,
    "text_content": "Directions, \u201dSN Computer Science 2, no. 3 (2021): 160, https://doi.org/10.1007/s42979-021-00592-x . 6. M. Gerstung, D. Liu, M. Ghassemi, et al., \u201cArtificial Intelligence, \u201d Cancer Cell 42, no. 6 (2024): 915 \u2013 918, https://doi.org/10.1016/j.ccell. 2024.05.021 . 7. R. L. Siegel, A. N. Giaquinto, and A. Jemal, \u201cCancer Statistics, 2024, \u201d CA: A Cancer Journal for Clinicians 74, no. 1 (2024): 12 \u201349,https://doi. org/10.3322/caac.21820 . 8. E. Agostinetto, J. Gligorov, and M. Piccart, \u201cSystemic Therap",
    "full_text_length": 81068,
    "chunk_length": 1423
  },
  {
    "chunk_id": 1687,
    "paper_filename": "kaixiang_2024_artificial_intelligence_and_breast_cancer_management_from_data_to_clinic.pdf",
    "paper_title": "Kaixiang 2024 Artificial Intelligence And Breast Cancer Management From Data To Clinic",
    "chunk_index": 47,
    "total_chunks": 75,
    "text_content": "Clinical Oncology 19, no. 2 (2021): 91\u2013113, https://doi.org/10.1038/s41571-021-00565-2 . 11. US Preventive Services Task Force. \u201cScreening for Breast Cancer, \u201d Journal of the American Medical Association 331, no. 22 (2024): 1973 \u20131974, https://doi.org/10.1001/jama.2024.5535 . 12. H. Nakahara, K. Namba, A. Fukami, et al., \u201cComputer \u2010Aided Diagnosis (CAD) for Mammography: Preliminary Results, \u201dBreast Cancer 5, no. 4 (1998): 401 \u2013405, https://doi.org/10.1007/bf02967438 . 13. Y. Gao, K. J. Geras, A.",
    "full_text_length": 81068,
    "chunk_length": 1414
  },
  {
    "chunk_id": 1688,
    "paper_filename": "kaixiang_2024_artificial_intelligence_and_breast_cancer_management_from_data_to_clinic.pdf",
    "paper_title": "Kaixiang 2024 Artificial Intelligence And Breast Cancer Management From Data To Clinic",
    "chunk_index": 48,
    "total_chunks": 75,
    "text_content": "D. Miller, et al., \u201cBreast Cancer Statistics, 2022, \u201dCA: A Cancer Journal for Clinicians 72, no. 6 (2022): 524 \u2013541, https://doi.org/10.3322/caac.21754 . 16. T. A. Retson and M. Eghtedari, \u201cExpanding Horizons: The Realities of CAD, the Promise of Artificial Intelligence, and Machine Learning's Role in Breast Imaging Beyond Screening Mammography, \u201dDiagnostics 13, no. 13 (2023): 2133, https://doi.org/10.3390/diagnostics13132133 . 17. H. E. Kim, H. H. Kim, B. K. Han, et al., \u201cChanges in Cancer Dete",
    "full_text_length": 81068,
    "chunk_length": 1398
  },
  {
    "chunk_id": 1689,
    "paper_filename": "kaixiang_2024_artificial_intelligence_and_breast_cancer_management_from_data_to_clinic.pdf",
    "paper_title": "Kaixiang 2024 Artificial Intelligence And Breast Cancer Management From Data To Clinic",
    "chunk_index": 49,
    "total_chunks": 75,
    "text_content": "Breast and Axillary Ultrasound, \u201dClinical Imaging 100 (2023): 64 \u201368,https://doi.org/10.1016/j.clinimag.2023. 05.007 . 20. S. E. Song, O. H. Woo, Y. Cho, K. R. Cho, K. H. Park, and J. W. Kim, \u201cPrediction of Axillary Lymph Node Metastasis in Early \u2010Stage Triple \u2010 Negative Breast Cancer Using Multiparametric and Radiomic Features 10 of 14 Cancer Innovation , 2025 27709183, 2025, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/cai2.159 by INASP/HINARI - PAKISTAN, Wiley Online Library",
    "full_text_length": 81068,
    "chunk_length": 1435
  },
  {
    "chunk_id": 1690,
    "paper_filename": "kaixiang_2024_artificial_intelligence_and_breast_cancer_management_from_data_to_clinic.pdf",
    "paper_title": "Kaixiang 2024 Artificial Intelligence And Breast Cancer Management From Data To Clinic",
    "chunk_index": 50,
    "total_chunks": 75,
    "text_content": "Cancer: A Systematic Review and Meta \u2010Analysis, \u201d BMC Cancer 20, no. 1 (2020): 499, https://doi.org/10.1186/s12885-020- 06992-1 . 23. Y. Shen, F. E. Shamout, J. R. Oliver, et al., \u201cArtificial Intelligence System Reduces False \u2010Positive Findings in the Interpretation of Breast Ultrasound Exams, \u201dNature Communications 12, no. 1 (2021): 5645, https://doi.org/10.1038/s41467-021-26023-2 . 2 4 . J .L i a o ,Y .G u i ,Z .L i ,e ta l . , \u201cArtificial Intelligence \u2010Assisted Ultrasound Image Analysis to Di",
    "full_text_length": 81068,
    "chunk_length": 1346
  },
  {
    "chunk_id": 1691,
    "paper_filename": "kaixiang_2024_artificial_intelligence_and_breast_cancer_management_from_data_to_clinic.pdf",
    "paper_title": "Kaixiang 2024 Artificial Intelligence And Breast Cancer Management From Data To Clinic",
    "chunk_index": 51,
    "total_chunks": 75,
    "text_content": "\u201cTowards Patient \u2010Centered Decision \u2010Making in Breast Cancer Surgery Machine Learning to Predict Individual Patient \u2010 Reported Outcomes at 1 \u2010Year Follow \u2010Up,\u201dAnnals of Surgery 277, no. 1 (2023): e144 \u2013e152, https://doi.org/10.1097/Sla.0000000000004862 . 27. C. Ming, V. Viassolo, N. Probst \u2010Hensch, I. D. Dinov, P. O. Chappuis, and M. C. Katapodi, \u201cMachine Learning \u2010Based Lifetime Breast Cancer Risk Reclassification Compared With the Boadicea Model: Impact on Screening Recommendations, \u201dBritish J",
    "full_text_length": 81068,
    "chunk_length": 1333
  },
  {
    "chunk_id": 1692,
    "paper_filename": "kaixiang_2024_artificial_intelligence_and_breast_cancer_management_from_data_to_clinic.pdf",
    "paper_title": "Kaixiang 2024 Artificial Intelligence And Breast Cancer Management From Data To Clinic",
    "chunk_index": 52,
    "total_chunks": 75,
    "text_content": "2536 \u20132545, https://doi.org/10.1200/Jco.22.01564 . 30. T. Portnoi, A. Yala, T. Schuster, et al., \u201cDeep Learning Model to Assess Cancer Risk on the Basis of a Breast MR Image Alone, \u201d American Journal of Roentgenology 213, no. 1 (2019): 227 \u2013233, https:// doi.org/10.2214/Ajr.18.20813 . 31. K. L\u00e5ng, V. Josefsson, A. M. Larsson, et al., \u201cArtificial Intelligence \u2010 Supported Screen Reading Versus Standard Double Reading in the Mammography Screening With Artificial Intelligence Trial (Masai): A Clinic",
    "full_text_length": 81068,
    "chunk_length": 1409
  },
  {
    "chunk_id": 1693,
    "paper_filename": "kaixiang_2024_artificial_intelligence_and_breast_cancer_management_from_data_to_clinic.pdf",
    "paper_title": "Kaixiang 2024 Artificial Intelligence And Breast Cancer Management From Data To Clinic",
    "chunk_index": 53,
    "total_chunks": 75,
    "text_content": "\u201cEvaluation of an AI Model to Assess Future Breast Cancer Risk, \u201dRadiology 307, no. 5 (2023): e222679, https://doi.org/10.1148/radiol.222679 . 34. Y. Liu, D. Han, A. V. Parwani, and Z. Li, \u201cApplications of Artificial Intelligence in Breast Pathology, \u201dArchives of Pathology & Laboratory Medicine 147, no. 9 (2023): 1003 \u20131013, https://doi.org/10.5858/arpa. 2022-0457-RA .35. M. Yousif, P. J. van Diest, A. Laurinavicius, et al., \u201cArtificial Intel- ligence Applied to Breast Pathology, \u201dVirchows Archi",
    "full_text_length": 81068,
    "chunk_length": 1450
  },
  {
    "chunk_id": 1694,
    "paper_filename": "kaixiang_2024_artificial_intelligence_and_breast_cancer_management_from_data_to_clinic.pdf",
    "paper_title": "Kaixiang 2024 Artificial Intelligence And Breast Cancer Management From Data To Clinic",
    "chunk_index": 54,
    "total_chunks": 75,
    "text_content": "K. Tiemann, T. Krech, et al., \u201cNoninferiority of Artificial Intelligence \u2013Assisted Analysis of Ki \u201067 and Estrogen/Progesterone Receptor in Breast Cancer Routine Diagnostics, \u201dModern Pathology 36, no. 3 (2023): 100033, https://doi.org/10.1016/j.modpat.2022.100033 . 39. S. M. Swain, M. Shastry, and E. Hamilton, \u201cTargeting Her2 \u2010Positive Breast Cancer: Advances and Future Directions, \u201dNature Reviews Drug Discovery 22, no. 2 (2022): 101 \u2013126, https://doi.org/10.1038/s41573-022- 00579-0 . 40. S. Wu,",
    "full_text_length": 81068,
    "chunk_length": 1427
  },
  {
    "chunk_id": 1695,
    "paper_filename": "kaixiang_2024_artificial_intelligence_and_breast_cancer_management_from_data_to_clinic.pdf",
    "paper_title": "Kaixiang 2024 Artificial Intelligence And Breast Cancer Management From Data To Clinic",
    "chunk_index": 55,
    "total_chunks": 75,
    "text_content": "Genetics 20, no. 5 (2019): 273 \u2013282, https://doi.org/10.1038/s41576-018- 0088-9 . 43. Z. Zeng, Y. Li, Y. Li, and Y. Luo, \u201cStatistical and Machine Learning Methods for Spatially Resolved Transcriptomics Data Analysis, \u201dGenome Biology 23, no. 1 (2022): 83, https://doi.org/10.1186/s13059-022-02653-7 . 44. M. Ignatiadis, G. W. Sledge, and S. S. Jeffrey, \u201cLiquid Biopsy Enters the Clinic \u2010Implementation Issues and Future Challenges, \u201dNature Reviews Clinical Oncology 18, no. 5 (2021): 297 \u2013312, https:/",
    "full_text_length": 81068,
    "chunk_length": 1395
  },
  {
    "chunk_id": 1696,
    "paper_filename": "kaixiang_2024_artificial_intelligence_and_breast_cancer_management_from_data_to_clinic.pdf",
    "paper_title": "Kaixiang 2024 Artificial Intelligence And Breast Cancer Management From Data To Clinic",
    "chunk_index": 56,
    "total_chunks": 75,
    "text_content": "Detection of Circulating Tumour DNA via Deep Methylation Sequencing Aided by Machine Learning, \u201dNature Biomedical Engineering 5, no. 6 (2021): 586 \u2013599, https://doi.org/10.1038/s41551-021-00746-5 . 48. K. T. Helzer, M. N. Sharifi, J. M. Sperger, et al., \u201cFragmentomic Analysis of Circulating Tumor DNA \u2010Targeted Cancer Panels, \u201dAnnals of Oncology 34, no. 9 (2023): 813 \u2013825, https://doi.org/10.1016/j.annonc. 2023.06.001 . 49. M. Baum, \u201cThe Curability of Breast Cancer, \u201dBMJ 1, no. 6007 (1976): 439\u20134",
    "full_text_length": 81068,
    "chunk_length": 1447
  },
  {
    "chunk_id": 1697,
    "paper_filename": "kaixiang_2024_artificial_intelligence_and_breast_cancer_management_from_data_to_clinic.pdf",
    "paper_title": "Kaixiang 2024 Artificial Intelligence And Breast Cancer Management From Data To Clinic",
    "chunk_index": 57,
    "total_chunks": 75,
    "text_content": "https://onlinelibrary.wiley.com/doi/10.1002/cai2.159 by INASP/HINARI - PAKISTAN, Wiley Online Library on [12/03/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License 52. D. Lu, X. Long, W. Fu, B. Liu, X. Zhou, and S. Sun, \u201cPredictive Value of Machine Learning for Breast Cancer Recurrence: A SystematicReview and Meta \u2010Analysis, \u201dJournal of Cancer Resea",
    "full_text_length": 81068,
    "chunk_length": 1462
  },
  {
    "chunk_id": 1698,
    "paper_filename": "kaixiang_2024_artificial_intelligence_and_breast_cancer_management_from_data_to_clinic.pdf",
    "paper_title": "Kaixiang 2024 Artificial Intelligence And Breast Cancer Management From Data To Clinic",
    "chunk_index": 58,
    "total_chunks": 75,
    "text_content": "K. Clift, D. Dodwell, S. Lord, et al, \u201cDevelopment and Internal \u2010 External Validation of Statistical and Machine Learning Models for Breast Cancer Prognostication: Cohort Study, \u201dBMJ 381 (2023): 073800, https://doi.org/10.1136/bmj-2022-073800 . 56. X. Li, J. Yang, L. Peng, et al., \u201cTriple \u2010Negative Breast Cancer Has Worse Overall Survival and Cause \u2010Specific Survival Than Non \u2010Triple \u2010 Negative Breast Cancer, \u201dBreast Cancer Research and Treatment 161, no. 2 (2016): 279 \u2013287, https://doi.org/10.1",
    "full_text_length": 81068,
    "chunk_length": 1445
  },
  {
    "chunk_id": 1699,
    "paper_filename": "kaixiang_2024_artificial_intelligence_and_breast_cancer_management_from_data_to_clinic.pdf",
    "paper_title": "Kaixiang 2024 Artificial Intelligence And Breast Cancer Management From Data To Clinic",
    "chunk_index": 59,
    "total_chunks": 75,
    "text_content": "et al., \u201cDeep Learning Supported Dis- covery of Biomarkers for Clinical Prognosis of Liver Cancer, \u201dNature Machine Intelligence 5, no. 4 (2023): 408 \u2013420, https://doi.org/10.1038/ s42256-023-00635-3 . 60. E. Senkus, S. Kyriakides, F. Penault \u2010Llorca, et al., \u201cPrimary Breast Cancer: ESMO Clinical Practice Guidelines for Diagnosis, Treatment and Follow \u2010Up,\u201dAnnals of Oncology 24 (2013): vi7 \u2013vi23, https://doi. org/10.1093/annonc/mdt284 . 61. Z. Mirza, M. S. Ansari, M. S. Iqbal, et al., \u201cIdentifica",
    "full_text_length": 81068,
    "chunk_length": 1435
  },
  {
    "chunk_id": 1700,
    "paper_filename": "kaixiang_2024_artificial_intelligence_and_breast_cancer_management_from_data_to_clinic.pdf",
    "paper_title": "Kaixiang 2024 Artificial Intelligence And Breast Cancer Management From Data To Clinic",
    "chunk_index": 60,
    "total_chunks": 75,
    "text_content": "F. Chin, et al., \u201cMulti \u2010Omic Machine Learning Predictor of Breast Cancer Therapy Response, \u201d Nature 601, no. 7894 (2022): 623 \u2013629, https://doi.org/10.1038/s41586- 021-04278-5 . 64. T. Li, Y. Li, X. Zhu, et al., \u201cArtificial Intelligence in Cancer Immu- notherapy: Applications in Neoantigen Recognition, Antibody Design and Immunotherapy Response Prediction, \u201dSeminars in Cancer Biology 91 (2023): 50 \u201369,https://doi.org/10.1016/j.semcancer.2023.02.007 . 65. S. Park, E. Silva, A. Singhal, et al., \u201c",
    "full_text_length": 81068,
    "chunk_length": 1472
  },
  {
    "chunk_id": 1701,
    "paper_filename": "kaixiang_2024_artificial_intelligence_and_breast_cancer_management_from_data_to_clinic.pdf",
    "paper_title": "Kaixiang 2024 Artificial Intelligence And Breast Cancer Management From Data To Clinic",
    "chunk_index": 61,
    "total_chunks": 75,
    "text_content": "Using Ultrasound Images: A Retrospective, Multicentre Study, \u201d eClinicalMedicine 69 (2024): 102499, https://doi.org/10.1016/j.eclinm. 2024.102499 . 68. K. H. Leung, S. P. Rowe, M. S. Sadaghiani, et al., \u201cDeep Semi- supervised Transfer Learning for Fully Automated Whole \u2010Body Tumor Quantification and Prognosis of Cancer on PET/CT, \u201dJournal of Nuclear Medicine 65, no. 4 (2024): 643 \u2013650, https://doi.org/10.2967/jnumed.123. 267048 . 69. W. Aswolinskiy, E. Munari, H. M. Horlings, et al., \u201cProacting:",
    "full_text_length": 81068,
    "chunk_length": 1458
  },
  {
    "chunk_id": 1702,
    "paper_filename": "kaixiang_2024_artificial_intelligence_and_breast_cancer_management_from_data_to_clinic.pdf",
    "paper_title": "Kaixiang 2024 Artificial Intelligence And Breast Cancer Management From Data To Clinic",
    "chunk_index": 62,
    "total_chunks": 75,
    "text_content": "\u201cPretreatment Ultrasound \u2010 Based Deep Learning Radiomics Model for the Early Prediction of Pathologic Response to Neoadjuvant Chemotherapy in Breast Cancer, \u201d European Radiology 33, no. 8 (2023): 5634 \u20135644, https://doi.org/10. 1007/s00330-023-09555-7 . 72. Z. Shi, X. Huang, Z. Cheng, et al, \u201cMRI \u2010Based Quantification of Intratumoral Heterogeneity for Predicting Treatment Response toNeoadjuvant Chemotherapy in Breast Cancer, \u201dRadiology 308, no. 1 (2023): e222830, https://doi.org/10.1148/radiol.2",
    "full_text_length": 81068,
    "chunk_length": 1545
  },
  {
    "chunk_id": 1703,
    "paper_filename": "kaixiang_2024_artificial_intelligence_and_breast_cancer_management_from_data_to_clinic.pdf",
    "paper_title": "Kaixiang 2024 Artificial Intelligence And Breast Cancer Management From Data To Clinic",
    "chunk_index": 63,
    "total_chunks": 75,
    "text_content": "Common Mechanism of Action of Cancer Drugs Undergoing Clinical Trials, \u201dScience Translational Medicine 11, no. 509 (2019): eaaw8412, https://doi.org/10.1126/scitranslmed.aaw8412 . 76. A. V. Sadybekov and V. Katritch, \u201cComputational Approaches Streamlining Drug Discovery, \u201dNature 616, no. 7958 (2023): 673 \u2013685, https://doi.org/10.1038/s41586-023-05905-z . 77. H. Zhang, L. Zhang, A. Lin, et al., \u201cAlgorithm for Optimized mRNA Design Improves Stability and Immunogenicity, \u201dNature 621, no. 7978 (2023",
    "full_text_length": 81068,
    "chunk_length": 1550
  },
  {
    "chunk_id": 1704,
    "paper_filename": "kaixiang_2024_artificial_intelligence_and_breast_cancer_management_from_data_to_clinic.pdf",
    "paper_title": "Kaixiang 2024 Artificial Intelligence And Breast Cancer Management From Data To Clinic",
    "chunk_index": 64,
    "total_chunks": 75,
    "text_content": "Net- work, \u201dScience 373, no. 6557 (2021): 871 \u2013876, https://doi.org/10.1126/ science.abj8754 . 12 of 14 Cancer Innovation , 2025 27709183, 2025, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/cai2.159 by INASP/HINARI - PAKISTAN, Wiley Online Library on [12/03/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License 81. E. Callaway, \u201c\u2018The ",
    "full_text_length": 81068,
    "chunk_length": 1453
  },
  {
    "chunk_id": 1705,
    "paper_filename": "kaixiang_2024_artificial_intelligence_and_breast_cancer_management_from_data_to_clinic.pdf",
    "paper_title": "Kaixiang 2024 Artificial Intelligence And Breast Cancer Management From Data To Clinic",
    "chunk_index": 65,
    "total_chunks": 75,
    "text_content": "11903, https://doi.org/10.3390/ijms241511903 . 84. S. Verma, D. Miles, L. Gianni, et al., \u201cTrastuzumab Emtansine for Her2 \u2010Positive Advanced Breast Cancer, \u201dNew England Journal of Medicine 367, no. 19 (2012): 1783 \u20131791, https://doi.org/10.1056/ NEJMoa1209124 . 85. C. H. Chau, P. S. Steeg, and W. D. Figg, \u201cAntibody \u2010Drug Conjugates for Cancer, \u201dLancet 394, no. 10200 (2019): 793 \u2013804, https://doi.org/10. 1016/S0140-6736(19)31774-X . 86. H. S. Rugo, A. Bardia, F. Marm\u00e9, et al., \u201cOverall Survival W",
    "full_text_length": 81068,
    "chunk_length": 1381
  },
  {
    "chunk_id": 1706,
    "paper_filename": "kaixiang_2024_artificial_intelligence_and_breast_cancer_management_from_data_to_clinic.pdf",
    "paper_title": "Kaixiang 2024 Artificial Intelligence And Breast Cancer Management From Data To Clinic",
    "chunk_index": 66,
    "total_chunks": 75,
    "text_content": "023-00139-0 . 88. C. L. Soh, V. Shah, A. Arjomandi Rad, et al., \u201cPresent and Future of Machine Learning in Breast Surgery: Systematic Review, \u201dBritish Journal of Surgery 109, no. 11 (2022): 1053 \u20131062, https://doi.org/10. 1093/bjs/znac224 . 89. E. Borsting, R. DeSimone, M. Ascha, and M. Ascha, \u201cApplied Deep Learning in Plastic Surgery: Classifying Rhinoplasty With a Mobile App, \u201dJournal of Craniofacial Surgery 31, no. 1 (2020): 102 \u2013106, https:// doi.org/10.1097/Scs.0000000000005905 . 90. M. R. ",
    "full_text_length": 81068,
    "chunk_length": 1391
  },
  {
    "chunk_id": 1707,
    "paper_filename": "kaixiang_2024_artificial_intelligence_and_breast_cancer_management_from_data_to_clinic.pdf",
    "paper_title": "Kaixiang 2024 Artificial Intelligence And Breast Cancer Management From Data To Clinic",
    "chunk_index": 67,
    "total_chunks": 75,
    "text_content": "R. Abdullah, \u201cUtilizing Machine Learning Methods for Preoperative Prediction of Postsurgical Mortality and Intensive Care Unit Admission, \u201dAnnals of Surgery 272, no. 6 (2020): 1133 \u20131139, https://doi.org/10.1097/Sla. 0000000000003297 . 93. A. S. Ahuja, B. W. Polascik, D. Doddapaneni, E. S. Byrnes, and J. Sridhar, \u201cThe Digital Metaverse: Applications in Artificial Intelli- gence, Medical Education, and Integrative Health, \u201dIntegrative Medicine Research 12, no. 1 (2023): 100917, https://doi.org/10",
    "full_text_length": 81068,
    "chunk_length": 1490
  },
  {
    "chunk_id": 1708,
    "paper_filename": "kaixiang_2024_artificial_intelligence_and_breast_cancer_management_from_data_to_clinic.pdf",
    "paper_title": "Kaixiang 2024 Artificial Intelligence And Breast Cancer Management From Data To Clinic",
    "chunk_index": 68,
    "total_chunks": 75,
    "text_content": "Implants for Breast Cancer Manage- ment, \u201dInternational Journal of Pharmaceutics 642 (2023): 123154, https://doi.org/10.1016/j.ijpharm.2023.123154 . 97. H. F. Ma, Y. Lu, and J. Shen, \u201cBibliometric Analysis of Robotic Surgery Research in Breast Cancer Conducted Between 2008 and 2022, \u201d Gland Surgery 12, no. 6 (2023): 767 \u2013779, https://doi.org/10.21037/gs- 22-540 . 98. M. Yip, S. Salcudean, K. Goldberg, et al., \u201cArtificial Intelligence Meets Medical Robotics, \u201dScience 381, no. 6654 (2023): 141 \u201314",
    "full_text_length": 81068,
    "chunk_length": 1396
  },
  {
    "chunk_id": 1709,
    "paper_filename": "kaixiang_2024_artificial_intelligence_and_breast_cancer_management_from_data_to_clinic.pdf",
    "paper_title": "Kaixiang 2024 Artificial Intelligence And Breast Cancer Management From Data To Clinic",
    "chunk_index": 69,
    "total_chunks": 75,
    "text_content": ". 101. Y. Zhong, Y. Guo, Y. Fang, Z. Wu, J. Wang, and W. Hu, \u201cGeometric and Dosimetric Evaluation of Deep Learning Based Auto \u2010Segmentation for Clinical Target Volume on Breast Cancer, \u201dJournal of Applied Clinical Medical Physics 24, no. 7 (2023): e13951, https://doi.org/10. 1002/acm2.13951 . 102. P. Buelens, S. Willems, L. Vandewinckele, W. Crijns, F. Maes, and C. G. Weltens, \u201cClinical Evaluation of a Deep Learning Model for Segmentation of Target Volumes in Breast Cancer Radiotherapy, \u201d Radiot",
    "full_text_length": 81068,
    "chunk_length": 1397
  },
  {
    "chunk_id": 1710,
    "paper_filename": "kaixiang_2024_artificial_intelligence_and_breast_cancer_management_from_data_to_clinic.pdf",
    "paper_title": "Kaixiang 2024 Artificial Intelligence And Breast Cancer Management From Data To Clinic",
    "chunk_index": 70,
    "total_chunks": 75,
    "text_content": "and D. M. Collins, \u201cImmune Checkpoint Inhibitors: Key Trials and an Emerging Role in Breast Cancer, \u201d Seminars in Cancer Biology 79 (2022): 44 \u201357,https://doi.org/10.1016/j. semcancer.2020.06.016 . 106. J. Cortes, H. S. Rugo, D. W. Cescon, et al., \u201cPembrolizumab Plus Chemotherapy in Advanced Triple \u2010Negative Breast Cancer, \u201dNew England Journal of Medicine 387, no. 3 (2022): 217 \u2013226, https://doi. org/10.1056/NEJMoa2202809 . 107. V. Geurts and M. Kok, \u201cImmunotherapy for Metastatic Triple Negative",
    "full_text_length": 81068,
    "chunk_length": 1451
  },
  {
    "chunk_id": 1711,
    "paper_filename": "kaixiang_2024_artificial_intelligence_and_breast_cancer_management_from_data_to_clinic.pdf",
    "paper_title": "Kaixiang 2024 Artificial Intelligence And Breast Cancer Management From Data To Clinic",
    "chunk_index": 71,
    "total_chunks": 75,
    "text_content": "\u201dAnnals of Oncology 35, no. 1 (2024): 29 \u201365,https://doi.org/10. 1016/j.annonc.2023.10.125 . 110. T. G. Chang, Y. Cao, H. J. Sfreddo, et al., \u201cLoris Robustly Predicts Patient Outcomes With Immune Checkpoint Blockade Therapy Using Common Clinical, Pathologic and Genomic Features, \u201dNature Cancer 5, no. 7 (2024): 1158 \u20131175, https://doi.org/10.1038/s43018-024-00772-7 . 111. J. Li, P. Dong, X. Wang, et al., \u201cArtificial Intelligence Enhances Whole \u2010Slide Interpretation of PD \u2010L1 CPS in Triple \u2010Negati",
    "full_text_length": 81068,
    "chunk_length": 1490
  },
  {
    "chunk_id": 1712,
    "paper_filename": "kaixiang_2024_artificial_intelligence_and_breast_cancer_management_from_data_to_clinic.pdf",
    "paper_title": "Kaixiang 2024 Artificial Intelligence And Breast Cancer Management From Data To Clinic",
    "chunk_index": 72,
    "total_chunks": 75,
    "text_content": "Across Multiple Cancer Types, \u201dJournal for Immunotherapy of Cancer 12, no. 2 (2024): e008339, https://doi.org/10.1136/jitc-2023- 008339 . 113. C. Liu, J. Xie, B. Lin, et al., \u201cPan\u2010Cancer Single \u2010Cell and Spatial \u2010 Resolved Profiling Reveals the Immunosuppressive Role of APOE+Macrophages in Immune Checkpoint Inhibitor Therapy, \u201dAdvanced Science 11, no. 23 (2024): e2401061, https://doi.org/10.1002/advs. 202401061 . 114. J. Zhao, Z. Sun, Y. Yu, et al., \u201cRadiomic and Clinical Data Inte- gration Usin",
    "full_text_length": 81068,
    "chunk_length": 1426
  },
  {
    "chunk_id": 1713,
    "paper_filename": "kaixiang_2024_artificial_intelligence_and_breast_cancer_management_from_data_to_clinic.pdf",
    "paper_title": "Kaixiang 2024 Artificial Intelligence And Breast Cancer Management From Data To Clinic",
    "chunk_index": 73,
    "total_chunks": 75,
    "text_content": "System and Clinicians in Breast Cancer, \u201dHeliyon 9, no. 5 (2023): e16059, https://doi.org/10.1016/j.heliyon.2023.e16059 . 117. G. Manzo, Y. Pannatier, P. Duflot, et al., \u201cBreast Cancer Survival Analysis Agents for Clinical Decision Support, \u201dComputer Methods and Programs in Biomedicine 231 (2023): 107373, https://doi.org/10.1016/j. cmpb.2023.107373 . 118. G. C Manikis, N. J. Simos, K. Kourou, et al., \u201cPersonalized Risk Analysis to Improve the Psychological Resilience of Women Under- going Treatm",
    "full_text_length": 81068,
    "chunk_length": 1461
  },
  {
    "chunk_id": 1714,
    "paper_filename": "kaixiang_2024_artificial_intelligence_and_breast_cancer_management_from_data_to_clinic.pdf",
    "paper_title": "Kaixiang 2024 Artificial Intelligence And Breast Cancer Management From Data To Clinic",
    "chunk_index": 74,
    "total_chunks": 75,
    "text_content": "\u201cDevelopment of Redcap \u2010Based Architecture for a Clinical Decision Support Tool Linked to the Electronic Health Record for Assessment of Medication Appropriateness, \u201dJAMIA Open 6, no. 2 (2023): ooad041, https://doi.org/10.1093/jamiaopen/ooad041 . 121. J. Rosa, M. Tajerian, Y. Zin, et al., \u201cDevelopment, Implementation and Initial Results of CDSS Recommendations for Patients at Risk of Hereditary Breast Cancer, \u201dStudies in Health Technology and Informatics 290 (2022): 340 \u2013344, https://doi.org/10.",
    "full_text_length": 81068,
    "chunk_length": 1173
  },
  {
    "chunk_id": 1715,
    "paper_filename": "kento_2024_automated_detection_of_cancer_suspicious_finding_in_radiology_reprots_with_nlp_tecnique.pdf",
    "paper_title": "Kento 2024 Automated Detection Of Cancer Suspicious Finding In Radiology Reprots With Nlp Tecnique",
    "chunk_index": 0,
    "total_chunks": 40,
    "text_content": "Vol.:(0123456789)Journal of Imaging Informatics in Medicine https://doi.org/10.1007/s10278-024-01338-w ORIGINAL PAPER Automated Detection of Cancer\u2011Suspicious Findings in Japanese Radiology Reports with Natural Language Processing: A Multicenter Study Kento Sugimoto1 \u00b7 Shoya Wada1,2 \u00b7 Shozo Konishi1 \u00b7 Junya Sato3 \u00b7 Katsuki Okada1 \u00b7 Shoji Kido4,5 \u00b7 Noriyuki Tomiyama6 \u00b7 Yasushi Matsumura1,7 \u00b7 Toshihiro Takeda1 Received: 16 September 2024 / Revised: 7 November 2024 / Accepted: 8 November 2024 \u00a9 The",
    "full_text_length": 42034,
    "chunk_length": 1426
  },
  {
    "chunk_id": 1716,
    "paper_filename": "kento_2024_automated_detection_of_cancer_suspicious_finding_in_radiology_reprots_with_nlp_tecnique.pdf",
    "paper_title": "Kento 2024 Automated Detection Of Cancer Suspicious Finding In Radiology Reprots With Nlp Tecnique",
    "chunk_index": 1,
    "total_chunks": 40,
    "text_content": "create the gold standard, reports were annotated by two experienced physicians. Data were statistically analyzed using precision, recall and F1 score with 1000 bootstrap iterations. BERT was used as a baseline deep learning model, and its performance was compared with the proposed rule-based method. At the report level of detection, the overall precision, recall, and F-1 score were 0.886, 0.886, and 0.883, respectively, for the rule-based algorithm, which were higher than those of the deep learn",
    "full_text_length": 42034,
    "chunk_length": 1430
  },
  {
    "chunk_id": 1717,
    "paper_filename": "kento_2024_automated_detection_of_cancer_suspicious_finding_in_radiology_reprots_with_nlp_tecnique.pdf",
    "paper_title": "Kento 2024 Automated Detection Of Cancer Suspicious Finding In Radiology Reprots With Nlp Tecnique",
    "chunk_index": 2,
    "total_chunks": 40,
    "text_content": "\u00b7 Information extraction \u00b7 Radiology report \u00b7 Actionable findings Introduction Radiology reports are important documents for radiologists to communicate with referring physicians. Although radi- ologists document examination results in their report, busy referring physicians may overlook the content [1 ]. These communication problems were among the serious medical errors in the past, resulting in delays or missed patient fol- low-up [2 , 3]. Failure to properly follow-up and treat serious diseas",
    "full_text_length": 42034,
    "chunk_length": 1430
  },
  {
    "chunk_id": 1718,
    "paper_filename": "kento_2024_automated_detection_of_cancer_suspicious_finding_in_radiology_reprots_with_nlp_tecnique.pdf",
    "paper_title": "Kento 2024 Automated Detection Of Cancer Suspicious Finding In Radiology Reprots With Nlp Tecnique",
    "chunk_index": 3,
    "total_chunks": 40,
    "text_content": "for non-urgent critical findings, such as cancers, because * Kento Sugimoto sugimoto.kento@hp-info.med.osaka-u.ac.jp 1 Department of Medical Informatics, Osaka University Graduate School of Medicine, 2-2 Yamadaoka, Suita, 565-0871 Osaka, Japan 2 Department of Transformative System for Medical Information, Osaka University Graduate School of Medicine, 2-2, Yamadaoka, Suita, 565-0871 Osaka, Japan 3 Department of Artificial Intelligence in Diagnostic Radiology, Osaka University Graduate School of M",
    "full_text_length": 42034,
    "chunk_length": 1495
  },
  {
    "chunk_id": 1719,
    "paper_filename": "kento_2024_automated_detection_of_cancer_suspicious_finding_in_radiology_reprots_with_nlp_tecnique.pdf",
    "paper_title": "Kento 2024 Automated Detection Of Cancer Suspicious Finding In Radiology Reprots With Nlp Tecnique",
    "chunk_index": 4,
    "total_chunks": 40,
    "text_content": "they increase the workload of radiologists and rely on their subjective judgment in identifying action- able findings [1 , 6, 7]. To automate this process, Natural Language Processing (NLP) offers a promising solution [8\u201314]. This solution has two main approaches: rule and machine learning based. The rule-based approach can be implemented using several methods, such as those based on specific phrases or terms (e.g., \u201cfollow-up\u201d) [ 8\u201310] and those that rely on the syntactic rules of sentences [11",
    "full_text_length": 42034,
    "chunk_length": 1341
  },
  {
    "chunk_id": 1720,
    "paper_filename": "kento_2024_automated_detection_of_cancer_suspicious_finding_in_radiology_reprots_with_nlp_tecnique.pdf",
    "paper_title": "Kento 2024 Automated Detection Of Cancer Suspicious Finding In Radiology Reprots With Nlp Tecnique",
    "chunk_index": 5,
    "total_chunks": 40,
    "text_content": "both approaches is shown in Table 1. Here, we propose a new rule-based NLP algorithm to cover a wide range of actionable findings by incorporating context-based semantic information as well as specific phrases and terms. Through this approach, the high interpretability of rule-based systems can be maintained, and a more flex- ible detection can be achieved by considering the meaning of text within its context. In this study, we focused on find- ings suggestive of cancer rather than any actionabl",
    "full_text_length": 42034,
    "chunk_length": 1360
  },
  {
    "chunk_id": 1721,
    "paper_filename": "kento_2024_automated_detection_of_cancer_suspicious_finding_in_radiology_reprots_with_nlp_tecnique.pdf",
    "paper_title": "Kento 2024 Automated Detection Of Cancer Suspicious Finding In Radiology Reprots With Nlp Tecnique",
    "chunk_index": 6,
    "total_chunks": 40,
    "text_content": "board (Permission number: 24017). Informed consent was waived because of the retrospective nature of the study.Rule\u2011Based Algorithm System Pipeline Our overall system pipeline is illustrated in Fig. 1. First, our system converted a free-text report into a predefined structured schema. Second, clinical terms in a structured report were matched with our developed dictionary to assign concept codes. Finally, cancer-suspicious findings were detected by classification algorithm. Structuring Radiology",
    "full_text_length": 42034,
    "chunk_length": 1479
  },
  {
    "chunk_id": 1722,
    "paper_filename": "kento_2024_automated_detection_of_cancer_suspicious_finding_in_radiology_reprots_with_nlp_tecnique.pdf",
    "paper_title": "Kento 2024 Automated Detection Of Cancer Suspicious Finding In Radiology Reprots With Nlp Tecnique",
    "chunk_index": 7,
    "total_chunks": 40,
    "text_content": "(e.g., upper lobe, 3-cm) are extracted from free-text radiology reports [16]. \u2022 In relation extraction, correct relations between the extracted entities are obtained [17]. \u2022 In certainty classification, a five-level diagnostic certainty scale (definite, likely, may represent, unlikely, denial) is assigned to the observation and clinical finding entities [18]. They described their methods in detail in their research [16\u201318]. We extended this model to include follow-up entities because of their im",
    "full_text_length": 42034,
    "chunk_length": 1398
  },
  {
    "chunk_id": 1723,
    "paper_filename": "kento_2024_automated_detection_of_cancer_suspicious_finding_in_radiology_reprots_with_nlp_tecnique.pdf",
    "paper_title": "Kento 2024 Automated Detection Of Cancer Suspicious Finding In Radiology Reprots With Nlp Tecnique",
    "chunk_index": 8,
    "total_chunks": 40,
    "text_content": "context \u2713 Easy to scale Weaknesses \u2713 Limited to phrases or terms \u2713 Difficult to scale\u2713 Needs large labeled dataset \u2713 Difficult to interpret Journal of Imaging Informatics in Medicine Fig. 1 Overview of our natural language processing (NLP) system pipeline. The system is divided into several modules: 1) Entity extrac- tion, 2) Relation extraction, 3) Certainty classification, Dictionary matching, and the Classification algorithm. First, free-text reports are structured and processed through entit",
    "full_text_length": 42034,
    "chunk_length": 1380
  },
  {
    "chunk_id": 1724,
    "paper_filename": "kento_2024_automated_detection_of_cancer_suspicious_finding_in_radiology_reprots_with_nlp_tecnique.pdf",
    "paper_title": "Kento 2024 Automated Detection Of Cancer Suspicious Finding In Radiology Reprots With Nlp Tecnique",
    "chunk_index": 9,
    "total_chunks": 40,
    "text_content": "anatomical location are visualized as actionable labels (high, low, or no action- able) at the bottom of the figure Journal of Imaging Informatics in Medicine dictionary database for this study. Our database consists of two main types of tables: surfaces and concepts. The sur - face table contains phrases and words that appear in reports, while the concept table standardizes these phrases and words in the surface table. The concept table is divided into the following three categories: \u2022 In the f",
    "full_text_length": 42034,
    "chunk_length": 1322
  },
  {
    "chunk_id": 1725,
    "paper_filename": "kento_2024_automated_detection_of_cancer_suspicious_finding_in_radiology_reprots_with_nlp_tecnique.pdf",
    "paper_title": "Kento 2024 Automated Detection Of Cancer Suspicious Finding In Radiology Reprots With Nlp Tecnique",
    "chunk_index": 10,
    "total_chunks": 40,
    "text_content": "and a body part code. \u2022 In the change concept table, words or phrases extracted as change modifier entities are matched. This table includes a standardized status code attribute (increasing, decreas- ing, new, unchanged). Words and phrases extracted as specific entities (observa- tion, clinical finding, anatomical location modifier, change modifier) in a report were string-matched with the dictionary to look up their concept id. The process of dictionary match- ing is visualized in Fig. 2. The d",
    "full_text_length": 42034,
    "chunk_length": 1425
  },
  {
    "chunk_id": 1726,
    "paper_filename": "kento_2024_automated_detection_of_cancer_suspicious_finding_in_radiology_reprots_with_nlp_tecnique.pdf",
    "paper_title": "Kento 2024 Automated Detection Of Cancer Suspicious Finding In Radiology Reprots With Nlp Tecnique",
    "chunk_index": 11,
    "total_chunks": 40,
    "text_content": "analyzed to reference relevant radiology reports. Through our radiology reporting system, radiolo- gists can flag reports that include critical findings during routine clinical practice. Then, cancer-suspicious reports were manually collected from the flagged reports. Our analysis revealed four distinct writing patterns flagged by radiologists as cancer-suspicious findings. The four patterns include: \u2022 P1 (malignant) are findings explicitly indicating a sus- picion of malignancy; \u2022 P2 (indetermi",
    "full_text_length": 42034,
    "chunk_length": 1445
  },
  {
    "chunk_id": 1727,
    "paper_filename": "kento_2024_automated_detection_of_cancer_suspicious_finding_in_radiology_reprots_with_nlp_tecnique.pdf",
    "paper_title": "Kento 2024 Automated Detection Of Cancer Suspicious Finding In Radiology Reprots With Nlp Tecnique",
    "chunk_index": 12,
    "total_chunks": 40,
    "text_content": "during the structuring step. Figure 3 provides a visual represen- tation of this step, outlining the flow of the classification algorithm. Fig. 2 Overview of the dictionary matching process. The example sentence, \u201cA 3-cm increased nodule in the right upper lobe suggests lung cancer,\u201d is processed by matching entities to the dictionary data- base. Each word or phrase is matched to the corresponding concepts in three tables: findings concepts table, anatomical location concepts table, and change c",
    "full_text_length": 42034,
    "chunk_length": 1309
  },
  {
    "chunk_id": 1728,
    "paper_filename": "kento_2024_automated_detection_of_cancer_suspicious_finding_in_radiology_reprots_with_nlp_tecnique.pdf",
    "paper_title": "Kento 2024 Automated Detection Of Cancer Suspicious Finding In Radiology Reprots With Nlp Tecnique",
    "chunk_index": 13,
    "total_chunks": 40,
    "text_content": "the diagnostic cer - tainty scale of each entity. This scale, provided by the cer - tainty classification module in the structuring step, catego- rizes the findings into a five-level scale: definite, likely, may represent, unlikely, and denial. We specifically excluded the findings labeled as \u201cdenial.\u201d Second, for the remaining observation and clinical enti- ties, classification rule was developed to categorize them into one of the four patterns or as not applicable. Specifi- cally, entities wer",
    "full_text_length": 42034,
    "chunk_length": 1298
  },
  {
    "chunk_id": 1729,
    "paper_filename": "kento_2024_automated_detection_of_cancer_suspicious_finding_in_radiology_reprots_with_nlp_tecnique.pdf",
    "paper_title": "Kento 2024 Automated Detection Of Cancer Suspicious Finding In Radiology Reprots With Nlp Tecnique",
    "chunk_index": 14,
    "total_chunks": 40,
    "text_content": "observation and clinical entities and their modifiers. Specifically, entities that were mapped to finding concepts with a malignancy code of \u201cbenign\u201d or \u201cpossible,\u201d and had follow-up entity modifiers were classified as P3. Entities with change entity modifiers and their status code of \u201cnew\u201d or \u201cincreasing\u201d were classified as P4. Entities related to non- neoplastic disease codes (e.g., pneumonia) were excluded although they met the P3 or P4 criteria. Entities that were not mapped to a finding con",
    "full_text_length": 42034,
    "chunk_length": 1317
  },
  {
    "chunk_id": 1730,
    "paper_filename": "kento_2024_automated_detection_of_cancer_suspicious_finding_in_radiology_reprots_with_nlp_tecnique.pdf",
    "paper_title": "Kento 2024 Automated Detection Of Cancer Suspicious Finding In Radiology Reprots With Nlp Tecnique",
    "chunk_index": 15,
    "total_chunks": 40,
    "text_content": "location of a given finding was identi- fied by referencing either the finding itself or any related anatomical location modifier entities. Deep Learning\u2011Based Algorithm As a baseline, we developed a deep learning-based clas- sification algorithm. We applied BERT [20], a widely used NLP model. As a straightforward method, we formulated this task as a multi-class classification problem by using a Fig. 3 Flowchart of classifica- tion algorithm. The algorithm classifies findings suggestive of cance",
    "full_text_length": 42034,
    "chunk_length": 1346
  },
  {
    "chunk_id": 1731,
    "paper_filename": "kento_2024_automated_detection_of_cancer_suspicious_finding_in_radiology_reprots_with_nlp_tecnique.pdf",
    "paper_title": "Kento 2024 Automated Detection Of Cancer Suspicious Finding In Radiology Reprots With Nlp Tecnique",
    "chunk_index": 16,
    "total_chunks": 40,
    "text_content": "was divided into development (January 2020\u2013June 2023), and test sets (July 2023\u2013December 2023). The test set was composed of internal and external validation sets from our institution and five other institutions, respectively. Development Sets A total of 955 radiology reports from our institution were used for algorithm development and refinement. For the deep learning-based algorithm, these reports were used to fine-tune the model. Test Set A total of 900 reports were collected from the Japan M",
    "full_text_length": 42034,
    "chunk_length": 1400
  },
  {
    "chunk_id": 1732,
    "paper_filename": "kento_2024_automated_detection_of_cancer_suspicious_finding_in_radiology_reprots_with_nlp_tecnique.pdf",
    "paper_title": "Kento 2024 Automated Detection Of Cancer Suspicious Finding In Radiology Reprots With Nlp Tecnique",
    "chunk_index": 17,
    "total_chunks": 40,
    "text_content": "Annotations Reports were annotated by two experienced physicians who read the findings and impression sections of the reports. After a guideline for cancer-suspicious findings was provided, annotators were asked to read the findings and impression sections of the reports and label them as \u201chigh actionable,\u201d \u201clow actionable,\u201d or \u201cno action- able.\u201d They were instructed to assign actionable labels for each anatomical location (e.g., lung, breast, pancreas, etc.), which were predefined by experts. C",
    "full_text_length": 42034,
    "chunk_length": 1374
  },
  {
    "chunk_id": 1733,
    "paper_filename": "kento_2024_automated_detection_of_cancer_suspicious_finding_in_radiology_reprots_with_nlp_tecnique.pdf",
    "paper_title": "Kento 2024 Automated Detection Of Cancer Suspicious Finding In Radiology Reprots With Nlp Tecnique",
    "chunk_index": 18,
    "total_chunks": 40,
    "text_content": "Disagreements between the annotators were resolved by discussion. Statistical Analysis Precision, recall, and F1 score were used to evaluate our algorithm. Overall results were calculated using a macro average, and 95% confidence intervals (CI) were calculated by 1000 bootstrap iterations.Table 2 Classification rules and examples for each pattern (The bold with underline text highlights the key elements in each example for clas- sification rules) Pattern Rule Category Example P1 1. The malignanc",
    "full_text_length": 42034,
    "chunk_length": 1472
  },
  {
    "chunk_id": 1734,
    "paper_filename": "kento_2024_automated_detection_of_cancer_suspicious_finding_in_radiology_reprots_with_nlp_tecnique.pdf",
    "paper_title": "Kento 2024 Automated Detection Of Cancer Suspicious Finding In Radiology Reprots With Nlp Tecnique",
    "chunk_index": 19,
    "total_chunks": 40,
    "text_content": "or \u201cbenign\u201d 2. A finding is related to \u201cchange modifier entity,\u201d and its status code is \u201cnew\u201d or \u201cincreasing\u201dlow actionable A lung nodule[possible] has increased[increasing] in size. (original) \u80ba\u7d50\u7bc0[possible]\u304c\u5897\u5927[increasing]\u3057\u3066\u3044\u307e\u3059\u3002 Journal of Imaging Informatics in Medicine Results Our algorithm and the evaluation dataset were labeled at the anatomical location level. Therefore, in the evaluation at the report level, the following rules were used to assign labels to the reports: \u2022 The report was la",
    "full_text_length": 42034,
    "chunk_length": 1365
  },
  {
    "chunk_id": 1735,
    "paper_filename": "kento_2024_automated_detection_of_cancer_suspicious_finding_in_radiology_reprots_with_nlp_tecnique.pdf",
    "paper_title": "Kento 2024 Automated Detection Of Cancer Suspicious Finding In Radiology Reprots With Nlp Tecnique",
    "chunk_index": 20,
    "total_chunks": 40,
    "text_content": "level. The rule-based algorithm achieved a precision, recall, and F-1 score of 0.886 (95% CI, 0.850\u20130.918), 0.886 (95% CI, 0.852\u20130.920), and 0.883 (95% CI, 0.850\u20130.914), respectively; conversely, the deep learning-based algorithm achieved 0.851 (95% CI, 0.801\u20130.896), 0.679 (95% CI, 0.635\u20130.720), and 0.773 (95% CI, 0.688\u20130.776), respec- tively. The difference (delta) between the rule-based and deep learning-based algorithms in Table 3 indicates that performance of the rule-based was statistically",
    "full_text_length": 42034,
    "chunk_length": 1392
  },
  {
    "chunk_id": 1736,
    "paper_filename": "kento_2024_automated_detection_of_cancer_suspicious_finding_in_radiology_reprots_with_nlp_tecnique.pdf",
    "paper_title": "Kento 2024 Automated Detection Of Cancer Suspicious Finding In Radiology Reprots With Nlp Tecnique",
    "chunk_index": 21,
    "total_chunks": 40,
    "text_content": "0.986 (95% CI, 0.968\u20131.000), and 0.920 (95% CI, 0.892\u20130.946), respectively; for \u201clow actionable,\u201d they were 0.813 (95% CI, 0.714\u20130.902), 0.724 (95% CI, 0.627\u20130.816), and 0.764 (95% CI, 0.685\u20130.835), respec- tively. For \u201cno actionable,\u201d the overall precision, recall, and F-1 score were 0.983 (95% CI, 0.972\u20130.993), 0.950 (95% CI, 0.932\u20130.966), and 0.966 (95% CI, 0.955\u20130.976), respec- tively. \u201cHigh actionable\u201d and \u201cno actionable\u201d demonstrated a high overall performance with small statistical variat",
    "full_text_length": 42034,
    "chunk_length": 1414
  },
  {
    "chunk_id": 1737,
    "paper_filename": "kento_2024_automated_detection_of_cancer_suspicious_finding_in_radiology_reprots_with_nlp_tecnique.pdf",
    "paper_title": "Kento 2024 Automated Detection Of Cancer Suspicious Finding In Radiology Reprots With Nlp Tecnique",
    "chunk_index": 22,
    "total_chunks": 40,
    "text_content": "set showed approxi- mately 0.05 higher values; however, the difference in confi- dence intervals (delta) indicated no statistical significance. Table 3 Comparison results between rule-based and deep learning-based algorithm Data in parentheses are 95% confidence intervalsAlgorithm Precision Recall F1-score Rule-based 0.886 (0.850, 0.918) 0.886 (0.852, 0.920) 0.883 (0.850, 0.914) Deep learning-based 0.851 (0.801, 0.896) 0.679 (0.635, 0.720) 0.733 (0.688, 0.776) Delta 0.035 (\u22120.025, 0.094) 0.208 (",
    "full_text_length": 42034,
    "chunk_length": 1433
  },
  {
    "chunk_id": 1738,
    "paper_filename": "kento_2024_automated_detection_of_cancer_suspicious_finding_in_radiology_reprots_with_nlp_tecnique.pdf",
    "paper_title": "Kento 2024 Automated Detection Of Cancer Suspicious Finding In Radiology Reprots With Nlp Tecnique",
    "chunk_index": 23,
    "total_chunks": 40,
    "text_content": "in parentheses are 95% confidence intervalsDataset Precision Recall F1-score N Internal 0.929 (0.840, 0.997) 0.929 (0.841, 0.997) 0.927 (0.845, 0.986) 150 External 0.875 (0.837, 0.909) 0.879 (0.842, 0.916) 0.873 (0.837, 0.907) 750 Delta 0.053 (\u22120.049, 0.135) 0.050 (\u22120.048, 0.132) 0.053 (\u22120.034, 0.126) 900 Journal of Imaging Informatics in Medicine Figure 4 presents the comparison result of F-1 scores by the institutions. \u201cINT\u201d and \u201cEXT\u201d refer to the internal and external sets, respectively. The ",
    "full_text_length": 42034,
    "chunk_length": 1372
  },
  {
    "chunk_id": 1739,
    "paper_filename": "kento_2024_automated_detection_of_cancer_suspicious_finding_in_radiology_reprots_with_nlp_tecnique.pdf",
    "paper_title": "Kento 2024 Automated Detection Of Cancer Suspicious Finding In Radiology Reprots With Nlp Tecnique",
    "chunk_index": 24,
    "total_chunks": 40,
    "text_content": "in Supplemental Table 2. Discussion Error Analysis Table 3 shows that the performance of the rule-based algo- rithm was significantly better than that of the deep learning algorithm. Classifying cancer-suspicious findings is a com- plex task that involves recognizing cancer-related terms and detecting negations. We considered that the performance of the deep learning algorithm was lower likely because of the insufficient sample size of the training set for solving such a complex task. For exampl",
    "full_text_length": 42034,
    "chunk_length": 1335
  },
  {
    "chunk_id": 1740,
    "paper_filename": "kento_2024_automated_detection_of_cancer_suspicious_finding_in_radiology_reprots_with_nlp_tecnique.pdf",
    "paper_title": "Kento 2024 Automated Detection Of Cancer Suspicious Finding In Radiology Reprots With Nlp Tecnique",
    "chunk_index": 25,
    "total_chunks": 40,
    "text_content": "F1 score for \u201clow actionable\u201d was 0.764, which was lower than that for \u201chigh actionable\u201d and \u201cno actionable\u201d (Table 4). \u201cHigh actionable\u201d is a straight- forward task that primarily identifies phrases such as \u201ccan- cer,\u201d while \u201clow actionable\u201d is more challenging because it requires understanding the context of the findings. We observed that annotation disagreements were most frequent in \u201clow actionable,\u201d showing that even human annotators, including experienced physicians, struggled to determine",
    "full_text_length": 42034,
    "chunk_length": 1430
  },
  {
    "chunk_id": 1741,
    "paper_filename": "kento_2024_automated_detection_of_cancer_suspicious_finding_in_radiology_reprots_with_nlp_tecnique.pdf",
    "paper_title": "Kento 2024 Automated Detection Of Cancer Suspicious Finding In Radiology Reprots With Nlp Tecnique",
    "chunk_index": 26,
    "total_chunks": 40,
    "text_content": "No actionable 0.998 (0.997, 0.998) 0.994 (0.993, 0.995) 0.996 (0.995, 0.997) Total 0.821 (0.785, 0.855) 0.843 (0.812, 0.876) 0.816 (0.785, 0.845) Journal of Imaging Informatics in Medicine appropriate actions for relatively less important findings that still require follow-up. For example, in the follow-up of a lesion suspected of a non-neoplastic disease, one physi- cian determined it as \u201cno actionable,\u201d while another iden- tified it as \u201clow actionable\u201d (Table 7, no. 1) because the possibility ",
    "full_text_length": 42034,
    "chunk_length": 1363
  },
  {
    "chunk_id": 1742,
    "paper_filename": "kento_2024_automated_detection_of_cancer_suspicious_finding_in_radiology_reprots_with_nlp_tecnique.pdf",
    "paper_title": "Kento 2024 Automated Detection Of Cancer Suspicious Finding In Radiology Reprots With Nlp Tecnique",
    "chunk_index": 27,
    "total_chunks": 40,
    "text_content": "tion (Table 7, no. 2, 3). Although annotators could integrate information from the findings and diagnostic sections, our algorithm could not perform this integration because it inde- pendently extracted findings from each section. We demonstrated that the internal and external sets did not significantly differ based on the difference in their con- fidence intervals (Table 5); the F-1 scores did not also sig- nificantly vary between the internal set and four out of the five external sets (Figs. 4",
    "full_text_length": 42034,
    "chunk_length": 1288
  },
  {
    "chunk_id": 1743,
    "paper_filename": "kento_2024_automated_detection_of_cancer_suspicious_finding_in_radiology_reprots_with_nlp_tecnique.pdf",
    "paper_title": "Kento 2024 Automated Detection Of Cancer Suspicious Finding In Radiology Reprots With Nlp Tecnique",
    "chunk_index": 28,
    "total_chunks": 40,
    "text_content": "\u201clow actionable\u201d because the whole con- text did not explicitly indicate any cancer suspicion (Table 7, no. 4, 5). This error pattern was particularly frequent in EXT5, which explained the lower performance of EXT5. The macro-average F1 score of the classification perfor - mance at the anatomical location level was approximately 6.7% lower than that at the report level in the entire dataset (Tables 3 and 4 ). The most frequent error is a case wherein the lesion\u2019s location is indicated with posit",
    "full_text_length": 42034,
    "chunk_length": 1295
  },
  {
    "chunk_id": 1744,
    "paper_filename": "kento_2024_automated_detection_of_cancer_suspicious_finding_in_radiology_reprots_with_nlp_tecnique.pdf",
    "paper_title": "Kento 2024 Automated Detection Of Cancer Suspicious Finding In Radiology Reprots With Nlp Tecnique",
    "chunk_index": 29,
    "total_chunks": 40,
    "text_content": "our anatomical location dictionary. In the sec- ond example, the algorithm predicted \u201ckidney\u201d when our anatomical location dictionary was searched. The lesion\u2019s location is often indicated by positional words, especially in abdominal CT reports. Therefore, we considered expanding the dictionary to accommodate these cases. Table 7 Error analysis examples No Findings Impression Gold Prediction 1 Multiple nodules in the lower lobe of the left lung probably suggest pulmonary cryptococcosis. Addition",
    "full_text_length": 42034,
    "chunk_length": 1645
  },
  {
    "chunk_id": 1745,
    "paper_filename": "kento_2024_automated_detection_of_cancer_suspicious_finding_in_radiology_reprots_with_nlp_tecnique.pdf",
    "paper_title": "Kento 2024 Automated Detection Of Cancer Suspicious Finding In Radiology Reprots With Nlp Tecnique",
    "chunk_index": 30,
    "total_chunks": 40,
    "text_content": "left lung: suggestive of post-inflammatory change. (original) \u5de6\u80ba\u7d50\u7bc0:\u708e\u75c7\u5f8c\u5909\u5316\u3092\u7591\u3044\u307e\u3059\u3002Low actionable High actionable 5 Multiple uterine fibroids are suspected. Exclusion of malig- nancy is necessary. (original) \u591a\u767a\u5b50\u5bae\u7b4b\u816b\u304c\u7591\u308f\u308c\u307e\u3059\u3002\u60aa\u6027\u306e\u9664\u5916\u304c\u5fc5\u8981\u3067\u3059\u3002Suspicious of multiple uterine fibroids. (original) \u591a\u767a\u5b50\u5bae\u7b4b\u816b\u306e\u7591\u3044\u3002Low actionable High actionable Journal of Imaging Informatics in Medicine Limitations and Future Work Our study has several limitations. First, because of the bur - den on annotators and time constraints, we onl",
    "full_text_length": 42034,
    "chunk_length": 1382
  },
  {
    "chunk_id": 1746,
    "paper_filename": "kento_2024_automated_detection_of_cancer_suspicious_finding_in_radiology_reprots_with_nlp_tecnique.pdf",
    "paper_title": "Kento 2024 Automated Detection Of Cancer Suspicious Finding In Radiology Reprots With Nlp Tecnique",
    "chunk_index": 31,
    "total_chunks": 40,
    "text_content": "reports. Lastly, although our algorithm achieved a high perfor - mance, a gap remains between our algorithm and a practical system for clinical use. In this study, we conducted experi- ments to classify cancer-suspicious findings from a single report. Therefore, we did not consider the chronological progresses in the patient\u2019s clinical condition. Notifying the users would be more effective only when a cancer-suspicious finding was detected for the first time in each anatomical location rather th",
    "full_text_length": 42034,
    "chunk_length": 1395
  },
  {
    "chunk_id": 1747,
    "paper_filename": "kento_2024_automated_detection_of_cancer_suspicious_finding_in_radiology_reprots_with_nlp_tecnique.pdf",
    "paper_title": "Kento 2024 Automated Detection Of Cancer Suspicious Finding In Radiology Reprots With Nlp Tecnique",
    "chunk_index": 32,
    "total_chunks": 40,
    "text_content": "the feasibility of our system in real-world clinical practice. Supplementary Information The online version contains supplemen- tary material available at https:// doi. org/ 10. 1007/ s10278- 024- 01338-w . Acknowledgments This study used the Japan Medical Image Database (J-MID), a multi-center database for the external validation of our algo- rithm. We would like to express our sincere gratitude to the radiology departments of the following institutions for their invaluable coop- eration: Junte",
    "full_text_length": 42034,
    "chunk_length": 1524
  },
  {
    "chunk_id": 1748,
    "paper_filename": "kento_2024_automated_detection_of_cancer_suspicious_finding_in_radiology_reprots_with_nlp_tecnique.pdf",
    "paper_title": "Kento 2024 Automated Detection Of Cancer Suspicious Finding In Radiology Reprots With Nlp Tecnique",
    "chunk_index": 33,
    "total_chunks": 40,
    "text_content": "- Review & EditingShoji Kido: Resources, Writing - Review & Editing Noriyuki Tomiyama: Resources, Writing - Review & Editing Yasushi Matsumura: Conceptualization, Methodology, Writing - Review & Editing, Supervision Toshihiro Takeda: Conceptualization, Methodology, Writing - Review & Editing, Supervision, Project administration, Funding acquisition Funding A part of this work was performed for Council for Sci- ence, Technology and Innovation (CSTI), Cross-ministerial Strategic Innovation Promoti",
    "full_text_length": 42034,
    "chunk_length": 1420
  },
  {
    "chunk_id": 1749,
    "paper_filename": "kento_2024_automated_detection_of_cancer_suspicious_finding_in_radiology_reprots_with_nlp_tecnique.pdf",
    "paper_title": "Kento 2024 Automated Detection Of Cancer Suspicious Finding In Radiology Reprots With Nlp Tecnique",
    "chunk_index": 34,
    "total_chunks": 40,
    "text_content": "International License, which permits use, sharing, adapta- tion, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article\u2019s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article",
    "full_text_length": 42034,
    "chunk_length": 1311
  },
  {
    "chunk_id": 1750,
    "paper_filename": "kento_2024_automated_detection_of_cancer_suspicious_finding_in_radiology_reprots_with_nlp_tecnique.pdf",
    "paper_title": "Kento 2024 Automated Detection Of Cancer Suspicious Finding In Radiology Reprots With Nlp Tecnique",
    "chunk_index": 35,
    "total_chunks": 40,
    "text_content": "A key to malprac- tice prevention. JAMA. 1994;272(20):1619. 3. Lucey LL, Kushner DC. The ACR Guideline on Communica- tion: To Be or Not to Be, That Is the Question. J Am Coll Radiol. 2010;7(2):109\u201314. 4. Berlin L. Failure of radiologic communication: An increasing cause of malpractice litigation and harm to patients. Appl Radiol. 2010;17\u201323. 5. Larson PA, Berland LL, Griffith B, Kahn CE Jr, Liebscher LA. Actionable Findings and the Role of IT Support: Report of the ACR Actionable Reporting Work ",
    "full_text_length": 42034,
    "chunk_length": 1324
  },
  {
    "chunk_id": 1751,
    "paper_filename": "kento_2024_automated_detection_of_cancer_suspicious_finding_in_radiology_reprots_with_nlp_tecnique.pdf",
    "paper_title": "Kento 2024 Automated Detection Of Cancer Suspicious Finding In Radiology Reprots With Nlp Tecnique",
    "chunk_index": 36,
    "total_chunks": 40,
    "text_content": "AT. Automated detection using natural language processing of radiologists Journal of Imaging Informatics in Medicine recommendations for additional imaging of incidental findings. Ann Emerg Med. 2013;62(2):162\u20139. 9. Mabotuwana T, Hall CS, Dalal S, Tieder J, Gunn ML. Extract- ing Follow-Up Recommendations and Associated Anat- omy from Radiology Reports. Stud Health Technol Inform. 2017;245:1090\u20134. 10. Kang SK, Garry K, Chung R, et al. Natural Language Process- ing for Identification of Incidental",
    "full_text_length": 42034,
    "chunk_length": 1393
  },
  {
    "chunk_id": 1752,
    "paper_filename": "kento_2024_automated_detection_of_cancer_suspicious_finding_in_radiology_reprots_with_nlp_tecnique.pdf",
    "paper_title": "Kento 2024 Automated Detection Of Cancer Suspicious Finding In Radiology Reprots With Nlp Tecnique",
    "chunk_index": 37,
    "total_chunks": 40,
    "text_content": "ML, Xia F, Payne TH. A text process - ing pipeline to extract recommendations from radiology reports. J Biomed Inform. 2013;46(2):354\u201362. 14. Banerjee I, Davis MA, Vey BL, et al. Natural Language Pro- cessing Model for Identifying Critical Findings-A Multi-Insti- tutional Study. J Digit Imaging. 2023;36(1):105\u201313. 15. Nakamura Y, Hanaoka S, Nomura Y, et al. Automatic detection of actionable radiology reports using bidirectional encoder rep- resentations from transformers. BMC Med Inform Decis Ma",
    "full_text_length": 42034,
    "chunk_length": 1380
  },
  {
    "chunk_id": 1753,
    "paper_filename": "kento_2024_automated_detection_of_cancer_suspicious_finding_in_radiology_reprots_with_nlp_tecnique.pdf",
    "paper_title": "Kento 2024 Automated Detection Of Cancer Suspicious Finding In Radiology Reprots With Nlp Tecnique",
    "chunk_index": 38,
    "total_chunks": 40,
    "text_content": "Technol Inform. 2024;310:569\u201373. 19. N\u00e9v\u00e9ol A, Dalianis H, Velupillai S, Savova G, Zweigenbaum P. Clinical Natural Language Processing in languages other than English: opportunities and challenges. J Biomed Semantics. 2018;9(1). 20. Devlin J, Chang M-W, Lee K, Toutanova K. BERT: Pre-training of Deep Bidirectional Transformers for Language Understand- ing. In: Proceedings of the 2019 Conference of the North Amer - ican Chapter of the Association for Computational Linguistics: Human Language Techn",
    "full_text_length": 42034,
    "chunk_length": 1389
  },
  {
    "chunk_id": 1754,
    "paper_filename": "kento_2024_automated_detection_of_cancer_suspicious_finding_in_radiology_reprots_with_nlp_tecnique.pdf",
    "paper_title": "Kento 2024 Automated Detection Of Cancer Suspicious Finding In Radiology Reprots With Nlp Tecnique",
    "chunk_index": 39,
    "total_chunks": 40,
    "text_content": "language processing: how much data is enough for automated radiology report classification? Br J Radiol. 2023;96(1149):20220769. 25. van der Sijs H, Aarts J, Vulto A, Berg M. Overriding of drug safety alerts in computerized physician order entry. J Am Med Inform Assoc. 2006;13(2):138\u201347. 26. Kesselheim AS, Cresswell K, Phansalkar S, Bates DW, Sheikh A. Clinical Decision Support Systems Could Be Modified To Reduce \u2018Alert Fatigue\u2019 While Still Minimizing The Risk Of Litigation. Health Affairs. 2011",
    "full_text_length": 42034,
    "chunk_length": 847
  },
  {
    "chunk_id": 1755,
    "paper_filename": "kinji_2025_improving_benign_and_malignant_ classification in_mammography_using_machine_learning.pdf",
    "paper_title": "Kinji 2025 Improving Benign And Malignant  Classification In Mammography Using Machine Learning",
    "chunk_index": 0,
    "total_chunks": 27,
    "text_content": "Article Not peer-reviewed version Improving Benign and Malignant Classification in Mammography with ROI-Aware Deep Learning Kenji Yoshitsugu * , Kazumasa Kishimoto , Tadamasa Takemura Posted Date: 18 July 2025 doi: 10.20944/preprints202507.1497.v1 Keywords: deep learning; mammography; region of interest Preprints.org is a free multidisciplinary platform providing preprint service that is dedicated to making early versions of research outputs permanently available and citable. Preprints posted at",
    "full_text_length": 28646,
    "chunk_length": 1498
  },
  {
    "chunk_id": 1756,
    "paper_filename": "kinji_2025_improving_benign_and_malignant_ classification in_mammography_using_machine_learning.pdf",
    "paper_title": "Kinji 2025 Improving Benign And Malignant  Classification In Mammography Using Machine Learning",
    "chunk_index": 1,
    "total_chunks": 27,
    "text_content": "Technology and Administration Planning * Correspondence: smokyjp@gmail.com ; Tel.: +81 -090-3262 -7027 Abstract Deep learning has achieved widespread adoption for medical image diagnosis, with extensive research dedicated to mammographic image analysis for breast cancer screening. This study investigates the hypothesis that incorporating region -of-interest (ROI) mask information for individual mammographic images d uring deep learning can improve the accuracy of benign/malignant diagnoses. We u",
    "full_text_length": 28646,
    "chunk_length": 1516
  },
  {
    "chunk_id": 1757,
    "paper_filename": "kinji_2025_improving_benign_and_malignant_ classification in_mammography_using_machine_learning.pdf",
    "paper_title": "Kinji 2025 Improving Benign And Malignant  Classification In Mammography Using Machine Learning",
    "chunk_index": 2,
    "total_chunks": 27,
    "text_content": "CESM/ConvNeXtV2 (0.65, 0.65, 0.65, 0.65). Subsequent analysis with ROI -based separation demonstrated marked improvements in these metrics: VinDr/Swin Transformer (0.93, 0.87, 0.90, 0.87), VinDr/ConvNeXtV2 (0.90, 0.86, 0.88, 0.87), CDD -CESM/Swin Transformer (0.65, 0.65, 0.65, 0.65), and CDD -CESM/ConvNeXtV2 (0.74, 0.61, 0.67, 0.68 ). These findings provide compelling evidence validating our hypothesis and affirming the utility of considering ROI mask information for enhanced diagnostic accuracy",
    "full_text_length": 28646,
    "chunk_length": 1534
  },
  {
    "chunk_id": 1758,
    "paper_filename": "kinji_2025_improving_benign_and_malignant_ classification in_mammography_using_machine_learning.pdf",
    "paper_title": "Kinji 2025 Improving Benign And Malignant  Classification In Mammography Using Machine Learning",
    "chunk_index": 3,
    "total_chunks": 27,
    "text_content": "clinicians often perform highly invasive procedures such as cytologi cal and histological examinations for definitive diagnosis. If deep learning -based image analysis of minimally invasive mammographic images can achieve high diagnostic accuracy, then it could reduce the need for highly invasive procedures. This approach wo uld simultaneously alleviate burden s on radiologists and breast surgeons responsible for interpreting these images. Recent rapid advancements in artificial intelligence (AI",
    "full_text_length": 28646,
    "chunk_length": 1501
  },
  {
    "chunk_id": 1759,
    "paper_filename": "kinji_2025_improving_benign_and_malignant_ classification in_mammography_using_machine_learning.pdf",
    "paper_title": "Kinji 2025 Improving Benign And Malignant  Classification In Mammography Using Machine Learning",
    "chunk_index": 4,
    "total_chunks": 27,
    "text_content": "disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions, or products referred to in the content. \u00a9 2025 by the author(s). Distributed under a Creative Commons CC BY license. 2 of 8 exceeding human capabilities. For medical image diagnosis, these technologies often achieve superior accuracy and efficiency compared to conventional methodologies. Many studies have explored deep learning applications for mammographic image diagnosis. For instance,",
    "full_text_length": 28646,
    "chunk_length": 1347
  },
  {
    "chunk_id": 1760,
    "paper_filename": "kinji_2025_improving_benign_and_malignant_ classification in_mammography_using_machine_learning.pdf",
    "paper_title": "Kinji 2025 Improving Benign And Malignant  Classification In Mammography Using Machine Learning",
    "chunk_index": 5,
    "total_chunks": 27,
    "text_content": "private dataset can reduce interval cancer rates without supplementary screening. Zhu et al. [4] predicted future breast cancer development in negative subjects during an eight -year period using a deep learning model with a private dataset. Kerschke et al. [5] compared human versus deep learning AI accuracy for benign \u2013malignant screening using a private dataset, highlighting the need for prospective studies. Nica et al. [6] reported high -accuracy benign \u2013malignant classific ation of cranio -c",
    "full_text_length": 28646,
    "chunk_length": 1334
  },
  {
    "chunk_id": 1761,
    "paper_filename": "kinji_2025_improving_benign_and_malignant_ classification in_mammography_using_machine_learning.pdf",
    "paper_title": "Kinji 2025 Improving Benign And Malignant  Classification In Mammography Using Machine Learning",
    "chunk_index": 6,
    "total_chunks": 27,
    "text_content": "the best performance. Tzortzis et al. [9] demonstrated superior performanc e for efficiently detecting abnormalities on the public INBreast dataset using their tensor -based deep learning model, particularly showing robustness with limited data and reduced computational requirements. Pawar et al. [10] and Hsu et al. [11] both reported high -accuracy Breast Imaging Reporting and Data System (BIRADS) category classification , respectively , using proprietary multi -channel DenseNet architecture an",
    "full_text_length": 28646,
    "chunk_length": 1350
  },
  {
    "chunk_id": 1762,
    "paper_filename": "kinji_2025_improving_benign_and_malignant_ classification in_mammography_using_machine_learning.pdf",
    "paper_title": "Kinji 2025 Improving Benign And Malignant  Classification In Mammography Using Machine Learning",
    "chunk_index": 7,
    "total_chunks": 27,
    "text_content": "a YOLOv5 object detection model and an anomaly detection model for mass screening on the public VinDr and Mini -DDSM datasets. Ellis et al. [15] , using the UK-national OPTIMAM dataset , developed a deep learning AI model for predicting future cancer risk in patients with negative mammograms. Elhakim et al. [16] further investigated replac ement of one or both readers with AI in double -reading mammography, emphasizing clinical implications for accuracy and workload. Sait et al. [17] reported hi",
    "full_text_length": 28646,
    "chunk_length": 1306
  },
  {
    "chunk_id": 1763,
    "paper_filename": "kinji_2025_improving_benign_and_malignant_ classification in_mammography_using_machine_learning.pdf",
    "paper_title": "Kinji 2025 Improving Benign And Malignant  Classification In Mammography Using Machine Learning",
    "chunk_index": 8,
    "total_chunks": 27,
    "text_content": "four binary tasks using a CNN and a private mammography image dataset, suggesting a potential to reduce unnecessary breast biopsies. Finally, Park et al. [20] reported improved diagnostic accuracy, especially in challenging ACR BIRADS categories 3 and 4 with breast density exceeding 50%, by learning both benign \u2013malignant classification and lesion boundaries using a ViT -B DINO -v2 model on the public CBIS -DDSM dataset. AlMansour et al. [21] reported high - accuracy BIRADS classification using ",
    "full_text_length": 28646,
    "chunk_length": 1473
  },
  {
    "chunk_id": 1764,
    "paper_filename": "kinji_2025_improving_benign_and_malignant_ classification in_mammography_using_machine_learning.pdf",
    "paper_title": "Kinji 2025 Improving Benign And Malignant  Classification In Mammography Using Machine Learning",
    "chunk_index": 9,
    "total_chunks": 27,
    "text_content": "(www.preprints.org) | NOT PEER-REVIEWED | Posted: Posted: 18 July 2025 doi:10.20944/preprints202507.1497.v1 \u00a9 2025 by the author(s). Distributed under a Creative Commons CC BY license. 3 of 8 studies combining mammographic images with other modality images require specific data combinations, thereby complicating claim reproduction. Given these considerations, we prioritized reproducible research by particularly addressing studies using publicly available datasets and open -source deep learning m",
    "full_text_length": 28646,
    "chunk_length": 1410
  },
  {
    "chunk_id": 1765,
    "paper_filename": "kinji_2025_improving_benign_and_malignant_ classification in_mammography_using_machine_learning.pdf",
    "paper_title": "Kinji 2025 Improving Benign And Malignant  Classification In Mammography Using Machine Learning",
    "chunk_index": 10,
    "total_chunks": 27,
    "text_content": "regions of interest. Using two public datasets and two deep learn ing models , we validated this hypothesis, particularly addressing the presence or absence of annotated mask information as a novel feature. 2. Materials and Methods 2.1. Materials This study used two publicly available mammography datasets with region of interest (ROI) annotations: VinDr [22] and CDD -CESM [23]. Both datasets include ROI mask information, but not all mammographic images within them have corresponding mask images ",
    "full_text_length": 28646,
    "chunk_length": 1319
  },
  {
    "chunk_id": 1766,
    "paper_filename": "kinji_2025_improving_benign_and_malignant_ classification in_mammography_using_machine_learning.pdf",
    "paper_title": "Kinji 2025 Improving Benign And Malignant  Classification In Mammography Using Machine Learning",
    "chunk_index": 11,
    "total_chunks": 27,
    "text_content": "malignant. The CDD -CESM dataset includes predefined normal, benign, and malignant classifications. For this analysis, we used benign and malignant data exclusively. Because the CDD -CESM dataset does not provide a predefined train \u2013test split, we divided the data into training and testing sets with a 10:1 ratio. Composition s of the respective datasets are presented in Tables 1 \u20134. Table 1. Distribution of Cases in Training and Testing Subsets of the VinDr Dataset. train ing testing total malig",
    "full_text_length": 28646,
    "chunk_length": 1202
  },
  {
    "chunk_id": 1767,
    "paper_filename": "kinji_2025_improving_benign_and_malignant_ classification in_mammography_using_machine_learning.pdf",
    "paper_title": "Kinji 2025 Improving Benign And Malignant  Classification In Mammography Using Machine Learning",
    "chunk_index": 12,
    "total_chunks": 27,
    "text_content": "NOT PEER-REVIEWED | Posted: Posted: 18 July 2025 doi:10.20944/preprints202507.1497.v1 \u00a9 2025 by the author(s). Distributed under a Creative Commons CC BY license. 4 of 8 Table 4. Presence of ROIs in the CDD -CESM Dataset. with ROIs without ROIs total malignant 326 5 331 benign 322 9 331 total 648 14 662 2.2. Methods 2.2.1. Image Preprocessing For preprocessing, window processing was applied during conversion of DICOM images to JPEG format. This preprocessing was followed by contrast adjustment u",
    "full_text_length": 28646,
    "chunk_length": 1368
  },
  {
    "chunk_id": 1768,
    "paper_filename": "kinji_2025_improving_benign_and_malignant_ classification in_mammography_using_machine_learning.pdf",
    "paper_title": "Kinji 2025 Improving Benign And Malignant  Classification In Mammography Using Machine Learning",
    "chunk_index": 13,
    "total_chunks": 27,
    "text_content": "divided into four standard views: right craniocaudal (RCC), left craniocaudal (LCC), right mediolateral oblique (RMLO), and left mediolateral oblique (LMLO). 3. Training was performed on mammographic images without ROI mask images, with separate training for each view. 4. Prediction was performed on mammographic images without ROI mask images, with separate prediction for each view. 5. Training was then performed on mammographic images with ROI mask images, again with separate training for each ",
    "full_text_length": 28646,
    "chunk_length": 1345
  },
  {
    "chunk_id": 1769,
    "paper_filename": "kinji_2025_improving_benign_and_malignant_ classification in_mammography_using_machine_learning.pdf",
    "paper_title": "Kinji 2025 Improving Benign And Malignant  Classification In Mammography Using Machine Learning",
    "chunk_index": 14,
    "total_chunks": 27,
    "text_content": "LMLO. 2. Training was performed on mammographic images without ROI mask images, separately for each view. 3. Prediction was performed on mammographic images without ROI mask images, separately for each view. 4. Training was performed on mammographic images with ROI mask images, separately for each view. 5. Prediction was performed on mammographic images with ROI mask images, separately for each view. 6. The prediction results were merged. Preprints.org (www.preprints.org) | NOT PEER-REVIEWED | P",
    "full_text_length": 28646,
    "chunk_length": 1344
  },
  {
    "chunk_id": 1770,
    "paper_filename": "kinji_2025_improving_benign_and_malignant_ classification in_mammography_using_machine_learning.pdf",
    "paper_title": "Kinji 2025 Improving Benign And Malignant  Classification In Mammography Using Machine Learning",
    "chunk_index": 15,
    "total_chunks": 27,
    "text_content": "on a system running Windows 11 Pro, equipped with a 13th Gen Intel(R) Core(TM) i9 -13900KF 3.00 GHz processor, 128 GB of memory, and an NVIDIA RTX 3090 GPU. 3. Results Our mammographic image classification results are presented for two scenarios: with and without the inclusion of ROI mask images. For the VinDr dataset, the classification results obtained using Swin Transformer and ConvNeXtV2 are shown respectively in Table s 5 and 6. Similarly for the CDD -CESM dataset, Table s 7 and 8 present t",
    "full_text_length": 28646,
    "chunk_length": 1272
  },
  {
    "chunk_id": 1771,
    "paper_filename": "kinji_2025_improving_benign_and_malignant_ classification in_mammography_using_machine_learning.pdf",
    "paper_title": "Kinji 2025 Improving Benign And Malignant  Classification In Mammography Using Machine Learning",
    "chunk_index": 16,
    "total_chunks": 27,
    "text_content": "Specificity 1.00 0.86 F-score 0.00 0.88 Accuracy 0.85 0.87 Table 7. SwinTransformer / CDD -DESM. without consideration of ROI mask image presence with consideration of ROI mask image presence Sensitiv ity 0.29 0.65 Specificity 0.68 0.65 F-score 0.41 0.65 Accuracy 0.48 0.65 Table 8. ConvNeXt2 / CDD -DESM. without consideration of ROI mask image presence with consideration of ROI mask image presence Sensitivity 0.65 0.74 Specificity 0.65 0.61 F-score 0.65 0.67 Accuracy 0.65 0.68 Preprints.org (www",
    "full_text_length": 28646,
    "chunk_length": 1284
  },
  {
    "chunk_id": 1772,
    "paper_filename": "kinji_2025_improving_benign_and_malignant_ classification in_mammography_using_machine_learning.pdf",
    "paper_title": "Kinji 2025 Improving Benign And Malignant  Classification In Mammography Using Machine Learning",
    "chunk_index": 17,
    "total_chunks": 27,
    "text_content": "diagnosis reached despite the absence of a clear ROI on the image likely reflects corroborating results from other diagnostic modalities such as biopsies. Also observed are instances of data diagnosed as normal but exhibiting an ROI. The presence of an ROI in a \u201cnormal\u201d diagnosis looks stra nge and suggests a potential misrepresentation or artifact in the diagnostic labeling process. Such anomalous data points, whether they involve a malignant diagnosis without a discernible ROI or a normal diag",
    "full_text_length": 28646,
    "chunk_length": 1374
  },
  {
    "chunk_id": 1773,
    "paper_filename": "kinji_2025_improving_benign_and_malignant_ classification in_mammography_using_machine_learning.pdf",
    "paper_title": "Kinji 2025 Improving Benign And Malignant  Classification In Mammography Using Machine Learning",
    "chunk_index": 18,
    "total_chunks": 27,
    "text_content": "images. However, mammographic images requiring benign \u2013malignant classification do not always have corresponding mask images available. Therefore, future research should specifically examine generati on of mask images for mammographic data lacking existing masks, employing techniques such as semantic segmentation or object detection, and subsequently validating these approaches. The deep learning models used for this study, Swin Transformer and ConvNeXtV2, demonstrated superior accuracy in both ",
    "full_text_length": 28646,
    "chunk_length": 1482
  },
  {
    "chunk_id": 1774,
    "paper_filename": "kinji_2025_improving_benign_and_malignant_ classification in_mammography_using_machine_learning.pdf",
    "paper_title": "Kinji 2025 Improving Benign And Malignant  Classification In Mammography Using Machine Learning",
    "chunk_index": 19,
    "total_chunks": 27,
    "text_content": "If successful, this capability would enable diagnostic prediction for a broader range of arbitrary mammographic data. 5. Conclusion s Our findings indicate that pre -segment ation of mammographic data based on the presence or absence of ROI mask images , followed by separate training and prediction processes for each segment and subsequent merging of the results, can enhance classification accuracy for the benign \u2013 malignant classification of mammographic images using deep learning. References 1",
    "full_text_length": 28646,
    "chunk_length": 1393
  },
  {
    "chunk_id": 1775,
    "paper_filename": "kinji_2025_improving_benign_and_malignant_ classification in_mammography_using_machine_learning.pdf",
    "paper_title": "Kinji 2025 Improving Benign And Malignant  Classification In Mammography Using Machine Learning",
    "chunk_index": 20,
    "total_chunks": 27,
    "text_content": "reduce the interval cancer rate in mammography screening?. European Radiology 2021 , 31(8), 5940\u20135947. doi:10.1007/s00330 - 021-07686 -3 4. Zhu, X.; Wolfgruber, T.K.; Leong, L.; Jensen, M.; Scott, C.; Winham, S.; Sadowski, P.; Vachon, C.; Kerlikowske, K.; Shepherd, J.A. Deep Learning Predicts Interval and Screening -detected Cancer from Screening Mammograms: A Case \u2013Case\u2013Control Study in 6369 Wo men. Radiology 2021 , 301(3), 50\u2013558. doi:10.1148/radiol.2021203758 Preprints.org (www.preprints.org)",
    "full_text_length": 28646,
    "chunk_length": 1453
  },
  {
    "chunk_id": 1776,
    "paper_filename": "kinji_2025_improving_benign_and_malignant_ classification in_mammography_using_machine_learning.pdf",
    "paper_title": "Kinji 2025 Improving Benign And Malignant  Classification In Mammography Using Machine Learning",
    "chunk_index": 21,
    "total_chunks": 27,
    "text_content": "Method for Histological Class Prediction of Breast Tumors in Mammography. Journal of Digital Imaging 2021 , 34(5). 1190\u20131198. doi:10.1007/s10278 -021-00508 -4 7. Rehman, K.U.; Li, J.; Pei, Y.; Yasin, A.; Ali, S.; Saeed, Y. Architectural Distortion -Based Digital Mammograms Classification Using Depth Wise Convolutional Neural Network. Biology 2021 , 11(1), 15. doi:10.3390/biology11010015 8. Kizildag Yirgin, I.; Koyluoglu, Y.O.; Seker, M.E.; Ozkan Gurdal, S.; Ozaydin, A.N.; Ozcinar, B.; Cabio\u011flu, ",
    "full_text_length": 28646,
    "chunk_length": 1478
  },
  {
    "chunk_id": 1777,
    "paper_filename": "kinji_2025_improving_benign_and_malignant_ classification in_mammography_using_machine_learning.pdf",
    "paper_title": "Kinji 2025 Improving Benign And Malignant  Classification In Mammography Using Machine Learning",
    "chunk_index": 22,
    "total_chunks": 27,
    "text_content": "DenseNet Architecture for Classification of Mammographic Breast Density for Breast Cancer Detection. Frontiers in Public Health 2022 , 10, 885212. doi:10.3389/fpubh.2022.885212 11. Hsu, S.Y.; Wang, C.Y.; Kao, Y.K.; Liu, K.Y.; Lin, M.C.; Yeh, L.R.; Wang, Y.M.; Chen, C.I.; Kao, F.C. Using Deep Neural Network Approach for Multiple -Class Assessment of Digital Mammography. Healthcare (Basel, Switzerland) 2022 , 10(12), 2382. doi:10.3390/healthcare10122382 12. Elhakim, M.T.; Stougaard, S.W.; Graumann",
    "full_text_length": 28646,
    "chunk_length": 1448
  },
  {
    "chunk_id": 1778,
    "paper_filename": "kinji_2025_improving_benign_and_malignant_ classification in_mammography_using_machine_learning.pdf",
    "paper_title": "Kinji 2025 Improving Benign And Malignant  Classification In Mammography Using Machine Learning",
    "chunk_index": 23,
    "total_chunks": 27,
    "text_content": "F.G.; Debelee, T.G.; Aleme, M.; Bedane, W.; Mezgebu, B.; Merga, Z.C. Dual view deep learning for enhanced breast cancer screening using mammography. Scientific Reports 2024 , 14(1), 3839. doi:10.1038/s41598 -023-50797 -8 15. Ellis, S.; Gomes, S.; Trumble, M.; Halling -Brown, M.D.; Young, K.C.; Chaudhry, N.S.; Harris, P.; Warren, L.M. Deep Learning for Breast Cancer Risk Prediction: Application to a Large Representative UK Screening Cohort. Radiology. Artificial Intelligence 2024 , 6(4), e230431.",
    "full_text_length": 28646,
    "chunk_length": 1489
  },
  {
    "chunk_id": 1779,
    "paper_filename": "kinji_2025_improving_benign_and_malignant_ classification in_mammography_using_machine_learning.pdf",
    "paper_title": "Kinji 2025 Improving Benign And Malignant  Classification In Mammography Using Machine Learning",
    "chunk_index": 24,
    "total_chunks": 27,
    "text_content": "Detection in Mammography Images Using Deep Convolutional Neural Networks and Fuzzy Ensemble Modeling Techniques. Diagnostics (Basel, Switzerland) 2022 , 12(8), 1812. doi:10.3390/diagnostics12081812 19. Liu, C.; Sun, M.; Arefan, D.; Zuley, M.; Sumkin, J.; Wu, S. Deep learning of mammogram images to reduce unnecessary breast biopsies: a preliminary study. Breast Cancer Research: BCR 2024 , 26(1), 82. doi:10.1186/s13058 -024-01830 -9 20. Park, J.H.; Lim, J.H.; Kim, S.; Heo, J. A Multi -label Artifi",
    "full_text_length": 28646,
    "chunk_length": 1461
  },
  {
    "chunk_id": 1780,
    "paper_filename": "kinji_2025_improving_benign_and_malignant_ classification in_mammography_using_machine_learning.pdf",
    "paper_title": "Kinji 2025 Improving Benign And Malignant  Classification In Mammography Using Machine Learning",
    "chunk_index": 25,
    "total_chunks": 27,
    "text_content": "doi:10.3390/diagnostics15030285 22. Nguyen, H.T.; Nguyen, H.Q.; Pham, H.H.; Lam, K.; Le, L.T.; Dao, M.; Vu, V. VinDr -Mammo: A large -scale benchmark dataset for computer -aided diagnosis in full -field digital mammography. Scientific Data 2023 , 10(1), 277. doi:10.1038/s41597 -023-02100 -7 23. Khaled, R.; Helal, M.; Alfarghaly, O.; Mokhtar, O.; Elkorany , A.; El Kassas, H.; Fahmy, A. Categorized contrast enhanced mammography dataset for diagnostic and artificial intelligence research. Scientifi",
    "full_text_length": 28646,
    "chunk_length": 1382
  },
  {
    "chunk_id": 1781,
    "paper_filename": "kinji_2025_improving_benign_and_malignant_ classification in_mammography_using_machine_learning.pdf",
    "paper_title": "Kinji 2025 Improving Benign And Malignant  Classification In Mammography Using Machine Learning",
    "chunk_index": 26,
    "total_chunks": 27,
    "text_content": "and Pattern Recognition (CVPR), Vancouver, BC, Canada, 2023, pp. 16133 \u201316142, doi: 10.1109/CVPR52729.2023.01548. Disclaimer/Publisher\u2019s Note: The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property re sulting from any ideas, methods, instructions or products referred to in the content. Preprints.org (",
    "full_text_length": 28646,
    "chunk_length": 683
  },
  {
    "chunk_id": 1782,
    "paper_filename": "kinkar_2024_breast_cancer_dignosisi_and radiology_practice_uisng_artifical_intellegence_for_contrast_enhance_mammography.pdf",
    "paper_title": "Kinkar 2024 Breast Cancer Dignosisi And Radiology Practice Uisng Artifical Intellegence For Contrast Enhance Mammography",
    "chunk_index": 0,
    "total_chunks": 61,
    "text_content": "EDITED BY Maria Evelina Fantacci, University of Pisa, Italy REVIEWED BY Annarita Fanizzi, National Cancer Institute Foundation (IRCCS), ItalyMin-Ying Lydia Su, University of California, Irvine, United States * CORRESPONDENCE Bino A. Varghese bino.varghese@med.usc.edu \u2020These authors have contributed equally to this work RECEIVED 24 October 2023 ACCEPTED 07 December 2023 PUBLISHED 05 January 2024 CITATION Kinkar KK, Fields BKK, Yamashita MW andVarghese BA (2024) Empowering breast cancer diagnosis ",
    "full_text_length": 63067,
    "chunk_length": 1522
  },
  {
    "chunk_id": 1783,
    "paper_filename": "kinkar_2024_breast_cancer_dignosisi_and radiology_practice_uisng_artifical_intellegence_for_contrast_enhance_mammography.pdf",
    "paper_title": "Kinkar 2024 Breast Cancer Dignosisi And Radiology Practice Uisng Artifical Intellegence For Contrast Enhance Mammography",
    "chunk_index": 1,
    "total_chunks": 61,
    "text_content": "comply with these terms.Empowering breast cancer diagnosis and radiology practice: advances in arti \ufb01cial intelligence for contrast-enhancedmammography Ketki K. Kinkar1\u2020, Brandon K. K. Fields2, Mary W. Yamashita3and Bino A. Varghese3*\u2020 1Viterbi School of Engineering, University of Southern California, Los Angeles, CA, United States, 2Department of Radiology & Biomedical Imaging, University of California, San Francisco, San Francisco, CA, United States,3Department of Radiology, Keck School of Med",
    "full_text_length": 63067,
    "chunk_length": 1533
  },
  {
    "chunk_id": 1784,
    "paper_filename": "kinkar_2024_breast_cancer_dignosisi_and radiology_practice_uisng_artifical_intellegence_for_contrast_enhance_mammography.pdf",
    "paper_title": "Kinkar 2024 Breast Cancer Dignosisi And Radiology Practice Uisng Artifical Intellegence For Contrast Enhance Mammography",
    "chunk_index": 2,
    "total_chunks": 61,
    "text_content": "i.e., mammography (MG) and/or conventional ultrasound (US), withcomparable accuracy to MRI (current diagnostic imaging benchmark), but at a much lower cost and higher throughput. This makes CEM an excellent tool for widespread breast lesion characterization for all women, includingunderserved and minority women. Underlining the critical need for early detection and accurate diagnosis of breast cancer, this review examines the limitations of conventional approaches and reveals how AI can help ove",
    "full_text_length": 63067,
    "chunk_length": 1477
  },
  {
    "chunk_id": 1785,
    "paper_filename": "kinkar_2024_breast_cancer_dignosisi_and radiology_practice_uisng_artifical_intellegence_for_contrast_enhance_mammography.pdf",
    "paper_title": "Kinkar 2024 Breast Cancer Dignosisi And Radiology Practice Uisng Artifical Intellegence For Contrast Enhance Mammography",
    "chunk_index": 3,
    "total_chunks": 61,
    "text_content": "cancer detection 1 Introduction Breast cancer is the second most leading cause of cancer death in women globally ( 1), and early detection is crucial for improved prognosis ( 2\u20135). Digital Mammography (DM) is known to reduce breast cancer related deaths by 40%. However, among speci \ufb01c patients, heightened breast density poses a challenge in detecting early-stage small cancers, resulting in a higher rate of false positive callbacks and interval cancers ( 6,7). Currently 43% of allTYPE Review PUBL",
    "full_text_length": 63067,
    "chunk_length": 1307
  },
  {
    "chunk_id": 1786,
    "paper_filename": "kinkar_2024_breast_cancer_dignosisi_and radiology_practice_uisng_artifical_intellegence_for_contrast_enhance_mammography.pdf",
    "paper_title": "Kinkar 2024 Breast Cancer Dignosisi And Radiology Practice Uisng Artifical Intellegence For Contrast Enhance Mammography",
    "chunk_index": 4,
    "total_chunks": 61,
    "text_content": "dual-energy DM to create a recombined or iodine image that highlights just the enhancing lesion in the breast ( 11,12). CEM has comparable sensitivity to MRI with a much higher speci \ufb01city, potentially at a much lower cost and higher throughput ( 13\u201315). As a natural progression, multiple studies report of the bene \ufb01ts of using CEM for the screening, diagnosis of breast cancers as a cost-effective and viable alternative to the current standards, particularly in women withdense breasts and at rel",
    "full_text_length": 63067,
    "chunk_length": 1305
  },
  {
    "chunk_id": 1787,
    "paper_filename": "kinkar_2024_breast_cancer_dignosisi_and radiology_practice_uisng_artifical_intellegence_for_contrast_enhance_mammography.pdf",
    "paper_title": "Kinkar 2024 Breast Cancer Dignosisi And Radiology Practice Uisng Artifical Intellegence For Contrast Enhance Mammography",
    "chunk_index": 5,
    "total_chunks": 61,
    "text_content": "can be useful to identify vascularity of a particular lesion ( 12). Studies have suggested, low energy mammograms obtained as part of CEM protocols is comparable to conventional mammography ( 19,20) and though with the added advantage of emphasizing regions of contrast enhancement ( 21). CEM is currently offered on \ufb01ve different systems across 4 vendors ( 11, 22). Although a general consensus on how to perform CEM has been followed, a standardized implementation has not been established. This is",
    "full_text_length": 63067,
    "chunk_length": 1304
  },
  {
    "chunk_id": 1788,
    "paper_filename": "kinkar_2024_breast_cancer_dignosisi_and radiology_practice_uisng_artifical_intellegence_for_contrast_enhance_mammography.pdf",
    "paper_title": "Kinkar 2024 Breast Cancer Dignosisi And Radiology Practice Uisng Artifical Intellegence For Contrast Enhance Mammography",
    "chunk_index": 6,
    "total_chunks": 61,
    "text_content": "3 mGy threshold dose limit set by Mammography Quality Standards guidelines ( 24,25). Furthermore, despite enhanced sensitivity of CEM, certain breast lesions may still be undetectable due to their location within the breast; supplementary breast MRI may be required if lesions are anticipated in these areas such as region near chest wall ( 26). Finally, due to use subtraction techniques, certain CEM-speci \ufb01c artifacts may be visible on the recombined image which likewise can obscure subtle lesion",
    "full_text_length": 63067,
    "chunk_length": 1367
  },
  {
    "chunk_id": 1789,
    "paper_filename": "kinkar_2024_breast_cancer_dignosisi_and radiology_practice_uisng_artifical_intellegence_for_contrast_enhance_mammography.pdf",
    "paper_title": "Kinkar 2024 Breast Cancer Dignosisi And Radiology Practice Uisng Artifical Intellegence For Contrast Enhance Mammography",
    "chunk_index": 7,
    "total_chunks": 61,
    "text_content": "of systematically presenting research \ufb01ndings resonate deeply within the academic and scienti \ufb01c community. Their assertionhighlights that, beyond achieving optimal results in research, the meticulous and structured presentation of these \ufb01ndings is of paramount signi \ufb01cance. These guidelines promote transparency, reproducibility, and the ability to generalize research \ufb01ndings. They standardize reporting, elevate research quality, and ensure clinical relevance, providing a shared foundation for r",
    "full_text_length": 63067,
    "chunk_length": 1367
  },
  {
    "chunk_id": 1790,
    "paper_filename": "kinkar_2024_breast_cancer_dignosisi_and radiology_practice_uisng_artifical_intellegence_for_contrast_enhance_mammography.pdf",
    "paper_title": "Kinkar 2024 Breast Cancer Dignosisi And Radiology Practice Uisng Artifical Intellegence For Contrast Enhance Mammography",
    "chunk_index": 8,
    "total_chunks": 61,
    "text_content": "in the clinical \ufb01eld. 2 Method The literature review was conducted on the use of contrast- enhanced mammography (CEM) and arti \ufb01cial intelligence (AI) techniques for predicting malignancy. PubMed database was searched for articles published between 1st January 2018 and 5th October, 2023, using a query: \u201c(contrast-enhanced mammography) AND (deep learning OR radiomics OR arti \ufb01cial intelligence OR quantitative analysis) AND (classi \ufb01cation OR detection) \u201d. 53 articles that met this initial criteri",
    "full_text_length": 63067,
    "chunk_length": 1350
  },
  {
    "chunk_id": 1791,
    "paper_filename": "kinkar_2024_breast_cancer_dignosisi_and radiology_practice_uisng_artifical_intellegence_for_contrast_enhance_mammography.pdf",
    "paper_title": "Kinkar 2024 Breast Cancer Dignosisi And Radiology Practice Uisng Artifical Intellegence For Contrast Enhance Mammography",
    "chunk_index": 9,
    "total_chunks": 61,
    "text_content": "is presented in Figure 2 . 3 Image acquisition The availability of CEM in commercial systems from vendors like GE Healthcare, Hologic, and Siemens Healthineers represents a signi \ufb01cant advancement in breast imaging, as demonstrated in Table 1 , with information sourced from Jeukens (32), Jochelson et al. ( 11). While optimal imaging parameters for CEM have not been extensively documented in published studies, there are a few generally accepted guidelines. Commonly, low- osmolarity iodine-based c",
    "full_text_length": 63067,
    "chunk_length": 1294
  },
  {
    "chunk_id": 1792,
    "paper_filename": "kinkar_2024_breast_cancer_dignosisi_and radiology_practice_uisng_artifical_intellegence_for_contrast_enhance_mammography.pdf",
    "paper_title": "Kinkar 2024 Breast Cancer Dignosisi And Radiology Practice Uisng Artifical Intellegence For Contrast Enhance Mammography",
    "chunk_index": 10,
    "total_chunks": 61,
    "text_content": "is important to acknowledge that providing explicit details regarding image acquisition methods and the sources of ground truth data is essential for establishing a commonKinkar et al. 10.3389/fradi.2023.1326831 Frontiers in Radiology 02 frontiersin.org platform for comparing existing studies. While the majority of researchers have embraced transparency and rigor in their research processes, there are exceptions where such critical information remains undisclosed. This underscores the importance",
    "full_text_length": 63067,
    "chunk_length": 1533
  },
  {
    "chunk_id": 1793,
    "paper_filename": "kinkar_2024_breast_cancer_dignosisi_and radiology_practice_uisng_artifical_intellegence_for_contrast_enhance_mammography.pdf",
    "paper_title": "Kinkar 2024 Breast Cancer Dignosisi And Radiology Practice Uisng Artifical Intellegence For Contrast Enhance Mammography",
    "chunk_index": 11,
    "total_chunks": 61,
    "text_content": "CEM within a multicenter study. Studies exploring harmonization/standardization strategies prior to using multivendor CEM data for multicenter studies are warranted. 4 Image pre-processing Image preprocessing is crucial for models in contrast-enhanced imaging datasets, overcoming challenges like noise and artifacts. Steps like noise reduction, removal of background pixels, contrast enhancement, and data normalization improve image quality ( 47). FIGURE 1 Diagram of systematic evaluation for arti",
    "full_text_length": 63067,
    "chunk_length": 1550
  },
  {
    "chunk_id": 1794,
    "paper_filename": "kinkar_2024_breast_cancer_dignosisi_and radiology_practice_uisng_artifical_intellegence_for_contrast_enhance_mammography.pdf",
    "paper_title": "Kinkar 2024 Breast Cancer Dignosisi And Radiology Practice Uisng Artifical Intellegence For Contrast Enhance Mammography",
    "chunk_index": 12,
    "total_chunks": 61,
    "text_content": "in enhancing the accuracy of diagnoses. Several techniques have been developed for artifact removal, including thresholding, clustering, graph-cut algorithms,and deep learning methods. Thresholding is particularly effective in addressing large and well-de \ufb01ned artifacts ( 47,48). Clustering, on the other hand, groups similar pixels together to tackle artifact removal ( 48). Otsu \u2019s thresholding method has been applied in two notable studies ( 33,43) for malignancy detection. In the case of ( 33)",
    "full_text_length": 63067,
    "chunk_length": 1475
  },
  {
    "chunk_id": 1795,
    "paper_filename": "kinkar_2024_breast_cancer_dignosisi_and radiology_practice_uisng_artifical_intellegence_for_contrast_enhance_mammography.pdf",
    "paper_title": "Kinkar 2024 Breast Cancer Dignosisi And Radiology Practice Uisng Artifical Intellegence For Contrast Enhance Mammography",
    "chunk_index": 13,
    "total_chunks": 61,
    "text_content": "choice of artifact removal technique hinges on the speci \ufb01c image characteristics FIGURE 2 Flowchart for malignancy detection in CEM images. TABLE 1 Vendors for CEM imaging acquisition system. Vendors/system characteristicLow\u2014energy imaging: anode and \ufb01lter material and thicknessHigh\u2014energy imaging: anode and \ufb01lter material and thicknessMean glandular doseTotal acquisition time Vendor 1 Mo/Mo, Mo/Rh, Rh/Rh Mo (0.03 mm), Rh (0.025 mm)Mo/AI + Cu, Rh/AI + Cu Al (0.3 mm), Cu (0.3 mm)1.6\u20132.8 mGy 3 \u20138",
    "full_text_length": 63067,
    "chunk_length": 1246
  },
  {
    "chunk_id": 1796,
    "paper_filename": "kinkar_2024_breast_cancer_dignosisi_and radiology_practice_uisng_artifical_intellegence_for_contrast_enhance_mammography.pdf",
    "paper_title": "Kinkar 2024 Breast Cancer Dignosisi And Radiology Practice Uisng Artifical Intellegence For Contrast Enhance Mammography",
    "chunk_index": 14,
    "total_chunks": 61,
    "text_content": "offers Siemens Healthineers Mammomat Revelation Titanium CEM system. The low-energy tube voltage range for these vendors spans 26 to 34 kV, while the high-energy range extends from 45 to 49 kV. Essential anode and \ufb01lter materials include silver (Ag), aluminium (Al), copper (Cu), molybdenum (Mo), rhodium (Rh), titanium (Ti), and tungsten (W). The data within this table is sourced from Jeukens ( 32), Jochelson at al ( 11).Kinkar et al. 10.3389/fradi.2023.1326831 Frontiers in Radiology 04 frontiers",
    "full_text_length": 63067,
    "chunk_length": 1385
  },
  {
    "chunk_id": 1797,
    "paper_filename": "kinkar_2024_breast_cancer_dignosisi_and radiology_practice_uisng_artifical_intellegence_for_contrast_enhance_mammography.pdf",
    "paper_title": "Kinkar 2024 Breast Cancer Dignosisi And Radiology Practice Uisng Artifical Intellegence For Contrast Enhance Mammography",
    "chunk_index": 15,
    "total_chunks": 61,
    "text_content": "an image. Image acquisition timing impacts appearance and generalization. Standardizing resolution and acquisition times enhances dataset consistency and diminishes model variance, ultimately reducing false negatives, thus improving model performance ( 51). Wang et al. ( 34) conducted a study that used data from two different centers and successfully standardized their dataset using resampling techniques. In study by Wang et al. ( 38) they performed resampling before featureextraction. Resamplin",
    "full_text_length": 63067,
    "chunk_length": 1398
  },
  {
    "chunk_id": 1798,
    "paper_filename": "kinkar_2024_breast_cancer_dignosisi_and radiology_practice_uisng_artifical_intellegence_for_contrast_enhance_mammography.pdf",
    "paper_title": "Kinkar 2024 Breast Cancer Dignosisi And Radiology Practice Uisng Artifical Intellegence For Contrast Enhance Mammography",
    "chunk_index": 16,
    "total_chunks": 61,
    "text_content": "to \ufb02uctuations in these parameters than others. Normalization techniques offer a critical solution to mitigate these sensitivities. By normalizing CEM images, the impact of variable brightness, contrast, and color balance is minimized (52). This, in turn, enhances the reliability and precision of feature extraction processes from CEM images. For instance, in a study by Zheng et al. ( 43) the researchers used data from threeTABLE 2 Review of existing work in AI for CEM imaging. Research workMetho",
    "full_text_length": 63067,
    "chunk_length": 1231
  },
  {
    "chunk_id": 1799,
    "paper_filename": "kinkar_2024_breast_cancer_dignosisi_and radiology_practice_uisng_artifical_intellegence_for_contrast_enhance_mammography.pdf",
    "paper_title": "Kinkar 2024 Breast Cancer Dignosisi And Radiology Practice Uisng Artifical Intellegence For Contrast Enhance Mammography",
    "chunk_index": 17,
    "total_chunks": 61,
    "text_content": "GE Healthcare182 64/118 91.67% Sensitivity = 0.90 Speci \ufb01city = 0.92 Wang et al. ( 36) Logistic regression GE Healthcare 226 101/125 94.6% AUC = 0.96 Sensitivity = 0.97 Speci \ufb01city = 0.91 Fusco et al. ( 37) Support Vector Machine Hologic, USA and GE Healthcare104 39/65 87% AUC = 0.90 Sensitivity = 0.86 Speci \ufb01city = 0.87 Wang et al. ( 38) Least absolute shrinkage and selection operator (LASSOGE Healthcare 223 101/122 \u2013 AUC = 0.940 Sun et al. ( 39) Least absolute shrinkage and selection operator ",
    "full_text_length": 63067,
    "chunk_length": 1134
  },
  {
    "chunk_id": 1800,
    "paper_filename": "kinkar_2024_breast_cancer_dignosisi_and radiology_practice_uisng_artifical_intellegence_for_contrast_enhance_mammography.pdf",
    "paper_title": "Kinkar 2024 Breast Cancer Dignosisi And Radiology Practice Uisng Artifical Intellegence For Contrast Enhance Mammography",
    "chunk_index": 18,
    "total_chunks": 61,
    "text_content": "\ufb01city = 0.86 Jailin et al. ( 42) YOLOv5 with CSPDarknet as backbone GE HealthCare, USA 7,443 3,739/3,704 estimated 90% AUC = 0.964 FPR = 0.128 Zheng et al. ( 43)R e \ufb01neNet and Xception + Pyramid pooling (PPM) GE Healthcare 1,802 493/1,309 87.6% Sensitivity = 0.95 Speci \ufb01city = 0.70 Savaridas et al. ( 44) Arti \ufb01cial Neural Network (ANN) Hologic and GE Healthcare269 14/255 91.4% AUC = 0.97 Sensitivity = 0.95 Speci \ufb01city = 0.89 Chen et al. ( 45) DenseNet 121 with Convolutional Neural Network (CNN)G",
    "full_text_length": 63067,
    "chunk_length": 1235
  },
  {
    "chunk_id": 1801,
    "paper_filename": "kinkar_2024_breast_cancer_dignosisi_and radiology_practice_uisng_artifical_intellegence_for_contrast_enhance_mammography.pdf",
    "paper_title": "Kinkar 2024 Breast Cancer Dignosisi And Radiology Practice Uisng Artifical Intellegence For Contrast Enhance Mammography",
    "chunk_index": 19,
    "total_chunks": 61,
    "text_content": "two for external testing. To ensure that the dataset was consistent, they used normalization. Qian et al. ( 46) enhanced CEM images by adjusting pixel values to improve contrast and highlight lesions and then performed min-max normalization. This normalization process was essential for harmonizing the diverse data sources and ensuring that the dataset was coherent and free of inconsistencies. Adopting these steps in studies is strongly recommended as they strengthen the reliability of their \ufb01ndi",
    "full_text_length": 63067,
    "chunk_length": 1349
  },
  {
    "chunk_id": 1802,
    "paper_filename": "kinkar_2024_breast_cancer_dignosisi_and radiology_practice_uisng_artifical_intellegence_for_contrast_enhance_mammography.pdf",
    "paper_title": "Kinkar 2024 Breast Cancer Dignosisi And Radiology Practice Uisng Artifical Intellegence For Contrast Enhance Mammography",
    "chunk_index": 20,
    "total_chunks": 61,
    "text_content": "valuable strategy. Data augmentation techniques, such as horizontal image mirroring, global intensity adjustments, realistic transformations of breast geometry ( 53), horizontal \ufb02ipping, rotation, scaling, reducing size (54) and horizontal and vertical shifting have been effectively used in studies by Jailin et al. ( 42), Zheng et al. ( 43), Qian et al. (46). These techniques increase the diversity of the dataset, which improves the robustness of research \ufb01ndings. 4.5 Lesion segmentation In the ",
    "full_text_length": 63067,
    "chunk_length": 1405
  },
  {
    "chunk_id": 1803,
    "paper_filename": "kinkar_2024_breast_cancer_dignosisi_and radiology_practice_uisng_artifical_intellegence_for_contrast_enhance_mammography.pdf",
    "paper_title": "Kinkar 2024 Breast Cancer Dignosisi And Radiology Practice Uisng Artifical Intellegence For Contrast Enhance Mammography",
    "chunk_index": 21,
    "total_chunks": 61,
    "text_content": "was the chosen method. This approach underscored the importance of detailed and careful delineation of lesion contours, taking into account both the CC and MLO views, thus emphasizing its role in achieving precision and accuracy in radiological assessments. It is crucial to recognize that manual segmentation, despite its accuracy and reliability, demands a substantial investment of time and effort. The involvement of skilled radiologists is paramount to its success. If radiologist availability i",
    "full_text_length": 63067,
    "chunk_length": 1446
  },
  {
    "chunk_id": 1804,
    "paper_filename": "kinkar_2024_breast_cancer_dignosisi_and radiology_practice_uisng_artifical_intellegence_for_contrast_enhance_mammography.pdf",
    "paper_title": "Kinkar 2024 Breast Cancer Dignosisi And Radiology Practice Uisng Artifical Intellegence For Contrast Enhance Mammography",
    "chunk_index": 22,
    "total_chunks": 61,
    "text_content": "CEM image analysis. Automatic segmentation methods leverage the power of deep learning models to develop a comprehensive understanding of lesion features in contrast-enhanced mammography (CEM) images, enabling them to autonomously outline lesion contours. Alternatively, whole-organ analysis, the analysis of the entire breast, can be performed instead of lesion- speci\ufb01c segmentation. Consequently, automatic segmentation methods have the potential to reduce analysis time and effort, while also enh",
    "full_text_length": 63067,
    "chunk_length": 1406
  },
  {
    "chunk_id": 1805,
    "paper_filename": "kinkar_2024_breast_cancer_dignosisi_and radiology_practice_uisng_artifical_intellegence_for_contrast_enhance_mammography.pdf",
    "paper_title": "Kinkar 2024 Breast Cancer Dignosisi And Radiology Practice Uisng Artifical Intellegence For Contrast Enhance Mammography",
    "chunk_index": 23,
    "total_chunks": 61,
    "text_content": "images. Meanwhile, Beuque et al. ( 33) utilized the Mask R-CNN model ( 57), a region-based deep learning model that is optimized for object detection and segmentation. Jailen et al. ( 42) employed the YOLO v5 model, a single-stage deep learning model that is faster and more generalized than Mask R-CNN. These examples exemplify the diversity of approaches within the realm of automatic segmentation, and highlight the different trade-offs between accuracy, speed, and generalization. However, it is ",
    "full_text_length": 63067,
    "chunk_length": 1368
  },
  {
    "chunk_id": 1806,
    "paper_filename": "kinkar_2024_breast_cancer_dignosisi_and radiology_practice_uisng_artifical_intellegence_for_contrast_enhance_mammography.pdf",
    "paper_title": "Kinkar 2024 Breast Cancer Dignosisi And Radiology Practice Uisng Artifical Intellegence For Contrast Enhance Mammography",
    "chunk_index": 24,
    "total_chunks": 61,
    "text_content": "automatic segmentation methods pose a formidable hurdle, especially in clinical settings characterized by limited resources. 5 Feature extraction Feature extraction is a critical technique for training CEM model training, enhancing the accuracy, ef \ufb01ciency, and interpretability of deep learning models ( 58). Common techniques include shape features, texture features, and kinetic features. Shape features describe the shape of the lesion, texture features describe its brightness, contrast, and hom",
    "full_text_length": 63067,
    "chunk_length": 1436
  },
  {
    "chunk_id": 1807,
    "paper_filename": "kinkar_2024_breast_cancer_dignosisi_and radiology_practice_uisng_artifical_intellegence_for_contrast_enhance_mammography.pdf",
    "paper_title": "Kinkar 2024 Breast Cancer Dignosisi And Radiology Practice Uisng Artifical Intellegence For Contrast Enhance Mammography",
    "chunk_index": 25,
    "total_chunks": 61,
    "text_content": "context of lesion segmentation. This approach, as observed in the reviewed studies, provides valuable insights into the characteristics of the lesion. These handcrafted features have been extracted using tools such as the PyRadiomics package and the Texture toolbox by MATLAB according to Image Biomarker Standardization Initiative (IBSI) ( 59), as elaborated in (33\u201337). Once these features are extracted, it becomes imperative to re \ufb01ne them to enhance data quality. This often involves normalizati",
    "full_text_length": 63067,
    "chunk_length": 1394
  },
  {
    "chunk_id": 1808,
    "paper_filename": "kinkar_2024_breast_cancer_dignosisi_and radiology_practice_uisng_artifical_intellegence_for_contrast_enhance_mammography.pdf",
    "paper_title": "Kinkar 2024 Breast Cancer Dignosisi And Radiology Practice Uisng Artifical Intellegence For Contrast Enhance Mammography",
    "chunk_index": 26,
    "total_chunks": 61,
    "text_content": "+ ACC). Strati \ufb01ed 10-fold cross- validation is used in the XG Boost classi \ufb01er to perform feature elimination ( 33). This process ensures that only the most informative and non-redundant features are retained for model training. 5.2 Transfer learning Transfer learning is a valuable technique in deep learning pipelines for feature extraction. It utilizes pre-trained models to ef\ufb01ciently extract relevant features from new data, enhancing performance. This approach is particularly bene \ufb01cial when ",
    "full_text_length": 63067,
    "chunk_length": 1399
  },
  {
    "chunk_id": 1809,
    "paper_filename": "kinkar_2024_breast_cancer_dignosisi_and radiology_practice_uisng_artifical_intellegence_for_contrast_enhance_mammography.pdf",
    "paper_title": "Kinkar 2024 Breast Cancer Dignosisi And Radiology Practice Uisng Artifical Intellegence For Contrast Enhance Mammography",
    "chunk_index": 27,
    "total_chunks": 61,
    "text_content": "the speci \ufb01c model being developed. Handcrafted radiomics requires lesion segmentation for feature extraction, while transfer learning allows for the utilization of either entire images or patches of lesions. This adaptability underscores the importance of selecting the most suitable approach based on the objectives and requirements of the model under consideration. In essence, feature extraction serves as thelinchpin in the AI pipeline for malignancy detection and segmentation, determining the ",
    "full_text_length": 63067,
    "chunk_length": 1428
  },
  {
    "chunk_id": 1810,
    "paper_filename": "kinkar_2024_breast_cancer_dignosisi_and radiology_practice_uisng_artifical_intellegence_for_contrast_enhance_mammography.pdf",
    "paper_title": "Kinkar 2024 Breast Cancer Dignosisi And Radiology Practice Uisng Artifical Intellegence For Contrast Enhance Mammography",
    "chunk_index": 28,
    "total_chunks": 61,
    "text_content": "its representation in the training dataset. Techniques like SMOTE (Synthetic Minority Over-sampling Technique) ( 44) and ROSE (Random Over-Sampling Examples) can be employed for this purpose. For example, in studies ( 33, 35) the authors utilized Adaptive Synthetic Sampling (ADASYN). 6.2 Under-sampling In contrast, under-sampling entails removing samples from the majority class to diminish its presence in the training dataset. Various techniques, such as random under-sampling and Tomek links, ca",
    "full_text_length": 63067,
    "chunk_length": 1360
  },
  {
    "chunk_id": 1811,
    "paper_filename": "kinkar_2024_breast_cancer_dignosisi_and radiology_practice_uisng_artifical_intellegence_for_contrast_enhance_mammography.pdf",
    "paper_title": "Kinkar 2024 Breast Cancer Dignosisi And Radiology Practice Uisng Artifical Intellegence For Contrast Enhance Mammography",
    "chunk_index": 29,
    "total_chunks": 61,
    "text_content": "a higher cost to the minority class, this approach compels the model to give more attention to it, often resulting in improved performance on imbalanced datasets as done in study ( 41). 6.4 Ensemble learning Ensemble learning entails training multiple models on different subsets of the data and then averaging their predictions. This technique helps reduce model variance and enhances performance on imbalanced datasets. These methods illustrate the versatility required to address data imbalance ef",
    "full_text_length": 63067,
    "chunk_length": 1428
  },
  {
    "chunk_id": 1812,
    "paper_filename": "kinkar_2024_breast_cancer_dignosisi_and radiology_practice_uisng_artifical_intellegence_for_contrast_enhance_mammography.pdf",
    "paper_title": "Kinkar 2024 Breast Cancer Dignosisi And Radiology Practice Uisng Artifical Intellegence For Contrast Enhance Mammography",
    "chunk_index": 30,
    "total_chunks": 61,
    "text_content": "extracted features to rigorous measurement, quanti \ufb01cation, and analysis before using these features for model training. Univariate and multivariate analysis represent two primary categories of quantitative methodologies extensively employed for the examination of handcrafted features within CEM images. 7.1 Univariate analysis Univariate analysis is a fundamental statistical method focused on analyzing a single variable. It helps describe the variable \u2019s distribution, detect outliers, and identi",
    "full_text_length": 63067,
    "chunk_length": 1489
  },
  {
    "chunk_id": 1813,
    "paper_filename": "kinkar_2024_breast_cancer_dignosisi_and radiology_practice_uisng_artifical_intellegence_for_contrast_enhance_mammography.pdf",
    "paper_title": "Kinkar 2024 Breast Cancer Dignosisi And Radiology Practice Uisng Artifical Intellegence For Contrast Enhance Mammography",
    "chunk_index": 31,
    "total_chunks": 61,
    "text_content": "comparing CEM radiomics features, ensuring robust and reliable research results. Another technique in study ( 37,38) is the Intraclass Correlation Coef\ufb01cient (ICC), which plays a vital role in univariate analysis for handcrafted radiomics features in CEM. The ICC assesses measurement reliability, identi \ufb01es variability sources, aids in quality control, informs study design, facilitates feature reliability comparison, and determines clinical utility. By ensuring the consistency and trustworthines",
    "full_text_length": 63067,
    "chunk_length": 1392
  },
  {
    "chunk_id": 1814,
    "paper_filename": "kinkar_2024_breast_cancer_dignosisi_and radiology_practice_uisng_artifical_intellegence_for_contrast_enhance_mammography.pdf",
    "paper_title": "Kinkar 2024 Breast Cancer Dignosisi And Radiology Practice Uisng Artifical Intellegence For Contrast Enhance Mammography",
    "chunk_index": 32,
    "total_chunks": 61,
    "text_content": "for interpreting the performance of features, particularly in diagnostic or predictive modeling scenarios. Univariate analysis by Sun et al. ( 39) revealed that larger lesion sizes and rim or ripple artifacts were associated with more misclassi \ufb01cations of benign lesions and smaller lesion sizes were associated with more misclassi \ufb01cations of malignant lesions.7.2 Multivariate analysis Multivariate analysis involves the simultaneous examination of multiple variables, offering a powerful approach",
    "full_text_length": 63067,
    "chunk_length": 1445
  },
  {
    "chunk_id": 1815,
    "paper_filename": "kinkar_2024_breast_cancer_dignosisi_and radiology_practice_uisng_artifical_intellegence_for_contrast_enhance_mammography.pdf",
    "paper_title": "Kinkar 2024 Breast Cancer Dignosisi And Radiology Practice Uisng Artifical Intellegence For Contrast Enhance Mammography",
    "chunk_index": 33,
    "total_chunks": 61,
    "text_content": "(SVMs) for high-dimensional data analysis. These methods offer diverse approaches to extract insights from radiomics data, but their choice depends on research objectives and data characteristics. We recommend selecting the analytics technique that aligns with the speci \ufb01c criteria and research objectives. Multivariate analysis by Sun et al. ( 39) revealed that smaller lesion size and air trapping artifacts were associated with the misclassi \ufb01cation of malignant lesions. Our \ufb01ndings indicate tha",
    "full_text_length": 63067,
    "chunk_length": 1372
  },
  {
    "chunk_id": 1816,
    "paper_filename": "kinkar_2024_breast_cancer_dignosisi_and radiology_practice_uisng_artifical_intellegence_for_contrast_enhance_mammography.pdf",
    "paper_title": "Kinkar 2024 Breast Cancer Dignosisi And Radiology Practice Uisng Artifical Intellegence For Contrast Enhance Mammography",
    "chunk_index": 34,
    "total_chunks": 61,
    "text_content": "for malignancy detection is to train a model to classify the data according to respective standards of ground truth. This can be done in two ways using machine learning techniques or using convolutional neural networks (CNNs). 8.1 Machine learning approach Machine learning techniques play a vital role in malignancy detection from CEM images by distinguishing between malignant and benign lesions. In a review of 14 studies using CEM datasets as mentioned in Table 2 , it was found that 7 of them us",
    "full_text_length": 63067,
    "chunk_length": 1375
  },
  {
    "chunk_id": 1817,
    "paper_filename": "kinkar_2024_breast_cancer_dignosisi_and radiology_practice_uisng_artifical_intellegence_for_contrast_enhance_mammography.pdf",
    "paper_title": "Kinkar 2024 Breast Cancer Dignosisi And Radiology Practice Uisng Artifical Intellegence For Contrast Enhance Mammography",
    "chunk_index": 35,
    "total_chunks": 61,
    "text_content": "where handcraftedfeatures are used as the training data. Machine learningKinkar et al. 10.3389/fradi.2023.1326831 Frontiers in Radiology 08 frontiersin.org techniques can effectively harness the valuable insights extracted from handcrafted features to develop robust models for making informed predictions. Here is a comprehensive overview of the key methodologies: 8.1.1 Logistic regression Logistic regression is a binary classi \ufb01cation technique known for its simplicity and effectiveness in disti",
    "full_text_length": 63067,
    "chunk_length": 1429
  },
  {
    "chunk_id": 1818,
    "paper_filename": "kinkar_2024_breast_cancer_dignosisi_and radiology_practice_uisng_artifical_intellegence_for_contrast_enhance_mammography.pdf",
    "paper_title": "Kinkar 2024 Breast Cancer Dignosisi And Radiology Practice Uisng Artifical Intellegence For Contrast Enhance Mammography",
    "chunk_index": 36,
    "total_chunks": 61,
    "text_content": "high- dimensional radiomics data derived from CEM images, making it an invaluable asset in the pursuit of precise malignancy detection, as used by ( 37). 8.1.3 Random forest Random forest is a robust ensemble learning technique that combines multiple decision trees to improve prediction accuracy. Its innate resistance to noise and over \ufb01tting makes it dependable choices for navigating the complexities of radiomics data, emerging as steadfast allies when precision is of paramount concern as used ",
    "full_text_length": 63067,
    "chunk_length": 1311
  },
  {
    "chunk_id": 1819,
    "paper_filename": "kinkar_2024_breast_cancer_dignosisi_and radiology_practice_uisng_artifical_intellegence_for_contrast_enhance_mammography.pdf",
    "paper_title": "Kinkar 2024 Breast Cancer Dignosisi And Radiology Practice Uisng Artifical Intellegence For Contrast Enhance Mammography",
    "chunk_index": 37,
    "total_chunks": 61,
    "text_content": "networks to learn from data. Neural networks are inspired by the human brain and can learn complex patterns from data. CNN is well-suited for image analysis tasks, including malignancy detection in CEM images. CNN models can learn to identify subtle features in images that may be dif \ufb01cult or impossible for humans to see, making them very effective at distinguishing between malignant and benign lesions. In a review of 14 studies, 7 used CNNs for model training. 6 out of 7 studies used transfer l",
    "full_text_length": 63067,
    "chunk_length": 1241
  },
  {
    "chunk_id": 1820,
    "paper_filename": "kinkar_2024_breast_cancer_dignosisi_and radiology_practice_uisng_artifical_intellegence_for_contrast_enhance_mammography.pdf",
    "paper_title": "Kinkar 2024 Breast Cancer Dignosisi And Radiology Practice Uisng Artifical Intellegence For Contrast Enhance Mammography",
    "chunk_index": 38,
    "total_chunks": 61,
    "text_content": "to different datasets and tasks. In addition to ResNet, other pre- trained networks such as XceptionNet, CSPDarkNet, and Inception models ( 64) were also used in the reviewed studies. Some studies using CNNs have not provided adequate information about key hyperparameters, such as learning rate schedule, optimization algorithm, minibatch size, dropout rates, and regularization parameters. Additionally, studies often fail to discuss why speci \ufb01c objective functions were chosen or how they align w",
    "full_text_length": 63067,
    "chunk_length": 1402
  },
  {
    "chunk_id": 1821,
    "paper_filename": "kinkar_2024_breast_cancer_dignosisi_and radiology_practice_uisng_artifical_intellegence_for_contrast_enhance_mammography.pdf",
    "paper_title": "Kinkar 2024 Breast Cancer Dignosisi And Radiology Practice Uisng Artifical Intellegence For Contrast Enhance Mammography",
    "chunk_index": 39,
    "total_chunks": 61,
    "text_content": "more likely to over \ufb01t. Cross- validation assesses a model \u2019s ability to generalize by repeatedly testing it on different data subsets. It helps with model selection, hyperparameter tuning, and providing a robust performance estimate, ensuring reliable results in medical diagnosis. Commonly used CV methods encompass K-fold Cross- Validation, as indicated in ( 34,39,40,43) which divides the data into subsets for rigorous evaluation. Strati \ufb01ed K-fold Cross- Validation is particularly bene \ufb01cial f",
    "full_text_length": 63067,
    "chunk_length": 1437
  },
  {
    "chunk_id": 1822,
    "paper_filename": "kinkar_2024_breast_cancer_dignosisi_and radiology_practice_uisng_artifical_intellegence_for_contrast_enhance_mammography.pdf",
    "paper_title": "Kinkar 2024 Breast Cancer Dignosisi And Radiology Practice Uisng Artifical Intellegence For Contrast Enhance Mammography",
    "chunk_index": 40,
    "total_chunks": 61,
    "text_content": "factors like dataset size, class distribution, and research objectives, with Strati \ufb01ed K-fold commonly favored in CEM datasets to ensure equitable evaluation of model performance. 10 Integration with clinical data The integration of clinical data with CEM datasets is a promising multi-modal approach for enhancing the accuracy and clinical utility of machine learning models for malignancy detection. This integration allows for a more holistic assessment of breast lesions by incorporating not onl",
    "full_text_length": 63067,
    "chunk_length": 1396
  },
  {
    "chunk_id": 1823,
    "paper_filename": "kinkar_2024_breast_cancer_dignosisi_and radiology_practice_uisng_artifical_intellegence_for_contrast_enhance_mammography.pdf",
    "paper_title": "Kinkar 2024 Breast Cancer Dignosisi And Radiology Practice Uisng Artifical Intellegence For Contrast Enhance Mammography",
    "chunk_index": 41,
    "total_chunks": 61,
    "text_content": "combination of modalities can vary depending on the research objectives and data availability. In the study by Miller et al. ( 40), they found that incorporating demographic and clinical information into their models led to a notably improved AUC-ROC compared to using only density images, contrast images, or the combination of density and contrast images. It is observed in study by Wang et al. ( 36), the inclusion of clinical features to the radiomics features for model training resulted in a si",
    "full_text_length": 63067,
    "chunk_length": 1274
  },
  {
    "chunk_id": 1824,
    "paper_filename": "kinkar_2024_breast_cancer_dignosisi_and radiology_practice_uisng_artifical_intellegence_for_contrast_enhance_mammography.pdf",
    "paper_title": "Kinkar 2024 Breast Cancer Dignosisi And Radiology Practice Uisng Artifical Intellegence For Contrast Enhance Mammography",
    "chunk_index": 42,
    "total_chunks": 61,
    "text_content": "employed by studies to attain their results. However, there exists a compelling scope in the realm of multimodal approaches, particularly considering the persistent challenge of data scarcity in medical image datasets. The incorporation of multimodal data holds the potential to revolutionize the \ufb01eld by augmenting the accuracy, sensitivity, and AUC of detection models. The rationale behind exploring multimodal approaches is rooted in the inherent strengths of deep learning. This robust tool enab",
    "full_text_length": 63067,
    "chunk_length": 1430
  },
  {
    "chunk_id": 1825,
    "paper_filename": "kinkar_2024_breast_cancer_dignosisi_and radiology_practice_uisng_artifical_intellegence_for_contrast_enhance_mammography.pdf",
    "paper_title": "Kinkar 2024 Breast Cancer Dignosisi And Radiology Practice Uisng Artifical Intellegence For Contrast Enhance Mammography",
    "chunk_index": 43,
    "total_chunks": 61,
    "text_content": "and clinical decision-making. The future of malignancy detection research lies in strategic utilization of multimodal data, overcoming individual limitations and paving the way for more robust and accurate detection models. The integration of multimodal approaches holds the potential to rede \ufb01ne malignancy detection research. 12 Conclusion In conclusion, advances in the \ufb01eld of Arti \ufb01cial Intelligence in Contrast-Enhanced Mammography (CEM) have occurred, holding enormous potential for changing b",
    "full_text_length": 63067,
    "chunk_length": 1457
  },
  {
    "chunk_id": 1826,
    "paper_filename": "kinkar_2024_breast_cancer_dignosisi_and radiology_practice_uisng_artifical_intellegence_for_contrast_enhance_mammography.pdf",
    "paper_title": "Kinkar 2024 Breast Cancer Dignosisi And Radiology Practice Uisng Artifical Intellegence For Contrast Enhance Mammography",
    "chunk_index": 44,
    "total_chunks": 61,
    "text_content": "but it also aids medical experts in to their decision-making processes. However, there is a lack of suf \ufb01cient reliable labeled training data and handling variability between imaging systems, and protocols. Therefore, while AI analysis shows promise for improving CEM diagnosis, larger studies assessing its clinical value and real-world effectiveness are required. For such studies to be designed and implemented, it is critical that researchers, doctors, and technologists continue tointeract and p",
    "full_text_length": 63067,
    "chunk_length": 1368
  },
  {
    "chunk_id": 1827,
    "paper_filename": "kinkar_2024_breast_cancer_dignosisi_and radiology_practice_uisng_artifical_intellegence_for_contrast_enhance_mammography.pdf",
    "paper_title": "Kinkar 2024 Breast Cancer Dignosisi And Radiology Practice Uisng Artifical Intellegence For Contrast Enhance Mammography",
    "chunk_index": 45,
    "total_chunks": 61,
    "text_content": "MWY: Validation, Writing \u2013review & editing. BAV: Methodology, Supervision, Validation, Writing \u2013original draft, Writing \u2013review & editing. Funding The author(s) declare that no \ufb01nancial support was received for the research, authorship, and/or publication of this article. Con\ufb02ict of interest The authors declare that the research was conducted in the absence of any commercial or \ufb01nancial relationships that could be construed as a potential con \ufb02ict of interest. The reviewer MYLS declared a shared",
    "full_text_length": 63067,
    "chunk_length": 1276
  },
  {
    "chunk_id": 1828,
    "paper_filename": "kinkar_2024_breast_cancer_dignosisi_and radiology_practice_uisng_artifical_intellegence_for_contrast_enhance_mammography.pdf",
    "paper_title": "Kinkar 2024 Breast Cancer Dignosisi And Radiology Practice Uisng Artifical Intellegence For Contrast Enhance Mammography",
    "chunk_index": 46,
    "total_chunks": 61,
    "text_content": "the publisher, the editors and the reviewers. Any product that may be evaluated in this article, or claim that may be made by its manufacturer, is not guaranteed or endorsed by the publisher.Kinkar et al. 10.3389/fradi.2023.1326831 Frontiers in Radiology 10 frontiersin.org References 1. Ghoncheh M, Pournamdar Z, Salehiniya H. Incidence and mortality and epidemiology of breast cancer in the world. Asian Pac J Cancer Prev . (2016) 17 (S3):43 \u20136. doi: 10.7314/APJCP.2016.17.S3.43 2. DeSantis CE, Ma ",
    "full_text_length": 63067,
    "chunk_length": 1329
  },
  {
    "chunk_id": 1829,
    "paper_filename": "kinkar_2024_breast_cancer_dignosisi_and radiology_practice_uisng_artifical_intellegence_for_contrast_enhance_mammography.pdf",
    "paper_title": "Kinkar 2024 Breast Cancer Dignosisi And Radiology Practice Uisng Artifical Intellegence For Contrast Enhance Mammography",
    "chunk_index": 47,
    "total_chunks": 61,
    "text_content": "Cost effectiveness of the NHS breast screening programme: life table model. Br Med J . (2013) 346:f2618. doi: 10.1136/bmj.f2618 5. Breast-cancer screening \u2014viewpoint of the IARC working group. NEJM. Available online at: https://www.nejm.org/doi/full/10.1056/NEJMsr1504363 (CitedJuly 30, 2023). 6. Chetlen A, Mack J, Chan T. Breast cancer screening controversies: who, when, why, and how? Clin Imaging . (2016) 40(2):279 \u201382. doi: 10.1016/j.clinimag.2015.05.017 7. Zhao H, Zou L, Geng X, Zheng S. Limi",
    "full_text_length": 63067,
    "chunk_length": 1473
  },
  {
    "chunk_id": 1830,
    "paper_filename": "kinkar_2024_breast_cancer_dignosisi_and radiology_practice_uisng_artifical_intellegence_for_contrast_enhance_mammography.pdf",
    "paper_title": "Kinkar 2024 Breast Cancer Dignosisi And Radiology Practice Uisng Artifical Intellegence For Contrast Enhance Mammography",
    "chunk_index": 48,
    "total_chunks": 61,
    "text_content": "doi: 10.1016/j.ejrad.2022.110513 10. Cozzi A, Magni V, Zanardo M, Schiaf \ufb01no S, Sardanelli F. Contrast-enhanced mammography: a systematic review and meta-analysis of diagnostic performance.Radiology . (2022) 302(3):568 \u201381. doi: 10.1148/radiol.211412 11. Jochelson MS, Lobbes MBI. Contrast-enhanced mammography: state of the art. Radiology . (2021) 299(1):36 \u201348. doi: 10.1148/radiol.2021201948 12. Dromain C, Balleyguier C. Contrast-Enhanced digital mammography. In: Bick U, Diekmann F, editors. Dig",
    "full_text_length": 63067,
    "chunk_length": 1493
  },
  {
    "chunk_id": 1831,
    "paper_filename": "kinkar_2024_breast_cancer_dignosisi_and radiology_practice_uisng_artifical_intellegence_for_contrast_enhance_mammography.pdf",
    "paper_title": "Kinkar 2024 Breast Cancer Dignosisi And Radiology Practice Uisng Artifical Intellegence For Contrast Enhance Mammography",
    "chunk_index": 49,
    "total_chunks": 61,
    "text_content": "F, Berghian A, Defta D, Vera P, Modzelewski R, et al. Deep learning analysis of contrast-enhanced spectral mammography to determine histoprognostic factors of malignant breast tumours. Eur Radiol . (2022) 32 (7):4834 \u201344. doi: 10.1007/s00330-022-08538-4 16. Prabhu S, Naveen DK, Bangera S, Bhat BS. Production of x-rays using x-ray tube. J Phys Conf Ser . (2020) 1712(1):012036. doi: 10.1088/1742-6596/1712/1/012036 17. Goo HW, Goo JM. Dual-energy CT: new horizon in medical imaging. Korean J Radiol ",
    "full_text_length": 63067,
    "chunk_length": 1409
  },
  {
    "chunk_id": 1832,
    "paper_filename": "kinkar_2024_breast_cancer_dignosisi_and radiology_practice_uisng_artifical_intellegence_for_contrast_enhance_mammography.pdf",
    "paper_title": "Kinkar 2024 Breast Cancer Dignosisi And Radiology Practice Uisng Artifical Intellegence For Contrast Enhance Mammography",
    "chunk_index": 50,
    "total_chunks": 61,
    "text_content": "(2014) 83(8):1350 \u20135. doi: 10.1016/j.ejrad.2014.05.015 20. Lalji UC, Jeukens CRLPN, Houben I, Nelemans PJ, van Engen RE, van Wylick E, et al. Evaluation of low-energy contrast-enhanced spectral mammography images bycomparing them to full- \ufb01eld digital mammography using EUREF image quality criteria. Eur Radiol . (2015) 25(10):2813 \u201320. doi: 10.1007/s00330-015-3695-2 21. Ghaderi KF, Phillips J, Perry H, Lot \ufb01P, Mehta TS. Contrast-enhanced mammography: current applications and future directions. Ra",
    "full_text_length": 63067,
    "chunk_length": 1618
  },
  {
    "chunk_id": 1833,
    "paper_filename": "kinkar_2024_breast_cancer_dignosisi_and radiology_practice_uisng_artifical_intellegence_for_contrast_enhance_mammography.pdf",
    "paper_title": "Kinkar 2024 Breast Cancer Dignosisi And Radiology Practice Uisng Artifical Intellegence For Contrast Enhance Mammography",
    "chunk_index": 51,
    "total_chunks": 61,
    "text_content": "of contrast-enhanced spectral mammography compared with full-\ufb01eld digital mammography. Invest Radiol . (2014) 49(10):659. doi: 10.1097/RLI. 0000000000000068 25. James JR, Pavlicek W, Hanson JA, Boltz TF, Patel BK. Breast radiation dose with CESM compared with 2D FFDM and 3D tomosynthesis mammography. Am J Roentgenol . (2017) 208(2):362 \u201372. doi: 10.2214/AJR.16.16743 26. Argus A, Mahoney MC. Indications for breast MRI: case-based review. Am J Roentgenol . (2011) 196(3_supplement):WS1 \u20134. 27. Sung",
    "full_text_length": 63067,
    "chunk_length": 1386
  },
  {
    "chunk_id": 1834,
    "paper_filename": "kinkar_2024_breast_cancer_dignosisi_and radiology_practice_uisng_artifical_intellegence_for_contrast_enhance_mammography.pdf",
    "paper_title": "Kinkar 2024 Breast Cancer Dignosisi And Radiology Practice Uisng Artifical Intellegence For Contrast Enhance Mammography",
    "chunk_index": 52,
    "total_chunks": 61,
    "text_content": "doi: 10.1186/s13244-019-0811-x 29. Gluskin J, Click M, Fleischman R, Dromain C, Morris EA, Jochelson MS. Contamination artifact that mimics in-situ carcinoma on contrast-enhanced digitalmammography. Eur J Radiol . (2017) 95:147 \u201354. doi: 10.1016/j.ejrad.2017.08.002 30. Mongan J, Moy L, Charles E, Kahn J. Checklist for arti \ufb01cial intelligence in medical imaging (CLAIM): a guide for authors and reviewers. Radiol Artif Intell . (2020) 2(2):e200029. doi: 10.1148/ryai.2020200029 31. McInnes MDF, Mohe",
    "full_text_length": 63067,
    "chunk_length": 1412
  },
  {
    "chunk_id": 1835,
    "paper_filename": "kinkar_2024_breast_cancer_dignosisi_and radiology_practice_uisng_artifical_intellegence_for_contrast_enhance_mammography.pdf",
    "paper_title": "Kinkar 2024 Breast Cancer Dignosisi And Radiology Practice Uisng Artifical Intellegence For Contrast Enhance Mammography",
    "chunk_index": 53,
    "total_chunks": 61,
    "text_content": "deep learning and handcrafted radiomics for classi \ufb01cation of suspicious lesions on contrast-enhanced mammograms. Radiology . (2023) 307(5): e221843. doi: 10.1148/radiol.221843 34. Wang S, Mao N, Duan S, Li Q, Li R, Jiang T, et al. Radiomic analysis of contrast- enhanced mammography with different image types: classi \ufb01cation of breast lesions. Front Oncol . (2021) 11:1873 \u2013918. doi: 10.3389/fonc.2021.600546 35. Petrillo A, Fusco R, Di Bernardo E, Petrosino T, Barretta ML, Porto A, et al. Predict",
    "full_text_length": 63067,
    "chunk_length": 1363
  },
  {
    "chunk_id": 1836,
    "paper_filename": "kinkar_2024_breast_cancer_dignosisi_and radiology_practice_uisng_artifical_intellegence_for_contrast_enhance_mammography.pdf",
    "paper_title": "Kinkar 2024 Breast Cancer Dignosisi And Radiology Practice Uisng Artifical Intellegence For Contrast Enhance Mammography",
    "chunk_index": 54,
    "total_chunks": 61,
    "text_content": "Petrosino T, et al. Radiomics and arti \ufb01cial intelligence analysis with textural metrics extracted by contrast-enhanced mammography in the breast lesions classi \ufb01cation. Diagnostics . (2021) 11(5):815. doi: 10.3390/diagnostics11050815 38. Wang S, Sun Y, Li R, Mao N, Li Q, Jiang T, et al. Diagnostic performance of perilesional radiomics analysis of contrast-enhanced mammography for thedifferentiation of benign and malignant breast lesions. Eur Radiol . (2022) 32 (1):639 \u201349. doi: 10.1007/s00330-0",
    "full_text_length": 63067,
    "chunk_length": 1401
  },
  {
    "chunk_id": 1837,
    "paper_filename": "kinkar_2024_breast_cancer_dignosisi_and radiology_practice_uisng_artifical_intellegence_for_contrast_enhance_mammography.pdf",
    "paper_title": "Kinkar 2024 Breast Cancer Dignosisi And Radiology Practice Uisng Artifical Intellegence For Contrast Enhance Mammography",
    "chunk_index": 55,
    "total_chunks": 61,
    "text_content": "al. SD-CNN: a shallow-deep CNN for improved breast cancer diagnosis. Comput Med Imaging Graph . (2018) 70:53 \u201362. doi: 10.1016/j.compmedimag.2018.09.004 42. Jailin C, Mohamed S, Iordache R, Milioni De Carvalho P, Ahmed SY, Abdel Sattar EA, et al. AI-based cancer detection model for contrast-enhancedmammography. Bioengineering . (2023) 10(8):974. doi: 10.3390/ bioengineering10080974 43. Zheng T, Lin F, Li X, Chu T, Gao J, Zhang S, et al. Deep learning-enabled fully automated pipeline system for s",
    "full_text_length": 63067,
    "chunk_length": 1423
  },
  {
    "chunk_id": 1838,
    "paper_filename": "kinkar_2024_breast_cancer_dignosisi_and radiology_practice_uisng_artifical_intellegence_for_contrast_enhance_mammography.pdf",
    "paper_title": "Kinkar 2024 Breast Cancer Dignosisi And Radiology Practice Uisng Artifical Intellegence For Contrast Enhance Mammography",
    "chunk_index": 56,
    "total_chunks": 61,
    "text_content": "S, et al. Detection and classi \ufb01cation of breast lesions using multiple information on contrast-enhanced mammography by a multiprocess deep-lea rning system: a multicenter study. Chin J Cancer Res . (2023) 35(4):408 \u201323. doi: 10.21147/j.issn.1000-9604.2023.04.07 46. Qian N, Jiang W, Guo Y, Zhu J, Qiu J, Yu H, et al. Breast cancer diagnosis from contrast-enhanced mammography using multi-feature fusion neural network. Eur Radiol . (2023) :1 \u201311. doi: 10.1007/s00330-023-10170-9 47. Perumal S, Thamb",
    "full_text_length": 63067,
    "chunk_length": 1359
  },
  {
    "chunk_id": 1839,
    "paper_filename": "kinkar_2024_breast_cancer_dignosisi_and radiology_practice_uisng_artifical_intellegence_for_contrast_enhance_mammography.pdf",
    "paper_title": "Kinkar 2024 Breast Cancer Dignosisi And Radiology Practice Uisng Artifical Intellegence For Contrast Enhance Mammography",
    "chunk_index": 57,
    "total_chunks": 61,
    "text_content": "50. Don S, Choi E, Min D. Breast mass segmentation in digital mammography using graph cuts. In: Lee G, Howard D, \u015al\u0119zak D, editors. Convergence and Hybrid Information Technology . Berlin, Heidelberg: Springer (2011). p. 88 \u201396. (Communications in Computer and Information Science). 51. Avanzo M, Wei L, Stancanello J, Valli\u00e8res M, Rao A, Morin O, et al. Machine and deep learning methods for radiomics. Med Phys . (2020) 47(5):e185 \u2013202. doi: 10.1002/mp.13678 52. Veldkamp WJH, Karssemeijer N. Normal",
    "full_text_length": 63067,
    "chunk_length": 1349
  },
  {
    "chunk_id": 1840,
    "paper_filename": "kinkar_2024_breast_cancer_dignosisi_and radiology_practice_uisng_artifical_intellegence_for_contrast_enhance_mammography.pdf",
    "paper_title": "Kinkar 2024 Breast Cancer Dignosisi And Radiology Practice Uisng Artifical Intellegence For Contrast Enhance Mammography",
    "chunk_index": 58,
    "total_chunks": 61,
    "text_content": "a python library for ef \ufb01cient loading, preprocessing, augmentation and patch-based sampling of medical imagesin deep learning. Comput Methods Programs Biomed . (2021) 208:106236. doi: 10. 1016/j.cmpb.2021.106236 55. Marino MA, Pinker K, Leithner D, Sung J, Avendano D, Morris EA, et al. Contrast-enhanced mammography and radiomics analysis for noninvasive breastcancer characterization: initial results. Mol Imaging Biol . (2020) 22(3):780 \u20137. doi: 10.1007/s11307-019-01423-5 56. Wang K, Patel BK, W",
    "full_text_length": 63067,
    "chunk_length": 1474
  },
  {
    "chunk_id": 1841,
    "paper_filename": "kinkar_2024_breast_cancer_dignosisi_and radiology_practice_uisng_artifical_intellegence_for_contrast_enhance_mammography.pdf",
    "paper_title": "Kinkar 2024 Breast Cancer Dignosisi And Radiology Practice Uisng Artifical Intellegence For Contrast Enhance Mammography",
    "chunk_index": 59,
    "total_chunks": 61,
    "text_content": "spectral mammography for identifying triple-negative breast cancer. Front Oncol . (2021) 11:773196. doi: 10.3389/fonc.2021. 773196 59. Zwanenburg A, Valli\u00e8res M, Abdalah MA, Aerts HJWL, Andrearczyk V, Apte A, et al. The image biomarker standardization initiative: standardized quantitative radiomics for high-throughput image-based phenotyping. Radiology . (2020) 295 (2):328 \u201338. doi: 10.1148/radiol.2020191145 60. Sauerbrei W, Royston P, Binder H. Selection of important variables and determination",
    "full_text_length": 63067,
    "chunk_length": 1617
  },
  {
    "chunk_id": 1842,
    "paper_filename": "kinkar_2024_breast_cancer_dignosisi_and radiology_practice_uisng_artifical_intellegence_for_contrast_enhance_mammography.pdf",
    "paper_title": "Kinkar 2024 Breast Cancer Dignosisi And Radiology Practice Uisng Artifical Intellegence For Contrast Enhance Mammography",
    "chunk_index": 60,
    "total_chunks": 61,
    "text_content": "on ImageNet classi \ufb01cation. (2015) 1026 \u201334. Available online at: https://openaccess.thecvf.com/content_iccv_2015/html/He_Delving_Deep_into_ICCV_2015_paper.html (Cited August 15, 2023) 64. Szegedy C, Vanhoucke V, Ioffe S, Shlens J, Wojna Z. Rethinking the inception architecture for computer vision. (2016) 2818 \u201326. Available online at: https://www.cv- foundation.org/openaccess/content_cvpr_201 6/html/Szegedy_Rethinking_the_Inception_ CVPR_2016_paper.html (Cited August 22, 2023).Kinkar et al. 10.",
    "full_text_length": 63067,
    "chunk_length": 565
  },
  {
    "chunk_id": 1843,
    "paper_filename": "kshitiz_2021_MMBCD_multimodal_breast cancer detection_specially_for_mammography.pdf",
    "paper_title": "Kshitiz 2021 Mmbcd Multimodal Breast Cancer Detection Specially For Mammography",
    "chunk_index": 0,
    "total_chunks": 26,
    "text_content": "MMBCD: Multimodal Breast Cancer Detection from Mammograms with Clinical History Kshitiz Jain1, Aditya Bansal2\u22c6, Krithika Rangarajan1,3, and Chetan Arora1\u22c6\u22c6 1Indian Institute of Technology Delhi, Delhi, India 2Thapar Institute of Engineering and Technology, Punjab, India 3All India Institute of Medical Sciences Delhi, India Abstract. Mammography serves as a vital tool for breast cancer detec- tion, with screening and diagnostic modalities catering to distinct pa- tient populations. However, in re",
    "full_text_length": 26728,
    "chunk_length": 1485
  },
  {
    "chunk_id": 1844,
    "paper_filename": "kshitiz_2021_MMBCD_multimodal_breast cancer detection_specially_for_mammography.pdf",
    "paper_title": "Kshitiz 2021 Mmbcd Multimodal Breast Cancer Detection Specially For Mammography",
    "chunk_index": 1,
    "total_chunks": 26,
    "text_content": "history. Since, current implementations of ViTcan not handle large 4K\u00d74Kmammography scans, we device a novel framework to first de- tect region-of-interests, and then classify using multi-instance-learning strategy, while allowing text embedding from clinical history to attend to the visual regions of interest from the mammograms. Extensive ex- perimentation demonstrates that our model, MMBCD, successfully incorpo- rates contextual information while preserving image resolution and con- text, lea",
    "full_text_length": 26728,
    "chunk_length": 1498
  },
  {
    "chunk_id": 1845,
    "paper_filename": "kshitiz_2021_MMBCD_multimodal_breast cancer detection_specially_for_mammography.pdf",
    "paper_title": "Kshitiz 2021 Mmbcd Multimodal Breast Cancer Detection Specially For Mammography",
    "chunk_index": 2,
    "total_chunks": 26,
    "text_content": "This omis- sion of clinical history can undermine the reliability and performance of deep neural network models, as they fail to incorporate essential diagnostic features \u22c6Work done as an intern at Indian Institute of Technology Delhi \u22c6\u22c6Corresponding author 2 K. Jain et al. valued by trained radiologists. For the readers of this paper who may not have a medical background, screening mammograms are similar to annual preventive exams based on the risk factors, and may not involve any specific comp",
    "full_text_length": 26728,
    "chunk_length": 1327
  },
  {
    "chunk_id": 1846,
    "paper_filename": "kshitiz_2021_MMBCD_multimodal_breast cancer detection_specially_for_mammography.pdf",
    "paper_title": "Kshitiz 2021 Mmbcd Multimodal Breast Cancer Detection Specially For Mammography",
    "chunk_index": 3,
    "total_chunks": 26,
    "text_content": "of an imaging study may be diminished [2]. Collecting patient history is straight- forward and can be completed by patients themselves, either independently or with minimal assistance. For mammograms, history is often gathered through a cost-effective questionnaire that elicits details about breast-related symptoms (such as a lump, discharge, pain, etc), prior breast cancer diagnoses, surgeries, radiation therapy, and other risk factors such as the family history of breast cancer, concurrent ill",
    "full_text_length": 26728,
    "chunk_length": 1395
  },
  {
    "chunk_id": 1847,
    "paper_filename": "kshitiz_2021_MMBCD_multimodal_breast cancer detection_specially_for_mammography.pdf",
    "paper_title": "Kshitiz 2021 Mmbcd Multimodal Breast Cancer Detection Specially For Mammography",
    "chunk_index": 4,
    "total_chunks": 26,
    "text_content": "masks. Tang et al.did not release their dataset or share code or trained models, precluding direct comparison with our methodology. Zheng et al.[22] utilized tabular data alongside CT images to predict patient survival outcomes, while Liu et al.[13] leveraged the CLIP foundation model for organ segmentation in CT scans. Hager et al.[6] explored aligning cardiac MR images with patients\u2019 routine clinical data. Wang et al. [20] introduced MedCLIP, a foundation model trained on extensive image data,",
    "full_text_length": 26728,
    "chunk_length": 1377
  },
  {
    "chunk_id": 1848,
    "paper_filename": "kshitiz_2021_MMBCD_multimodal_breast cancer detection_specially_for_mammography.pdf",
    "paper_title": "Kshitiz 2021 Mmbcd Multimodal Breast Cancer Detection Specially For Mammography",
    "chunk_index": 5,
    "total_chunks": 26,
    "text_content": "minute cancerous lesions spanning just a few hundred pixels. However, current deep neural network models necessitate resizing the images to a smaller dimension, leading to diminished performance, particularly in detecting small cancers [16]. Our method addresses the challenge by treating a mammogram, not as a single sample, but as a set of Regions of Interest ( ROIs), where cancer may be present, and reformulate the problem as a multi-instance learning [4,9]. Contributions. (1) Changing the atte",
    "full_text_length": 26728,
    "chunk_length": 1425
  },
  {
    "chunk_id": 1849,
    "paper_filename": "kshitiz_2021_MMBCD_multimodal_breast cancer detection_specially_for_mammography.pdf",
    "paper_title": "Kshitiz 2021 Mmbcd Multimodal Breast Cancer Detection Specially For Mammography",
    "chunk_index": 6,
    "total_chunks": 26,
    "text_content": "architecture along with multi-instance-learning. The proposed architecture not only allows to use full resolution mammograms for training, and inference, but also helps our model to focus on salient regions (extremely important to learn from small datasets). (3)Our MMBCDmodel suc- cessfully incorporates contextual information while preserving image resolution and context, leading to superior results over existing methods as already men- tioned in the abstract of the manuscript. 2 Dataset Our dat",
    "full_text_length": 26728,
    "chunk_length": 1501
  },
  {
    "chunk_id": 1850,
    "paper_filename": "kshitiz_2021_MMBCD_multimodal_breast cancer detection_specially_for_mammography.pdf",
    "paper_title": "Kshitiz 2021 Mmbcd Multimodal Breast Cancer Detection Specially For Mammography",
    "chunk_index": 7,
    "total_chunks": 26,
    "text_content": "lexicon, when documenting indications and comments during patient diagnosis. Due to variations in patient demographics between the two datasets, the nature of comments provided by clinicians also exhibits differences. The first dataset predominantly includes complaints from patients, whereas the second dataset contains information concerning the patients\u2019 medical history. AIIMS 1. This dataset comprises 3,816 studies, each containing digital mam- mograms of patients along with their correspondin",
    "full_text_length": 26728,
    "chunk_length": 1448
  },
  {
    "chunk_id": 1851,
    "paper_filename": "kshitiz_2021_MMBCD_multimodal_breast cancer detection_specially_for_mammography.pdf",
    "paper_title": "Kshitiz 2021 Mmbcd Multimodal Breast Cancer Detection Specially For Mammography",
    "chunk_index": 8,
    "total_chunks": 26,
    "text_content": "evaluate our model\u2019s performance under real-world testing scenarios, enhancing its reliability. AIIMS 2. This dataset consists of 583 (58 malignancies) studies obtained from a different department within our hospital. Mimicking the specifications and procedures of AIIMS 1 dataset, this dataset serves as an external validation set to demonstrate the generalization capability of our model. This dataset is acquired from the studies that happened between January 2020 to December 2020. We are plannin",
    "full_text_length": 26728,
    "chunk_length": 1427
  },
  {
    "chunk_id": 1852,
    "paper_filename": "kshitiz_2021_MMBCD_multimodal_breast cancer detection_specially_for_mammography.pdf",
    "paper_title": "Kshitiz 2021 Mmbcd Multimodal Breast Cancer Detection Specially For Mammography",
    "chunk_index": 9,
    "total_chunks": 26,
    "text_content": "ranges from 1 to 45 words, with an average length of 8 words. These notes reflect the doctor\u2019s observations and speculations regarding the patient\u2019s complaints, excluding any pathology reports. The textual data encompassesdescriptionsofpriorbreastcancerconcernsandpatientdiscomforts. For patients undergoing regular screening, the clinical history often contains the term \u201cscreening\u201d. To process this data, we utilize the built-in tokenizer provided byRoBERTa [14]. In Fig. 1, we highlight class-spec",
    "full_text_length": 26728,
    "chunk_length": 1431
  },
  {
    "chunk_id": 1853,
    "paper_filename": "kshitiz_2021_MMBCD_multimodal_breast cancer detection_specially_for_mammography.pdf",
    "paper_title": "Kshitiz 2021 Mmbcd Multimodal Breast Cancer Detection Specially For Mammography",
    "chunk_index": 10,
    "total_chunks": 26,
    "text_content": "to effectively capture relevant fea- tures. Given that cancerous lesions in mammograms are often small, and the majority of the image area is benign, and looks similar across samples, it es- sentially converts our problem to a fine-grained visual classification task, where focusing on salient regions becomes extremely important. To help a foundational model in our architecture to focus on salient regions, we employ an object de- tection modules[10,17,21] trained to delineate bounding boxes aroun",
    "full_text_length": 26728,
    "chunk_length": 1370
  },
  {
    "chunk_id": 1854,
    "paper_filename": "kshitiz_2021_MMBCD_multimodal_breast cancer detection_specially_for_mammography.pdf",
    "paper_title": "Kshitiz 2021 Mmbcd Multimodal Breast Cancer Detection Specially For Mammography",
    "chunk_index": 11,
    "total_chunks": 26,
    "text_content": "by the textual representation of the clinical history. Our findings reveal the synergistic impact of textual, visual, and cross-attention em- beddings on the accuracy of breast cancer detection. a small subset comprising approximately 390 malignant mammograms from our training dataset. Additionally, we utilize 540 benign images, which do not re- quire bounding box annotations. This approach enables us to effectively train our module to provide ROIannotations over mammograms. To perform ROIex- tr",
    "full_text_length": 26728,
    "chunk_length": 1329
  },
  {
    "chunk_id": 1855,
    "paper_filename": "kshitiz_2021_MMBCD_multimodal_breast cancer detection_specially_for_mammography.pdf",
    "paper_title": "Kshitiz 2021 Mmbcd Multimodal Breast Cancer Detection Specially For Mammography",
    "chunk_index": 12,
    "total_chunks": 26,
    "text_content": "encoder, we treat the ROIsextracted from mammograms as a set {Rm j=1}i. For malignant images, this set comprises both malignant and benign regions of the mammogram, whereas for benign images, it consists solely of benign regions. In this setup, we leverage classical multi-instance learning ( MIL) strategy. We train our image encoder ( V) by inputting all the ROIsto it and extracting visual embeddings ( {ev}m j=1) from each ROI. Subsequently, we aggregate these embed- dings to create a collective",
    "full_text_length": 26728,
    "chunk_length": 1304
  },
  {
    "chunk_id": 1856,
    "paper_filename": "kshitiz_2021_MMBCD_multimodal_breast cancer detection_specially_for_mammography.pdf",
    "paper_title": "Kshitiz 2021 Mmbcd Multimodal Breast Cancer Detection Specially For Mammography",
    "chunk_index": 13,
    "total_chunks": 26,
    "text_content": "if the number of benign embeddings outweighs the number of malignant embeddings, potentially diminishing the rep- resentation of malignant features. Specifically, we utilize the Vision Transformer (ViT) architecture from the open-source DINO[3] repository. 6 K. Jain et al. Text encoder. For the text encoder, we utilize a sequence classification mod- ule to encode the clinical history ( Hn i=1) of the patients. In our experiments, we employ another text foundational model, RoBERTa [14] to extract",
    "full_text_length": 26728,
    "chunk_length": 1325
  },
  {
    "chunk_id": 1857,
    "paper_filename": "kshitiz_2021_MMBCD_multimodal_breast cancer detection_specially_for_mammography.pdf",
    "paper_title": "Kshitiz 2021 Mmbcd Multimodal Breast Cancer Detection Specially For Mammography",
    "chunk_index": 14,
    "total_chunks": 26,
    "text_content": "is important to note that our model operates under a single-view classification paradigm. For patients diagnosed with malignancy, during training, we do not include clinical history from the unaffected breast. 3.3 Cross-attention between clinical history and image ROIs To integrate both the visual ( {ev}m j=1) and clinical history ( ET) embeddings, we train a cross-attention module between these embeddings. In this setup, the clinicalhistory ETservesasthequery,whileeach {ev}jactsasthekeyandvalue",
    "full_text_length": 26728,
    "chunk_length": 1479
  },
  {
    "chunk_id": 1858,
    "paper_filename": "kshitiz_2021_MMBCD_multimodal_breast cancer detection_specially_for_mammography.pdf",
    "paper_title": "Kshitiz 2021 Mmbcd Multimodal Breast Cancer Detection Specially For Mammography",
    "chunk_index": 15,
    "total_chunks": 26,
    "text_content": "patient\u2019s clinical background. Finally, we concatenate the obtained embeddings ( EV, EA, and E T), and pass them through a Multi-Layer Perceptron ( MLP) to generate the malignancy score ( \u02c6y). 4 Experiments and Results Implementation Details. Ourdetectionmodelisinitializedwith COCOweights, while vision encoders leverage various publicly available pre-trained weights. We employ Non-Maximum Suppression ( NMS) with an Intersection over Union ( IoU) threshold of 0.1 on detection model\u2019s output. This",
    "full_text_length": 26728,
    "chunk_length": 1369
  },
  {
    "chunk_id": 1859,
    "paper_filename": "kshitiz_2021_MMBCD_multimodal_breast cancer detection_specially_for_mammography.pdf",
    "paper_title": "Kshitiz 2021 Mmbcd Multimodal Breast Cancer Detection Specially For Mammography",
    "chunk_index": 16,
    "total_chunks": 26,
    "text_content": "0.36 0.820 0.64 0.22 0.769 CLIP-R50[15] CLIP 0.76 0.34 0.759 0.73 0.21 0.694 CLIP-ViT[15] ImageNet 0.83 0.47 0.856 0.69 0.20 0.699 CLIP-ViT[15] DINO 0.84 0.5 0.895 0.91 0.27 0.741 LLaVA[12] LCS-558K 0.91 0.41 - 0.87 0.39 - OURS 0.96 0.82 0.973 0.95 0.68 0.950 is executed on a server equipped with 8 NVidia A100 GPUs, each possessing 80GB of memory. To ensure complete reproducibility, we release the source code of our model.4 4.1 Comparison with multi-modal foundational models We evaluate the perf",
    "full_text_length": 26728,
    "chunk_length": 1393
  },
  {
    "chunk_id": 1860,
    "paper_filename": "kshitiz_2021_MMBCD_multimodal_breast cancer detection_specially_for_mammography.pdf",
    "paper_title": "Kshitiz 2021 Mmbcd Multimodal Breast Cancer Detection Specially For Mammography",
    "chunk_index": 17,
    "total_chunks": 26,
    "text_content": "along with the textual embed- ding, undergoes contrastive loss learning. During testing, the model receives a mammogram and two prompts for binary classification. Prompts: <Cancer:{Yes/No}; Indication:{Clinical History }>(4) We experiment with various CLIPconfigurations by switching vision encoders, and weight initializations as shown in Table 1. LLava-1.5 [12].Wealsofine-tunestate-of-the-artfoundationalmodel LLaVa-1.5 using the LORA[8] method. Both the training and testing sets encompass com- p",
    "full_text_length": 26728,
    "chunk_length": 1403
  },
  {
    "chunk_id": 1861,
    "paper_filename": "kshitiz_2021_MMBCD_multimodal_breast cancer detection_specially_for_mammography.pdf",
    "paper_title": "Kshitiz 2021 Mmbcd Multimodal Breast Cancer Detection Specially For Mammography",
    "chunk_index": 18,
    "total_chunks": 26,
    "text_content": "hence we could not calculate theAUCmetric for it. 4.2 Ablation Study Table 2 illustrates the significance of each embedding within our network ar- chitecture. Notably, the inclusion of patient clinical history (text embeddings) significantly enhances the performance of our proposed module. Additionally, 4https://mammo-iitd-aiims.github.io/MMBCD 8 K. Jain et al. Fig. 3.The figure shows the ROIbounding boxes and the respective attention scores obtained from our proposed cross-attention layer for d",
    "full_text_length": 26728,
    "chunk_length": 1335
  },
  {
    "chunk_id": 1862,
    "paper_filename": "kshitiz_2021_MMBCD_multimodal_breast cancer detection_specially_for_mammography.pdf",
    "paper_title": "Kshitiz 2021 Mmbcd Multimodal Breast Cancer Detection Specially For Mammography",
    "chunk_index": 19,
    "total_chunks": 26,
    "text_content": "the effectiveness of our proposed cross-attention layer between text and visual embeddings is underscored by the attention visualization in Fig. 3, showcas- ing salient regions of mammograms receiving significant attention by the text embeddings. Note that for experiments without proposed attention module, we simply concatenate visual and text embeddings and pass it through an MLPfor prediction. Furthermore, our approach prevents overfitting during text module training through proposed masking o",
    "full_text_length": 26728,
    "chunk_length": 1396
  },
  {
    "chunk_id": 1863,
    "paper_filename": "kshitiz_2021_MMBCD_multimodal_breast cancer detection_specially_for_mammography.pdf",
    "paper_title": "Kshitiz 2021 Mmbcd Multimodal Breast Cancer Detection Specially For Mammography",
    "chunk_index": 20,
    "total_chunks": 26,
    "text_content": "automated techniques for breast cancer detection do not include clinical history in the inference process. Our proposed approach addresses this key deficiency. In the process, we develop a novel architecture which leverages recent advances in multi-modal foundational models to give an highly accurate breast cancer detection. MMBCD 9 Table 3. Ablation study by using various vision en- coders in proposed model. As observed, ViT-DINO works best in our settings, and has been used in all experiments.",
    "full_text_length": 26728,
    "chunk_length": 1367
  },
  {
    "chunk_id": 1864,
    "paper_filename": "kshitiz_2021_MMBCD_multimodal_breast cancer detection_specially_for_mammography.pdf",
    "paper_title": "Kshitiz 2021 Mmbcd Multimodal Breast Cancer Detection Specially For Mammography",
    "chunk_index": 21,
    "total_chunks": 26,
    "text_content": "in AI funded by Ministry of education, government of India, Central Project Management Unit, IIT Jammu with sanction number IITJMU/CPMU-AI/2024/0002. Kshitiz Jain is supported by Yardi School of Artificial Intelligence, IIT Delhi via its Publication Grant for Students. Disclosure of Interests. The authors have no competing interests to declare that are relevant to the content of this article. References 1. Burnside, E.S., Sickles, E.A., Bassett, L.W., Rubin, D.L., Lee, C.H., Ikeda, D.M., Mendels",
    "full_text_length": 26728,
    "chunk_length": 1520
  },
  {
    "chunk_id": 1865,
    "paper_filename": "kshitiz_2021_MMBCD_multimodal_breast cancer detection_specially_for_mammography.pdf",
    "paper_title": "Kshitiz 2021 Mmbcd Multimodal Breast Cancer Detection Specially For Mammography",
    "chunk_index": 22,
    "total_chunks": 26,
    "text_content": "(2021) 5 4. Dietterich, T.G., Lathrop, R.H., Lozano-P\u00e9rez, T.: Solving the multiple instance problem with axis-parallel rectangles. Artificial intelligence (1997) 2 5. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al.: An image is worth 16x16 words: Transformers for image recognition at scale. arXiv:2010.11929 (2020) 2 6. Hager,P.,Menten,M.J.,Rueckert,D.:Bestofbothworlds:Multimodalcontrastive learnin",
    "full_text_length": 26728,
    "chunk_length": 1450
  },
  {
    "chunk_id": 1866,
    "paper_filename": "kshitiz_2021_MMBCD_multimodal_breast cancer detection_specially_for_mammography.pdf",
    "paper_title": "Kshitiz 2021 Mmbcd Multimodal Breast Cancer Detection Specially For Mammography",
    "chunk_index": 23,
    "total_chunks": 26,
    "text_content": "A., Qiu, J.: YOLO by Ultralytics (Jan 2023), https:// github.com/ultralytics/ultralytics 4 11. Kooi, T., Litjens, G., Van Ginneken, B., Gubern-M\u00e9rida, A., S\u00e1nchez, C.I., Mann, R., den Heeten, A., Karssemeijer, N.: Large scale deep learning for computer aided detection of mammographic lesions. Medical image analysis (2017) 2 12. Liu, H., Li, C., Li, Y., Lee, Y.J.: Improved baselines with visual instruction tuning. arXiv preprint arXiv:2310.03744 (2023) 7 13. Liu, J., Zhang, Y., Chen, J.N., Xiao, ",
    "full_text_length": 26728,
    "chunk_length": 1314
  },
  {
    "chunk_id": 1867,
    "paper_filename": "kshitiz_2021_MMBCD_multimodal_breast cancer detection_specially_for_mammography.pdf",
    "paper_title": "Kshitiz 2021 Mmbcd Multimodal Breast Cancer Detection Specially For Mammography",
    "chunk_index": 24,
    "total_chunks": 26,
    "text_content": "Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al.: Learning transferable visual models from natural language supervision. In: International conference on machine learning. pp. 8748\u20138763. PMLR (2021) 7 16. Rangarajan, K., Gupta, A., Dasgupta, S., Marri, U., Gupta, A.K., Hari, S., Baner- jee, S., Arora, C.: Ultra-high resolution, multi-scale, context-aware approach for detection of small cancers on mammography. Nature Scientific Reports (2022) 2 17. Ren, S., ",
    "full_text_length": 26728,
    "chunk_length": 1282
  },
  {
    "chunk_id": 1868,
    "paper_filename": "kshitiz_2021_MMBCD_multimodal_breast cancer detection_specially_for_mammography.pdf",
    "paper_title": "Kshitiz 2021 Mmbcd Multimodal Breast Cancer Detection Specially For Mammography",
    "chunk_index": 25,
    "total_chunks": 26,
    "text_content": "20. Wang, Z., Wu, Z., Agarwal, D., Sun, J.: Medclip: Contrastive learning from un- paired medical images and text. arXiv preprint arXiv:2210.10163 (2022) 2 21. Yang, J., Li, C., Dai, X., Gao, J.: Focal modulation networks. NeurIPS (2022) 4, 5 22. Zheng, H., Lin, Z., Zhou, Q., Peng, X., Xiao, J., Zu, C., Jiao, Z., Wang, Y.: Multi- transsp: Multimodal transformer for survival prediction of nasopharyngeal carci- noma patients. In: International Conference on Medical Image Computing and Computer-Ass",
    "full_text_length": 26728,
    "chunk_length": 550
  },
  {
    "chunk_id": 1869,
    "paper_filename": "mahdi_2024_ask_in_any_modality_a_comprehensive_survey_on_multimodal_RAG.pdf",
    "paper_title": "Mahdi 2024 Ask In Any Modality A Comprehensive Survey On Multimodal Rag",
    "chunk_index": 0,
    "total_chunks": 146,
    "text_content": "Ask in Any Modality: A Comprehensive Survey on Multimodal Retrieval-Augmented Generation Mohammad Mahdi Abootorabi\u2020, Amirhosein Zobeiri\u22c4, Mahdi Dehghani\u00b6, Mohammadali Mohammadkhani\u00a7, Bardia Mohammadi\u00a7,Omid Ghahroodi\u2020,Mahdieh Soleymani Baghshah\u00a7,*,Ehsaneddin Asgari\u2020,* \u00a7Computer Engineering Department, Sharif University of Technology, Tehran, Iran, \u22c4College of Interdisciplinary Science and Technology, University of Tehran, Tehran, Iran, \u00b6Computer Engineering Department, K.N. Toosi University of Te",
    "full_text_length": 153423,
    "chunk_length": 1705
  },
  {
    "chunk_id": 1870,
    "paper_filename": "mahdi_2024_ask_in_any_modality_a_comprehensive_survey_on_multimodal_RAG.pdf",
    "paper_title": "Mahdi 2024 Ask In Any Modality A Comprehensive Survey On Multimodal Rag",
    "chunk_index": 1,
    "total_chunks": 146,
    "text_content": "offers a structured and comprehensive analysis of Multimodal RAG systems, covering datasets, metrics, benchmarks, evaluation, methodologies, and innovations in retrieval, fusion, augmentation, and generation. We precisely review training strategies, robustness enhancements, and loss functions, while also exploring the diverse Multimodal RAG scenarios. Furthermore, we discuss open challenges and future research directions to support advancements in this evolving field. This survey lays the founda",
    "full_text_length": 153423,
    "chunk_length": 1511
  },
  {
    "chunk_id": 1871,
    "paper_filename": "mahdi_2024_ask_in_any_modality_a_comprehensive_survey_on_multimodal_RAG.pdf",
    "paper_title": "Mahdi 2024 Ask In Any Modality A Comprehensive Survey On Multimodal Rag",
    "chunk_index": 2,
    "total_chunks": 146,
    "text_content": "2023; Qwen et al., 2025; Anil et al., 2023) has revolutionized natural language processing (NLP), demonstrat- ing unprecedented capabilities in a wide range of tasks including instruction following (Qin et al., 2024), sophisticated reasoning (Wei et al., 2024c), In-context Learning (Brown et al., 2020), and mul- tilingual machine translation (Zhu et al., 2024a). These advancements have elevated the performance of various NLP tasks, opening new avenues for re- search and application. Despite thei",
    "full_text_length": 153423,
    "chunk_length": 1424
  },
  {
    "chunk_id": 1872,
    "paper_filename": "mahdi_2024_ask_in_any_modality_a_comprehensive_survey_on_multimodal_RAG.pdf",
    "paper_title": "Mahdi 2024 Ask In Any Modality A Comprehensive Survey On Multimodal Rag",
    "chunk_index": 3,
    "total_chunks": 146,
    "text_content": "2020) has emerged as a promising so- lution to these limitations by enabling LLMs to re- trieve and incorporate external knowledge, improv- ing factual accuracy and reducing hallucinations (Shuster et al., 2021; Ding et al., 2024a). By dy- namically accessing vast external knowledge repos- itories, RAG systems enhance knowledge-intensive tasks while ensuring responses remain grounded in verifiable sources (Gao et al., 2023). In practice, RAG systems operate through a retriever-generator pipeline",
    "full_text_length": 153423,
    "chunk_length": 1401
  },
  {
    "chunk_id": 1873,
    "paper_filename": "mahdi_2024_ask_in_any_modality_a_comprehensive_survey_on_multimodal_RAG.pdf",
    "paper_title": "Mahdi 2024 Ask In Any Modality A Comprehensive Survey On Multimodal Rag",
    "chunk_index": 4,
    "total_chunks": 146,
    "text_content": "2024), and feedback-driven itera- tive refinement (Liu et al., 2024c; Asai et al., 2023), further enhance both retrieval and generation stages. However, traditional RAG architectures are primar- ily designed for textual information, limiting their ability to address multimodal challenges that re- quire integrating diverse data formats. Multimodal Learning Parallel to these develop- ments, significant advances in multimodal learning have reshaped artificial intelligence by enabling systems to int",
    "full_text_length": 153423,
    "chunk_length": 1424
  },
  {
    "chunk_id": 1874,
    "paper_filename": "mahdi_2024_ask_in_any_modality_a_comprehensive_survey_on_multimodal_RAG.pdf",
    "paper_title": "Mahdi 2024 Ask In Any Modality A Comprehensive Survey On Multimodal Rag",
    "chunk_index": 5,
    "total_chunks": 146,
    "text_content": "of multimodal approaches. By enabling sys- tems to process and understand diverse data types such as text, images, audio, and video, multimodal learning plays a key role in advancing artificial general intelligence (AGI) (Song et al., 2025). Multimodal RAG The extension of LLMs to mul- timodal LLMs (MLLMs) has further expanded their capabilities, allowing them to process, reason, and generate outputs across diverse modalities (Liu et al., 2023a; Team et al., 2024; Li et al., 2023b). For example,",
    "full_text_length": 153423,
    "chunk_length": 1371
  },
  {
    "chunk_id": 1875,
    "paper_filename": "mahdi_2024_ask_in_any_modality_a_comprehensive_survey_on_multimodal_RAG.pdf",
    "paper_title": "Mahdi 2024 Ask In Any Modality A Comprehensive Survey On Multimodal Rag",
    "chunk_index": 6,
    "total_chunks": 146,
    "text_content": "generated outputs while leveraging multimodal cues to improve the reasoning capa- bilities of MLLMs. However, these multimodal systems also present unique challenges, including determining which modalities to retrieve, effec-tively fusing diverse data types, and addressing the complexities of cross-modal relevance (Zhao et al., 2023a). Figure 1 illustrates the general pipeline of these systems. Task Formulation A mathematical formulation of the general task for multimodal RAG is pre- sented in t",
    "full_text_length": 153423,
    "chunk_length": 1381
  },
  {
    "chunk_id": 1876,
    "paper_filename": "mahdi_2024_ask_in_any_modality_a_comprehensive_survey_on_multimodal_RAG.pdf",
    "paper_title": "Mahdi 2024 Ask In Any Modality A Comprehensive Survey On Multimodal Rag",
    "chunk_index": 7,
    "total_chunks": 146,
    "text_content": "ment representation zwith respect to the query q, represented as R(q, z). To construct the retrieval- augmented multimodal context, the retrieval model selects the most relevant documents based on a modality-specific threshold: X={di|s(eq, zi)\u2265\u03c4Mdi} (2) where \u03c4Mdiis a relevancy threshold for the modal- ity of Mdi,eqis the encoded representation of q in the shared semantic space, and sis a scoring function that measures the relevance between the encoded query and document representations. The gen",
    "full_text_length": 153423,
    "chunk_length": 1324
  },
  {
    "chunk_id": 1877,
    "paper_filename": "mahdi_2024_ask_in_any_modality_a_comprehensive_survey_on_multimodal_RAG.pdf",
    "paper_title": "Mahdi 2024 Ask In Any Modality A Comprehensive Survey On Multimodal Rag",
    "chunk_index": 8,
    "total_chunks": 146,
    "text_content": "al., 2025), none provide a detailed and compre- hensive overview of advancements in multimodal RAGs. The only related survey to date (Zhao et al., 2023a) categorizes multimodal RAGs by group- ing relevant papers based on their applications and modalities. However, our survey provides a more detailed and innovation-driven perspective, offer- ing a detailed taxonomy and exploring emerging 2 Figure 1: Overview of the multimodal retrieval-augmented generation (RAG) pipeline, highlighting the advance",
    "full_text_length": 153423,
    "chunk_length": 1487
  },
  {
    "chunk_id": 1878,
    "paper_filename": "mahdi_2024_ask_in_any_modality_a_comprehensive_survey_on_multimodal_RAG.pdf",
    "paper_title": "Mahdi 2024 Ask In Any Modality A Comprehensive Survey On Multimodal Rag",
    "chunk_index": 9,
    "total_chunks": 146,
    "text_content": "generation stage incorporates innovations like Chain-of-Thought reasoning and source attribution for better outputs, with loss functions combining alignment loss and generation loss to optimize both retrieval and generation components. Noise management techniques are also applied to improve training stability and robustness. trends and challenges in depth. Furthermore, sig- nificant advancements have been made in the field since its publication, and interest in this topic has grown substantially",
    "full_text_length": 153423,
    "chunk_length": 1503
  },
  {
    "chunk_id": 1879,
    "paper_filename": "mahdi_2024_ask_in_any_modality_a_comprehensive_survey_on_multimodal_RAG.pdf",
    "paper_title": "Mahdi 2024 Ask In Any Modality A Comprehensive Survey On Multimodal Rag",
    "chunk_index": 10,
    "total_chunks": 146,
    "text_content": "on their primary contributions, highlighting methodological advancements and emerging fron- tiers. (iii)To support further research, we make resources, including datasets, benchmarks, and key innovations, publicly available. (iv)We identify current research trends and knowledge gaps, pro- viding insights and recommendations to guide fu- ture advancements in this evolving field. 2 Datasets and Benchmarks Multimodal RAG research employs diverse datasets and benchmarks to evaluate retrieval, in- te",
    "full_text_length": 153423,
    "chunk_length": 1403
  },
  {
    "chunk_id": 1880,
    "paper_filename": "mahdi_2024_ask_in_any_modality_a_comprehensive_survey_on_multimodal_RAG.pdf",
    "paper_title": "Mahdi 2024 Ask In Any Modality A Comprehensive Survey On Multimodal Rag",
    "chunk_index": 11,
    "total_chunks": 146,
    "text_content": "In the medical domain, MIMIC-CXR (Johnson et al., 2019) and CheXpert (Irvin et al., 2019) fa- cilitate tasks such as medical report generation. It is noteworthy that a number of these datasets are unimodal (e.g., solely text-based or image-based). Unimodal datasets are frequently employed to rep- resent a specific modality and are subsequently integrated with complementary datasets from other modalities. This modular approach allows each dataset to contribute its domain-specific strengths, there",
    "full_text_length": 153423,
    "chunk_length": 1405
  },
  {
    "chunk_id": 1881,
    "paper_filename": "mahdi_2024_ask_in_any_modality_a_comprehensive_survey_on_multimodal_RAG.pdf",
    "paper_title": "Mahdi 2024 Ask In Any Modality A Comprehensive Survey On Multimodal Rag",
    "chunk_index": 12,
    "total_chunks": 146,
    "text_content": "(Suri et al., 2024), test models on complex visual tasks. Dyn-VQA (Li et al., 2024b), MMBench (Liu et al., 2025), and ScienceQA (Saikh et al., 2022) evaluate dynamic retrieval and multi-hop reasoning across textual, visual, and diagrammatic inputs. Knowledge-intensive benchmarks, such as Triv- iaQA (Joshi et al., 2017) and Natural Ques- tions (Kwiatkowski et al., 2019), together with document-oriented evaluations such as Om- niDocBench (Ouyang et al., 2024), measure in- tegration of unstructured",
    "full_text_length": 153423,
    "chunk_length": 1356
  },
  {
    "chunk_id": 1882,
    "paper_filename": "mahdi_2024_ask_in_any_modality_a_comprehensive_survey_on_multimodal_RAG.pdf",
    "paper_title": "Mahdi 2024 Ask In Any Modality A Comprehensive Survey On Multimodal Rag",
    "chunk_index": 13,
    "total_chunks": 146,
    "text_content": "Appendix (\u00a7B). 3 Evaluation Evaluating multimodal RAG models is complex due to their varied input types and complex struc- ture. The evaluation combines metrics from VLMs, generative AI, and retrieval systems to assess ca- pabilities like text/image generation and informa- tion retrieval. Our review found about 60 different metrics used in the field. More details, includ- ing the formulas for the RAG evaluation metrics, can be found in Appendix (\u00a7C). In the following paragraphs, we will examine ",
    "full_text_length": 153423,
    "chunk_length": 1342
  },
  {
    "chunk_id": 1883,
    "paper_filename": "mahdi_2024_ask_in_any_modality_a_comprehensive_survey_on_multimodal_RAG.pdf",
    "paper_title": "Mahdi 2024 Ask In Any Modality A Comprehensive Survey On Multimodal Rag",
    "chunk_index": 14,
    "total_chunks": 146,
    "text_content": "evaluations primarily focus on text and image, assessing their alignment, text fluency, and image caption quality.For text evaluation, metrics include Exact Match (EM), BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), and METEOR (Banerjee and Lavie, 2005). MultiRAGen (Shohan et al., 2024) uses Multilin- gual ROUGE for multilingual settings. For image captioning, CIDEr (Consensus-Based Image Description Evaluation) (Vedantam et al., 2015) measures caption quality using TF-IDF and cosine similarit",
    "full_text_length": 153423,
    "chunk_length": 1260
  },
  {
    "chunk_id": 1884,
    "paper_filename": "mahdi_2024_ask_in_any_modality_a_comprehensive_survey_on_multimodal_RAG.pdf",
    "paper_title": "Mahdi 2024 Ask In Any Modality A Comprehensive Survey On Multimodal Rag",
    "chunk_index": 15,
    "total_chunks": 146,
    "text_content": "BERT embeddings (Sun et al., 2024b; Shohan et al., 2024), and evaluates fluency (Chen et al., 2022a; Zhi Lim et al., 2024; Ma et al., 2024c). CLIP Score (Hessel et al., 2021), used in (Shari- fymoghaddam et al., 2024; Zhang et al., 2024c), measures image-text similarity using CLIP (Rad- ford et al., 2021). For image quality, FID (Fr\u00e9chet Inception Distance) (Heusel et al., 2017) compares feature distributions (Yasunaga et al., 2023; Zhao et al., 2024; Sharifymoghaddam et al., 2024; Zhang et al.,",
    "full_text_length": 153423,
    "chunk_length": 1289
  },
  {
    "chunk_id": 1885,
    "paper_filename": "mahdi_2024_ask_in_any_modality_a_comprehensive_survey_on_multimodal_RAG.pdf",
    "paper_title": "Mahdi 2024 Ask In Any Modality A Comprehensive Survey On Multimodal Rag",
    "chunk_index": 16,
    "total_chunks": 146,
    "text_content": "specific variant of FID. System efficiency is measured through FLOPs, execution time, response time, and retrieval time per query (Nguyen et al., 2024; Strand et al., 2024; Dang, 2024; Zhou, 2024). Domain-specific metrics include geodesic distance for geographical accu- racy (Zhou et al., 2024e), and Clinical Relevance for medical applications (Lahiri and Hu, 2024). 4 Key Innovations and Methodologies 4.1 Retrieval Strategy 4.1.1 Efficient Search and Similarity Retrieval Modern multimodal RAG sy",
    "full_text_length": 153423,
    "chunk_length": 1328
  },
  {
    "chunk_id": 1886,
    "paper_filename": "mahdi_2024_ask_in_any_modality_a_comprehensive_survey_on_multimodal_RAG.pdf",
    "paper_title": "Mahdi 2024 Ask In Any Modality A Comprehensive Survey On Multimodal Rag",
    "chunk_index": 17,
    "total_chunks": 146,
    "text_content": "et al., 2024a), Zhai et al. (2023), Deeperimpact (Basnet et al., 2024), RetrievalAttention (Liu et al., 2024a), FactMM-RAG (Sun et al., 2024b) Multimodal EncodersCLIP (Radford et al., 2021), BLIP (Li et al., 2022a), MARVEL (Zhou et al., 2024c), ALIGN (Jia et al., 2021), FLA V A (Singh et al., 2022), UniVL-DR (Liu et al., 2023b), UniIR (Wei et al., 2024a), GME (Zhang et al., 2024i), VISTA (Zhou et al., 2024b), ColPali (Faysse et al., 2024), InternVideo (Wang et al., 2022), Ovis (Lu et al., 2024) ",
    "full_text_length": 153423,
    "chunk_length": 1215
  },
  {
    "chunk_id": 1887,
    "paper_filename": "mahdi_2024_ask_in_any_modality_a_comprehensive_survey_on_multimodal_RAG.pdf",
    "paper_title": "Mahdi 2024 Ask In Any Modality A Comprehensive Survey On Multimodal Rag",
    "chunk_index": 18,
    "total_chunks": 146,
    "text_content": "al., 2024), M2-RAG (Ma et al., 2024c) Vision-CentricVQA4CIR (Feng et al., 2023), Unifashion (Zhao et al., 2024), Jang et al. (Jang et al., 2024), Pic2word (Saito et al., 2023), eClip (Kumar and Marttinen, 2024), RAMM (Yuan et al., 2023), Joshi et al. (Joshi et al., 2024), VISA (Ma et al., 2024b), ImgRet (Shohan et al., 2024), EchoSight (Yan and Xie, 2024), Xue et al. (Xue et al., 2024) Video-CentriciRAG (Arefeen et al., 2024), VideoRAG (Ren et al., 2025), VideoRAG (Jeong et al., 2025), T-Mass (W",
    "full_text_length": 153423,
    "chunk_length": 1186
  },
  {
    "chunk_id": 1888,
    "paper_filename": "mahdi_2024_ask_in_any_modality_a_comprehensive_survey_on_multimodal_RAG.pdf",
    "paper_title": "Mahdi 2024 Ask In Any Modality A Comprehensive Survey On Multimodal Rag",
    "chunk_index": 19,
    "total_chunks": 146,
    "text_content": "Do- cLLM (Wang et al., 2024a), CREAM (Zhang et al., 2024b), mPLUG-DocOwl 1.5 (Hu et al., 2024a), mPLUG-DocOwl 2 (Hu et al., 2024b), VisDom (Suri et al., 2024), DSE (Ma et al., 2024a) Re-ranking Strategies (\u00a74.1.3)Optimized Ex- ample SelectionMSIER (Luo et al., 2024a), Hybrid RAG (Su et al., 2024a), RULE (Xia et al., 2024b), RAMM (Yuan et al., 2023), M2RAAP (Dong et al., 2024b) Relevance Score EvaluationRAG-Check (Mortaheb et al., 2025a,b), UniRaG (Zhi Lim et al., 2024), MR2AG (Zhang et al., 2024",
    "full_text_length": 153423,
    "chunk_length": 1218
  },
  {
    "chunk_id": 1889,
    "paper_filename": "mahdi_2024_ask_in_any_modality_a_comprehensive_survey_on_multimodal_RAG.pdf",
    "paper_title": "Mahdi 2024 Ask In Any Modality A Comprehensive Survey On Multimodal Rag",
    "chunk_index": 20,
    "total_chunks": 146,
    "text_content": "Lim et al. (2024), Sharifymoghaddam et al. (2024), REVEAL (Hu et al., 2023), RAG-Driver (Yuan et al., 2024), C3Net (Zhang et al., 2024c), LLM-RA (Jian et al., 2024), Riedler and Langer (2024), VISA (Ma et al., 2024b), MA-LMM (He et al., 2024), Xue et al. (2024), RA-BLIP (Ding et al., 2024b), Re-IMAGEN (Chen et al., 2022b), MegaPairs (Zhou et al., 2024a), Wiki-LLaV A (Caffagni et al., 2024), VISRAG (Yu et al., 2024) Attention-Based Mechanisms (\u00a74.2.2)RAMM (Yuan et al., 2023), EMERGE (Zhu et al., ",
    "full_text_length": 153423,
    "chunk_length": 1237
  },
  {
    "chunk_id": 1890,
    "paper_filename": "mahdi_2024_ask_in_any_modality_a_comprehensive_survey_on_multimodal_RAG.pdf",
    "paper_title": "Mahdi 2024 Ask In Any Modality A Comprehensive Survey On Multimodal Rag",
    "chunk_index": 21,
    "total_chunks": 146,
    "text_content": "et al., 2024b), M3DocRAG (Cho et al., 2024), DQU-CIR (Wen et al., 2024), PDF-MVQA (Ding et al., 2024c), SAM-RAG (Zhai, 2024), UFineBench (Zuo et al., 2024), Li et al. (2022a) Augmentation Techniques (\u00a74.3)Context- Enrichment (\u00a74.3.1)EMERGE (Zhu et al., 2024b), MiRAG (Adjali et al., 2024), Wiki-LLaV A (Caffagni et al., 2024), Video-RAG (Luo et al., 2024b), Img2Loc (Zhou et al., 2024e), Xue et al. (2024) Adaptive and Iterative Retrieval (\u00a74.3.2)SKURG (Yang et al., 2023), IRAMIG (Liu et al., 2024b)",
    "full_text_length": 153423,
    "chunk_length": 1257
  },
  {
    "chunk_id": 1891,
    "paper_filename": "mahdi_2024_ask_in_any_modality_a_comprehensive_survey_on_multimodal_RAG.pdf",
    "paper_title": "Mahdi 2024 Ask In Any Modality A Comprehensive Survey On Multimodal Rag",
    "chunk_index": 22,
    "total_chunks": 146,
    "text_content": "al., 2024), SAM-RAG (Zhai, 2024), LDRE (Yang et al., 2024) Instruction Tuning (\u00a74.4.3)RA-BLIP (Ding et al., 2024b), RAGPT (Lang et al., 2025), mR2AG (Zhang et al., 2024f), RagVL (Chen et al., 2024d), Jang et al. (2024), MMed- RAG (Xia et al., 2024a), MegaPairs (Zhou et al., 2024a), Surf (Sun et al., 2024a), Rule (Xia et al., 2024b) Source Attribution (\u00a74.4.4)MuRAR (Zhu et al., 2025), VISA (Ma et al., 2024b), OMG-QA (Nan et al., 2024) Training Strategies (\u00a74.5)Alignment (\u00a74.5.1)VISRAG (Yu et al.,",
    "full_text_length": 153423,
    "chunk_length": 1295
  },
  {
    "chunk_id": 1892,
    "paper_filename": "mahdi_2024_ask_in_any_modality_a_comprehensive_survey_on_multimodal_RAG.pdf",
    "paper_title": "Mahdi 2024 Ask In Any Modality A Comprehensive Survey On Multimodal Rag",
    "chunk_index": 23,
    "total_chunks": 146,
    "text_content": "2: Taxonomy of recent advances in Multimodal RAG. Refer to Appendix (\u00a7A) for further details. to enable direct cross-modal retrieval. Recent ad- vancements in CLIP-based (Radford et al., 2021) and BLIP-inspired (Li et al., 2022a) approaches have driven the evolution of contrastive learning strategies through novel multimodal retrieval archi- tectures and training methodologies (Zhou et al., 2024c; Wei et al., 2024b; Zhang et al., 2024i). As these multi-encoder models project different modal- iti",
    "full_text_length": 153423,
    "chunk_length": 1332
  },
  {
    "chunk_id": 1893,
    "paper_filename": "mahdi_2024_ask_in_any_modality_a_comprehensive_survey_on_multimodal_RAG.pdf",
    "paper_title": "Mahdi 2024 Ask In Any Modality A Comprehensive Survey On Multimodal Rag",
    "chunk_index": 24,
    "total_chunks": 146,
    "text_content": "vector and a large collec- tion of image\u2013text embeddings. Large-scale imple- mentations leverage distributed MIPS techniques, such as TPU-KNN (Chern et al., 2022), for high- speed retrieval. Additionally, ScaNN (Scalable Nearest Neighbors) (Guo et al., 2020), MAXSIM score (Chan and Ng, 2008; Cho et al., 2024), and approximate KNN methods (Caffagni et al., 2024) have been adopted for efficient similarity computa- tion. Recent advancements in MIPS optimization fo- cus on reducing retrieval latency",
    "full_text_length": 153423,
    "chunk_length": 1376
  },
  {
    "chunk_id": 1894,
    "paper_filename": "mahdi_2024_ask_in_any_modality_a_comprehensive_survey_on_multimodal_RAG.pdf",
    "paper_title": "Mahdi 2024 Ask In Any Modality A Comprehensive Survey On Multimodal Rag",
    "chunk_index": 25,
    "total_chunks": 146,
    "text_content": "This remains foundational in multimodal RAG systems, with both traditional methods like BM25 (Robertson and Zaragoza, 2009), and dense retrievers such as MiniLM (Wang et al., 2020a) and BGE-M3 (Chen et al., 2024b) dominate text-based evidence retrieval (Chen et al., 2022b; Suri et al., 2024; Nan et al., 2024). Novel approaches also address the need for fine-grained semantic matching and domain specificity: For in- stance, ColBERT (Khattab and Zaharia, 2020) and PreFLMR (Lin et al., 2024b) employ",
    "full_text_length": 153423,
    "chunk_length": 1324
  },
  {
    "chunk_id": 1895,
    "paper_filename": "mahdi_2024_ask_in_any_modality_a_comprehensive_survey_on_multimodal_RAG.pdf",
    "paper_title": "Mahdi 2024 Ask In Any Modality A Comprehensive Survey On Multimodal Rag",
    "chunk_index": 26,
    "total_chunks": 146,
    "text_content": "retrieve visually similar content by using reference images as queries. In addition, composed Image Retrieval (CMI) models (Feng et al., 2023; Zhao et al., 2024; Jang et al., 2024) enhance retrieval by integrat- ing multiple image features into a unified query representation. Similarly, Pic2word (Saito et al., 2023) maps visual content to textual descriptions, enabling zero-shot image retrieval. Video-Centric Retrieval These methods extend vision-based techniques by incorporating tempo- ral dyna",
    "full_text_length": 153423,
    "chunk_length": 1452
  },
  {
    "chunk_id": 1896,
    "paper_filename": "mahdi_2024_ask_in_any_modality_a_comprehensive_survey_on_multimodal_RAG.pdf",
    "paper_title": "Mahdi 2024 Ask In Any Modality A Comprehensive Survey On Multimodal Rag",
    "chunk_index": 27,
    "total_chunks": 146,
    "text_content": "oRAG (Ren et al., 2025) -employs dual-channel ar- chitectures with graph-based knowledge grounding for extreme-length videos. For temporal reasoning, CTCH (Shen et al., 2024) uses contrastive trans- former hashing to model long-term dependencies, while RTime (Du et al., 2024) introduces reversed- video hard negatives to rigorously benchmark tem- poral causality. Meanwhile, for complex video un- derstanding, OmAgent (Zhang et al., 2024e) adopts a divide-and-conquer framework, and DRVideo (Ma et a",
    "full_text_length": 153423,
    "chunk_length": 1445
  },
  {
    "chunk_id": 1897,
    "paper_filename": "mahdi_2024_ask_in_any_modality_a_comprehensive_survey_on_multimodal_RAG.pdf",
    "paper_title": "Mahdi 2024 Ask In Any Modality A Comprehensive Survey On Multimodal Rag",
    "chunk_index": 28,
    "total_chunks": 146,
    "text_content": "ColQwen2 (Wang et al., 2024d; Khattab and Zaharia, 2020) and M3DocVQA (Cho et al., 2024) extend this paradigm with dynamic resolution handling and holistic multi-page reasoning. Newer frameworks refine efficiency and layout understanding: ViTLP (Mao et al., 2024) and Do- cLLM (Wang et al., 2024a) pre-train generative models to align spatial layouts with text, while CREAM (Zhang et al., 2024b) employs coarse-to- fine retrieval with multimodal efficient tuning to balance accuracy and computation c",
    "full_text_length": 153423,
    "chunk_length": 1425
  },
  {
    "chunk_id": 1898,
    "paper_filename": "mahdi_2024_ask_in_any_modality_a_comprehensive_survey_on_multimodal_RAG.pdf",
    "paper_title": "Mahdi 2024 Ask In Any Modality A Comprehensive Survey On Multimodal Rag",
    "chunk_index": 29,
    "total_chunks": 146,
    "text_content": "integrating both 6 supervised and unsupervised selection strategies (Luo et al., 2024a; Yuan et al., 2023). For in- stance, (Su et al., 2024a) enhance multimodal in- puts using probabilistic control keywords to im- prove credibility, RULE (Xia et al., 2024b) cal- ibrates retrieved context selection via statistical methods like the Bonferroni correction to mitigate factuality risks, and clustering-based key-frame se- lection ensures diversity in video-based retrieval (Dong et al., 2024b). Relevan",
    "full_text_length": 153423,
    "chunk_length": 1429
  },
  {
    "chunk_id": 1899,
    "paper_filename": "mahdi_2024_ask_in_any_modality_a_comprehensive_survey_on_multimodal_RAG.pdf",
    "paper_title": "Mahdi 2024 Ask In Any Modality A Comprehensive Survey On Multimodal Rag",
    "chunk_index": 30,
    "total_chunks": 146,
    "text_content": "scores for improved ranking (Zhang et al., 2024g; Yan and Xie, 2024; Xu et al., 2024a). LDRE (Yang et al., 2024) employs seman- tic ensemble methods to adaptively weigh multiple caption features, whereas RAGTrans (Cheng et al., 2024) and OMG-QA (Nan et al., 2024) incorporate traditional ranking functions like BM25 (Robertson and Zaragoza, 2009). Filtering Mechasnism This ensures high-quality retrieval by eliminating irrelevant data. Hard nega- tive mining, as used in GME (Zhang et al., 2024i) an",
    "full_text_length": 153423,
    "chunk_length": 1376
  },
  {
    "chunk_id": 1900,
    "paper_filename": "mahdi_2024_ask_in_any_modality_a_comprehensive_survey_on_multimodal_RAG.pdf",
    "paper_title": "Mahdi 2024 Ask In Any Modality A Comprehensive Survey On Multimodal Rag",
    "chunk_index": 31,
    "total_chunks": 146,
    "text_content": "data, thereby enhancing the discriminative capacity and overall robustness of multi-modal retrieval systems. 4.2 Fusion Mechanisms 4.2.1 Score Fusion and Alignment Models in this category utilize distinct strategies to align multimodal representations. Zhi Lim et al.(2024) convert text, tables, and images into a sin- gle textual format using a cross-encoder trained for relevance scoring. Sharifymoghaddam et al. (2024) introduce interleaved image\u2013text pairs that verti- cally merge multiple few-sh",
    "full_text_length": 153423,
    "chunk_length": 1312
  },
  {
    "chunk_id": 1901,
    "paper_filename": "mahdi_2024_ask_in_any_modality_a_comprehensive_survey_on_multimodal_RAG.pdf",
    "paper_title": "Mahdi 2024 Ask In Any Modality A Comprehensive Survey On Multimodal Rag",
    "chunk_index": 32,
    "total_chunks": 146,
    "text_content": "both into a shared embedding space. REVEAL (Hu et al., 2023) injects retrieval scores into attention layers to minimize L2-norm differences between query and knowledge embed- dings, and MA-LMM (He et al., 2024) aligns video- text embeddings via a BLIP-inspired Query Trans- former (Li et al., 2022a). LLM-RA (Jian et al., 2024) concatenates text and visual embeddings into joint queries to reduce retrieval noise, while RA-BLIP (Ding et al., 2024b) employs a 3-layer BERT-based adaptive fusion module",
    "full_text_length": 153423,
    "chunk_length": 1392
  },
  {
    "chunk_id": 1902,
    "paper_filename": "mahdi_2024_ask_in_any_modality_a_comprehensive_survey_on_multimodal_RAG.pdf",
    "paper_title": "Mahdi 2024 Ask In Any Modality A Comprehensive Survey On Multimodal Rag",
    "chunk_index": 33,
    "total_chunks": 146,
    "text_content": "states, prioritizing later tokens for rel- evance, and RAG-Driver (Yuan et al., 2024) aligns visual\u2013language embeddings using visual instruc- tion tuning and an MLP projector. 4.2.2 Attention-Based Mechanisms Attention-based methods dynamically weight cross- modal interactions to support task-specific rea- soning. EMERGE (Zhu et al., 2024b), MORE (Cui et al., 2024), and AlzheimerRAG (Lahiri and Hu, 2024) integrate heterogeneous data via cross- attention. RAMM (Yuan et al., 2023) employs a dual-s",
    "full_text_length": 153423,
    "chunk_length": 1463
  },
  {
    "chunk_id": 1903,
    "paper_filename": "mahdi_2024_ask_in_any_modality_a_comprehensive_survey_on_multimodal_RAG.pdf",
    "paper_title": "Mahdi 2024 Ask In Any Modality A Comprehensive Survey On Multimodal Rag",
    "chunk_index": 34,
    "total_chunks": 146,
    "text_content": "similarity and frame-to-token at- tention, generating refined frame-specific text rep- resentations. Xu et al. (2024a) condition text gener- ation on visual features using gated cross-attention, and Mu-RAG (Chen et al., 2022a) employs inter- mediate cross-attention for open-domain QA. Kim et al. (2024) leverage cross-modal memory retrieval with pre-trained CLIP ViT-L/14 to map video-text pairs into a shared space, enabling dense caption- ing through the attention-based fusion of retrieved memori",
    "full_text_length": 153423,
    "chunk_length": 1431
  },
  {
    "chunk_id": 1904,
    "paper_filename": "mahdi_2024_ask_in_any_modality_a_comprehensive_survey_on_multimodal_RAG.pdf",
    "paper_title": "Mahdi 2024 Ask In Any Modality A Comprehensive Survey On Multimodal Rag",
    "chunk_index": 35,
    "total_chunks": 146,
    "text_content": "et al., 2024) unifies raw data by converting images into text captions for complex queries and overlaying text onto images for sim- ple ones, then fusing embeddings via MLP-learned weights. SAM-RAG (Zhai, 2024) aligns image- text modalities by generating captions for images, converting the multimodal input into unimodal text for subsequent processing. UFineBench (Zuo et al., 2024) utilizes a shared granularity decoder for ultra- fine text\u2013person retrieval. Nguyen et al. (2024) in- troduce Dense2",
    "full_text_length": 153423,
    "chunk_length": 1440
  },
  {
    "chunk_id": 1905,
    "paper_filename": "mahdi_2024_ask_in_any_modality_a_comprehensive_survey_on_multimodal_RAG.pdf",
    "paper_title": "Mahdi 2024 Ask In Any Modality A Comprehensive Survey On Multimodal Rag",
    "chunk_index": 36,
    "total_chunks": 146,
    "text_content": "interpretation, structuring, and integration (Gao et al., 2023). 4.3.1 Context Enrichment This focuses on enhancing the relevance of re- trieved knowledge by refining or expanding re- trieved data. General approaches incorporate addi- tional contextual elements (e.g., text chunks, image tokens, structured data) to provide a richer ground- ing for generation (Caffagni et al., 2024; Xue et al., 2024). EMERGE (Zhu et al., 2024b) enriches con- text by integrating entity relationships and seman- tic ",
    "full_text_length": 153423,
    "chunk_length": 1416
  },
  {
    "chunk_id": 1906,
    "paper_filename": "mahdi_2024_ask_in_any_modality_a_comprehensive_survey_on_multimodal_RAG.pdf",
    "paper_title": "Mahdi 2024 Ask In Any Modality A Comprehensive Survey On Multimodal Rag",
    "chunk_index": 37,
    "total_chunks": 146,
    "text_content": "implausible locations for its predictions. 4.3.2 Adaptive and Iterative Retrieval For more complex queries, dynamic retrieval mech- anisms have proven effective. Adaptive retrieval ap- proaches optimize relevance by adjusting retrieval dynamically. SKURG (Yang et al., 2023) deter- mines the number of retrieval hops based on query complexity. SAM-RAG (Zhai, 2024) and mR2AG (Zhang et al., 2024f) dynamically assess the need for external knowledge and filter irrelevant con- tent using MLLMs to retai",
    "full_text_length": 153423,
    "chunk_length": 1425
  },
  {
    "chunk_id": 1907,
    "paper_filename": "mahdi_2024_ask_in_any_modality_a_comprehensive_survey_on_multimodal_RAG.pdf",
    "paper_title": "Mahdi 2024 Ask In Any Modality A Comprehensive Survey On Multimodal Rag",
    "chunk_index": 38,
    "total_chunks": 146,
    "text_content": "based on retrieved content. Similarly, OMG-QA 8 (Nan et al., 2024) introduces a multi-round re- trieval strategy, where each retrieval step incorpo- rates episodic memory to refine subsequent queries. An evaluation module assesses retrieval effective- ness at each step, guiding the refinement of subse- quent retrieval efforts through feedback. RAGAR (Khaliq et al., 2024) further enhances contextual consistency by iteratively adjusting retrieval based on prior responses and multimodal analysis. 4",
    "full_text_length": 153423,
    "chunk_length": 1471
  },
  {
    "chunk_id": 1908,
    "paper_filename": "mahdi_2024_ask_in_any_modality_a_comprehensive_survey_on_multimodal_RAG.pdf",
    "paper_title": "Mahdi 2024 Ask In Any Modality A Comprehensive Survey On Multimodal Rag",
    "chunk_index": 39,
    "total_chunks": 146,
    "text_content": "2024) refines ICL by retrieving relevant driving experiences from a mem- ory database, ensuring scenario-specific contextual alignment. MSIER (Luo et al., 2024a) further en- hances example selection through a Multimodal Supervised In-Context Examples Retrieval frame- work, leveraging a foundation MLLM scorer to evaluate both textual and visual relevance. Mean- while, Raven (Rao et al., 2024) introduces Fusion- in-Context Learning, a novel approach that en- riches ICL by incorporating a more dive",
    "full_text_length": 153423,
    "chunk_length": 1448
  },
  {
    "chunk_id": 1909,
    "paper_filename": "mahdi_2024_ask_in_any_modality_a_comprehensive_survey_on_multimodal_RAG.pdf",
    "paper_title": "Mahdi 2024 Ask In Any Modality A Comprehensive Survey On Multimodal Rag",
    "chunk_index": 40,
    "total_chunks": 146,
    "text_content": "accuracy, while SAM-RAG (Zhai, 2024) employs reasoning chains alongside multi-stage answer ver- ification to enhance the relevance, utility, and sup-port of generated responses. Meanwhile, LDRE (Yang et al., 2024) leverages LLMs for divergent compositional reasoning, generating refined cap- tions by incorporating dense captions and modifi- cation text. 4.4.3 Instruction Tuning Several works have fine-tuned or instruct-tuned generation components for specific applications. RA-BLIP (Ding et al., 2",
    "full_text_length": 153423,
    "chunk_length": 1422
  },
  {
    "chunk_id": 1910,
    "paper_filename": "mahdi_2024_ask_in_any_modality_a_comprehensive_survey_on_multimodal_RAG.pdf",
    "paper_title": "Mahdi 2024 Ask In Any Modality A Comprehensive Survey On Multimodal Rag",
    "chunk_index": 41,
    "total_chunks": 146,
    "text_content": "of MLLMs, serving them as a re-ranker for filtering the top-k retrieved images. Jang et al. (2024) fo- cus on distinguishing image differences to generate descriptive textual responses. MMed-RAG (Xia et al., 2024a) applies preference fine-tuning to help models balance retrieved knowledge with internal reasoning. To improve generation quality, MegaPairs (Zhou et al., 2024a) and Surf (Sun et al., 2024a) construct multimodal instruction-tuning datasets from prior LLM errors, while Rule (Xia et al.,",
    "full_text_length": 153423,
    "chunk_length": 1415
  },
  {
    "chunk_id": 1911,
    "paper_filename": "mahdi_2024_ask_in_any_modality_a_comprehensive_survey_on_multimodal_RAG.pdf",
    "paper_title": "Mahdi 2024 Ask In Any Modality A Comprehensive Survey On Multimodal Rag",
    "chunk_index": 42,
    "total_chunks": 146,
    "text_content": "evi- dence in retrieved document screenshots. Similarly, OMG-QA (Nan et al., 2024) ensures transparency by prompting the LLM to explicitly cite evidence in generated responses. 9 4.5 Training Strategies Training multimodal RAG models involves a multi- stage process to effectively handle cross-modal in- teractions (Chen et al., 2022a). Pretraining estab- lishes the foundation using large paired datasets to learn cross-modal relationships while fine-tuning adapts models to specific tasks by levera",
    "full_text_length": 153423,
    "chunk_length": 1403
  },
  {
    "chunk_id": 1912,
    "paper_filename": "mahdi_2024_ask_in_any_modality_a_comprehensive_survey_on_multimodal_RAG.pdf",
    "paper_title": "Mahdi 2024 Ask In Any Modality A Comprehensive Survey On Multimodal Rag",
    "chunk_index": 43,
    "total_chunks": 146,
    "text_content": "a cross-entropy objective trains the model for tasks like VQA and image caption- ing. Details of formulas for widely used RAG loss functions can be found in Appendix (\u00a7D). 4.5.1 Alignment Contrastive learning improves representation qual- ity by pulling positive pairs closer and pushing negative pairs apart in the embedding space. A common objective is the InfoNCE loss (van den Oord et al., 2019), which maximizes the mutual in- formation between positive pairs while minimizing similarity to nega",
    "full_text_length": 153423,
    "chunk_length": 1357
  },
  {
    "chunk_id": 1913,
    "paper_filename": "mahdi_2024_ask_in_any_modality_a_comprehensive_survey_on_multimodal_RAG.pdf",
    "paper_title": "Mahdi 2024 Ask In Any Modality A Comprehensive Survey On Multimodal Rag",
    "chunk_index": 44,
    "total_chunks": 146,
    "text_content": "by incorporating hard negative documents, helping the model distin- guish relevant contexts from noise. The eCLIP loss (Kumar and Marttinen, 2024) extends contrastive training by integrating expert- annotated data and an auxiliary Mean Squared Er- ror (MSE) loss to refine embedding quality. Mixup strategies further augment training by generat- ing synthetic positive pairs, improving generaliza- tion in contrastive learning (Kumar and Marttinen, 2024). Dense2Sparse (Nguyen et al., 2024) in- corpo",
    "full_text_length": 153423,
    "chunk_length": 1437
  },
  {
    "chunk_id": 1914,
    "paper_filename": "mahdi_2024_ask_in_any_modality_a_comprehensive_survey_on_multimodal_RAG.pdf",
    "paper_title": "Mahdi 2024 Ask In Any Modality A Comprehensive Survey On Multimodal Rag",
    "chunk_index": 45,
    "total_chunks": 146,
    "text_content": "ous loss functions, such as Binary Cross-Entropy Loss, Minimax Loss, Wasserstein Loss (WGAN), and Hinge Loss. Diffusion Models utilize Mean Squared Error (MSE) Loss for noise prediction, a common approach in Denoising Diffusion Proba- bilistic Models (DDPMs) (Ho et al., 2020). 4.5.3 Robustness and Noise Management Multimodal training faces challenges such as noise and modality-specific biases Buettner and Ko- vashka (2024). Managing noisy retrieval inputs is critical for maintaining model perfor",
    "full_text_length": 153423,
    "chunk_length": 1464
  },
  {
    "chunk_id": 1915,
    "paper_filename": "mahdi_2024_ask_in_any_modality_a_comprehensive_survey_on_multimodal_RAG.pdf",
    "paper_title": "Mahdi 2024 Ask In Any Modality A Comprehensive Survey On Multimodal Rag",
    "chunk_index": 46,
    "total_chunks": 146,
    "text_content": "filter rele- vant knowledge for generation through a denoising- enhanced loss term, eliminating the need for fine- tuning. This approach achieves strong performance compared to baselines while significantly reducing computational overhead by minimizing trainable parameters. RagVL (Chen et al., 2024d) improves robustness through noise-injected training by adding hard neg- ative samples at the data level and applying Gaus- sian noise with loss reweighting at the token level, enhancing the model\u2019s ",
    "full_text_length": 153423,
    "chunk_length": 1397
  },
  {
    "chunk_id": 1916,
    "paper_filename": "mahdi_2024_ask_in_any_modality_a_comprehensive_survey_on_multimodal_RAG.pdf",
    "paper_title": "Mahdi 2024 Ask In Any Modality A Comprehensive Survey On Multimodal Rag",
    "chunk_index": 47,
    "total_chunks": 146,
    "text_content": "et al., 2023; Rao et al., 2024) and text-to-image synthesis (Yasunaga et al., 2023; Chen et al., 2022b) by retrieving rele- vant contextual information. They also improve coherence in visual storytelling and ensure fac- tual alignment in multimodal summarization (Ton- moy et al., 2024). In knowledge-intensive appli- cations, multimodal RAG supports open-domain and knowledge-seeking question answering (Chen et al., 2024d; Ding et al., 2024b; Yuan et al., 2023), video-based QA (Luo et al., 2024b),",
    "full_text_length": 153423,
    "chunk_length": 1434
  },
  {
    "chunk_id": 1917,
    "paper_filename": "mahdi_2024_ask_in_any_modality_a_comprehensive_survey_on_multimodal_RAG.pdf",
    "paper_title": "Mahdi 2024 Ask In Any Modality A Comprehensive Survey On Multimodal Rag",
    "chunk_index": 48,
    "total_chunks": 146,
    "text_content": "enables natural language-driven visual search, document understanding, and multimodal reasoning. The taxonomy of application domains can be seen in Figure 3. The following sections explore domain-specific adaptations of these tech- niques in greater depth. Healthcare and Medicine Multimodal RAG en- hances clinical decision-making through integrated analysis of medical imaging, electronic health records, and biomedical literature. Systems like MMED-RAG (Xia et al., 2024a) address diag- nostic unc",
    "full_text_length": 153423,
    "chunk_length": 1526
  },
  {
    "chunk_id": 1918,
    "paper_filename": "mahdi_2024_ask_in_any_modality_a_comprehensive_survey_on_multimodal_RAG.pdf",
    "paper_title": "Mahdi 2024 Ask In Any Modality A Comprehensive Survey On Multimodal Rag",
    "chunk_index": 49,
    "total_chunks": 146,
    "text_content": "2024a) ad- vances privacy-preserving architectures for feder- ated clinical data integration. FactMM-RAG (Sun et al., 2024b) automates radiology report drafting by retrieving biomarker correlations from medical ontologies, exemplifying the paradigm\u2019s capacity to operationalize expert knowledge at scale. Software Engineering Code generation systems leverage multimodal RAG to synthesize context- aware solutions from technical documentation and version histories. DocPrompting (Zhou et al., 2023) im",
    "full_text_length": 153423,
    "chunk_length": 1586
  },
  {
    "chunk_id": 1919,
    "paper_filename": "mahdi_2024_ask_in_any_modality_a_comprehensive_survey_on_multimodal_RAG.pdf",
    "paper_title": "Mahdi 2024 Ask In Any Modality A Comprehensive Survey On Multimodal Rag",
    "chunk_index": 50,
    "total_chunks": 146,
    "text_content": "by jointly embed- ding garment images and textual descriptors, while Dang (2024) reduces search friction through mul- timodal query expansion. LLM4DESIGN (Chen et al., 2024c) demonstrates architectural design au- tomation by retrieving compliance constraints and environmental impact assessments, underscoring RAG\u2019s adaptability to creative domains. Entertainment and Social Computing Multime- dia analytics benefit from RAG\u2019s capacity to cor- relate heterogeneous signals. SoccerRAG (Strand et al., ",
    "full_text_length": 153423,
    "chunk_length": 1546
  },
  {
    "chunk_id": 1920,
    "paper_filename": "mahdi_2024_ask_in_any_modality_a_comprehensive_survey_on_multimodal_RAG.pdf",
    "paper_title": "Mahdi 2024 Ask In Any Modality A Comprehensive Survey On Multimodal Rag",
    "chunk_index": 51,
    "total_chunks": 146,
    "text_content": "Img2Loc (Zhou et al., 2024e) advance image geolocalization through cross-modal landmark cor- relation. 6 Open Problems and Future Directions Despite rapid advancements in multimodal RAG systems, fundamental challenges remain in achiev- ing robust, efficient, and human-like reasoning across modalities. 6.1 Generalization, Explainability, and Robustness Multimodal RAG systems often struggle with do- main adaptation and exhibit modality biases, fre- quently over-relying on text for both retrieval a",
    "full_text_length": 153423,
    "chunk_length": 1370
  },
  {
    "chunk_id": 1921,
    "paper_filename": "mahdi_2024_ask_in_any_modality_a_comprehensive_survey_on_multimodal_RAG.pdf",
    "paper_title": "Mahdi 2024 Ask In Any Modality A Comprehensive Survey On Multimodal Rag",
    "chunk_index": 52,
    "total_chunks": 146,
    "text_content": "sources may differ in quality compared to those requiring a combination of text and image inputs (Baltrusaitis et al., 2019). They are also vulnera- ble to adversarial perturbations, such as misleading images influencing textual outputs, and their per- formance can degrade when relying on low-quality or outdated sources (Chen et al., 2022b). While the trustworthiness of unimodal RAGs has been stud- ied (Zhou et al., 2024d), enhancing the robustness of multimodal RAGs remains an open challenge an",
    "full_text_length": 153423,
    "chunk_length": 1396
  },
  {
    "chunk_id": 1922,
    "paper_filename": "mahdi_2024_ask_in_any_modality_a_comprehensive_survey_on_multimodal_RAG.pdf",
    "paper_title": "Mahdi 2024 Ask In Any Modality A Comprehensive Survey On Multimodal Rag",
    "chunk_index": 53,
    "total_chunks": 146,
    "text_content": "for entity-aware retrieval, is es- sential. Moreover, despite the potential of knowl-edge graphs to enrich cross-modal reasoning, they remain largely underexplored in multimodal RAGs compared to text-based RAGs (Zhang et al., 2024f; Procko and Ochoa, 2024). Retrieval biases such as position sensitivity (Hu et al., 2024c), redundant retrieval (Nan et al., 2024), and biases propagated from training data or retrieved content (Zhai, 2024), pose significant challenges that require further attention. ",
    "full_text_length": 153423,
    "chunk_length": 1466
  },
  {
    "chunk_id": 1923,
    "paper_filename": "mahdi_2024_ask_in_any_modality_a_comprehensive_survey_on_multimodal_RAG.pdf",
    "paper_title": "Mahdi 2024 Ask In Any Modality A Comprehensive Survey On Multimodal Rag",
    "chunk_index": 54,
    "total_chunks": 146,
    "text_content": "self- guided decision-making to iteratively refine out- puts. Existing feedback mechanisms often fail to accurately determine whether errors stem from re- trieval, generation, or other stages (Dong et al., 2024b). The incorporation of reinforcement learn- ing and end-to-end human-aligned feedback into multimodal RAGs remains largely unexplored but holds significant potential for enhancing these sys- tems. These methods could enable multimodal RAGs to assess whether retrieval is necessary, eval- ",
    "full_text_length": 153423,
    "chunk_length": 1509
  },
  {
    "chunk_id": 1924,
    "paper_filename": "mahdi_2024_ask_in_any_modality_a_comprehensive_survey_on_multimodal_RAG.pdf",
    "paper_title": "Mahdi 2024 Ask In Any Modality A Comprehensive Survey On Multimodal Rag",
    "chunk_index": 55,
    "total_chunks": 146,
    "text_content": "and physics-informed reason- ing. Bridging retrieval-based reasoning with real- world agency brings these systems closer to AGI. 12 6.4 Long-Context Processing, Efficiency, Scalability, and Personalization High computational costs in video frame sampling and memory bottlenecks in processing multi-page documents with images remain key challenges in long-context processing. Fixed extraction rates struggle to capture relevant frames, requiring adap- tive selection based on content complexity and mo",
    "full_text_length": 153423,
    "chunk_length": 1570
  },
  {
    "chunk_id": 1925,
    "paper_filename": "mahdi_2024_ask_in_any_modality_a_comprehensive_survey_on_multimodal_RAG.pdf",
    "paper_title": "Mahdi 2024 Ask In Any Modality A Comprehensive Survey On Multimodal Rag",
    "chunk_index": 56,
    "total_chunks": 146,
    "text_content": "a comprehensive review of mul- timodal Retrieval-Augmented Generation (Multi- modal RAG) literature. Specifically, we explore and categorize key advancements across different aspects of multimodal RAG systems, including re- trieval, multimodal fusion, augmentation, genera- tion, and training strategies. Additionally, we exam- ine the tasks these systems address, their domain- specific applications, and the datasets, benchmarks, and evaluation methods. We also discuss open challenges and limitati",
    "full_text_length": 153423,
    "chunk_length": 1483
  },
  {
    "chunk_id": 1926,
    "paper_filename": "mahdi_2024_ask_in_any_modality_a_comprehensive_survey_on_multimodal_RAG.pdf",
    "paper_title": "Mahdi 2024 Ask In Any Modality A Comprehensive Survey On Multimodal Rag",
    "chunk_index": 57,
    "total_chunks": 146,
    "text_content": "methodologies are necessarily concise. Second, al- though we curate studies from major venues (e.g., ACL, EMNLP, NeurIPS, CVPR, ICLR, ICML,ACM Multimedia) and arXiv, our selection may in- advertently overlook emerging or domain-specific research, with a primary focus on recent advance- ments. Additionally, this work does not include a comparative performance evaluation of the various models, as task definitions, evaluation metrics, and implementation details vary significantly across studies, an",
    "full_text_length": 153423,
    "chunk_length": 1422
  },
  {
    "chunk_id": 1927,
    "paper_filename": "mahdi_2024_ask_in_any_modality_a_comprehensive_survey_on_multimodal_RAG.pdf",
    "paper_title": "Mahdi 2024 Ask In Any Modality A Comprehensive Survey On Multimodal Rag",
    "chunk_index": 58,
    "total_chunks": 146,
    "text_content": "research on multimodal RAG systems, offering insights that we believe will be valuable to re- searchers in this evolving field. All the studies, datasets, and benchmarks analyzed in this work are publicly available, with only a very small number of papers requiring institutional access. Addition- ally, this survey does not involve personal data or user interactions, and we adhere to ethical guide- lines throughout. Since this work is purely a survey of exist- ing literature and does not introduc",
    "full_text_length": 153423,
    "chunk_length": 1440
  },
  {
    "chunk_id": 1928,
    "paper_filename": "mahdi_2024_ask_in_any_modality_a_comprehensive_survey_on_multimodal_RAG.pdf",
    "paper_title": "Mahdi 2024 Ask In Any Modality A Comprehensive Survey On Multimodal Rag",
    "chunk_index": 59,
    "total_chunks": 146,
    "text_content": "of sensitive multimodal data also poses privacy risks, while content gener- ation raises concerns about proper attribution and copyright compliance. Addressing these challenges requires careful dataset curation, bias mitigation strategies, and transparent evaluation of retrieval and generation mechanisms. 13 References Mohammad Mahdi Abootorabi and Ehsaneddin Asgari. 2024. Clasp: Contrastive language-speech pretrain- ing for multilingual multimodal information retrieval. Preprint , arXiv:2412.13",
    "full_text_length": 153423,
    "chunk_length": 1538
  },
  {
    "chunk_id": 1929,
    "paper_filename": "mahdi_2024_ask_in_any_modality_a_comprehensive_survey_on_multimodal_RAG.pdf",
    "paper_title": "Mahdi 2024 Ask In Any Modality A Comprehensive Survey On Multimodal Rag",
    "chunk_index": 60,
    "total_chunks": 146,
    "text_content": "Hasson, Karel Lenc, Arthur Mensch, Katie Millicah, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, An- drew Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, and Karen Simonyan. 2024. Flamingo: a visual language model for few-shot learning. In Proceedings of the 36th International Conference on Neural Information Processing Sy",
    "full_text_length": 153423,
    "chunk_length": 1439
  },
  {
    "chunk_id": 1930,
    "paper_filename": "mahdi_2024_ask_in_any_modality_a_comprehensive_survey_on_multimodal_RAG.pdf",
    "paper_title": "Mahdi 2024 Ask In Any Modality A Comprehensive Survey On Multimodal Rag",
    "chunk_index": 61,
    "total_chunks": 146,
    "text_content": "Dai, Orhan Firat, Melvin John- son, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, Eric Chu, Jonathan H. Clark, Laurent El Shafey, Yanping Huang, Kathy Meier-Hellstern, Gau- rav Mishra, Erica Moreira, Mark Omernick, Kevin Robinson, Sebastian Ruder, Yi Tay, Kefan Xiao, Yuanzhong Xu, Yujing Zhang, Gustavo Hernandez Abrego, Junwhan Ahn, Jacob Austin, Paul Barham, Jan Botha, James Bradbury, Siddhartha Brahma, Kevin Brooks, Michele Catasta, Yong Cheng, ",
    "full_text_length": 153423,
    "chunk_length": 1429
  },
  {
    "chunk_id": 1931,
    "paper_filename": "mahdi_2024_ask_in_any_modality_a_comprehensive_survey_on_multimodal_RAG.pdf",
    "paper_title": "Mahdi 2024 Ask In Any Modality A Comprehensive Survey On Multimodal Rag",
    "chunk_index": 62,
    "total_chunks": 146,
    "text_content": ". Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Mar- garet Mitchell, Dhruv Batra, C Lawrence Zitnick, and Devi Parikh. 2015. Vqa: Visual question answering. Proceedings of the IEEE International Conference on Computer Vision , pages 2425\u20132433. Md Adnan Arefeen, Biplob Debnath, Md Yusuf Sarwar Uddin, and Srimat Chakradhar. 2024. irag: Advanc- ing rag for videos with an incremental approach. In Proceedings of the 33rd ACM International Confer- ence on Information and Knowledge Management , CIKM \u2019",
    "full_text_length": 153423,
    "chunk_length": 1410
  },
  {
    "chunk_id": 1932,
    "paper_filename": "mahdi_2024_ask_in_any_modality_a_comprehensive_survey_on_multimodal_RAG.pdf",
    "paper_title": "Mahdi 2024 Ask In Any Modality A Comprehensive Survey On Multimodal Rag",
    "chunk_index": 63,
    "total_chunks": 146,
    "text_content": "understanding. arXiv preprint arXiv:2301.02560 . Adil Bahaj and Mounir Ghogho. 2024. Asthmabot: Multi-modal, multi-lingual retrieval augmented gen- eration for asthma patient support. arXiv preprint arXiv:2409.15815 . Max Bain, Arsha Nagrani, Gul Varol, and Andrew Zis- serman. 2021. Frozen in time: A joint video and image encoder for end-to-end retrieval. Proceedings of the IEEE/CVF International Conference on Com- puter Vision (ICCV) , pages 1\u201310. Alberto Baldrati, Marco Bertini, and Alberto De",
    "full_text_length": 153423,
    "chunk_length": 1437
  },
  {
    "chunk_id": 1933,
    "paper_filename": "mahdi_2024_ask_in_any_modality_a_comprehensive_survey_on_multimodal_RAG.pdf",
    "paper_title": "Mahdi 2024 Ask In Any Modality A Comprehensive Survey On Multimodal Rag",
    "chunk_index": 64,
    "total_chunks": 146,
    "text_content": "Measures for Machine Transla- tion and/or Summarization , pages 65\u201372, Ann Arbor, 14 Michigan. Association for Computational Linguis- tics. Soyuj Basnet, Jerry Gou, Antonio Mallia, and Torsten Suel. 2024. Deeperimpact: Optimiz- ing sparse learned index structures. Preprint , arXiv:2405.17093. Ali Furkan Biten, Lluis Gomez, Marcal Rusinol, and Dimosthenis Karatzas. 2022. Viquae: A dataset for visual question answering on events. arXiv preprint arXiv:2204.03485 . Miko\u0142aj Bi \u00b4nkowski, Dougal J. Sut",
    "full_text_length": 153423,
    "chunk_length": 1449
  },
  {
    "chunk_id": 1934,
    "paper_filename": "mahdi_2024_ask_in_any_modality_a_comprehensive_survey_on_multimodal_RAG.pdf",
    "paper_title": "Mahdi 2024 Ask In Any Modality A Comprehensive Survey On Multimodal Rag",
    "chunk_index": 65,
    "total_chunks": 146,
    "text_content": "Dario Amodei. 2020. Language models are few-shot learners. In Ad- vances in Neural Information Processing Systems , volume 33, pages 1877\u20131901. Curran Associates, Inc. Kyle Buettner and Adriana Kovashka. 2024. Quantify- ing the gaps between translation and native percep- tion in training for multimodal, multilingual retrieval. InProceedings of the 2024 Conference on Empiri- cal Methods in Natural Language Processing , pages 5863\u20135870, Miami, Florida, USA. Association for Computational Linguistic",
    "full_text_length": 153423,
    "chunk_length": 1446
  },
  {
    "chunk_id": 1935,
    "paper_filename": "mahdi_2024_ask_in_any_modality_a_comprehensive_survey_on_multimodal_RAG.pdf",
    "paper_title": "Mahdi 2024 Ask In Any Modality A Comprehensive Survey On Multimodal Rag",
    "chunk_index": 66,
    "total_chunks": 146,
    "text_content": "Gao, and Yong Jae Lee. 2025. Matryoshka multimodal models. In The Thir- teenth International Conference on Learning Repre- sentations . Jamie Callan, Matthew Hoy, Anagha Kulkarni, et al. 2022. Clueweb22: 10 billion web documents with visual and semantic information. arXiv preprint arXiv:2211.15848 .Yee Seng Chan and Hwee Tou Ng. 2008. Maxsim: A maximum similarity metric for machine translation evaluation. In Proceedings of ACL-08: HLT , pages 55\u201362. Angel X Chang, Thomas Funkhouser, Leonidas Gui",
    "full_text_length": 153423,
    "chunk_length": 1382
  },
  {
    "chunk_id": 1936,
    "paper_filename": "mahdi_2024_ask_in_any_modality_a_comprehensive_survey_on_multimodal_RAG.pdf",
    "paper_title": "Mahdi 2024 Ask In Any Modality A Comprehensive Survey On Multimodal Rag",
    "chunk_index": 67,
    "total_chunks": 146,
    "text_content": "IEEE/CVF Conference on Computer Vi- sion and Pattern Recognition (CVPR) , pages 16495\u2013 16504. David L Chen and William B Dolan. 2011. Collect- ing highly parallel data for paraphrase evaluation. Proceedings of the 49th Annual Meeting of the Asso- ciation for Computational Linguistics: Human Lan- guage Technologies , pages 190\u2013200. Jianlyu Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, and Zheng Liu. 2024a. M3- embedding: Multi-linguality, multi-functionality, multi-granularity text embedd",
    "full_text_length": 153423,
    "chunk_length": 1455
  },
  {
    "chunk_id": 1937,
    "paper_filename": "mahdi_2024_ask_in_any_modality_a_comprehensive_survey_on_multimodal_RAG.pdf",
    "paper_title": "Mahdi 2024 Ask In Any Modality A Comprehensive Survey On Multimodal Rag",
    "chunk_index": 68,
    "total_chunks": 146,
    "text_content": "environmental design. arXiv preprint arXiv:2407.12025 . Wenhu Chen, Hexiang Hu, Xi Chen, Pat Verga, and William Cohen. 2022a. Murag: Multimodal retrieval- augmented generator for open question answering over images and text. In Proceedings of the 2022 Conference on Empirical Methods in Natural Lan- guage Processing , pages 5558\u20135570, Abu Dhabi, United Arab Emirates. Association for Computa- tional Linguistics. Wenhu Chen, Hexiang Hu, Chitwan Saharia, and William W. Cohen. 2022b. Re-imagen: Retri",
    "full_text_length": 153423,
    "chunk_length": 1415
  },
  {
    "chunk_id": 1938,
    "paper_filename": "mahdi_2024_ask_in_any_modality_a_comprehensive_survey_on_multimodal_RAG.pdf",
    "paper_title": "Mahdi 2024 Ask In Any Modality A Comprehensive Survey On Multimodal Rag",
    "chunk_index": 69,
    "total_chunks": 146,
    "text_content": "445\u2013455, New York, NY , USA. Association for Computing Machin- ery. Felix Chern, Blake Hechtman, Andy Davis, Ruiqi Guo, David Majnemer, and Sanjiv Kumar. 2022. Tpu-knn: K nearest neighbor search at peak flop/s. Advances in Neural Information Processing Systems , 35:15489\u2013 15501. Jaemin Cho, Debanjan Mahata, Ozan Irsoy, Yu- jie He, and Mohit Bansal. 2024. M3docrag: Multi-modal retrieval is what you need for multi- page multi-document understanding. Preprint , arXiv:2411.04952. Yunjey Choi, Minje ",
    "full_text_length": 153423,
    "chunk_length": 1434
  },
  {
    "chunk_id": 1939,
    "paper_filename": "mahdi_2024_ask_in_any_modality_a_comprehensive_survey_on_multimodal_RAG.pdf",
    "paper_title": "Mahdi 2024 Ask In Any Modality A Comprehensive Survey On Multimodal Rag",
    "chunk_index": 70,
    "total_chunks": 146,
    "text_content": "structblip: towards general-purpose vision-language models with instruction tuning. In Proceedings of the 37th International Conference on Neural Information Processing Systems , NIPS \u201923, Red Hook, NY , USA. Curran Associates Inc. Quang-Vinh Dang. 2024. Multi-modal retrieval aug- mented generation for product query. Library of Progress-Library Science, Information Technology & Computer , 44(3). Ringki Das and Thoudam Doren Singh. 2023. Mul- timodal sentiment analysis: A survey of methods, trend",
    "full_text_length": 153423,
    "chunk_length": 1422
  },
  {
    "chunk_id": 1940,
    "paper_filename": "mahdi_2024_ask_in_any_modality_a_comprehensive_survey_on_multimodal_RAG.pdf",
    "paper_title": "Mahdi 2024 Ask In Any Modality A Comprehensive Survey On Multimodal Rag",
    "chunk_index": 71,
    "total_chunks": 146,
    "text_content": "models. Preprint , arXiv:2402.10612. Muhe Ding, Yang Ma, Pengda Qin, Jianlong Wu, Yuhong Li, and Liqiang Nie. 2024b. Ra- blip: Multimodal adaptive retrieval-augmented boot- strapping language-image pre-training. Preprint , arXiv:2410.14154. Yihao Ding, Kaixuan Ren, Jiabin Huang, Siwen Luo, and Soyeon Caren Han. 2024c. Pdf-mvqa: A dataset for multimodal information retrieval in pdf-based visual question answering. Preprint , arXiv:2404.12720. Jialin Dong, Bahare Fatemi, Bryan Perozzi, Lin F. Yang",
    "full_text_length": 153423,
    "chunk_length": 1427
  },
  {
    "chunk_id": 1941,
    "paper_filename": "mahdi_2024_ask_in_any_modality_a_comprehensive_survey_on_multimodal_RAG.pdf",
    "paper_title": "Mahdi 2024 Ask In Any Modality A Comprehensive Survey On Multimodal Rag",
    "chunk_index": 72,
    "total_chunks": 146,
    "text_content": "Jin. 2024. Reversed in time: A novel temporal-emphasized benchmark for cross- modal video-text retrieval. In Proceedings of the 32nd ACM International Conference on Multimedia , MM \u201924, page 5260\u20135269, New York, NY , USA. Association for Computing Machinery. Desmond Elliott, Stella Frank, Khalil Sima\u2019an, and Lu- cia Specia. 2016. Multi30k: Multilingual english- german image descriptions. Proceedings of the 5th Workshop on Vision and Language , pages 70\u201374. Angela Fan, Claire Gardent, Chlo\u00e9 Braud",
    "full_text_length": 153423,
    "chunk_length": 1347
  },
  {
    "chunk_id": 1942,
    "paper_filename": "mahdi_2024_ask_in_any_modality_a_comprehensive_survey_on_multimodal_RAG.pdf",
    "paper_title": "Mahdi 2024 Ask In Any Modality A Comprehensive Survey On Multimodal Rag",
    "chunk_index": 73,
    "total_chunks": 146,
    "text_content": "Boosting composed image retrieval with visual ques- tion answering. Preprint , arXiv:2312.12273. Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Qianyu Guo, Meng Wang, and Haofen Wang. 2023. Retrieval- augmented generation for large language models: A survey. ArXiv , abs/2312.10997. 16 Jort F Gemmeke, Daniel PW Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, et al. 2017. Audioset: An ontology and human-labeled dataset for audio events. Proceedings of ",
    "full_text_length": 153423,
    "chunk_length": 1357
  },
  {
    "chunk_id": 1943,
    "paper_filename": "mahdi_2024_ask_in_any_modality_a_comprehensive_survey_on_multimodal_RAG.pdf",
    "paper_title": "Mahdi 2024 Ask In Any Modality A Comprehensive Survey On Multimodal Rag",
    "chunk_index": 74,
    "total_chunks": 146,
    "text_content": "Advances in neural information processing systems , 27. Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. 2017a. Making the V in VQA matter: Elevating the role of image understanding in Visual Question Answering. In Conference on Computer Vision and Pattern Recognition (CVPR) . Yash Goyal, Tushar Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. 2017b. Vqa v2: Visual question answering. Proceedings of the IEEE/CVF Conference on Computer Vision and Pat- tern Reco",
    "full_text_length": 153423,
    "chunk_length": 1409
  },
  {
    "chunk_id": 1944,
    "paper_filename": "mahdi_2024_ask_in_any_modality_a_comprehensive_survey_on_multimodal_RAG.pdf",
    "paper_title": "Mahdi 2024 Ask In Any Modality A Comprehensive Survey On Multimodal Rag",
    "chunk_index": 75,
    "total_chunks": 146,
    "text_content": "Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Al- lonsius, Daniel Song, Danielle Pintz, Danny Livshits, Danny Wyatt, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, and et al. 2024. The llama 3 herd of models. Preprint , arXiv:2407.21783. Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furn",
    "full_text_length": 153423,
    "chunk_length": 1407
  },
  {
    "chunk_id": 1945,
    "paper_filename": "mahdi_2024_ask_in_any_modality_a_comprehensive_survey_on_multimodal_RAG.pdf",
    "paper_title": "Mahdi 2024 Ask In Any Modality A Comprehensive Survey On Multimodal Rag",
    "chunk_index": 76,
    "total_chunks": 146,
    "text_content": "and Tamara L Berg. 2018. Fashion-gen: The generative fashion dataset and chal- lenge. arXiv preprint arXiv:1806.08317 . Bo He, Hengduo Li, Young Kyun Jang, Menglin Jia, Xuefei Cao, Ashish Shah, Abhinav Shrivastava, and Ser-Nam Lim. 2024. Ma-lmm: Memory-augmented large multimodal model for long-term video under- standing. In Proceedings of the IEEE/CVF Confer- ence on Computer Vision and Pattern Recognition , pages 13504\u201313514. Konstantin Hemker, Nikola Simidjievski, and Mateja Jamnik. 2024. HEAL",
    "full_text_length": 153423,
    "chunk_length": 1417
  },
  {
    "chunk_id": 1946,
    "paper_filename": "mahdi_2024_ask_in_any_modality_a_comprehensive_survey_on_multimodal_RAG.pdf",
    "paper_title": "Mahdi 2024 Ask In Any Modality A Comprehensive Survey On Multimodal Rag",
    "chunk_index": 77,
    "total_chunks": 146,
    "text_content": "trained by a two time-scale update rule converge to a local nash equilibrium. Advances in neural informa- tion processing systems , 30. Jonathan Ho, Ajay Jain, and Pieter Abbeel. 2020. De- noising diffusion probabilistic models. Advances in neural information processing systems , 33:6840\u2013 6851. Anwen Hu, Haiyang Xu, Jiabo Ye, Ming Yan, Liang Zhang, Bo Zhang, Ji Zhang, Qin Jin, Fei Huang, and Jingren Zhou. 2024a. mPLUG-DocOwl 1.5: Uni- fied structure learning for OCR-free document under- standing",
    "full_text_length": 153423,
    "chunk_length": 1395
  },
  {
    "chunk_id": 1947,
    "paper_filename": "mahdi_2024_ask_in_any_modality_a_comprehensive_survey_on_multimodal_RAG.pdf",
    "paper_title": "Mahdi 2024 Ask In Any Modality A Comprehensive Survey On Multimodal Rag",
    "chunk_index": 78,
    "total_chunks": 146,
    "text_content": "retrieval- augmented multimodal models. arXiv preprint arXiv:2410.08182 . Ziniu Hu, Ahmet Iscen, Chen Sun, Zirui Wang, Kai- Wei Chang, Yizhou Sun, Cordelia Schmid, David A. Ross, and Alireza Fathi. 2023. Reveal: Retrieval- augmented visual-language pre-training with multi- source multimodal knowledge memory. In Proceed- ings of the IEEE/CVF Conference on Computer Vi- sion and Pattern Recognition (CVPR) , pages 23369\u2013 23379. 17 Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haot",
    "full_text_length": 153423,
    "chunk_length": 1415
  },
  {
    "chunk_id": 1948,
    "paper_filename": "mahdi_2024_ask_in_any_modality_a_comprehensive_survey_on_multimodal_RAG.pdf",
    "paper_title": "Mahdi 2024 Ask In Any Modality A Comprehensive Survey On Multimodal Rag",
    "chunk_index": 79,
    "total_chunks": 146,
    "text_content": "volume 33, pages 590\u2013597. Gautier Izacard, Mathilde Caron, Lucas Hosseini, Se- bastian Riedel, Piotr Bojanowski, Armand Joulin, and Edouard Grave. 2022. Unsupervised dense infor- mation retrieval with contrastive learning. Preprint , arXiv:2112.09118. Young Kyun Jang, Donghyun Kim, Zihang Meng, Dat Huynh, and Ser-Nam Lim. 2024. Visual delta generator with large multi-modal models for semi- supervised composed image retrieval. In Proceed- ings of the IEEE/CVF Conference on Computer Vi- sion and P",
    "full_text_length": 153423,
    "chunk_length": 1368
  },
  {
    "chunk_id": 1949,
    "paper_filename": "mahdi_2024_ask_in_any_modality_a_comprehensive_survey_on_multimodal_RAG.pdf",
    "paper_title": "Mahdi 2024 Ask In Any Modality A Comprehensive Survey On Multimodal Rag",
    "chunk_index": 80,
    "total_chunks": 146,
    "text_content": "pages 4904\u20134916. PMLR. Pu Jian, Donglei Yu, and Jiajun Zhang. 2024. Large language models know what is key visual entity: An LLM-assisted multimodal retrieval for VQA. In Pro- ceedings of the 2024 Conference on Empirical Meth- ods in Natural Language Processing , pages 10939\u2013 10956, Miami, Florida, USA. Association for Com- putational Linguistics. Chaoya Jiang, Haiyang Xu, Mengfan Dong, Jiaxing Chen, Wei Ye, Ming Yan, Qinghao Ye, Ji Zhang, Fei Huang, and Shikun Zhang. 2024. Hallucination augment",
    "full_text_length": 153423,
    "chunk_length": 1320
  },
  {
    "chunk_id": 1950,
    "paper_filename": "mahdi_2024_ask_in_any_modality_a_comprehensive_survey_on_multimodal_RAG.pdf",
    "paper_title": "Mahdi 2024 Ask In Any Modality A Comprehensive Survey On Multimodal Rag",
    "chunk_index": 81,
    "total_chunks": 146,
    "text_content": "Pollard, Nathaniel R Green- baum, Matthew P Lungren, Chih-ying Deng, Yi- fan Peng, et al. 2019. Mimic-cxr-jpg, a large pub- licly available database of labeled chest radiographs. arXiv preprint arXiv:1901.07042 . Alistair EW Johnson, Tom J Pollard, Lu Shen, H Lehman Li-wei, Mengling Feng, et al. 2016. Mimic-iii, a freely accessible critical care database. Scientific Data , 3:160035. Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised",
    "full_text_length": 153423,
    "chunk_length": 1438
  },
  {
    "chunk_id": 1951,
    "paper_filename": "mahdi_2024_ask_in_any_modality_a_comprehensive_survey_on_multimodal_RAG.pdf",
    "paper_title": "Mahdi 2024 Ask In Any Modality A Comprehensive Survey On Multimodal Rag",
    "chunk_index": 82,
    "total_chunks": 146,
    "text_content": "Matei, and Nistor Grozavu. 2024. Cadmr: Cross-attention and disentangled learning for multimodal recom- mender systems. Preprint , arXiv:2412.02295. Mohammed Abdul Khaliq, Paul Yu-Chun Chang, Mingyang Ma, Bernhard Pflugfelder, and Filip Mileti \u00b4c. 2024. RAGAR, your falsehood radar: RAG- augmented reasoning for political fact-checking us- ing multimodal large language models. In Proceed- ings of the Seventh Fact Extraction and VERification Workshop (FEVER) , pages 280\u2013296, Miami, Florida, USA. As",
    "full_text_length": 153423,
    "chunk_length": 1451
  },
  {
    "chunk_id": 1952,
    "paper_filename": "mahdi_2024_ask_in_any_modality_a_comprehensive_survey_on_multimodal_RAG.pdf",
    "paper_title": "Mahdi 2024 Ask In Any Modality A Comprehensive Survey On Multimodal Rag",
    "chunk_index": 83,
    "total_chunks": 146,
    "text_content": "and Gunhee Kim. 2019. Audiocaps: Generating cap- tions for audios in the wild. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Hu- man Language Technologies, Volume 1 (Long and Short Papers) , pages 119\u2013132, Minneapolis, Min- nesota. Association for Computational Linguistics. Minkuk Kim, Hyeon Bae Kim, Jinyoung Moon, Jinwoo Choi, and Seong Tae Kim. 2024. Do you remember? 18 dense video captioning with cross-modal memory re- tr",
    "full_text_length": 153423,
    "chunk_length": 1366
  },
  {
    "chunk_id": 1953,
    "paper_filename": "mahdi_2024_ask_in_any_modality_a_comprehensive_survey_on_multimodal_RAG.pdf",
    "paper_title": "Mahdi 2024 Ask In Any Modality A Comprehensive Survey On Multimodal Rag",
    "chunk_index": 84,
    "total_chunks": 146,
    "text_content": "(ICCV) . Hilde Kuehne, Ali Arslan, and Thomas Serre. 2014. The language of actions: Recovering the syntax and semantics of goal-directed human activities. In Pro- ceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) . Yogesh Kumar and Pekka Marttinen. 2024. Improving medical multi-modal contrastive learning with ex- pert annotations. In Computer Vision \u2013 ECCV 2024: 18th European Conference, Milan, Italy, September 29\u2013October 4, 2024, Proceedings, Part XX , page 468\u20134",
    "full_text_length": 153423,
    "chunk_length": 1463
  },
  {
    "chunk_id": 1954,
    "paper_filename": "mahdi_2024_ask_in_any_modality_a_comprehensive_survey_on_multimodal_RAG.pdf",
    "paper_title": "Mahdi 2024 Ask In Any Modality A Comprehensive Survey On Multimodal Rag",
    "chunk_index": 85,
    "total_chunks": 146,
    "text_content": "dynamic prompt tuning for incomplete multimodal learning. arXiv preprint arXiv:2501.01120 . Myeonghwa Lee, Seonho An, and Min-Soo Kim. 2024. PlanRAG: A plan-then-retrieval augmented genera- tion for generative large language models as decision makers. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech- nologies (Volume 1: Long Papers) , pages 6537\u20136555, Mexico City, Mexico. Association for Computational Lingui",
    "full_text_length": 153423,
    "chunk_length": 1464
  },
  {
    "chunk_id": 1955,
    "paper_filename": "mahdi_2024_ask_in_any_modality_a_comprehensive_survey_on_multimodal_RAG.pdf",
    "paper_title": "Mahdi 2024 Ask In Any Modality A Comprehensive Survey On Multimodal Rag",
    "chunk_index": 86,
    "total_chunks": 146,
    "text_content": "Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yix- iao Ge, and Ying Shan. 2023a. Seed-bench: Bench- marking multimodal llms with generative compre- hension. Preprint , arXiv:2307.16125. Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. 2023b. Blip-2: bootstrapping language-image pre- training with frozen image encoders and large lan- guage models. In Proceedings of the 40th Interna- tional Conference on Machine Learning , ICML\u201923. JMLR.org. Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. 202",
    "full_text_length": 153423,
    "chunk_length": 1381
  },
  {
    "chunk_id": 1956,
    "paper_filename": "mahdi_2024_ask_in_any_modality_a_comprehensive_survey_on_multimodal_RAG.pdf",
    "paper_title": "Mahdi 2024 Ask In Any Modality A Comprehensive Survey On Multimodal Rag",
    "chunk_index": 87,
    "total_chunks": 146,
    "text_content": "Pattern Recognition (CVPR) , pages 1\u201310. Muquan Li, Dongyang Zhang, Qiang Dong, Xiurui Xie, and Ke Qin. 2024a. Adaptive dataset quantization. Preprint , arXiv:2412.16895. Yangning Li, Yinghui Li, Xingyu Wang, Yong Jiang, Zhen Zhang, Xinran Zheng, Hui Wang, Hai-Tao Zheng, Philip S Yu, Fei Huang, et al. 2024b. Bench- marking multimodal retrieval augmented generation with dynamic vqa dataset and self-adaptive planning agent. arXiv preprint arXiv:2411.02937 . Zehan Li, Xin Zhang, Yanzhao Zhang, Ding",
    "full_text_length": 153423,
    "chunk_length": 1434
  },
  {
    "chunk_id": 1957,
    "paper_filename": "mahdi_2024_ask_in_any_modality_a_comprehensive_survey_on_multimodal_RAG.pdf",
    "paper_title": "Mahdi 2024 Ask In Any Modality A Comprehensive Survey On Multimodal Rag",
    "chunk_index": 88,
    "total_chunks": 146,
    "text_content": "Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C Lawrence Zitnick. 2014. Microsoft coco: Common objects in context. European Conference on Computer Vision , pages 740\u2013755. 19 Weizhe Lin, Jingbiao Mei, Jinghong Chen, and Bill Byrne. 2024b. PreFLMR: Scaling up fine-grained late-interaction multi-modal retrievers. In Proceed- ings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Pa- pers) , pages 5294\u20135316, Bangkok, Thailand. Associ- ation for Computationa",
    "full_text_length": 153423,
    "chunk_length": 1411
  },
  {
    "chunk_id": 1958,
    "paper_filename": "mahdi_2024_ask_in_any_modality_a_comprehensive_survey_on_multimodal_RAG.pdf",
    "paper_title": "Mahdi 2024 Ask In Any Modality A Comprehensive Survey On Multimodal Rag",
    "chunk_index": 89,
    "total_chunks": 146,
    "text_content": "image caption- ing via policy gradient optimization of spider. In Proceedings of the IEEE international conference on computer vision , pages 873\u2013881. Xingzu Liu, Mingbang Wang, Songhang Deng, Xinyue Peng, Yanming Liu, Ruilin Nong, David Williams, and Jiyuan Li. 2024b. Iterative retrieval augmen- tation for multi-modal knowledge integration and generation. Yanming Liu, Xinyue Peng, Xuhong Zhang, Weihao Liu, Jianwei Yin, Jiannan Cao, and Tianyu Du. 2024c. RA-ISF: Learning to answer and understand",
    "full_text_length": 153423,
    "chunk_length": 1421
  },
  {
    "chunk_id": 1959,
    "paper_filename": "mahdi_2024_ask_in_any_modality_a_comprehensive_survey_on_multimodal_RAG.pdf",
    "paper_title": "Mahdi 2024 Ask In Any Modality A Comprehensive Survey On Multimodal Rag",
    "chunk_index": 90,
    "total_chunks": 146,
    "text_content": "Ge Yu. 2023b. Universal vision-language dense retrieval: Learning a unified representation space for multi-modal retrieval. In Proceedings of ICLR . Zheyuan Liu, Cristian Rodriguez-Opazo, Damien Teney, and Stephen Gould. 2021. Image retrieval on real-life images with pre-trained vision-and-language mod- els. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) , pages 2125\u2013 2134. Ziwei Liu, Ping Luo, Shi Qiu, Xiaogang Wang, and Xiaoou Tang. 2016. Deepfashion: Powerin",
    "full_text_length": 153423,
    "chunk_length": 1419
  },
  {
    "chunk_id": 1960,
    "paper_filename": "mahdi_2024_ask_in_any_modality_a_comprehensive_survey_on_multimodal_RAG.pdf",
    "paper_title": "Mahdi 2024 Ask In Any Modality A Comprehensive Survey On Multimodal Rag",
    "chunk_index": 91,
    "total_chunks": 146,
    "text_content": "2024. Ovis: Structural embedding alignment for multimodal large language model. Preprint , arXiv:2405.20797. Yang Luo, Zangwei Zheng, Zirui Zhu, and Yang You. 2024a. How does the textual information affect the retrieval of multimodal in-context learning? In Pro- ceedings of the 2024 Conference on Empirical Meth- ods in Natural Language Processing , pages 5321\u2013 5335, Miami, Florida, USA. Association for Compu- tational Linguistics. Yongdong Luo, Xiawu Zheng, Xiao Yang, Guilin Li, Haojia Lin, Jinf",
    "full_text_length": 153423,
    "chunk_length": 1427
  },
  {
    "chunk_id": 1961,
    "paper_filename": "mahdi_2024_ask_in_any_modality_a_comprehensive_survey_on_multimodal_RAG.pdf",
    "paper_title": "Mahdi 2024 Ask In Any Modality A Comprehensive Survey On Multimodal Rag",
    "chunk_index": 92,
    "total_chunks": 146,
    "text_content": "Hu, Heyan Huang, and Xian-Ling Mao. 2024c. Multi-modal re- trieval augmented multi-modal generation: A bench- mark, evaluate metrics and strong baselines. Preprint , arXiv:2411.16365. Ziyu Ma, Chenhui Gou, Hengcan Shi, Bin Sun, Shutao Li, Hamid Rezatofighi, and Jianfei Cai. 2024d. Drvideo: Document retrieval based long video under- standing. Preprint , arXiv:2406.12846. Zhiming Mao, Haoli Bai, Lu Hou, Lifeng Shang, Xin Jiang, Qun Liu, and Kam-Fai Wong. 2024. Visu- ally guided generative text-lay",
    "full_text_length": 153423,
    "chunk_length": 1440
  },
  {
    "chunk_id": 1962,
    "paper_filename": "mahdi_2024_ask_in_any_modality_a_comprehensive_survey_on_multimodal_RAG.pdf",
    "paper_title": "Mahdi 2024 Ask In Any Modality A Comprehensive Survey On Multimodal Rag",
    "chunk_index": 93,
    "total_chunks": 146,
    "text_content": "Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac, Makarand Tapaswi, Ivan Laptev, and Josef Sivic. 2019. Howto100m: Learning a text-video embed- ding by watching hundred million narrated video clips. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) . Matin Mortaheb, Mohammad A. Amir Khojastepour, Srimat T. Chakradhar, and Sennur Ulukus. 2025a. Rag-check: Evaluating multimodal retrieval augmented generation performance. Preprint , arXiv:2501.03995. Matin Mortah",
    "full_text_length": 153423,
    "chunk_length": 1497
  },
  {
    "chunk_id": 1963,
    "paper_filename": "mahdi_2024_ask_in_any_modality_a_comprehensive_survey_on_multimodal_RAG.pdf",
    "paper_title": "Mahdi 2024 Ask In Any Modality A Comprehensive Survey On Multimodal Rag",
    "chunk_index": 94,
    "total_chunks": 146,
    "text_content": "for code-related few-shot learning. In 2023 IEEE/ACM 45th Interna- tional Conference on Software Engineering (ICSE) , pages 2450\u20132462. Humza Naveed, Asad Ullah Khan, Shi Qiu, Muhammad Saqib, Saeed Anwar, Muhammad Usman, Naveed Akhtar, Nick Barnes, and Ajmal Mian. 2024. A comprehensive overview of large language models. Preprint , arXiv:2307.06435. Ahmad M Nazar, Abdulkadir Celik, Mohamed Y Se- lim, Asmaa Abdallah, Daji Qiao, and Ahmed M Eltawil. 2024. Enwar: A rag-empowered multi- modal llm fram",
    "full_text_length": 153423,
    "chunk_length": 1408
  },
  {
    "chunk_id": 1964,
    "paper_filename": "mahdi_2024_ask_in_any_modality_a_comprehensive_survey_on_multimodal_RAG.pdf",
    "paper_title": "Mahdi 2024 Ask In Any Modality A Comprehensive Survey On Multimodal Rag",
    "chunk_index": 95,
    "total_chunks": 146,
    "text_content": "Zhiwu Lu, Xian-Sheng Hua, and Ji-Rong Wen. 2021. Counter- factual vqa: A cause-effect look at language bias. In Proceedings of the IEEE/CVF conference on com- puter vision and pattern recognition , pages 12700\u2013 12710.OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Ale- man, Diogo Almeida, Janko Altenschmidt, Sam Alt- man, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu, Haim- ing Bao, Mohammad Bavarian, Jeff ",
    "full_text_length": 153423,
    "chunk_length": 1386
  },
  {
    "chunk_id": 1965,
    "paper_filename": "mahdi_2024_ask_in_any_modality_a_comprehensive_survey_on_multimodal_RAG.pdf",
    "paper_title": "Mahdi 2024 Ask In Any Modality A Comprehensive Survey On Multimodal Rag",
    "chunk_index": 96,
    "total_chunks": 146,
    "text_content": "2024. Gpt-4 technical report. Preprint , arXiv:2303.08774. Weihua Ou, Yingjie Chen, Linqing Liang, Jianping Gou, Jiahao Xiong, Jiacheng Zhang, Lingge Lai, and Lei Zhang. 2025. Cross-modal retrieval of chest x-ray images and diagnostic reports based on report entity graph and dual attention: Cross-modal retrieval of chest x-ray images and diagnostic reports... Multime- dia Syst. , 31(1). Linke Ouyang, Yuan Qu, Hongbin Zhou, Jiawei Zhu, Rui Zhang, Qunshu Lin, Bin Wang, Zhiyuan Zhao, Man Jiang, Xia",
    "full_text_length": 153423,
    "chunk_length": 1376
  },
  {
    "chunk_id": 1966,
    "paper_filename": "mahdi_2024_ask_in_any_modality_a_comprehensive_survey_on_multimodal_RAG.pdf",
    "paper_title": "Mahdi 2024 Ask In Any Modality A Comprehensive Survey On Multimodal Rag",
    "chunk_index": 97,
    "total_chunks": 146,
    "text_content": "and Ryan Lowe. 2022. Training language models to follow instructions with human feedback. In Advances in Neural Information Processing Systems , volume 35, pages 27730\u201327744. Curran Associates, Inc. Vassil Panayotov, Guoguo Chen, Daniel Povey, and San- jeev Khudanpur. 2015. Librispeech: An asr corpus based on public domain audio books. Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) , pages 5206\u2013 5210. Kishore Papineni, Salim Roukos, Todd Ward",
    "full_text_length": 153423,
    "chunk_length": 1420
  },
  {
    "chunk_id": 1967,
    "paper_filename": "mahdi_2024_ask_in_any_modality_a_comprehensive_survey_on_multimodal_RAG.pdf",
    "paper_title": "Mahdi 2024 Ask In Any Modality A Comprehensive Survey On Multimodal Rag",
    "chunk_index": 98,
    "total_chunks": 146,
    "text_content": "cap- tioning. In Proceedings of the Second Workshop on Shortcomings in Vision and Language , pages 26\u201336, Minneapolis, Minnesota. Association for Computa- tional Linguistics. Shraman Pramanick, Li Jing, Sayan Nag, Jiachen Zhu, Hardik Shah, Yann LeCun, and Rama Chellappa. 2023. V olta: Vision-language transformer with weakly-supervised local-feature alignment. TMLR . Tyler Thomas Procko and Omar Ochoa. 2024. Graph retrieval-augmented generation for large language models: A survey. In 2024 Confere",
    "full_text_length": 153423,
    "chunk_length": 1338
  },
  {
    "chunk_id": 1968,
    "paper_filename": "mahdi_2024_ask_in_any_modality_a_comprehensive_survey_on_multimodal_RAG.pdf",
    "paper_title": "Mahdi 2024 Ask In Any Modality A Comprehensive Survey On Multimodal Rag",
    "chunk_index": 99,
    "total_chunks": 146,
    "text_content": "Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. 2025. Qwen2.5 technical report. Preprint , arXiv:2412.15115. Alec Radford, Jong Wook Kim, Chris ",
    "full_text_length": 153423,
    "chunk_length": 1367
  },
  {
    "chunk_id": 1969,
    "paper_filename": "mahdi_2024_ask_in_any_modality_a_comprehensive_survey_on_multimodal_RAG.pdf",
    "paper_title": "Mahdi 2024 Ask In Any Modality A Comprehensive Survey On Multimodal Rag",
    "chunk_index": 100,
    "total_chunks": 146,
    "text_content": "Herv\u00e9 D\u00e9jean, and St\u00e9phane Clinchant. 2024. Context embeddings for ef- ficient answer generation in rag. Preprint , arXiv:2407.09252. Xubin Ren, Lingrui Xu, Long Xia, Shuaiqiang Wang, Dawei Yin, and Chao Huang. 2025. Videorag: Retrieval-augmented generation with extreme long- context videos. Preprint , arXiv:2502.01549. Monica Riedler and Stefan Langer. 2024. Beyond text: Optimizing rag with multimodal inputs for industrial applications. Preprint , arXiv:2410.21943.Stephen Robertson and Hugo Zar",
    "full_text_length": 153423,
    "chunk_length": 1476
  },
  {
    "chunk_id": 1970,
    "paper_filename": "mahdi_2024_ask_in_any_modality_a_comprehensive_survey_on_multimodal_RAG.pdf",
    "paper_title": "Mahdi 2024 Ask In Any Modality A Comprehensive Survey On Multimodal Rag",
    "chunk_index": 101,
    "total_chunks": 146,
    "text_content": "Chen-Yu Lee, Kate Saenko, and Tomas Pfister. 2023. Pic2word: Mapping pictures to words for zero- shot composed image retrieval. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 19305\u201319314. Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. 2022. Laion-5b: An open large-scale dataset for training next generation image- text models. Advances",
    "full_text_length": 153423,
    "chunk_length": 1472
  },
  {
    "chunk_id": 1971,
    "paper_filename": "mahdi_2024_ask_in_any_modality_a_comprehensive_survey_on_multimodal_RAG.pdf",
    "paper_title": "Mahdi 2024 Ask In Any Modality A Comprehensive Survey On Multimodal Rag",
    "chunk_index": 102,
    "total_chunks": 146,
    "text_content": "Unirag: Universal re- trieval augmentation for multi-modal large language models. ArXiv , abs/2405.10311. Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. 2018. Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic im- age captioning. In Annual Meeting of the Association for Computational Linguistics . Xiaobo Shen, Qianxin Huang, Long Lan, and Yuhui Zheng. 2024. Contrastive transformer cross-modal hashing for video-text retrieval. In Proceedings of the Thi",
    "full_text_length": 153423,
    "chunk_length": 1489
  },
  {
    "chunk_id": 1972,
    "paper_filename": "mahdi_2024_ask_in_any_modality_a_comprehensive_survey_on_multimodal_RAG.pdf",
    "paper_title": "Mahdi 2024 Ask In Any Modality A Comprehensive Survey On Multimodal Rag",
    "chunk_index": 103,
    "total_chunks": 146,
    "text_content": "Shafiq Joty. 2024. XL-HeadTags: Leveraging multimodal retrieval aug- mentation for the multilingual generation of news headlines and tags. In Findings of the Association for Computational Linguistics: ACL 2024 , pages 12991\u201313024, Bangkok, Thailand. Association for Computational Linguistics. Kurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, and Jason Weston. 2021. Retrieval augmentation reduces hallucination in conversation. In Findings of the Association for Computational Linguistics: EMNLP 2",
    "full_text_length": 153423,
    "chunk_length": 1462
  },
  {
    "chunk_id": 1973,
    "paper_filename": "mahdi_2024_ask_in_any_modality_a_comprehensive_survey_on_multimodal_RAG.pdf",
    "paper_title": "Mahdi 2024 Ask In Any Modality A Comprehensive Survey On Multimodal Rag",
    "chunk_index": 104,
    "total_chunks": 146,
    "text_content": "Guillaume Couairon, Wojciech Galuba, Marcus Rohrbach, and Douwe Kiela. 2022. Flava: A founda- tional language and vision alignment model. In Pro- ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 15638\u201315650. Shezheng Song, Xiaopeng Li, Shasha Li, Shan Zhao, Jie Yu, Jun Ma, Xiaoguang Mao, Weimin Zhang, and Meng Wang. 2025. How to bridge the gap between modalities: Survey on multimodal large language model. IEEE Transactions on Knowledge and Data Engineering .",
    "full_text_length": 153423,
    "chunk_length": 1371
  },
  {
    "chunk_id": 1974,
    "paper_filename": "mahdi_2024_ask_in_any_modality_a_comprehensive_survey_on_multimodal_RAG.pdf",
    "paper_title": "Mahdi 2024 Ask In Any Modality A Comprehensive Survey On Multimodal Rag",
    "chunk_index": 105,
    "total_chunks": 146,
    "text_content": "Tien Pei Chou, Va- sudev Lal, and Phillip Howard. 2024b. Sk-vqa: Synthetic knowledge generation at scale for training context-augmented multimodal llms. arXiv preprint arXiv:2406.19593 .Jiashuo Sun, Jihai Zhang, Yucheng Zhou, Zhaochen Su, Xiaoye Qu, and Yu Cheng. 2024a. Surf: Teaching large vision-language models to selectively utilize retrieved information. In Proceedings of the 2024 Conference on Empirical Methods in Natural Lan- guage Processing , pages 7611\u20137629. Liwen Sun, James Zhao, Megan",
    "full_text_length": 153423,
    "chunk_length": 1460
  },
  {
    "chunk_id": 1975,
    "paper_filename": "mahdi_2024_ask_in_any_modality_a_comprehensive_survey_on_multimodal_RAG.pdf",
    "paper_title": "Mahdi 2024 Ask In Any Modality A Comprehensive Survey On Multimodal Rag",
    "chunk_index": 106,
    "total_chunks": 146,
    "text_content": "images. In International Conference on Learning Representations . Cheng Tan, Jingxuan Wei, Linzhuang Sun, Zhangyang Gao, Siyuan Li, Bihui Yu, Ruifeng Guo, and Stan Z. Li. 2024. Retrieval meets reasoning: Even high- school textbook knowledge benefits multimodal rea- soning. Preprint , arXiv:2405.20834. Yansong Tang, Xiaohan Wang, Jingdong Wang, et al. 2019. Coin: A large-scale dataset for comprehen- sive instructional video analysis. Proceedings of the IEEE Conference on Computer Vision and Patte",
    "full_text_length": 153423,
    "chunk_length": 1390
  },
  {
    "chunk_id": 1976,
    "paper_filename": "mahdi_2024_ask_in_any_modality_a_comprehensive_survey_on_multimodal_RAG.pdf",
    "paper_title": "Mahdi 2024 Ask In Any Modality A Comprehensive Survey On Multimodal Rag",
    "chunk_index": 107,
    "total_chunks": 146,
    "text_content": "Jack Krawczyk, Cosmo Du, Ed Chi, Heng- Tze Cheng, Eric Ni, Purvi Shah, Patrick Kane, Betty Chan, Manaal Faruqui, Aliaksei Severyn, Hanzhao Lin, YaGuang Li, Yong Cheng, Abe Ittycheriah, Mahdis Mahdieh, Mia Chen, Pei Sun, Dustin Tran, Sumit Bagri, Balaji Lakshminarayanan, and et al. 2024. Gemini: A family of highly capable multi- modal models. Preprint , arXiv:2312.11805. Mo Tiwari, Ryan Kang, Jaeyong Lee, Donghyun Lee, Christopher J Piech, Sebastian Thrun, Ilan Shomorony, and Martin Jinye Zhang. ",
    "full_text_length": 153423,
    "chunk_length": 1380
  },
  {
    "chunk_id": 1977,
    "paper_filename": "mahdi_2024_ask_in_any_modality_a_comprehensive_survey_on_multimodal_RAG.pdf",
    "paper_title": "Mahdi 2024 Ask In Any Modality A Comprehensive Survey On Multimodal Rag",
    "chunk_index": 108,
    "total_chunks": 146,
    "text_content": "models. arXiv preprint arXiv:2401.01313 . Hugo Touvron, Louis Martin, Kevin Stone, Peter Al- bert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, An- thony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel K",
    "full_text_length": 153423,
    "chunk_length": 1414
  },
  {
    "chunk_id": 1978,
    "paper_filename": "mahdi_2024_ask_in_any_modality_a_comprehensive_survey_on_multimodal_RAG.pdf",
    "paper_title": "Mahdi 2024 Ask In Any Modality A Comprehensive Survey On Multimodal Rag",
    "chunk_index": 109,
    "total_chunks": 146,
    "text_content": "Illia Polosukhin. 2017. Attention is all you need. In Proceedings of the 31st International Conference on Neural Information Processing Sys- tems, NIPS\u201917, page 6000\u20136010, Red Hook, NY , USA. Curran Associates Inc. Ramakrishna Vedantam, C Lawrence Zitnick, and Devi Parikh. 2015. Cider: Consensus-based image de- scription evaluation. In Proceedings of the IEEE conference on computer vision and pattern recogni- tion, pages 4566\u20134575. Dongsheng Wang, Natraj Raman, Mathieu Sibue, Zhiqiang Ma, Petr B",
    "full_text_length": 153423,
    "chunk_length": 1404
  },
  {
    "chunk_id": 1979,
    "paper_filename": "mahdi_2024_ask_in_any_modality_a_comprehensive_survey_on_multimodal_RAG.pdf",
    "paper_title": "Mahdi 2024 Ask In Any Modality A Comprehensive Survey On Multimodal Rag",
    "chunk_index": 110,
    "total_chunks": 146,
    "text_content": "Proceedings of the IEEE/CVF Conference on Com- puter Vision and Pattern Recognition (CVPR) , pages 16551\u201316560.Mengzhao Wang, Xiangyu Ke, Xiaoliang Xu, Lu Chen, Yunjun Gao, Pinpin Huang, and Runkai Zhu. 2024c. Must: An effective and scalable framework for mul- timodal search of target modality. In 2024 IEEE 40th International Conference on Data Engineering (ICDE) , pages 4747\u20134759. IEEE. Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhi- hao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, ",
    "full_text_length": 153423,
    "chunk_length": 1361
  },
  {
    "chunk_id": 1980,
    "paper_filename": "mahdi_2024_ask_in_any_modality_a_comprehensive_survey_on_multimodal_RAG.pdf",
    "paper_title": "Mahdi 2024 Ask In Any Modality A Comprehensive Survey On Multimodal Rag",
    "chunk_index": 111,
    "total_chunks": 146,
    "text_content": "Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, and Ming Zhou. 2020a. Minilm: Deep self- attention distillation for task-agnostic compression of pre-trained transformers. Advances in Neural In- formation Processing Systems , 33:5776\u20135788. Xin Wang, Jiawei Wu, Junkun Chen, et al. 2019. Vatex: A large-scale, high-quality multilingual dataset for video-and-language research. Proceedings of the IEEE International Conference on Computer Vision (ICCV) , pages 1\u201310. Yi Wang, Kunchang Li, Yizhuo Li, Yina",
    "full_text_length": 153423,
    "chunk_length": 1357
  },
  {
    "chunk_id": 1981,
    "paper_filename": "mahdi_2024_ask_in_any_modality_a_comprehensive_survey_on_multimodal_RAG.pdf",
    "paper_title": "Mahdi 2024 Ask In Any Modality A Comprehensive Survey On Multimodal Rag",
    "chunk_index": 112,
    "total_chunks": 146,
    "text_content": "Chen. 2024a. Uniir: Training and benchmarking univer- sal multimodal information retrievers. In European Conference on Computer Vision , pages 387\u2013404. Springer. Cong Wei, Yang Chen, Haonan Chen, Hexiang Hu, Ge Zhang, Jie Fu, Alan Ritter, and Wenhu Chen. 2024b. Uniir: Training and benchmarking univer- sal multimodal information retrievers. In Computer Vision \u2013 ECCV 2024: 18th European Conference, Milan, Italy, September 29\u2013October 4, 2024, Pro- ceedings, Part LXXXVII , page 387\u2013404, Berlin, Hei-",
    "full_text_length": 153423,
    "chunk_length": 1356
  },
  {
    "chunk_id": 1982,
    "paper_filename": "mahdi_2024_ask_in_any_modality_a_comprehensive_survey_on_multimodal_RAG.pdf",
    "paper_title": "Mahdi 2024 Ask In Any Modality A Comprehensive Survey On Multimodal Rag",
    "chunk_index": 113,
    "total_chunks": 146,
    "text_content": "fusion for composed image retrieval. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval , SIGIR 2024, page 229\u2013239. ACM. Thomas Winterbottom, Sarah Xiao, Alistair McLean, and Noura Al Moubayed. 2020. On modality bias in the tvqa dataset. Preprint , arXiv:2012.10210. Chenyun Wu, Yuting Liu, and Gang Hua. 2019. Fashion iq: A new dataset towards retrieving images by natu- ral language feedback. Proceedings of the IEEE/CVF Conference on",
    "full_text_length": 153423,
    "chunk_length": 1366
  },
  {
    "chunk_id": 1983,
    "paper_filename": "mahdi_2024_ask_in_any_modality_a_comprehensive_survey_on_multimodal_RAG.pdf",
    "paper_title": "Mahdi 2024 Ask In Any Modality A Comprehensive Survey On Multimodal Rag",
    "chunk_index": 114,
    "total_chunks": 146,
    "text_content": "International Con- ference on Machine Learning , pages 53366\u201353397. Peng Xia, Kangyu Zhu, Haoran Li, Tianze Wang, Wei- jia Shi, Sheng Wang, Linjun Zhang, James Zou, and Huaxiu Yao. 2024a. Mmed-rag: Versatile multi- modal rag system for medical vision language mod- els.Preprint , arXiv:2410.13085. Peng Xia, Kangyu Zhu, Haoran Li, Hongtu Zhu, Yun Li, Gang Li, Linjun Zhang, and Huaxiu Yao. 2024b. RULE: Reliable multimodal RAG for factuality in medical vision language models. In Proceedings of the 2",
    "full_text_length": 153423,
    "chunk_length": 1305
  },
  {
    "chunk_id": 1984,
    "paper_filename": "mahdi_2024_ask_in_any_modality_a_comprehensive_survey_on_multimodal_RAG.pdf",
    "paper_title": "Mahdi 2024 Ask In Any Modality A Comprehensive Survey On Multimodal Rag",
    "chunk_index": 115,
    "total_chunks": 146,
    "text_content": "Hu, Yaya Shi, Guangwei Xu, Chenliang Li, Qi Qian, Maofei Que, Ji Zhang, Xiao Zeng, and Fei Huang. 2023. Youku- mplug: A 10 million large-scale chinese video- language dataset for pre-training and benchmarks. Preprint , arXiv:2306.04362.Huazhe Xu, Yuan Gao, Fisher Yu, and Trevor Darrell. 2018. Bdd-x: A dataset for explainable driving be- havior. Proceedings of the European Conference on Computer Vision (ECCV) , pages 1\u201310. Jilan Xu, Yifei Huang, Junlin Hou, Guo Chen, Yuejie Zhang, Rui Feng, and W",
    "full_text_length": 153423,
    "chunk_length": 1314
  },
  {
    "chunk_id": 1985,
    "paper_filename": "mahdi_2024_ask_in_any_modality_a_comprehensive_survey_on_multimodal_RAG.pdf",
    "paper_title": "Mahdi 2024 Ask In Any Modality A Comprehensive Survey On Multimodal Rag",
    "chunk_index": 116,
    "total_chunks": 146,
    "text_content": "is inevitable: An innate limitation of large language models. Preprint , arXiv:2401.11817. Junxiao Xue, Quan Deng, Fei Yu, Yanhao Wang, Jun Wang, and Yuehua Li. 2024. Enhanced multi- modal rag-llm for accurate visual question answering. Preprint , arXiv:2412.20927. Shi-Qi Yan, Jia-Chen Gu, Yun Zhu, and Zhen-Hua Ling. 2024. Corrective retrieval augmented generation. Yibin Yan and Weidi Xie. 2024. Echosight: Advanc- ing visual-language models with wiki knowledge. In Findings of the Association for",
    "full_text_length": 153423,
    "chunk_length": 1389
  },
  {
    "chunk_id": 1986,
    "paper_filename": "mahdi_2024_ask_in_any_modality_a_comprehensive_survey_on_multimodal_RAG.pdf",
    "paper_title": "Mahdi 2024 Ask In Any Modality A Comprehensive Survey On Multimodal Rag",
    "chunk_index": 117,
    "total_chunks": 146,
    "text_content": "reasoning and ensemble for zero-shot com- posed image retrieval. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval , SIGIR \u201924, page 80\u201390, New York, NY , USA. Association for Computing Machinery. Barry Menglong Yao, Aditya Shah, Lichao Sun, Jin-Hee Cho, and Lifu Huang. 2023. End-to-end multimodal fact-checking and explanation generation: A chal- lenging dataset and models. In Proceedings of the 46th International ACM SIGIR Confere",
    "full_text_length": 153423,
    "chunk_length": 1411
  },
  {
    "chunk_id": 1987,
    "paper_filename": "mahdi_2024_ask_in_any_modality_a_comprehensive_survey_on_multimodal_RAG.pdf",
    "paper_title": "Mahdi 2024 Ask In Any Modality A Comprehensive Survey On Multimodal Rag",
    "chunk_index": 118,
    "total_chunks": 146,
    "text_content": "pages 10502\u201310511. Peter Young, Alice Lai, Micah Hodosh, and Julia Hock- enmaier. 2014. From image descriptions to visual denotations: New similarity metrics for semantic in- ference over event descriptions. Transactions of the Association for Computational Linguistics , 2:67\u201378. Shi Yu, Chaoyue Tang, Bokai Xu, Junbo Cui, Jun- hao Ran, Yukun Yan, Zhenghao Liu, Shuo Wang, Xu Han, Zhiyuan Liu, and Maosong Sun. 2024. Visrag: Vision-based retrieval-augmented gener- ation on multi-modality documents.",
    "full_text_length": 153423,
    "chunk_length": 1408
  },
  {
    "chunk_id": 1988,
    "paper_filename": "mahdi_2024_ask_in_any_modality_a_comprehensive_survey_on_multimodal_RAG.pdf",
    "paper_title": "Mahdi 2024 Ask In Any Modality A Comprehensive Survey On Multimodal Rag",
    "chunk_index": 119,
    "total_chunks": 146,
    "text_content": "Zheng Yuan, Qiao Jin, Chuanqi Tan, Zhengyun Zhao, Hongyi Yuan, Fei Huang, and Songfang Huang. 2023. Ramm: Retrieval-augmented biomedical visual ques- tion answering with multi-modal pre-training. In Proceedings of the 31st ACM International Confer- ence on Multimedia , pages 547\u2013556. Jiaqi Zhai, Zhaojie Gong, Yueming Wang, Xiao Sun, Zheng Yan, Fu Li, and Xing Liu. 2023. Revisiting neural retrieval on accelerators. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Dis- covery and Data",
    "full_text_length": 153423,
    "chunk_length": 1395
  },
  {
    "chunk_id": 1989,
    "paper_filename": "mahdi_2024_ask_in_any_modality_a_comprehensive_survey_on_multimodal_RAG.pdf",
    "paper_title": "Mahdi 2024 Ask In Any Modality A Comprehensive Survey On Multimodal Rag",
    "chunk_index": 120,
    "total_chunks": 146,
    "text_content": "for maximum inner product search. Proceedings of the AAAI Conference on Artificial Intelligence , 37(4):4875\u20134883.Jinxu Zhang, Yongqi Yu, and Yu Zhang. 2024b. Cream: Coarse-to-fine retrieval and multi-modal efficient tun- ing for document vqa. In Proceedings of the 32nd ACM International Conference on Multimedia , MM \u201924, page 925\u2013934, New York, NY , USA. Association for Computing Machinery. Juntao Zhang, Yuehuai Liu, Yu-Wing Tai, and Chi- Keung Tang. 2024c. C3net: Compound conditioned controlne",
    "full_text_length": 153423,
    "chunk_length": 1373
  },
  {
    "chunk_id": 1990,
    "paper_filename": "mahdi_2024_ask_in_any_modality_a_comprehensive_survey_on_multimodal_RAG.pdf",
    "paper_title": "Mahdi 2024 Ask In Any Modality A Comprehensive Survey On Multimodal Rag",
    "chunk_index": 121,
    "total_chunks": 146,
    "text_content": "understanding with task divide-and-conquer. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing , pages 10031\u201310045, Miami, Florida, USA. Association for Computational Lin- guistics. Tao Zhang, Ziqi Zhang, Zongyang Ma, Yuxin Chen, Zhongang Qi, Chunfen Yuan, Bing Li, Junfu Pu, Yuxuan Zhao, Zehua Xie, Jin Ma, Ying Shan, and Weiming Hu. 2024f. mr2ag: Multimodal retrieval- reflection-augmented generation for knowledge- based vqa. ArXiv , abs/2411.15041. Tao Zhan",
    "full_text_length": 153423,
    "chunk_length": 1395
  },
  {
    "chunk_id": 1991,
    "paper_filename": "mahdi_2024_ask_in_any_modality_a_comprehensive_survey_on_multimodal_RAG.pdf",
    "paper_title": "Mahdi 2024 Ask In Any Modality A Comprehensive Survey On Multimodal Rag",
    "chunk_index": 122,
    "total_chunks": 146,
    "text_content": "Weinberger, and Yoav Artzi. 2020. Bertscore: Evaluating text generation with bert. Preprint , arXiv:1904.09675. Xin Zhang, Yanzhao Zhang, Wen Xie, Mingxin Li, Ziqi Dai, Dingkun Long, Pengjun Xie, Meishan Zhang, Wenjie Li, and Min Zhang. 2024i. Gme: Improving universal multimodal retrieval by multimodal llms. Preprint , arXiv:2412.16855. Zhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao, George Karypis, and Alex Smola. 2023b. Multi- modal chain-of-thought reasoning in language mod- els.arXiv preprint",
    "full_text_length": 153423,
    "chunk_length": 1433
  },
  {
    "chunk_id": 1992,
    "paper_filename": "mahdi_2024_ask_in_any_modality_a_comprehensive_survey_on_multimodal_RAG.pdf",
    "paper_title": "Mahdi 2024 Ask In Any Modality A Comprehensive Survey On Multimodal Rag",
    "chunk_index": 123,
    "total_chunks": 146,
    "text_content": "Proc. VLDB Endow. , 16(5):1100\u20131112. Xiangyu Zhao, Yuehan Zhang, Wenlong Zhang, and Xiao-Ming Wu. 2024. Unifashion: A unified vision- language model for multimodal fashion retrieval and generation. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Process- ing, pages 1490\u20131507, Miami, Florida, USA. Associ- ation for Computational Linguistics. Chaofan Zheng, Xinyu Lyu, Lianli Gao, Bo Dai, and Jingkuan Song. 2023. Prototype-Based Embedding Network for Scene Graph Gener",
    "full_text_length": 153423,
    "chunk_length": 1395
  },
  {
    "chunk_id": 1993,
    "paper_filename": "mahdi_2024_ask_in_any_modality_a_comprehensive_survey_on_multimodal_RAG.pdf",
    "paper_title": "Mahdi 2024 Ask In Any Modality A Comprehensive Survey On Multimodal Rag",
    "chunk_index": 124,
    "total_chunks": 146,
    "text_content": "of the 47th International ACM SIGIR Conference on Research and Devel- opment in Information Retrieval , SIGIR \u201924, page 2579\u20132583, New York, NY , USA. Association for Computing Machinery. Xu Zhong, Jianbin Tang, and Antonio Jimeno Yepes. 2019. Publaynet: largest dataset ever for document layout analysis. In 2019 International conference on document analysis and recognition (ICDAR) , pages 1015\u20131022. IEEE. Junjie Zhou, Zheng Liu, Ze Liu, Shitao Xiao, Yueze Wang, Bo Zhao, Chen Jason Zhang, Defu Li",
    "full_text_length": 153423,
    "chunk_length": 1411
  },
  {
    "chunk_id": 1994,
    "paper_filename": "mahdi_2024_ask_in_any_modality_a_comprehensive_survey_on_multimodal_RAG.pdf",
    "paper_title": "Mahdi 2024 Ask In Any Modality A Comprehensive Survey On Multimodal Rag",
    "chunk_index": 125,
    "total_chunks": 146,
    "text_content": "of procedures from web instructional videos. Proceedings of the AAAI Con- ference on Artificial Intelligence , 32(1):1\u201310. Ren Zhou. 2024. Advanced embedding techniques in multimodal retrieval augmented generation a compre- hensive study on cross modal ai applications. Journal of Computing and Electronic Information Manage- ment , 13(3):16\u201322. Shuyan Zhou, Uri Alon, Frank F. Xu, Zhengbao Jiang, and Graham Neubig. 2023. Docprompting: Gener- ating code by retrieving the docs. In The Eleventh Inter",
    "full_text_length": 153423,
    "chunk_length": 1376
  },
  {
    "chunk_id": 1995,
    "paper_filename": "mahdi_2024_ask_in_any_modality_a_comprehensive_survey_on_multimodal_RAG.pdf",
    "paper_title": "Mahdi 2024 Ask In Any Modality A Comprehensive Survey On Multimodal Rag",
    "chunk_index": 126,
    "total_chunks": 146,
    "text_content": "S. Yu. 2024d. Trustworthiness in retrieval-augmented generation systems: A survey. volume abs/2409.10102. Zhongliang Zhou, Jielu Zhang, Zihan Guan, Mengx- uan Hu, Ni Lao, Lan Mu, Sheng Li, and Gengchen Mai. 2024e. Img2loc: Revisiting image geolocal- ization using multi-modality foundation models and image-based retrieval-augmented generation. In Pro- ceedings of the 47th International ACM SIGIR Con- ference on Research and Development in Information Retrieval , pages 2749\u20132754. Wenhao Zhu, Hongy",
    "full_text_length": 153423,
    "chunk_length": 1412
  },
  {
    "chunk_id": 1996,
    "paper_filename": "mahdi_2024_ask_in_any_modality_a_comprehensive_survey_on_multimodal_RAG.pdf",
    "paper_title": "Mahdi 2024 Ask In Any Modality A Comprehensive Survey On Multimodal Rag",
    "chunk_index": 127,
    "total_chunks": 146,
    "text_content": "generation. InProceedings of the 33rd ACM International Con- ference on Information and Knowledge Management , CIKM \u201924, page 3549\u20133559, New York, NY , USA. Association for Computing Machinery. Yinghao Zhu, Changyu Ren, Shiyun Xie, Shukai Liu, Hangyuan Ji, Zixiang Wang, Tao Sun, Long He, Zhoujun Li, Xi Zhu, and Chengwei Pan. 2024c. Realm: Rag-driven enhancement of multimodal elec- tronic health records analysis via large language mod- els.Preprint , arXiv:2402.07016. 27 Zhengyuan Zhu, Daniel Lee",
    "full_text_length": 153423,
    "chunk_length": 1373
  },
  {
    "chunk_id": 1997,
    "paper_filename": "mahdi_2024_ask_in_any_modality_a_comprehensive_survey_on_multimodal_RAG.pdf",
    "paper_title": "Mahdi 2024 Ask In Any Modality A Comprehensive Survey On Multimodal Rag",
    "chunk_index": 128,
    "total_chunks": 146,
    "text_content": "Conference on Computer Vision and Pattern Recognition , pages 22010\u201322019.A Taxonomy In this section, we provide more details regarding the taxonomy of multimodal RAG systems, pre- viously mentioned in Figure 2. Additionally, we present a classification of multimodal RAG appli- cation domains in Figure 3. Figure 2 provides an overview of recent ad- vances in multimodal retrieval-augmented genera- tion (RAG) systems. The taxonomy is organized into several key categories. \u2022Retrieval strategies cov",
    "full_text_length": 153423,
    "chunk_length": 1516
  },
  {
    "chunk_id": 1998,
    "paper_filename": "mahdi_2024_ask_in_any_modality_a_comprehensive_survey_on_multimodal_RAG.pdf",
    "paper_title": "Mahdi 2024 Ask In Any Modality A Comprehensive Survey On Multimodal Rag",
    "chunk_index": 129,
    "total_chunks": 146,
    "text_content": "as adaptive and iterative retrieval. \u2022Generation methods include in-context learn- ing, reasoning, instruction tuning, and source attribution. \u2022training strategies are characterized by ap- proaches to alignment and robustness. Detailed discussions of these categories are pro- vided in the corresponding sections. Figure 3 presents the taxonomy of application do- mains for multimodal RAG systems. The iden- tified domains include healthcare and medicine , software engineering ,fashion and e-commerc",
    "full_text_length": 153423,
    "chunk_length": 1383
  },
  {
    "chunk_id": 1999,
    "paper_filename": "mahdi_2024_ask_in_any_modality_a_comprehensive_survey_on_multimodal_RAG.pdf",
    "paper_title": "Mahdi 2024 Ask In Any Modality A Comprehensive Survey On Multimodal Rag",
    "chunk_index": 130,
    "total_chunks": 146,
    "text_content": "(Sun et al., 2024b) Software Engineering (\u00a75)DocPrompting (Zhou et al., 2023), RACE (Shi et al., 2022), CEDAR (Nashid et al., 2023), RED- CODER (Parvez et al., 2021), Fashion and E-Commerce (\u00a75) Unifashion (Zhao et al., 2024), Dang (2024), LLM4DESIGN (Chen et al., 2024c), Entertainment and Social Computing (\u00a75) SoccerRAG (Strand et al., 2024), MMRA (Zhong et al., 2024) Emerging Applications (\u00a75)RAG-Driver (Yuan et al., 2024), ENWAR (Nazar et al., 2024), Riedler and Langer (2024), Img2Loc (Zhou e",
    "full_text_length": 153423,
    "chunk_length": 1355
  },
  {
    "chunk_id": 2000,
    "paper_filename": "mahdi_2024_ask_in_any_modality_a_comprehensive_survey_on_multimodal_RAG.pdf",
    "paper_title": "Mahdi 2024 Ask In Any Modality A Comprehensive Survey On Multimodal Rag",
    "chunk_index": 131,
    "total_chunks": 146,
    "text_content": "andText-Focused . This classification facilitates a clearer understand- ing of each dataset or benchmark\u2019s role within a multimodal framework. \u2022Name: The official name of the dataset or benchmarks is provided along with a citation for reference. \u2022Statistics and Description: This column summarizes key details such as dataset size, the nature of the content (e.g., image\u2013text pairs, video captions, QA pairs), and the spe- cific tasks or applications for which the dataset or benchmarks are used. The",
    "full_text_length": 153423,
    "chunk_length": 1347
  },
  {
    "chunk_id": 2001,
    "paper_filename": "mahdi_2024_ask_in_any_modality_a_comprehensive_survey_on_multimodal_RAG.pdf",
    "paper_title": "Mahdi 2024 Ask In Any Modality A Comprehensive Survey On Multimodal Rag",
    "chunk_index": 132,
    "total_chunks": 146,
    "text_content": "additional re- sources for the dataset or benchmark, thereby facilitating further exploration of its properties and applications.C Metrics Accuracy : Accuracy is typically defined as the ratio of correctly predicted instances to the total instances. In retrieval-based tasks, Top-K Accuracy is defined as: Top-K Accuracy (y,\u02c6f) =1 nn\u22121X i=0kX j=1\u22ae(\u02c6fi,j=yi) (3) FID (Fr\u00e9chet inception distance) : FID is a metric used to assess the quality of images created by a generative model. The formula for FID",
    "full_text_length": 153423,
    "chunk_length": 1362
  },
  {
    "chunk_id": 2002,
    "paper_filename": "mahdi_2024_ask_in_any_modality_a_comprehensive_survey_on_multimodal_RAG.pdf",
    "paper_title": "Mahdi 2024 Ask In Any Modality A Comprehensive Survey On Multimodal Rag",
    "chunk_index": 133,
    "total_chunks": 146,
    "text_content": "P gramN\u2208RefCount (gramN) (5) ROUGE-L measures the longest common subse- quence (LCS) between generated and reference text. The formula for ROUGE-L is: ROUGE-L =LCS (X, Y) |Y|(6) 29 BLEU : BLEU is another metric used for assessing text generation. The formula for calculating BLEU is: BLEU (pn,BP) =BP\u00b7exp NX n=1wnlogpn! (7) Here, pnrepresents the precision of n-grams, wn denotes the weight assigned to the n-gram preci- sion, and the Brevity Penalty (BP) is defined as: BP=( 1 length > rl exp\u0000 1\u2212rl ",
    "full_text_length": 153423,
    "chunk_length": 1272
  },
  {
    "chunk_id": 2003,
    "paper_filename": "mahdi_2024_ask_in_any_modality_a_comprehensive_survey_on_multimodal_RAG.pdf",
    "paper_title": "Mahdi 2024 Ask In Any Modality A Comprehensive Survey On Multimodal Rag",
    "chunk_index": 134,
    "total_chunks": 146,
    "text_content": "embeddings (e.g., from BERT) for words ciandrjin the candidate and reference sentences, respectively. CLIPScore : CLIPScore is a metric that evaluates the similarity between the text and an image by using the CLIP model. The formula for calculating CLIPScore is: CLIPScore =cos(t,i) \u2225t\u2225 \u00b7 \u2225i\u2225(10) where t and i are text and image embedding respectively. Mean Reciprocal Rank (MRR) : MRR is a metric used to evaluate the performance of systems that return a list of results, such as search engines or ",
    "full_text_length": 153423,
    "chunk_length": 1265
  },
  {
    "chunk_id": 2004,
    "paper_filename": "mahdi_2024_ask_in_any_modality_a_comprehensive_survey_on_multimodal_RAG.pdf",
    "paper_title": "Mahdi 2024 Ask In Any Modality A Comprehensive Survey On Multimodal Rag",
    "chunk_index": 135,
    "total_chunks": 146,
    "text_content": "learning methods. The formula for InfoNCE loss is: LInfoNCE =\u2212logexp( sim(zi, zj)/\u03c4)PK k=1exp( sim(zi, zk)/\u03c4) (12) where ziandzjare the embeddings of a positive pair and \u03c4is a temperature parameter. GAN (Generative Adversarial Network) : The GAN loss consists of two parts: the discriminator loss and the generator loss. The discriminator loss formula is: LD=\u2212Ex\u223cpdata(x)[logD(x)]\u2212Ez\u223cpz(z)[log(1\u2212D(G(z)))] (13) where xis a real sample from the data distribution. G(z)is the generated sample from the ",
    "full_text_length": 153423,
    "chunk_length": 1442
  },
  {
    "chunk_id": 2005,
    "paper_filename": "mahdi_2024_ask_in_any_modality_a_comprehensive_survey_on_multimodal_RAG.pdf",
    "paper_title": "Mahdi 2024 Ask In Any Modality A Comprehensive Survey On Multimodal Rag",
    "chunk_index": 136,
    "total_chunks": 146,
    "text_content": "network. 30 Table 1: Overview of Popular Datasets in Multimodal RAG Research.CategoryName Statistics and Description Modalities LinkImage-Text GeneralLAION-400M (Schuhmann et al., 2021) 200M image\u2013text pairs; used for pre-training multimodal models. Image, Text LAION-400M Conceptual-Captions (CC) (Sharma et al., 2018) 15M image\u2013caption pairs; multilingual English\u2013German image descriptions. Image, Text Conceptual Captions CIRR (Liu et al., 2021) 36,554 triplets from 21,552 images; focuses on natu",
    "full_text_length": 153423,
    "chunk_length": 1448
  },
  {
    "chunk_id": 2006,
    "paper_filename": "mahdi_2024_ask_in_any_modality_a_comprehensive_survey_on_multimodal_RAG.pdf",
    "paper_title": "Mahdi 2024 Ask In Any Modality A Comprehensive Survey On Multimodal Rag",
    "chunk_index": 137,
    "total_chunks": 146,
    "text_content": "Text LAION-5B COCO-CN (Author and Author, 2018) 20,341 images for cross-lingual tagging and captioning with Chinese sentences. Image, Text COCO-CN CIRCO (Baldrati et al., 2023) 1,020 queries with an average of 4.53 ground truths per query; for composed image retrieval. Image, Text CIRCOVideo-TextBDD-X (Xu et al., 2018) 77 hours of driving videos with expert textual explanations; for explainable driving behavior. Video, Text BDD-X YouCook2 (Zhou et al., 2018) 2,000 cooking videos with aligned des",
    "full_text_length": 153423,
    "chunk_length": 1371
  },
  {
    "chunk_id": 2007,
    "paper_filename": "mahdi_2024_ask_in_any_modality_a_comprehensive_survey_on_multimodal_RAG.pdf",
    "paper_title": "Mahdi 2024 Ask In Any Modality A Comprehensive Survey On Multimodal Rag",
    "chunk_index": 138,
    "total_chunks": 146,
    "text_content": "approximately 40 captions per video. Video, Text MSVD LSMDC (Rohrbach et al., 2015) 118,081 video\u2013text pairs from 202 movies; a movie description dataset. Video, Text LSMDC DiDemo (Anne Hendricks et al., 2017) 10,000 videos with four concatenated captions per video; with temporal localization of events. Video, Text DiDemo Breakfast (Kuehne et al., 2014) 1,712 videos of breakfast preparation; one of the largest fully annotated video datasets. Video, Text Breakfast COIN (Tang et al., 2019) 11,827 ",
    "full_text_length": 153423,
    "chunk_length": 1406
  },
  {
    "chunk_id": 2008,
    "paper_filename": "mahdi_2024_ask_in_any_modality_a_comprehensive_survey_on_multimodal_RAG.pdf",
    "paper_title": "Mahdi 2024 Ask In Any Modality A Comprehensive Survey On Multimodal Rag",
    "chunk_index": 139,
    "total_chunks": 146,
    "text_content": "for online action prediction; egocentric vision dataset. Video, Text EPIC-KITCHENS-100 Ego4D (Grauman et al., 2022) 4.3M video\u2013text pairs for egocentric videos; massive\u2013scale egocentric video dataset. Video, Text Ego4D HowTo100M (Miech et al., 2019) 136M video clips with captions from 1.2M YouTube videos; for learning text\u2013video embeddings. Video, Text HowTo100M CharadesEgo (Sigurdsson et al., 2018) 68,536 activity instances from ego\u2013exo videos; used for evaluation. Video, Text Charades-Ego Acti",
    "full_text_length": 153423,
    "chunk_length": 1484
  },
  {
    "chunk_id": 2009,
    "paper_filename": "mahdi_2024_ask_in_any_modality_a_comprehensive_survey_on_multimodal_RAG.pdf",
    "paper_title": "Mahdi 2024 Ask In Any Modality A Comprehensive Survey On Multimodal Rag",
    "chunk_index": 140,
    "total_chunks": 146,
    "text_content": "video\u2013text pairs (refined to Youku-Refined-1M). Video, Text Youku-mPLUGAudio-TextLibriSpeech (Panayotov et al., 2015) 1,000 hours of read English speech with corresponding text; ASR corpus based on audiobooks. Audio, Text LibriSpeech SpeechBrown (Abootorabi and Asgari, 2024) 55K paired speech-text samples; 15 categories covering diverse topics from religion to fiction. Audio, Text SpeechBrown AudioCap (Kim et al., 2019) 46K audio clips paired with human-written text captions. Audio, Text AudioCa",
    "full_text_length": 153423,
    "chunk_length": 1456
  },
  {
    "chunk_id": 2010,
    "paper_filename": "mahdi_2024_ask_in_any_modality_a_comprehensive_survey_on_multimodal_RAG.pdf",
    "paper_title": "Mahdi 2024 Ask In Any Modality A Comprehensive Survey On Multimodal Rag",
    "chunk_index": 141,
    "total_chunks": 146,
    "text_content": "X-ray PubLayNet (Zhong et al., 2019) 100,000 training samples and 2,160 test samples built from PubLayNet (tailored for the medical domain). Image, Text PubLayNetFashionFashion-IQ (Wu et al., 2019) 77,684 images across three categories; evaluated with Recall@10 and Recall@50. Image, Text Fashion IQ FashionGen (Hadi Kiapour et al., 2018) 260.5K image\u2013text pairs of fashion images and item descriptions. Image, Text Fashion-Gen VITON-HD (Choi et al., 2021) 83K images for virtual try\u2013on; high\u2013resolut",
    "full_text_length": 153423,
    "chunk_length": 1378
  },
  {
    "chunk_id": 2011,
    "paper_filename": "mahdi_2024_ask_in_any_modality_a_comprehensive_survey_on_multimodal_RAG.pdf",
    "paper_title": "Mahdi 2024 Ask In Any Modality A Comprehensive Survey On Multimodal Rag",
    "chunk_index": 142,
    "total_chunks": 146,
    "text_content": "2021) 65M text\u2013based QA pairs; a large\u2013scale dataset. Text PAQ ELI5 (Fan et al., 2019) 270K complex and diverse questions augmented with web pages and images. Text ELI5 ViQuAE (Biten et al., 2022) 11.8M passages from Wikipedia covering 2,397 unique entities; knowledge\u2013intensive QA. Text ViQuAE OK-VQA (Marino et al., 2019) 14K questions requiring external knowledge for VQA. Image, Text OK-VQA WebQA (Li et al., 2022b) 46K queries that require reasoning across text and images. Text, Image WebQA Inf",
    "full_text_length": 153423,
    "chunk_length": 1333
  },
  {
    "chunk_id": 2012,
    "paper_filename": "mahdi_2024_ask_in_any_modality_a_comprehensive_survey_on_multimodal_RAG.pdf",
    "paper_title": "Mahdi 2024 Ask In Any Modality A Comprehensive Survey On Multimodal Rag",
    "chunk_index": 143,
    "total_chunks": 146,
    "text_content": "fine-tuning VQA models. Image, Text VQA v2 A-OKVQA (Schwenk et al., 2022) Benchmark for visual question answering using world knowledge; around 25K questions. Image, Text A-OKVQA XL-HeadTags (Shohan et al., 2024) 415K news headline-article pairs consist of 20 languages across six diverse language families. Text XL-HeadTags SEED-Bench (Li et al., 2023a) 19K multiple\u2013choice questions with accurate human annotations across 12 evaluation dimensions. Text SEED-BenchOtherImageNet (Deng et al., 2009) 1",
    "full_text_length": 153423,
    "chunk_length": 1509
  },
  {
    "chunk_id": 2013,
    "paper_filename": "mahdi_2024_ask_in_any_modality_a_comprehensive_survey_on_multimodal_RAG.pdf",
    "paper_title": "Mahdi 2024 Ask In Any Modality A Comprehensive Survey On Multimodal Rag",
    "chunk_index": 144,
    "total_chunks": 146,
    "text_content": "Multimodal RAG Research.CategoryName Statistics and Description Modalities LinkCross-Modal UnderstandingMRAG-Bench (Hu et al., 2024c) Evaluates visual retrieval, integration, and robustness to irrelevant visual information. Images MRAG-Bench M2RAG (Ma et al., 2024c) Benchmarks multimodal RAG; evaluates retrieval, multi-hop reasoning, and integration. Images + Text M2RAG Dyn-VQA (Li et al., 2024b) Focuses on dynamic retrieval, multi-hop reasoning, and robustness to changing information. Images + ",
    "full_text_length": 153423,
    "chunk_length": 1477
  },
  {
    "chunk_id": 2014,
    "paper_filename": "mahdi_2024_ask_in_any_modality_a_comprehensive_survey_on_multimodal_RAG.pdf",
    "paper_title": "Mahdi 2024 Ask In Any Modality A Comprehensive Survey On Multimodal Rag",
    "chunk_index": 145,
    "total_chunks": 146,
    "text_content": "Images + Text SMMQGText-FocusedTriviaQA (Joshi et al., 2017) Provides 650K question-answer pairs; reading comprehension dataset, adaptable for multimodal RAG. Text TriviaQA Natural Questions (Kwiatkowski et al., 2019) Contains 307,373 training examples; real-world search queries, adaptable with visual contexts. Text Natural Questions 32",
    "full_text_length": 153423,
    "chunk_length": 338
  },
  {
    "chunk_id": 2015,
    "paper_filename": "matthew_2024_state_of_the_art_for_contrast_enhnace_mammography.pdf",
    "paper_title": "Matthew 2024 State Of The Art For Contrast Enhnace Mammography",
    "chunk_index": 0,
    "total_chunks": 53,
    "text_content": "State-of-the-art for contrast-enhanced mammography Matthew F. Covington , MD1,2,\ufffd, Samantha Salmon, MD1, Bradley D. Weaver, PhD3, Laurie L. Fajardo, MD1 1Department of Radiology and Imaging Sciences, University of Utah, Salt Lake City, UT, 84112, United States 2Center for Quantitative Cancer Imaging, Huntsman Cancer Institute, Salt Lake City, UT, 84112, United States 3Spencer Fox Eccles School of Medicine, University of Utah, Salt Lake City, UT, 84112, United States \ufffdCorresponding author: Matthe",
    "full_text_length": 58800,
    "chunk_length": 1532
  },
  {
    "chunk_id": 2016,
    "paper_filename": "matthew_2024_state_of_the_art_for_contrast_enhnace_mammography.pdf",
    "paper_title": "Matthew 2024 State Of The Art For Contrast Enhnace Mammography",
    "chunk_index": 1,
    "total_chunks": 53,
    "text_content": "trials, and meta-analyses published from 2020 to 2023. The intent of this article is to supplement prior comprehen - sive reviews and summarize the current state-of-the-art of CEM. Keywords: contrast-enhanced mammography; CEM; contrast-enhanced spectral mammography; contrast-enhanced digital mammography; CESM; CEDM . Introduction Contrast-enhanced mammography (CEM) is an emerging technology that is expected to expand screening, diagnostic, and procedural capabilities in breast imaging. For examp",
    "full_text_length": 58800,
    "chunk_length": 1458
  },
  {
    "chunk_id": 2017,
    "paper_filename": "matthew_2024_state_of_the_art_for_contrast_enhnace_mammography.pdf",
    "paper_title": "Matthew 2024 State Of The Art For Contrast Enhnace Mammography",
    "chunk_index": 2,
    "total_chunks": 53,
    "text_content": "Like other breast imaging techniques such as 2D mam - mography, tomosynthesis, and molecular breast imaging, CEM exposes individuals to ionizing radiation with theoreti - cal downstream risk of cancer induction. CEM requires the intravenous (IV) injection of iodinated contrast material with potential for contrast reactions including the rare risk of ana- phylaxis. Like MRI, CEM may be limited in detection of ab- normal enhancement in cases of elevated background parenchymal enhancement. CEM-guid",
    "full_text_length": 58800,
    "chunk_length": 1405
  },
  {
    "chunk_id": 2018,
    "paper_filename": "matthew_2024_state_of_the_art_for_contrast_enhnace_mammography.pdf",
    "paper_title": "Matthew 2024 State Of The Art For Contrast Enhnace Mammography",
    "chunk_index": 3,
    "total_chunks": 53,
    "text_content": "the most recently published data.Technical performance of CEM A standard CEM exam provides 8 standard images for inter- pretation, comprised of paired low-energy and post-contrast recombined craniocaudal and mediolateral oblique views of each breast (Figure 1). The low-energy CEM images appear analogous to a 2D mammogram, with a recent retrospective review of 40 cancers reporting potentially improved visuali - zation of these cancers on low-energy CEM images compared with standard 2D mammography",
    "full_text_length": 58800,
    "chunk_length": 1483
  },
  {
    "chunk_id": 2019,
    "paper_filename": "matthew_2024_state_of_the_art_for_contrast_enhnace_mammography.pdf",
    "paper_title": "Matthew 2024 State Of The Art For Contrast Enhnace Mammography",
    "chunk_index": 4,
    "total_chunks": 53,
    "text_content": "study from China evaluating CEM perfor - mance for detection of histologically proven malignancy with delayed-phase imaging performed at 7-9min after injection suggested that the delayed-phase imaging yielded limited di- agnostic improvement.14However, whether delayed-phase imaging could be utilized to improve characterization of cer- tain breast cancers with delayed contrast perfusion or wash - out warrants further investigation. Many manufacturers also allow tomosynthesis to be added to a CEM ",
    "full_text_length": 58800,
    "chunk_length": 1482
  },
  {
    "chunk_id": 2020,
    "paper_filename": "matthew_2024_state_of_the_art_for_contrast_enhnace_mammography.pdf",
    "paper_title": "Matthew 2024 State Of The Art For Contrast Enhnace Mammography",
    "chunk_index": 5,
    "total_chunks": 53,
    "text_content": "an Open Access article distributed under the terms of the Creative Commons Attribution License (https://creativecommons.org/licenses/by/4.0/), which permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited. BJR, 2024, 97, 695\u2013704 https://doi.org/10.1093/bjr/tqae017 Advance access publication: 20 January 2024 Review Downloaded from https://academic.oup.com/bjr/article/97/1156/695/7582327 by guest on 11 August 2025 approach, the breast ",
    "full_text_length": 58800,
    "chunk_length": 1494
  },
  {
    "chunk_id": 2021,
    "paper_filename": "matthew_2024_state_of_the_art_for_contrast_enhnace_mammography.pdf",
    "paper_title": "Matthew 2024 State Of The Art For Contrast Enhnace Mammography",
    "chunk_index": 6,
    "total_chunks": 53,
    "text_content": "mammogram, and post-contrast recombined images require assessment of background paren - chymal enhancement and suspicious enhancing masses, foci, or areas of non-mass enhancement, like breast MRI. A recent publication shows that a standardized lexicon can be imple - mented with similar agreement to other BI-RADS lexicons currently use for other breast imaging modalities.15 More comprehensive descriptions of CEM implementation and technical performance are available in prior reviews.2-4,16-20 CEM",
    "full_text_length": 58800,
    "chunk_length": 1370
  },
  {
    "chunk_id": 2022,
    "paper_filename": "matthew_2024_state_of_the_art_for_contrast_enhnace_mammography.pdf",
    "paper_title": "Matthew 2024 State Of The Art For Contrast Enhnace Mammography",
    "chunk_index": 7,
    "total_chunks": 53,
    "text_content": "comparing radiation exposure from CEM to 2D mammography plus a single tomosynthesis view in 56 patients found a significantly lower average glan- dular dose for CEM.24Other studies have attempted to delin - eate radiation dose differences based on breast density. For example, a study evaluating 173 CEM exams reported a lower radiation exposure from CEM in patients with dense breasts compared to individuals with non-dense breasts.21 This finding is of significant interest given that CEM is in- cr",
    "full_text_length": 58800,
    "chunk_length": 1395
  },
  {
    "chunk_id": 2023,
    "paper_filename": "matthew_2024_state_of_the_art_for_contrast_enhnace_mammography.pdf",
    "paper_title": "Matthew 2024 State Of The Art For Contrast Enhnace Mammography",
    "chunk_index": 8,
    "total_chunks": 53,
    "text_content": "in CEM is CEM-guided biopsy capability.26-30CEM has emerged as an alternative method for visualizing breast lesion neovascularity, previously limited to contrast-enhanced breast MRI.8As studies comparing the two methods have shown non-inferiority of CEM to MRI for diagnostic use, a growing number of groups have investigated Figure 1. Standard contrast-enhanced mammography views are shown with craniocaudal (top row) and mediolateral oblique (bottom row) views of each breast depicted with low-ener",
    "full_text_length": 58800,
    "chunk_length": 1389
  },
  {
    "chunk_id": 2024,
    "paper_filename": "matthew_2024_state_of_the_art_for_contrast_enhnace_mammography.pdf",
    "paper_title": "Matthew 2024 State Of The Art For Contrast Enhnace Mammography",
    "chunk_index": 9,
    "total_chunks": 53,
    "text_content": "rate of 95% and a lower lesion non-visualization rate at time of procedure com- pared to reported values of MRI-guided breast biopsies.26 With this early success, CEM-guided biopsies will likely ex- pand in tandem with growth of CEM for screening and diag- nostic use. The greatest advantage of CEM-guided biopsies arises from the potential increased accessibility for patients.30In a study of 153 enhancing lesions on CEM in 144 patients, only 47 (31%) had a correlate on targeted ultrasound32suppor",
    "full_text_length": 58800,
    "chunk_length": 1352
  },
  {
    "chunk_id": 2025,
    "paper_filename": "matthew_2024_state_of_the_art_for_contrast_enhnace_mammography.pdf",
    "paper_title": "Matthew 2024 State Of The Art For Contrast Enhnace Mammography",
    "chunk_index": 10,
    "total_chunks": 53,
    "text_content": "is expected to be similar in the proce - dural setting as that reported for diagnostic imaging applications.33,34 Additional limitations for CEM-guided biopsies parallel those of diagnostic CEM, namely the risks associated with iodine-based contrast reactions and contrast-induced renal insufficiency, although absolute risk of reaction is low.8,35 Evaluation of the symptomatic breast Several recent studies have evaluated accuracy of CEM for evaluation of clinical breast complaints such as focal b",
    "full_text_length": 58800,
    "chunk_length": 1351
  },
  {
    "chunk_id": 2026,
    "paper_filename": "matthew_2024_state_of_the_art_for_contrast_enhnace_mammography.pdf",
    "paper_title": "Matthew 2024 State Of The Art For Contrast Enhnace Mammography",
    "chunk_index": 11,
    "total_chunks": 53,
    "text_content": "in nature. A prospective study comparing CEM to mammographic and ultrasound evaluation of 166 breast lesions in 130 symp - tomatic patients showed improved sensitivity for cancer de- tection in dense breasts with CEM, with sensitivity of 97%, compared to a sensitivity of 76% for 2D synthetic mammog - raphy. The sensitivity was additionally higher than the reported 83% sensitivity for tomosynthesis and 89% sensitiv - ity for tomosynthesis plus ultrasound.38These studies suggest that CEM could pot",
    "full_text_length": 58800,
    "chunk_length": 1375
  },
  {
    "chunk_id": 2027,
    "paper_filename": "matthew_2024_state_of_the_art_for_contrast_enhnace_mammography.pdf",
    "paper_title": "Matthew 2024 State Of The Art For Contrast Enhnace Mammography",
    "chunk_index": 12,
    "total_chunks": 53,
    "text_content": "yielding malignant results.39A separate study reported that CEM for pre-operative staging caused additional breast imaging in 23% of patients, yielding additional biopsies in 18% of patients with 49% of biopsies demonstrating malignancy.40 This additional CEM imaging altered surgical planning in up to 18% of patients (9% converting from breast conservation therapy to mastectomies) with no conversions to mastecto - mies resulting from false-positive CEM findings.40The most common reason for CEM n",
    "full_text_length": 58800,
    "chunk_length": 1357
  },
  {
    "chunk_id": 2028,
    "paper_filename": "matthew_2024_state_of_the_art_for_contrast_enhnace_mammography.pdf",
    "paper_title": "Matthew 2024 State Of The Art For Contrast Enhnace Mammography",
    "chunk_index": 13,
    "total_chunks": 53,
    "text_content": "patients (7%).43A multi-reader study on pre- operative staging of 78 patients with 100 lesions comparing CEM to diagnostic mammography plus tomosynthesis showed a higher detection rate of additional sites of malig - nancy on CEM, most notably in dense breasts.42A Polish study of 60 biopsy-proven breast cancers demonstrated that both CEM and MRI detected areas of multifocality and multicentricity at a similar rate, resulting in altered surgical planning in 25% of cases compared to evaluation with",
    "full_text_length": 58800,
    "chunk_length": 1286
  },
  {
    "chunk_id": 2029,
    "paper_filename": "matthew_2024_state_of_the_art_for_contrast_enhnace_mammography.pdf",
    "paper_title": "Matthew 2024 State Of The Art For Contrast Enhnace Mammography",
    "chunk_index": 14,
    "total_chunks": 53,
    "text_content": "be malignant, CEM detected only 1/6 additional malignancies compared to 6/6 for MRI.46This suggests that CEM may not be as sensitive as MRI for detection of additional occult lesions on standard of care mammogram and ultrasound. These studies suggest the use of CEM in pre-surgical plan- ning will offer similar benefit and rate of change to the surgi - cal plan compared to MRI and improved evaluation of extent of disease compared to 2D mammography, tomosynthesis, and ultrasound, though MRI may re",
    "full_text_length": 58800,
    "chunk_length": 1340
  },
  {
    "chunk_id": 2030,
    "paper_filename": "matthew_2024_state_of_the_art_for_contrast_enhnace_mammography.pdf",
    "paper_title": "Matthew 2024 State Of The Art For Contrast Enhnace Mammography",
    "chunk_index": 15,
    "total_chunks": 53,
    "text_content": "is additionally being eval- uated as a tool to further characterize breast findings that would otherwise be considered indeterminate.48-51Several groups have investigated whether CEM can further charac - terize calcifications indeterminate on diagnostic mammogra - phy as benign or malignant. Amir et al observed that calcifications with associated enhancement are associated with higher malignancy rates; however, there is limited BJR, 2024, Volume 97, Issue 1156 697 Downloaded from https://academi",
    "full_text_length": 58800,
    "chunk_length": 1407
  },
  {
    "chunk_id": 2031,
    "paper_filename": "matthew_2024_state_of_the_art_for_contrast_enhnace_mammography.pdf",
    "paper_title": "Matthew 2024 State Of The Art For Contrast Enhnace Mammography",
    "chunk_index": 16,
    "total_chunks": 53,
    "text_content": "sites of malignancy not enhancing on CEM, 75% had lesions less than 5mm with Ki- 67 values under 5% suggesting lesions below a certain size threshold may not benefit from CEM characterization.52 Further studies assessing the best uses and limitations of CEM to characterize indeterminate breast calcifications as benign or malignant would be helpful. For individuals with dense breast tissue, CEM has been proposed as a tool to evaluate lesions indeterminate on mam - mography and ultrasound.49An Egy",
    "full_text_length": 58800,
    "chunk_length": 1268
  },
  {
    "chunk_id": 2032,
    "paper_filename": "matthew_2024_state_of_the_art_for_contrast_enhnace_mammography.pdf",
    "paper_title": "Matthew 2024 State Of The Art For Contrast Enhnace Mammography",
    "chunk_index": 17,
    "total_chunks": 53,
    "text_content": "suspicious or indeterminate MRI findings showed that CEM detected a higher fraction of MRI lesions in comparison with ultrasound with 76/109 (70%) lesions identified on CEM and 50/109 (46%) lesions on iden- tified on ultrasound (P D.001).54This suggests that CEM could be an alternative or complementary method to ultra- sound or mammography for sampling MRI-detected lesions. Diagnostic evaluation of screening recalls Several recent studies have evaluated performance of CEM to further characterize",
    "full_text_length": 58800,
    "chunk_length": 1403
  },
  {
    "chunk_id": 2033,
    "paper_filename": "matthew_2024_state_of_the_art_for_contrast_enhnace_mammography.pdf",
    "paper_title": "Matthew 2024 State Of The Art For Contrast Enhnace Mammography",
    "chunk_index": 18,
    "total_chunks": 53,
    "text_content": "- tially lowering screening recall rates.57Thus, CEM for diagnostic evaluation of screening recalls may increase both sensitivity and specificity, and lower biopsy rates, compared to standard of care evaluation. However, whether CEM is su- perior to MRI for this indication is currently unclear. A single-institution study comparing CEM with MRI for lesions rated BI-RADS 3-5 on standard-of-care evaluation with mammography and/or ultrasound showed the best results in diagnostic accuracy for benign ",
    "full_text_length": 58800,
    "chunk_length": 1390
  },
  {
    "chunk_id": 2034,
    "paper_filename": "matthew_2024_state_of_the_art_for_contrast_enhnace_mammography.pdf",
    "paper_title": "Matthew 2024 State Of The Art For Contrast Enhnace Mammography",
    "chunk_index": 19,
    "total_chunks": 53,
    "text_content": "110 participants with 115 breast cancers showed CEM compared favourably to MRI for assessment of residual disease on pathology.60A study evaluating CEM as- sessment of residual disease following neoadjuvant therapy specifically for calcified lesions showed that CEM yielded a sensitivity of 96% for residual disease but demonstrated a low specificity of 14.3% when CEM was used in addition to stan- dard mammography due to the inclusion of calcifications on the low-energy CEM images.61A retrospectiv",
    "full_text_length": 58800,
    "chunk_length": 1398
  },
  {
    "chunk_id": 2035,
    "paper_filename": "matthew_2024_state_of_the_art_for_contrast_enhnace_mammography.pdf",
    "paper_title": "Matthew 2024 State Of The Art For Contrast Enhnace Mammography",
    "chunk_index": 20,
    "total_chunks": 53,
    "text_content": "in both breasts with no additional significant mammographic finding. (C) Post-contrast recombined CEM images demonstrate widespread non-mass enhancement throughout the left breast on the MLO view suggesting multicentric DCIS (white arrows). An oval mass is also seen on the right MLO view which was not seen on standard mammography. (D) T1-fat saturated post-contrast sagittal breast MRI views demonstrate a similar degree of multicentric non-mass enhancement in the left breast (white arrows), and a",
    "full_text_length": 58800,
    "chunk_length": 1409
  },
  {
    "chunk_id": 2036,
    "paper_filename": "matthew_2024_state_of_the_art_for_contrast_enhnace_mammography.pdf",
    "paper_title": "Matthew 2024 State Of The Art For Contrast Enhnace Mammography",
    "chunk_index": 21,
    "total_chunks": 53,
    "text_content": "whereas post-contrast recombined CEM images may underestimate lesion size by approximately 3 mm.62A survey of 18 patients who underwent contrast-enhanced digital breast tomosynthesis and breast MRI showed 77% preferred contrast-enhanced digital breast tomosynthesis.33These stud- ies show potential utility of CEM for neoadjuvant therapy re- sponse assessment, though CEM may have low specificity for calcified lesions, and may slightly over- or under-estimate size of residual malignancy according t",
    "full_text_length": 58800,
    "chunk_length": 1434
  },
  {
    "chunk_id": 2037,
    "paper_filename": "matthew_2024_state_of_the_art_for_contrast_enhnace_mammography.pdf",
    "paper_title": "Matthew 2024 State Of The Art For Contrast Enhnace Mammography",
    "chunk_index": 22,
    "total_chunks": 53,
    "text_content": "and 74%, respectively (P \u0088 .09).35Given that the sensitivity of CEM remains higher than non-contrasted modalities and is near that of MRI in early data, CEM will continue to be investigated and used as an al- ternative to contrast-enhanced breast MRI. Breast cancer screening including supplemental screening of dense breasts There is interest in the use of CEM as a screening modality for individuals with dense breast tissue with intermediate life- time risk of breast cancer.16,24,63,65A recent me",
    "full_text_length": 58800,
    "chunk_length": 1395
  },
  {
    "chunk_id": 2038,
    "paper_filename": "matthew_2024_state_of_the_art_for_contrast_enhnace_mammography.pdf",
    "paper_title": "Matthew 2024 State Of The Art For Contrast Enhnace Mammography",
    "chunk_index": 23,
    "total_chunks": 53,
    "text_content": "3. 53-year-old woman with histologically proven right breast invasive ductal carcinoma undergoing diagnostic imaging evaluation for response to neoadjuvant therapy. (A) Low-energy right breast mediolateral oblique (MLO) CEM image demonstrates a hyperdense spiculated mass in the superior posterior breast (white arrow) compatible with histologically proven right breast invasive ductal carcinoma. (B) Post-contrast recombined MLO view of the right breast shows the spiculated mass intensely enhances ",
    "full_text_length": 58800,
    "chunk_length": 1528
  },
  {
    "chunk_id": 2039,
    "paper_filename": "matthew_2024_state_of_the_art_for_contrast_enhnace_mammography.pdf",
    "paper_title": "Matthew 2024 State Of The Art For Contrast Enhnace Mammography",
    "chunk_index": 24,
    "total_chunks": 53,
    "text_content": "breast ultrasound image following neoadjuvant therapy demonstrates reduced size of the known malignancy with some residual mass-like components (white arrow) suggesting partial treatment response. (G) T1-subtracted post-contrast MRI sagittal view of the right breast prior to neoadjuvant therapy demonstrates an intensely enhancing mass (white arrow) compatible with known malignancy. (H) T1-subtracted post-contrast MRI sagittal view of the right breast following neoadjuvant therapy demonstrates mi",
    "full_text_length": 58800,
    "chunk_length": 1484
  },
  {
    "chunk_id": 2040,
    "paper_filename": "matthew_2024_state_of_the_art_for_contrast_enhnace_mammography.pdf",
    "paper_title": "Matthew 2024 State Of The Art For Contrast Enhnace Mammography",
    "chunk_index": 25,
    "total_chunks": 53,
    "text_content": "for individuals with dense breasts.67Availability of CEM-guided biopsy has been reported as potentially helpful to support the expanded use of CEM for dense breast screening.30 The Contrast Enhanced Mammography Imaging Screening Trial (CMIST) (NCT 05625659) is a multi-centre prospective trial on use of CEM for dense breast screening with anticipated enrolment of 2032 patients that is currently underway.11 Of note, breast cancer screening with CEM has been rec- ommended by the American College of",
    "full_text_length": 58800,
    "chunk_length": 1303
  },
  {
    "chunk_id": 2041,
    "paper_filename": "matthew_2024_state_of_the_art_for_contrast_enhnace_mammography.pdf",
    "paper_title": "Matthew 2024 State Of The Art For Contrast Enhnace Mammography",
    "chunk_index": 26,
    "total_chunks": 53,
    "text_content": "surveillance CEM, 73 patients were recalled yielding 38 cancer diagnoses (28 inva- sive, 9 DCIS), of which 41% were detected only on the post- contrast recombined image.69A separate study comparing imaging surveillance with CEM to 2D mammography dem- onstrated an increased cancer detection rate of 15.4 per 1000 exams compared to 6.2 per 1000 exams on 2D mammogra - phy.70These studies suggest potential clinical benefit of CEM for detection of breast cancer recurrence following defini - tive surge",
    "full_text_length": 58800,
    "chunk_length": 1382
  },
  {
    "chunk_id": 2042,
    "paper_filename": "matthew_2024_state_of_the_art_for_contrast_enhnace_mammography.pdf",
    "paper_title": "Matthew 2024 State Of The Art For Contrast Enhnace Mammography",
    "chunk_index": 27,
    "total_chunks": 53,
    "text_content": "it is feasible to use CEM for diagnostic and screening purposes in individuals with breast implants.72 Background parenchymal enhancement Like contrast-enhanced breast MRI, CEM may show benign background parenchymal enhancement of varying intensity on post-contrast recombined CEM images. A recent study reports that CEM background parenchymal enhancement was negatively associated with age, prior history of breast cancer, breast cancer treatment, and post-menopausal status and positively associate",
    "full_text_length": 58800,
    "chunk_length": 1404
  },
  {
    "chunk_id": 2043,
    "paper_filename": "matthew_2024_state_of_the_art_for_contrast_enhnace_mammography.pdf",
    "paper_title": "Matthew 2024 State Of The Art For Contrast Enhnace Mammography",
    "chunk_index": 28,
    "total_chunks": 53,
    "text_content": "with minimal to mild background paren - chymal enhancement (sensitivity of 91% and specificity of 92% for CEM vs sensitivity of 90% and specificity of 71% for MRI, P\u0088.002).75 Quantitative measurements of lesion-to-background enhance - ment have been reported to help in differentiating benign and malignant lesions.76Several scoring systems have also been pro- posed based on CEM enhancement to improve characterization of CEM-detected lesions as benign or malignant.57,77,78A po- tentially improved ",
    "full_text_length": 58800,
    "chunk_length": 1477
  },
  {
    "chunk_id": 2044,
    "paper_filename": "matthew_2024_state_of_the_art_for_contrast_enhnace_mammography.pdf",
    "paper_title": "Matthew 2024 State Of The Art For Contrast Enhnace Mammography",
    "chunk_index": 29,
    "total_chunks": 53,
    "text_content": "HER2-positive and triple-negative breast cancers,95and predict presence of axil- lary lymph node metastases.96Several of these studies show high accuracy of AI for differentiating benign from malignant breast lesions. For example, an attention-based deep learning model that evaluated 1,239 CEM exams with 805 malignant lesions and 288 benign lesions in a multi-centre study reported a sensitivity of 85% and a specificity of 100% for characterizing benign from malignant breast lesions.97 Conclusion",
    "full_text_length": 58800,
    "chunk_length": 1412
  },
  {
    "chunk_id": 2045,
    "paper_filename": "matthew_2024_state_of_the_art_for_contrast_enhnace_mammography.pdf",
    "paper_title": "Matthew 2024 State Of The Art For Contrast Enhnace Mammography",
    "chunk_index": 30,
    "total_chunks": 53,
    "text_content": "biopsy will inform and likely support an expanded use of CEM for breast cancer screening, diagno - sis, and biopsy. Multiple recent investigations suggest AI and radiomics may aid human interpretation of CEM images, and we anticipate these techniques will be adopted for clinical use pending further advancements and continued research. Much of the current data on CEM accuracy stems from ret- rospective studies in populations of individuals with histolog - ically proven breast cancer. Prospective ",
    "full_text_length": 58800,
    "chunk_length": 1400
  },
  {
    "chunk_id": 2046,
    "paper_filename": "matthew_2024_state_of_the_art_for_contrast_enhnace_mammography.pdf",
    "paper_title": "Matthew 2024 State Of The Art For Contrast Enhnace Mammography",
    "chunk_index": 31,
    "total_chunks": 53,
    "text_content": "spe- cificity compared to previously available data, potentially 700 BJR, 2024, Volume 97, Issue 1156 Downloaded from https://academic.oup.com/bjr/article/97/1156/695/7582327 by guest on 11 August 2025 due to evaluating prospective studies only and excluding studies performed solely on patients with known breast can- cer.98Likewise, the meta-analysis by Potsch et al35mentioned earlier concluded that MRI had a higher sensitivity for breast cancer detection than CEM. This questions what may be a c",
    "full_text_length": 58800,
    "chunk_length": 1406
  },
  {
    "chunk_id": 2047,
    "paper_filename": "matthew_2024_state_of_the_art_for_contrast_enhnace_mammography.pdf",
    "paper_title": "Matthew 2024 State Of The Art For Contrast Enhnace Mammography",
    "chunk_index": 32,
    "total_chunks": 53,
    "text_content": "small and biologically indolent cancers that are less likely to harm patients. Current barriers to clinical implementation include risks of iodinated contrast media, increased radiation exposure, workflow considerations, reimbursement challenges, and a need for additional evidence to support use for various clini- cal indications.100Acknowledging these barriers, CEM remains a breast imaging study with great potential for a wide variety of applications for breast cancer screening, diag- nosis, an",
    "full_text_length": 58800,
    "chunk_length": 1456
  },
  {
    "chunk_id": 2048,
    "paper_filename": "matthew_2024_state_of_the_art_for_contrast_enhnace_mammography.pdf",
    "paper_title": "Matthew 2024 State Of The Art For Contrast Enhnace Mammography",
    "chunk_index": 33,
    "total_chunks": 53,
    "text_content": "Healthcare Americas and Bayer Healthcare. Research support from 5 For the Fight. S.S.: None. B.W.: None. L.L.F.: Consultant for WhiteRabbit.ai; Institutional research support for contrast- enhanced mammography from Fujifilm Healthcare Americas and Bayer Healthcare. References 1.0 Lewin JM, Patel BK, Tanna A. Contrast-enhanced mammography: a scientific review. Journal of Breast Imaging. 2019;2(1):7\u201315. 2.0 Patel BK, Lobbes MBI, Lewin J. Contrast enhanced spectral mam - mography: a review. Semin U",
    "full_text_length": 58800,
    "chunk_length": 1533
  },
  {
    "chunk_id": 2049,
    "paper_filename": "matthew_2024_state_of_the_art_for_contrast_enhnace_mammography.pdf",
    "paper_title": "Matthew 2024 State Of The Art For Contrast Enhnace Mammography",
    "chunk_index": 34,
    "total_chunks": 53,
    "text_content": "K, et al. Diagnostic value of contrast- enhanced spectral mammography for screening breast cancer: sys- tematic review and meta-analysis. Clin Breast Cancer. 2018;18 (5):e985-e95. 7.0 Tagliafico AS, Bignotti B, Rossi F, et al. Diagnostic performance of contrast-enhanced spectral mammography: systematic review and meta-analysis. Breast. 2016;28:13-19. 8.0 Jochelson MS, Lobbes MBI. Contrast-enhanced mammography: state of the art. Radiology. 2021;299(1):36-48. 9.0 Kornecki A. Current status of cont",
    "full_text_length": 58800,
    "chunk_length": 1465
  },
  {
    "chunk_id": 2050,
    "paper_filename": "matthew_2024_state_of_the_art_for_contrast_enhnace_mammography.pdf",
    "paper_title": "Matthew 2024 State Of The Art For Contrast Enhnace Mammography",
    "chunk_index": 35,
    "total_chunks": 53,
    "text_content": "2D mammography. J Breast Imaging. 2022;4 (1):31-38. 14. Xu W, Zheng B, Chen W, et al. Can the delayed phase of quantita - tive contrast-enhanced mammography improve the diagnostic per- formance on breast masses? Quant Imaging Med Surg. 2021;11 (8):3684-3697. 15. Berg WA, Bandos AI, Zuley ML, Waheed UX. Training radiolog - ists to interpret contrast-enhanced mammography: toward a stan- dardized lexicon. J Breast Imaging. 2021;3(2):176-189. 16. Covington MF. Contrast-enhanced mammography implement",
    "full_text_length": 58800,
    "chunk_length": 1409
  },
  {
    "chunk_id": 2051,
    "paper_filename": "matthew_2024_state_of_the_art_for_contrast_enhnace_mammography.pdf",
    "paper_title": "Matthew 2024 State Of The Art For Contrast Enhnace Mammography",
    "chunk_index": 36,
    "total_chunks": 53,
    "text_content": "setting up a new clinical pro- gram. J Breast Imaging. 2021;3(3):369-376. 20. Neeter L, Raat H, Alcantara R, et al. Contrast-enhanced mam - mography: what the radiologist needs to know. BJR Open. 2021;3 (1):20210034.\\klwo! 41!Established and emerging indications for contrast-enhanced mammography (CEM). Established CEM indications\u2022Locoregional pre-surgical staging of breast cancer \u2022Neoadjuvant therapy response assessment \u2022Evaluation of indeterminate findings on other diagnostic breast imaging stu",
    "full_text_length": 58800,
    "chunk_length": 1572
  },
  {
    "chunk_id": 2052,
    "paper_filename": "matthew_2024_state_of_the_art_for_contrast_enhnace_mammography.pdf",
    "paper_title": "Matthew 2024 State Of The Art For Contrast Enhnace Mammography",
    "chunk_index": 37,
    "total_chunks": 53,
    "text_content": "P, et al. Single center evaluation of comparative breast radiation dose of contrast enhanced digital mammography (CEDM), digital mammography (DM) and digital breast tomosynthesis (DBT). Acad Radiol. 2022;29(9):1342-1349. 22. Fallenberg EM, Dromain C, Diekmann F, et al. Contrast-enhanced spectral mammography: does mammography provide additional clinical benefits or can some radiation exposure be avoided? Breast Cancer Res Treat. 2014;146(2):371-381. 23. Gennaro G, Cozzi A, Schiaffino S, Sardanell",
    "full_text_length": 58800,
    "chunk_length": 1502
  },
  {
    "chunk_id": 2053,
    "paper_filename": "matthew_2024_state_of_the_art_for_contrast_enhnace_mammography.pdf",
    "paper_title": "Matthew 2024 State Of The Art For Contrast Enhnace Mammography",
    "chunk_index": 38,
    "total_chunks": 53,
    "text_content": "biopsy: technical feasibility and first out- comes. Eur Radiol. 2023;33(1):417-428. 27. Weaver OO, Yang WT, Scoggins ME, et al. Challenging contrast- enhanced mammography-guided biopsies: practical approach us- ing real-time multimodality imaging and a proposed procedural al- gorithm. AJR Am J Roentgenol. 2023;220(4):512-523. 28. Kornecki A, Bhaduri M, Khan N, et al. Contrast-enhanced mammography-guided breast biopsy: single-center experience. AJR Am J Roentgenol. 2023;220(6):826-827. 29. Kowals",
    "full_text_length": 58800,
    "chunk_length": 1510
  },
  {
    "chunk_id": 2054,
    "paper_filename": "matthew_2024_state_of_the_art_for_contrast_enhnace_mammography.pdf",
    "paper_title": "Matthew 2024 State Of The Art For Contrast Enhnace Mammography",
    "chunk_index": 39,
    "total_chunks": 53,
    "text_content": "33. Savaridas SL, Whelehan P, Warwick VR, Vinnicombe SJ, Evans AJ. Contrast-enhanced digital breast tomosythesis and breast MRI to monitor response to neoadjuvant chemotherapy: patient toler- ance and preference. Br J Radiol. 2022;95(1134):20210779. 34. Hobbs MM, Taylor DB, Buzynski S, Peake RE. Contrast-enhanced spectral mammography (CESM) and contrast enhanced MRI (CEMRI): Patient preferences and tolerance. J Med Imaging Radiat Oncol. 2015;59(3):300-305. 35. Potsch N, Vatteroni G, Clauser P, H",
    "full_text_length": 58800,
    "chunk_length": 1479
  },
  {
    "chunk_id": 2055,
    "paper_filename": "matthew_2024_state_of_the_art_for_contrast_enhnace_mammography.pdf",
    "paper_title": "Matthew 2024 State Of The Art For Contrast Enhnace Mammography",
    "chunk_index": 40,
    "total_chunks": 53,
    "text_content": "Koppula V. Diagnostic accuracy of contrast-enhanced digital mammography in breast cancer detection in comparison to tomosynthesis, synthetic 2D mammography and tomosynthesis combined with ultrasound in women with dense breast. Br J Radiol. 2021;94(1118):20201046. 39. Ahsberg K, Gardfjell A, Nimeus E, et al. Added value of contrast- enhanced mammography (CEM) in staging of malignant breast lesions-a feasibility study. World J Surg Oncol. 2020;18(1):100.40. Bicchierai G, Tonelli P, Piacenti A, et ",
    "full_text_length": 58800,
    "chunk_length": 1447
  },
  {
    "chunk_id": 2056,
    "paper_filename": "matthew_2024_state_of_the_art_for_contrast_enhnace_mammography.pdf",
    "paper_title": "Matthew 2024 State Of The Art For Contrast Enhnace Mammography",
    "chunk_index": 41,
    "total_chunks": 53,
    "text_content": "breast tomosynthesis in the preoper - ative assessment of breast cancer. Radiol Med. 2021;126 (11):1407-1414. 43. Goh Y, Chou CP, Chan CW, et al. Impact of contrast-enhanced mammography in surgical management of breast cancers for women with dense breasts: a dual-center, multi-disciplinary study in Asia. Eur Radiol. 2022;32(12):8226-8237. 44. Montrognon F, Clatot F, Berghian A, et al. Impact of preoperative staging with contrast-enhanced mammography for localized breast cancer management. Br J R",
    "full_text_length": 58800,
    "chunk_length": 1416
  },
  {
    "chunk_id": 2057,
    "paper_filename": "matthew_2024_state_of_the_art_for_contrast_enhnace_mammography.pdf",
    "paper_title": "Matthew 2024 State Of The Art For Contrast Enhnace Mammography",
    "chunk_index": 42,
    "total_chunks": 53,
    "text_content": "2023;7(1):8. 47. Savaridas SL, Jin H. Costing analysis to introduce a contrast- enhanced mammography service to replace an existing breast MRI service for local staging of breast cancer. Clin Radiol. 2023;78 (5):340-346. 48. Amir T, Hogan MP, Jacobs S, Sevilimedu V, Sung J, Jochelson MS. Comparison of false-positive versus true-positive findings on contrast-enhanced digital mammography. AJR Am J Roentgenol. 2022;218(5):797-808. 49. Azzam H, Kamal RM, Hanafy MM, Youssef A, Hashem LMB. Comparative",
    "full_text_length": 58800,
    "chunk_length": 1443
  },
  {
    "chunk_id": 2058,
    "paper_filename": "matthew_2024_state_of_the_art_for_contrast_enhnace_mammography.pdf",
    "paper_title": "Matthew 2024 State Of The Art For Contrast Enhnace Mammography",
    "chunk_index": 43,
    "total_chunks": 53,
    "text_content": "lesions using contrast-enhanced mammography. Clin Breast Cancer. 2021;21 (3):256-262. 52. Depretto C, Borelli A, Liguori A, et al. Contrast-enhanced mam - mography in the evaluation of breast calcifications: preliminary experience. Tumori. 2020;106(6):491-496. 53. Y\u007fuzkan S, Cengiz D, Hekimsoy lI, Sezgin Okc\u00b8u \u007fO, Oktay A. Diagnostic performance of contrast-enhanced mammography: comparison With MRI and mammography. J Breast Imaging. 2021;3(4):448-454. 54. Nida BA, Rooney TB, Miller MM. Utility o",
    "full_text_length": 58800,
    "chunk_length": 1461
  },
  {
    "chunk_id": 2059,
    "paper_filename": "matthew_2024_state_of_the_art_for_contrast_enhnace_mammography.pdf",
    "paper_title": "Matthew 2024 State Of The Art For Contrast Enhnace Mammography",
    "chunk_index": 44,
    "total_chunks": 53,
    "text_content": "11 August 2025 57. Nicosia L, Bozzini AC, Palma S, et al. A score to predict the malig - nancy of a breast lesion based on different contrast enhancement patterns in contrast-enhanced spectral mammography. Cancers (Basel). 2022;14(17):1417. 58. Petrillo A, Fusco R, Vallone P, et al. Digital breast tomosynthesis and contrast-enhanced dual-energy digital mammography alone and in combination compared to 2D digital synthetized mammog - raphy and MR imaging in breast cancer detection and classifica -",
    "full_text_length": 58800,
    "chunk_length": 1410
  },
  {
    "chunk_id": 2060,
    "paper_filename": "matthew_2024_state_of_the_art_for_contrast_enhnace_mammography.pdf",
    "paper_title": "Matthew 2024 State Of The Art For Contrast Enhnace Mammography",
    "chunk_index": 45,
    "total_chunks": 53,
    "text_content": "in the assessment of response to neoadjuvant chemotherapy in breast cancer patients with calci- fications in the tumor bed. Diagnostics (Basel). 2021;11(3):113. 62. Steinhof-Radwa \ufffdnska K, Gralzy\ufffdnska A, Lorek A, et al. Contrast-en - hanced spectral mammography assessment of patients treated with neoadjuvant chemotherapy for breast cancer. Curr Oncol. 2021; 28(5):3448-3462. 63. Gelardi F, Ragaini EM, Sollini M, Bernardi D, Chiti A. Contrast- enhanced mammography versus breast magnetic resonance ",
    "full_text_length": 58800,
    "chunk_length": 1450
  },
  {
    "chunk_id": 2061,
    "paper_filename": "matthew_2024_state_of_the_art_for_contrast_enhnace_mammography.pdf",
    "paper_title": "Matthew 2024 State Of The Art For Contrast Enhnace Mammography",
    "chunk_index": 46,
    "total_chunks": 53,
    "text_content": "AJR Am J Roentgenol. 2021;216(6):1486-1491. 67. Rudnicki W, Piegza T, Rozum-Liszewska N, et al. The effectiveness of contrast-enhanced spectral mammography and magnetic reso- nance imaging in dense breasts. Pol J Radiol. 2021;86:e159-e64. 68. Monticciolo DL, Newell MS, Moy L, Lee CS, Destounis SV. Breast cancer screening for women at higher-than-average risk: updated recommendations from the ACR. J Am Coll Radiol. 2023;20 (9):902-914. 69. Elder K, Matheson J, Nickson C, et al. Contrast enhanced ",
    "full_text_length": 58800,
    "chunk_length": 1376
  },
  {
    "chunk_id": 2062,
    "paper_filename": "matthew_2024_state_of_the_art_for_contrast_enhnace_mammography.pdf",
    "paper_title": "Matthew 2024 State Of The Art For Contrast Enhnace Mammography",
    "chunk_index": 47,
    "total_chunks": 53,
    "text_content": "Feasibility of contrast-enhanced mammography in women with breast implants. Clin Imaging. 2023;93:31-33. 73. Wang S, Sun Y, You C, et al. Association of clinical factors and de- gree of early background parenchymal enhancement on contrast- enhanced mammography. AJR Am J Roentgenol. 2023;221 (1):45-55. 74. Karimi Z, Phillips J, Slanetz P, et al. Factors associated with back- ground parenchymal enhancement on contrast-enhanced mam - mography. AJR Am J Roentgenol. 2021;216(2):340-348. 75. Yuen S, M",
    "full_text_length": 58800,
    "chunk_length": 1334
  },
  {
    "chunk_id": 2063,
    "paper_filename": "matthew_2024_state_of_the_art_for_contrast_enhnace_mammography.pdf",
    "paper_title": "Matthew 2024 State Of The Art For Contrast Enhnace Mammography",
    "chunk_index": 48,
    "total_chunks": 53,
    "text_content": "X. Diagnostic perfor - mance of the Kaiser score in the evaluation of breast lesions on contrast-enhanced mammography. Eur J Radiol. 2022;156:110524. 78. Rong X, Kang Y, Xue J, et al. Value of contrast-enhanced mam - mography combined with the Kaiser score for clinical decision- making regarding tomosynthesis BI-RADS 4A lesions. Eur Radiol. 2022;32(11):7439-7447. 79. Gennaro G, Baldan E, Bezzon E, Caumo F. Artifact reduction in contrast-enhanced mammography. Insights Imaging. 2022;13 (1):90. 80.",
    "full_text_length": 58800,
    "chunk_length": 1435
  },
  {
    "chunk_id": 2064,
    "paper_filename": "matthew_2024_state_of_the_art_for_contrast_enhnace_mammography.pdf",
    "paper_title": "Matthew 2024 State Of The Art For Contrast Enhnace Mammography",
    "chunk_index": 49,
    "total_chunks": 53,
    "text_content": "cancer by contrast-enhanced spectral mammography images. Diagnostics (Basel). 2021;11(4):114. 83. Miller MM, Rubaiyat AHM, Rohde GK. Predicting malignancy of breast imaging findings using quantitative analysis of contrast- enhanced mammography (CEM). Diagnostics (Basel). 2023;13 (6):136. 84. Petrillo A, Fusco R, Di Bernardo E, et al. Prediction of breast can- cer histological outcome by radiomics and artificial intelligence analysis in contrast-enhanced mammography. Cancers (Basel). 2022;14(9):1",
    "full_text_length": 58800,
    "chunk_length": 1507
  },
  {
    "chunk_id": 2065,
    "paper_filename": "matthew_2024_state_of_the_art_for_contrast_enhnace_mammography.pdf",
    "paper_title": "Matthew 2024 State Of The Art For Contrast Enhnace Mammography",
    "chunk_index": 50,
    "total_chunks": 53,
    "text_content": "X, et al. Deep learning-enabled fully automated pipeline system for segmentation and classification of single-mass breast lesions using contrast-enhanced mammography: a prospec - tive, multicentre study. EClinicalMedicine. 2023;58:101913. 89. Fanizzi A, Losurdo L, Basile TMA, et al. Fully automated support system for diagnosis of breast cancer in contrast-enhanced spectral mammography images. J Clin Med. 2019;86. 90. Fusco R, Di Bernardo E, Piccirillo A, et al. Radiomic and artificial intelligen",
    "full_text_length": 58800,
    "chunk_length": 1574
  },
  {
    "chunk_id": 2066,
    "paper_filename": "matthew_2024_state_of_the_art_for_contrast_enhnace_mammography.pdf",
    "paper_title": "Matthew 2024 State Of The Art For Contrast Enhnace Mammography",
    "chunk_index": 51,
    "total_chunks": 53,
    "text_content": "Cancer Res. 2017;19(1):57.BJR, 2024, Volume 97, Issue 1156 703 Downloaded from https://academic.oup.com/bjr/article/97/1156/695/7582327 by guest on 11 August 2025 93. Marino MA, Leithner D, Sung J, et al. Radiomics for tumor char - acterization in breast cancer patients: a feasibility study comparing contrast-enhanced mammography and magnetic resonance imag - ing. Diagnostics (Basel) . 2020;10(7):107. 94. Marino MA, Pinker K, Leithner D, et al. Contrast-enhanced mam - mography and radiomics anal",
    "full_text_length": 58800,
    "chunk_length": 1468
  },
  {
    "chunk_id": 2067,
    "paper_filename": "matthew_2024_state_of_the_art_for_contrast_enhnace_mammography.pdf",
    "paper_title": "Matthew 2024 State Of The Art For Contrast Enhnace Mammography",
    "chunk_index": 52,
    "total_chunks": 53,
    "text_content": "learning for breast lesions classification on contrast enhanced spectral mammog - raphy: a multicentre study. Br J Cancer . 2023;128(5):793-804. 98. Suter MB, Pesapane F, Agazzi GM, et al. Diagnostic accu - racy of contrast-enhanced spectral mammography for breast lesions: A systematic review and meta-analysis. Breast . 2020; 53:8-17. https://www.sciencedirect.com/science/article/pii/S09 60977620301326?via%3Dihub 99. Xiang W, Rao H, Zhou L. A meta-analysis of contrast-enhanced spectral mammograp",
    "full_text_length": 58800,
    "chunk_length": 964
  },
  {
    "chunk_id": 2068,
    "paper_filename": "mayo_2024_multimodal_artificial_intelligence_models_for_radiology.pdf",
    "paper_title": "Mayo 2024 Multimodal Artificial Intelligence Models For Radiology",
    "chunk_index": 0,
    "total_chunks": 35,
    "text_content": "Multimodal artificial intelligence models for radiology Amara Tariq, PhD1, Imon Banerjee , PhD1, Hari Trivedi, MD2, Judy Gichoya , MD, MS\ufffd,2 1Mayo Clinic, Phoenix, AZ, 85054, United States 2Department of Radiology and Imaging Sciences, Emory University, Atlanta, GA 30322, United States \ufffdCorresponding author: Judy Gichoya, MD, MS, Department of Radiology and Imaging Sciences, Emory University, 1364 Clifton Rd NE, Suite AG08, Atlanta, GA 30322, United States (judywawira@emory.edu ) Abstract Artifi",
    "full_text_length": 37840,
    "chunk_length": 1410
  },
  {
    "chunk_id": 2069,
    "paper_filename": "mayo_2024_multimodal_artificial_intelligence_models_for_radiology.pdf",
    "paper_title": "Mayo 2024 Multimodal Artificial Intelligence Models For Radiology",
    "chunk_index": 1,
    "total_chunks": 35,
    "text_content": "fusion models capable of harnessing multi-modal data for advanced decision making. We present a broad overview of the landscape of research in multimodal AI for radiology covering a wide variety of approaches from traditional fusion modelling to modern vision-language models. We provide analysis of comparative merits and drawbacks of each approach to assist future research and highlight ethical consideration in developing multimodal AI. In practice, the quality and quantity of available training",
    "full_text_length": 37840,
    "chunk_length": 1394
  },
  {
    "chunk_id": 2070,
    "paper_filename": "mayo_2024_multimodal_artificial_intelligence_models_for_radiology.pdf",
    "paper_title": "Mayo 2024 Multimodal Artificial Intelligence Models For Radiology",
    "chunk_index": 2,
    "total_chunks": 35,
    "text_content": "typically focused on single modalities\u2014for example, imaging, labs, or a finite set of features from the electronic health record (EHR). Most ra- diology models today are developed using convolutional neu- ral networks (CNNs) applied to a single image type like chest X-rays or head CTs.1,2Even basic elements like patient dem- ographics are rarely included in the model pipeline. While image-only models can perform well on image-only tasks such as haemorrhage detection, there remains a fundamental ",
    "full_text_length": 37840,
    "chunk_length": 1366
  },
  {
    "chunk_id": 2071,
    "paper_filename": "mayo_2024_multimodal_artificial_intelligence_models_for_radiology.pdf",
    "paper_title": "Mayo 2024 Multimodal Artificial Intelligence Models For Radiology",
    "chunk_index": 3,
    "total_chunks": 35,
    "text_content": "postoperative, for whom intracranial blood products are expected thus unneces - sarily disrupting the radiologist workflow. Additionally, we observed a 10% performance drop in outpatients in a pulmo - nary embolism triage model compared to the reported FDA validation metrics. This is concerning as the outpatients are the most vulnerable when they have critical findings as they cannot receive timely intervention. Building single modality models without clinical context (available from multimodal ",
    "full_text_length": 37840,
    "chunk_length": 1425
  },
  {
    "chunk_id": 2072,
    "paper_filename": "mayo_2024_multimodal_artificial_intelligence_models_for_radiology.pdf",
    "paper_title": "Mayo 2024 Multimodal Artificial Intelligence Models For Radiology",
    "chunk_index": 4,
    "total_chunks": 35,
    "text_content": "and release of many multimodal large datasets like INSPECT,7ROCO,8All of Us,9and UK Biobank.10Many new multi-modal fusion modelling techni - ques have also been developed that combine radiology images along with clinical information from the EHR to make diag- noses or predictions.11-15This survey paper reviews various computational approaches that can be applied to multimodal datasets in radiology based on model architecture and dis- cusses them from technical and application perspectives in- cl",
    "full_text_length": 37840,
    "chunk_length": 1408
  },
  {
    "chunk_id": 2073,
    "paper_filename": "mayo_2024_multimodal_artificial_intelligence_models_for_radiology.pdf",
    "paper_title": "Mayo 2024 Multimodal Artificial Intelligence Models For Radiology",
    "chunk_index": 5,
    "total_chunks": 35,
    "text_content": "frameworks with complex pretrained feature extraction pipelines (Figure 1).11 Received: 26 May 2024; Revised: 1 November 2024; Accepted: 13 November 2024 \u00a9 The Author(s) 2025. Published by Oxford University Press on behalf of the British Institute of Radiology. This is an Open Access article distributed under the terms of the Creative Commons Attribution License (https://creativecommons.org/licenses/by/4.0/), which permits unrestricted reuse, distribution, and reproduction in any medium, provide",
    "full_text_length": 37840,
    "chunk_length": 1494
  },
  {
    "chunk_id": 2074,
    "paper_filename": "mayo_2024_multimodal_artificial_intelligence_models_for_radiology.pdf",
    "paper_title": "Mayo 2024 Multimodal Artificial Intelligence Models For Radiology",
    "chunk_index": 6,
    "total_chunks": 35,
    "text_content": "modalities like images and tabular data. However, this technique is limited in that the fusion model cannot learn complementary information from differ - ent modalities as the features or probabilities are already extracted and frozen from each modality before fusion. Although limited compared to more advanced methods, many models have successfully used this approach to com- bine radiology images like chest X-rays with clinical data, achieving greater performance compared to individual mo- dalit",
    "full_text_length": 37840,
    "chunk_length": 1346
  },
  {
    "chunk_id": 2075,
    "paper_filename": "mayo_2024_multimodal_artificial_intelligence_models_for_radiology.pdf",
    "paper_title": "Mayo 2024 Multimodal Artificial Intelligence Models For Radiology",
    "chunk_index": 7,
    "total_chunks": 35,
    "text_content": "instead of raw data before they can be fused. This suggests that the quality of the pre- trained feature extractor can significantly influence the per- formance of the final classification model, highlighting the necessity of careful selection of the feature extractor, and po- tential need to experiment with multiple variations of feature extraction. For example, ImageNet-trained CNN models are not ideal feature extractors for chest X-rays due to differences between natural images (ImageNet data",
    "full_text_length": 37840,
    "chunk_length": 1414
  },
  {
    "chunk_id": 2076,
    "paper_filename": "mayo_2024_multimodal_artificial_intelligence_models_for_radiology.pdf",
    "paper_title": "Mayo 2024 Multimodal Artificial Intelligence Models For Radiology",
    "chunk_index": 8,
    "total_chunks": 35,
    "text_content": "1. Traditional fusion models\u2014(A) early fusion, (B) joint/middle fusion, and (C) late fusion. These architectures differ based on the point of merging multiple features early, middle, or late in the model pipeline.2 BJR|Artificial Intelligence, 2025, Volume 2, Issue 1 Downloaded from https://academic.oup.com/bjrai/article/2/1/ubae017/7959794 by guest on 24 March 2025 extraction backbone can be updated to better suit the down - stream prediction task. A limitation of all traditional fusion approac",
    "full_text_length": 37840,
    "chunk_length": 1385
  },
  {
    "chunk_id": 2077,
    "paper_filename": "mayo_2024_multimodal_artificial_intelligence_models_for_radiology.pdf",
    "paper_title": "Mayo 2024 Multimodal Artificial Intelligence Models For Radiology",
    "chunk_index": 9,
    "total_chunks": 35,
    "text_content": "where the same curation techniques may not apply. These limitations have led to the development of innovative fusion methods with ca- pacity to integrate both explicit and implicit information with minimal human curation. Graph-based fusion models While traditional fusion paradigms result in rigid models with restrictions on features that can be processed, graph convolutional neural networks (GCNs) provide an opportu - nity to fuse implicit information about clinical similarity be- tween samples",
    "full_text_length": 37840,
    "chunk_length": 1401
  },
  {
    "chunk_id": 2078,
    "paper_filename": "mayo_2024_multimodal_artificial_intelligence_models_for_radiology.pdf",
    "paper_title": "Mayo 2024 Multimodal Artificial Intelligence Models For Radiology",
    "chunk_index": 10,
    "total_chunks": 35,
    "text_content": "can learn the relationship between brain images of a specific patient (node feature vec- tor) and brain images of other clinically similar patients (fea- ture vectors of other connected nodes) as defined by neighbourhoods in the graph structure that connects patients with similar demographic characteristics (edge feature vectors). This approach has proven its merit in several use cases including diagnosis and clinical event prediction.14 GCNs have been adapted for a wide range of modalities in- ",
    "full_text_length": 37840,
    "chunk_length": 1333
  },
  {
    "chunk_id": 2079,
    "paper_filename": "mayo_2024_multimodal_artificial_intelligence_models_for_radiology.pdf",
    "paper_title": "Mayo 2024 Multimodal Artificial Intelligence Models For Radiology",
    "chunk_index": 11,
    "total_chunks": 35,
    "text_content": "GCN applicable. GCNs have also been modified to include temporal information by in- cluding temporal convolutional layers.19Such models allow incorporation of time-varying clinical information, like labo- ratory values or vital signs (Figure 2). For example, a tempo - ral model was used to build edges of the graph such that patients (nodes) with similarly evolving clinical status were connected and used to predict the risk of blood transfusion for each patient.23To demonstrate the variable perfo",
    "full_text_length": 37840,
    "chunk_length": 1343
  },
  {
    "chunk_id": 2080,
    "paper_filename": "mayo_2024_multimodal_artificial_intelligence_models_for_radiology.pdf",
    "paper_title": "Mayo 2024 Multimodal Artificial Intelligence Models For Radiology",
    "chunk_index": 12,
    "total_chunks": 35,
    "text_content": "test sets. For the first task, models were trained using cohort of Emory University Hospital patients and externally validated on a cohort from Mayo Clinic. For the second task, the model was trained us- ing patient cohort from Mayo Clinic and externally validated on the publicly available MIMIC-III dataset \u2013 see result sum- mary in Table 1. Although GCNs are powerful, unstructured modalities like images and text may still need pretrained feature extractors for generating node feature vectors. C",
    "full_text_length": 37840,
    "chunk_length": 1367
  },
  {
    "chunk_id": 2081,
    "paper_filename": "mayo_2024_multimodal_artificial_intelligence_models_for_radiology.pdf",
    "paper_title": "Mayo 2024 Multimodal Artificial Intelligence Models For Radiology",
    "chunk_index": 13,
    "total_chunks": 35,
    "text_content": "are included in the model. For example, imaging fea- tures extracted from pretrained CNN pipelines for different chest X-rays have similar characteristics as they share overall image structures such as a dark background and human- shaped foreground object. When used as node features, the first layer of graph convolution will process one patient\u2019s chest X-ray (node feature vectors) along with chest X-rays of its directly connected neighbours, often through weighted combinations. A second layer wi",
    "full_text_length": 37840,
    "chunk_length": 1351
  },
  {
    "chunk_id": 2082,
    "paper_filename": "mayo_2024_multimodal_artificial_intelligence_models_for_radiology.pdf",
    "paper_title": "Mayo 2024 Multimodal Artificial Intelligence Models For Radiology",
    "chunk_index": 14,
    "total_chunks": 35,
    "text_content": "2 forms of explanation\u2014explanation of important node fea- tures and importance of graph neighbourhood or subgraphs. Explanatory frameworks like GNNExplainer have been pro- posed which randomly sample graph neighbourhoods and estimate their effect on model output, thus assigning impor - tance weights to graph edges.15,25However, clinical applica - bility of such explanations is still under-explored due to their complexity. Joint embedding of multimodal data In recent years, large transformer mode",
    "full_text_length": 37840,
    "chunk_length": 1402
  },
  {
    "chunk_id": 2083,
    "paper_filename": "mayo_2024_multimodal_artificial_intelligence_models_for_radiology.pdf",
    "paper_title": "Mayo 2024 Multimodal Artificial Intelligence Models For Radiology",
    "chunk_index": 15,
    "total_chunks": 35,
    "text_content": "as image-to-image translation.26-28 Recently developed vision-language models (VLM) have shown surprisingly good performance on a variety of image and text-related downstream tasks like radiology report gen- eration and visual question answering.29-31Research in this field is rapidly moving towards multi-modal large models such as VLM that have joint embedding spaces to process vi- sual and textual data to learn text-image pair interdepend - ence (Figure 3). VLMs are generally composed of 2 Tabl",
    "full_text_length": 37840,
    "chunk_length": 1543
  },
  {
    "chunk_id": 2084,
    "paper_filename": "mayo_2024_multimodal_artificial_intelligence_models_for_radiology.pdf",
    "paper_title": "Mayo 2024 Multimodal Artificial Intelligence Models For Radiology",
    "chunk_index": 16,
    "total_chunks": 35,
    "text_content": "81.6 [80.3-82.9] GCN 84.7 [82.3-87.9] 82.5 [81.1-83.9] 90.1 [89.0-91.3] 81.4 [79.6-83.5] 74.6 [73.2-76.0] 85.3 [84.2-86.5] Transfusion risk prediction Late fusion 64.0 [62.1-66.0] 64.2 [62.9-65.5] 69.9 [68.6-71.2] 60.0 [50.0-75.0] 54.0 [50.8-57.9] 44.4 [31.3-56.0] GCN 73.8 [72.0-75.6] 65.4 [64.1-66.7] 77.4 [76.3-78.5] 80.0 [66.0-95.0] 69.8 [66.7-73.6] 70.8 [62.5-84.7] Figure 3. Joint embedding space generation using vision-language model (VLM)\u2014image and text.4 BJR|Artificial Intelligence, 2025, ",
    "full_text_length": 37840,
    "chunk_length": 1487
  },
  {
    "chunk_id": 2085,
    "paper_filename": "mayo_2024_multimodal_artificial_intelligence_models_for_radiology.pdf",
    "paper_title": "Mayo 2024 Multimodal Artificial Intelligence Models For Radiology",
    "chunk_index": 17,
    "total_chunks": 35,
    "text_content": "recent VLMs like ViLT in- cluded tasks like masked word prediction, image patch-word alignment, and image-text retrieval. The primary challenge for training VLMs is that the inher - ently large size of these models requires huge training data- sets\u2014in the range of million text-image pairs. Examples of open-source datasets used to train these models are Public Multimodal Dataset (PMD)33which contains about 70M image-text pairs, and several smaller datasets including Conceptual Captions,34WIT,35Lo",
    "full_text_length": 37840,
    "chunk_length": 1454
  },
  {
    "chunk_id": 2086,
    "paper_filename": "mayo_2024_multimodal_artificial_intelligence_models_for_radiology.pdf",
    "paper_title": "Mayo 2024 Multimodal Artificial Intelligence Models For Radiology",
    "chunk_index": 18,
    "total_chunks": 35,
    "text_content": "As the availability of radiology datasets and computational resources improves, VLMs are increasingly being applied for several radiology tasks. The inherently self-supervised nature of VLM training where a model is optimized to learn correla - tion between image-text pairs, for example, radiology image and report pairs, is particularly suitable for radiology where the cost of manual annotation of images remains expensive. While VLMs were initially applied to 2-dimensional radiol - ogy images li",
    "full_text_length": 37840,
    "chunk_length": 1409
  },
  {
    "chunk_id": 2087,
    "paper_filename": "mayo_2024_multimodal_artificial_intelligence_models_for_radiology.pdf",
    "paper_title": "Mayo 2024 Multimodal Artificial Intelligence Models For Radiology",
    "chunk_index": 19,
    "total_chunks": 35,
    "text_content": "cal trial datasets which tend to be very structured and have a very strict exclusion criterion.44This is because the radiology study must be matched to the pathology slide, and given var- ied appearance it\u2019s impossible to generate overlays and image registrations. Therefore, it is unlikely that these types of mod- els that are generated from clinical trial databases will demonstrate robustness when they encounter out-of- distribution data in a real-world setting.44Another challenge of these mode",
    "full_text_length": 37840,
    "chunk_length": 1325
  },
  {
    "chunk_id": 2088,
    "paper_filename": "mayo_2024_multimodal_artificial_intelligence_models_for_radiology.pdf",
    "paper_title": "Mayo 2024 Multimodal Artificial Intelligence Models For Radiology",
    "chunk_index": 20,
    "total_chunks": 35,
    "text_content": "challenging to inter- pret and explain due to the millions of model parameters. For multimodal models, traditional approaches may offer better explainability compared to VLMs. It is not clear if the large technical and data burden of multimodal data fusion outper - forms single modality models or those that use simple regres - sion approaches, and more research is necessary to fill this knowledge gap. The ability of deep learning models to encode \u201chidden characteristics\u201d on images like self-repo",
    "full_text_length": 37840,
    "chunk_length": 1291
  },
  {
    "chunk_id": 2089,
    "paper_filename": "mayo_2024_multimodal_artificial_intelligence_models_for_radiology.pdf",
    "paper_title": "Mayo 2024 Multimodal Artificial Intelligence Models For Radiology",
    "chunk_index": 21,
    "total_chunks": 35,
    "text_content": "is a risk of strengthening historical biases that attempt to show different biological occurrences across different race groups. Conclusion Large multimodal models like VLMs are pushing the bound - aries of AI in radiology, but caution should be exercised to se- lect the appropriate fusion approach for a task due to challenges of handling high-dimensional heterogeneous data in an end-to-end model. While existing pretrained VLMs are suitable for natural image tasks, careful finetuning may be re- ",
    "full_text_length": 37840,
    "chunk_length": 1393
  },
  {
    "chunk_id": 2090,
    "paper_filename": "mayo_2024_multimodal_artificial_intelligence_models_for_radiology.pdf",
    "paper_title": "Mayo 2024 Multimodal Artificial Intelligence Models For Radiology",
    "chunk_index": 22,
    "total_chunks": 35,
    "text_content": "to delivering precision medicine by incorporating data from different modalities including imaging, text, tabu- lar, and genomic sequences. As computational approaches advance, there will be several radiology toolkits available for harnessing multimodal data, as well as ethical pitfalls that must be addressed to ensure benefit for all patients. Table 2 summarises the pros and cons of various fusion strategies.BJR|Artificial Intelligence, 2025, Volume 2, Issue 1 5 Downloaded from https://academic",
    "full_text_length": 37840,
    "chunk_length": 1422
  },
  {
    "chunk_id": 2091,
    "paper_filename": "mayo_2024_multimodal_artificial_intelligence_models_for_radiology.pdf",
    "paper_title": "Mayo 2024 Multimodal Artificial Intelligence Models For Radiology",
    "chunk_index": 23,
    "total_chunks": 35,
    "text_content": "networks tech- niques for medical image analysis. Neural Comput Appl. 2022;34 (8):5791-5812. 02. Anwar S, Majid M, Qayyum A, Awais M, Alnowami M, Khan M. Medical image analysis using convolutional neural networks: a re- view. J Med Syst. 2018;42(11):226. 03. Huang SC, Huo Z, Steinberg E, et al. INSPECT: a multimodal dataset for patient outcome prediction of pulmonary embolisms. Adv Neural Inf Process Syst. 2024;36. 04. Pelka O, Koitka S, R\u007fuckert J, Nensa F, Friedrich CM. Radiology objects in co",
    "full_text_length": 37840,
    "chunk_length": 1374
  },
  {
    "chunk_id": 2092,
    "paper_filename": "mayo_2024_multimodal_artificial_intelligence_models_for_radiology.pdf",
    "paper_title": "Mayo 2024 Multimodal Artificial Intelligence Models For Radiology",
    "chunk_index": 24,
    "total_chunks": 35,
    "text_content": "deep learning: a systematic review and implementation guidelines. NPJ Digit Med. 2020;3(1):136. 06. Huang SC, Pareek A, Zamanian R, Banerjee I, Lungren MP. Multimodal fusion with deep neural networks for leveraging CT imaging and electronic health record: a case-study in pulmonary embolism detection. Sci Rep. 2020;10(1):22147. 07. Mohsen F, Ali H, El Hajj N, Shah Z. Artificial intelligence-based methods for fusion of electronic health records and imaging data. Sci Rep. 2022;12(1):17981. 08. Tari",
    "full_text_length": 37840,
    "chunk_length": 1353
  },
  {
    "chunk_id": 2093,
    "paper_filename": "mayo_2024_multimodal_artificial_intelligence_models_for_radiology.pdf",
    "paper_title": "Mayo 2024 Multimodal Artificial Intelligence Models For Radiology",
    "chunk_index": 25,
    "total_chunks": 35,
    "text_content": "CT and EHR. arXiv, arXiv:2111.11665, preprint: not peer reviewed. 11. Tariq A, Patel BN, Sensakovic WF, Fahrenholtz SJ, Banerjee I. Opportunistic screening for low bone density using abdominopel - vic computed tomography scans. Med Phys. 2023;50 (7):4296-4307. 12. Tang S, Tariq A, Dunnmon JA, et al. Predicting 30-day all-cause hospital readmission using multimodal spatiotemporal graph neu- ral networks. IEEE J Biomed Health Inform. 2023;27 (4):2071-2082. 13. Cao M, Yang M, Qin C, et al. Using De",
    "full_text_length": 37840,
    "chunk_length": 1405
  },
  {
    "chunk_id": 2094,
    "paper_filename": "mayo_2024_multimodal_artificial_intelligence_models_for_radiology.pdf",
    "paper_title": "Mayo 2024 Multimodal Artificial Intelligence Models For Radiology",
    "chunk_index": 26,
    "total_chunks": 35,
    "text_content": "Traditional\u2014early fusion \ufffdExplainable \ufffdAbility to extract complimentary informa - tion in the various modalities \ufffdMust extract features from heterogeneous data (instead of using the raw data)\u2014hence can be affected by the quality of feature extractor \ufffdInability to handle missing data \ufffdRisk of overfitting due to high feature dimensionality \ufffdHuman-based feature curation and engineering can be biased and is time-intensive Traditional\u2014middle/joint fusion\ufffdArchitectural innovation to support parallel e",
    "full_text_length": 37840,
    "chunk_length": 1487
  },
  {
    "chunk_id": 2095,
    "paper_filename": "mayo_2024_multimodal_artificial_intelligence_models_for_radiology.pdf",
    "paper_title": "Mayo 2024 Multimodal Artificial Intelligence Models For Radiology",
    "chunk_index": 27,
    "total_chunks": 35,
    "text_content": "generalizability to new datasets and missing data \ufffdUnstructured data may still need pretrained fea- ture extractors \ufffdHomogenization effect may occur when too many graph layers are included in the model \ufffdNot explainable Joint embedding of multimodal data\ufffdRequire minimal dataset curation \ufffdRequire large training datasets and compute \ufffdNo benchmark datasets for robust VLM perfor - mance evaluation 6 BJR|Artificial Intelligence, 2025, Volume 2, Issue 1 Downloaded from https://academic.oup.com/bjrai/ar",
    "full_text_length": 37840,
    "chunk_length": 1402
  },
  {
    "chunk_id": 2096,
    "paper_filename": "mayo_2024_multimodal_artificial_intelligence_models_for_radiology.pdf",
    "paper_title": "Mayo 2024 Multimodal Artificial Intelligence Models For Radiology",
    "chunk_index": 28,
    "total_chunks": 35,
    "text_content": "using graph neural networks. medRxiv 23287599. https://doi.org/10. 1101/2023.03.22.23287599, March 25, 2023, preprint: not peer reviewed. 18. Ying Z, Bourgeois D, You J, Zitnik M, Leskovec J. GNNExplainer: generating explanations for graph neural net- works. Adv Neural Inf Process Syst. 2019;32:9240-9251. 19. Guo J, Li J, Li D, et al. From images to textual prompts: zero-shot vi- sual question answering with frozen large language models. 2023; 10867-10877. https://openaccess.thecvf.com/content/C",
    "full_text_length": 37840,
    "chunk_length": 1455
  },
  {
    "chunk_id": 2097,
    "paper_filename": "mayo_2024_multimodal_artificial_intelligence_models_for_radiology.pdf",
    "paper_title": "Mayo 2024 Multimodal Artificial Intelligence Models For Radiology",
    "chunk_index": 29,
    "total_chunks": 35,
    "text_content": "Lee H, Shin W, Kim YH, Choi E. Multi-modal under - standing and generation for medical images and text via vision- language pre-training. IEEE J Biomed Health Inform. 2022;26 (12):6070-6080. 24. Zhang J, Huang J, Jin S, Lu S. Vision-language models for vision tasks: a survey. IEEE Trans Pattern Anal Mach Intell. 2024;46 (8):5625-5644. 25. Kim W, Son B, Kim I. ViLT: vision-and-language transformer without convolution or region supervision. In: International Conference on Machine Learning 2021. PM",
    "full_text_length": 37840,
    "chunk_length": 1443
  },
  {
    "chunk_id": 2098,
    "paper_filename": "mayo_2024_multimodal_artificial_intelligence_models_for_radiology.pdf",
    "paper_title": "Mayo 2024 Multimodal Artificial Intelligence Models For Radiology",
    "chunk_index": 30,
    "total_chunks": 35,
    "text_content": "dataset for multimodal multilingual machine learning. SIGIR'21: Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval. 2021:2443-2449. https://dl.acm.org/doi/ 10.1145/3404835.3463257 29. Pont-Tuset J, Uijlings J, Changpinyo S, Soricut R, Ferrari V. Connecting vision and language with localized narratives. In: Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings 16, Part V. Springer Internationa",
    "full_text_length": 37840,
    "chunk_length": 1476
  },
  {
    "chunk_id": 2099,
    "paper_filename": "mayo_2024_multimodal_artificial_intelligence_models_for_radiology.pdf",
    "paper_title": "Mayo 2024 Multimodal Artificial Intelligence Models For Radiology",
    "chunk_index": 31,
    "total_chunks": 35,
    "text_content": "Johnson AE, Pollard TJ, Berkowitz SJ, et al. MIMIC-CXR, a de- identified publicly available database of chest radiographs with free-text reports. Sci Data. 2019;6(1):317. 34. Lau JJ, Gayen S, Ben Abacha A, Demner-Fushman D. A dataset of clinically generated visual questions and answers about radiology images. Sci Data. 2018;5:180251. 35. Ramirez AH, Sulieman L, Schlueter DJ, et al. The All of Us Research Program: data quality, utility, and diversity. Patterns. 2022;3(8):100570. 36. Matthews PM, ",
    "full_text_length": 37840,
    "chunk_length": 1468
  },
  {
    "chunk_id": 2100,
    "paper_filename": "mayo_2024_multimodal_artificial_intelligence_models_for_radiology.pdf",
    "paper_title": "Mayo 2024 Multimodal Artificial Intelligence Models For Radiology",
    "chunk_index": 32,
    "total_chunks": 35,
    "text_content": "checkpoint inhibitor response within PD-L1 high/low/negative expression categories in stage IV NSCLC. 2023. https://ascopubs.org/doi/10.1200/JCO.2023.41.16_suppl.1517 40. Sako C, Jordan P, McCall R, et al. Multi-center real-world data curation and assessment of tumor growth rate and overall survival in advanced NSCLC treated with PD-(L)1 immune checkpoint in- hibitor therapy. 2023. https://digitalcommons.providence.org/pub lications/8002/ 41. Mutha P, Khorrami M, Sonuga B, et al. Correlation of ",
    "full_text_length": 37840,
    "chunk_length": 1495
  },
  {
    "chunk_id": 2101,
    "paper_filename": "mayo_2024_multimodal_artificial_intelligence_models_for_radiology.pdf",
    "paper_title": "Mayo 2024 Multimodal Artificial Intelligence Models For Radiology",
    "chunk_index": 33,
    "total_chunks": 35,
    "text_content": "Mamtani R, Sondhi A, Cohen AB, Parikh RB. Evaluating generalizability of practice-changing randomized clini- cal trials in non-small cell lung cancer using machine learning- based in-silico trials. J Clin Oncol. 2023;141(16_suppl):9130. 45. Bera K, Braman N, Gupta A, Velcheti V, Madabhushi A. Predicting cancer outcomes with radiomics and artificial intelli - gence in radiology. Nat Rev Clin Oncol. 2022;19(2):132-146. 46. Gichoya JW, Banerjee I, Bhimireddy AR, et al. AI recognition of patient rac",
    "full_text_length": 37840,
    "chunk_length": 1402
  },
  {
    "chunk_id": 2102,
    "paper_filename": "mayo_2024_multimodal_artificial_intelligence_models_for_radiology.pdf",
    "paper_title": "Mayo 2024 Multimodal Artificial Intelligence Models For Radiology",
    "chunk_index": 34,
    "total_chunks": 35,
    "text_content": "and reproduction in any medium, provided the original work is properly cited. BJRyArtificial Intelligence, 2025, 2, 1\u20137 https://doi.org/10.1093/bjrai/ubae017 ReviewBJR|Artificial Intelligence, 2025, Volume 2, Issue 1 7 Downloaded from https://academic.oup.com/bjrai/article/2/1/ubae017/7959794 by guest on 24 March 2025",
    "full_text_length": 37840,
    "chunk_length": 319
  },
  {
    "chunk_id": 2103,
    "paper_filename": "micheal_2025_traige_wit_ai_rule_ot_framework_forrisk_and_benifits_for_mammography_automation.pdf",
    "paper_title": "Micheal 2025 Traige Wit Ai Rule Ot Framework Forrisk And Benifits For Mammography Automation",
    "chunk_index": 0,
    "total_chunks": 51,
    "text_content": "Title: Triage with AI: A Rule-out Framework Quantifying the Risks and Benefits of Screening Mammogram Automation Authors & Affiliations: *Micheal H. Bernstein, PhD1, *Maggie Chung, MD2, Adam Yala, PhD3, and Grayson L. Baird, PhD1. 1. Brown Radiology Human Factors Lab, Department of Diagnostic Imaging, The Warren Alpert Medical School, Brown University, and Brown University Health, Providence, RI 2. Department of Radiology and Biomedical Imaging, University of California, San Francisco, CA 3. Com",
    "full_text_length": 47589,
    "chunk_length": 1372
  },
  {
    "chunk_id": 2104,
    "paper_filename": "micheal_2025_traige_wit_ai_rule_ot_framework_forrisk_and_benifits_for_mammography_automation.pdf",
    "paper_title": "Micheal 2025 Traige Wit Ai Rule Ot Framework Forrisk And Benifits For Mammography Automation",
    "chunk_index": 1,
    "total_chunks": 51,
    "text_content": "review and should not be used to guide clinical practice. Background: AI has been proposed as a triage or \u201crule out\u201d device to reduce radiologist workload, but it is presently unclear how an AI triage threshold should be determined. We present a framework for determining an optimal threshold. Materials and Methods: 114,229 bilateral 2D digital screening mammograms were retrospectively analyzed from 2006-2023. All mammograms were given an AI score using Mirai, an open-source deep-learning model. ",
    "full_text_length": 47589,
    "chunk_length": 1299
  },
  {
    "chunk_id": 2105,
    "paper_filename": "micheal_2025_traige_wit_ai_rule_ot_framework_forrisk_and_benifits_for_mammography_automation.pdf",
    "paper_title": "Micheal 2025 Traige Wit Ai Rule Ot Framework Forrisk And Benifits For Mammography Automation",
    "chunk_index": 2,
    "total_chunks": 51,
    "text_content": "Net False Omission Rate (30%) (AN-FOR[30%]; N-FOR adjusted for the hypothetical scenario where radiologists detect an extra 30% of breast cancers among AI retained cases). The two thresholds were severity scores of 0.2 (Yuden\u2019s J) and 0.05 (AN-FOR[30%]=0). The former is mathematically optimal; the latter reflects a threshold where AI triage does not introduce any total increase in False Negatives. Results: At the 0.20 threshold, G-FOR, N-FOR, and AN-FOR(30%) were 0.26%, 0.017%, and 0.14%, respec",
    "full_text_length": 47589,
    "chunk_length": 1328
  },
  {
    "chunk_id": 2106,
    "paper_filename": "micheal_2025_traige_wit_ai_rule_ot_framework_forrisk_and_benifits_for_mammography_automation.pdf",
    "paper_title": "Micheal 2025 Traige Wit Ai Rule Ot Framework Forrisk And Benifits For Mammography Automation",
    "chunk_index": 3,
    "total_chunks": 51,
    "text_content": "no additional missed cancers at a 36% caseload reduction. Introduction The increasing use of artificial intelligence (AI) in radiology has prompted considerations about its potential in addressing the field\u2019s mounting challenges. In recent years, the . CC-BY-ND 4.0 International license It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)The copyright holder for this preprint this vers",
    "full_text_length": 47589,
    "chunk_length": 1345
  },
  {
    "chunk_id": 2107,
    "paper_filename": "micheal_2025_traige_wit_ai_rule_ot_framework_forrisk_and_benifits_for_mammography_automation.pdf",
    "paper_title": "Micheal 2025 Traige Wit Ai Rule Ot Framework Forrisk And Benifits For Mammography Automation",
    "chunk_index": 4,
    "total_chunks": 51,
    "text_content": "individuals entering radiology residency has not kept pace with the rise of imaging volume, 5 creating a growing workforce imbalance that is unlikely to be resolved in the near future. AI has the potential to alleviate some of this burden. Studies have found that AI can reduce medical imaging interpretation times. 6-8 One promising application to improve efficiency is using AI as a triage or \u201crule out\u201d device. By identifying mammograms that are extremely low-risk, AI can reduce the number of cas",
    "full_text_length": 47589,
    "chunk_length": 1247
  },
  {
    "chunk_id": 2108,
    "paper_filename": "micheal_2025_traige_wit_ai_rule_ot_framework_forrisk_and_benifits_for_mammography_automation.pdf",
    "paper_title": "Micheal 2025 Traige Wit Ai Rule Ot Framework Forrisk And Benifits For Mammography Automation",
    "chunk_index": 5,
    "total_chunks": 51,
    "text_content": "fewer than 1% of cases are positives. 9,10 The goal is to safely exclude the majority of normal cases and allow radiologists to concentrate on more suspicious exams. Using AI triage for \u201crule out\u201d in radiology has been proposed by several groups.11-18 Recently, empirical studies have suggested that AI triage can perform comparably to, and in some cases better than, standard of care where radiologists interpret all mammograms.12,19-21 One critical component of AI triage is determining the appropr",
    "full_text_length": 47589,
    "chunk_length": 1296
  },
  {
    "chunk_id": 2109,
    "paper_filename": "micheal_2025_traige_wit_ai_rule_ot_framework_forrisk_and_benifits_for_mammography_automation.pdf",
    "paper_title": "Micheal 2025 Traige Wit Ai Rule Ot Framework Forrisk And Benifits For Mammography Automation",
    "chunk_index": 6,
    "total_chunks": 51,
    "text_content": "into \u201cruled out\u201d and \u201cretained\u201d cases. . CC-BY-ND 4.0 International license It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)The copyright holder for this preprint this version posted June 10, 2025. ; https://doi.org/10.1101/2025.04.25.25326396doi: medRxiv preprint In this article, we present a framework for determining the optimal AI triage threshold for screening mammogram automat",
    "full_text_length": 47589,
    "chunk_length": 1392
  },
  {
    "chunk_id": 2110,
    "paper_filename": "micheal_2025_traige_wit_ai_rule_ot_framework_forrisk_and_benifits_for_mammography_automation.pdf",
    "paper_title": "Micheal 2025 Traige Wit Ai Rule Ot Framework Forrisk And Benifits For Mammography Automation",
    "chunk_index": 7,
    "total_chunks": 51,
    "text_content": "waived the requirement for written informed consent. Operational definitions Benefits and Risks in Ruled-out Cases. The primary benefit of ruling out cases is caseload reduction, which can be quantified as the Caseload Reduction Rate (CRR) (Table 1) . The higher the threshold, the higher the CRR; that is, the caseload reduction for radiologists will be higher when a more stri ngent (i.e. higher) threshold is set, ruling out a larger pool of cases. However, the benefit of a higher CRR must be wei",
    "full_text_length": 47589,
    "chunk_length": 1256
  },
  {
    "chunk_id": 2111,
    "paper_filename": "micheal_2025_traige_wit_ai_rule_ot_framework_forrisk_and_benifits_for_mammography_automation.pdf",
    "paper_title": "Micheal 2025 Traige Wit Ai Rule Ot Framework Forrisk And Benifits For Mammography Automation",
    "chunk_index": 8,
    "total_chunks": 51,
    "text_content": "by the radiologist in standard care (interpretating mammograms without AI triaging), potentially leading to unnecessary, costly, and stress-inducing diagnostic imaging and biopsies that turn out to be benign. The aforementioned benefit must be carefully weighed against the AI Gross False Omission Rate (G-FOR, or 1-AI-NPV), which is the probability that a patient ruled out by AI actually has breast cancer. As the threshold is raised to exclude more cases, the G-FOR increases. That is, the CRR and",
    "full_text_length": 47589,
    "chunk_length": 1276
  },
  {
    "chunk_id": 2112,
    "paper_filename": "micheal_2025_traige_wit_ai_rule_ot_framework_forrisk_and_benifits_for_mammography_automation.pdf",
    "paper_title": "Micheal 2025 Traige Wit Ai Rule Ot Framework Forrisk And Benifits For Mammography Automation",
    "chunk_index": 9,
    "total_chunks": 51,
    "text_content": "that not all cancers missed by AI in the ruled-out cases would have been detected by radiologists under standard practice (SP ) (i.e., radiologist workflow absent an AI triaging model). That is, some cancer cases would likely have been missed regardless of whether AI triaging was used. To account for this, we define the AI Net False Omission Rate (N-FOR) as the G-FOR minus cancer cases that would have been missed by AI and radiologists (i.e. \u201cdeduct\u201d cancer cases mutually missed by both radiolog",
    "full_text_length": 47589,
    "chunk_length": 1225
  },
  {
    "chunk_id": 2113,
    "paper_filename": "micheal_2025_traige_wit_ai_rule_ot_framework_forrisk_and_benifits_for_mammography_automation.pdf",
    "paper_title": "Micheal 2025 Traige Wit Ai Rule Ot Framework Forrisk And Benifits For Mammography Automation",
    "chunk_index": 10,
    "total_chunks": 51,
    "text_content": "triage threshold, the more cases AI will rule out (i.e. the larger the CCR). This means that remaining (i.e. retained) cases are more likely to be true positives, which increases the AI-PPV and reduces the AI-FDR. However, decreasing the number of retained cases (and by definition also increasing the number of ruled-out cases) can have important implications for how they are interpreted. Radiologist performance may improve as the retained reading pool size decreases due to reading fewer cases, 2",
    "full_text_length": 47589,
    "chunk_length": 1291
  },
  {
    "chunk_id": 2114,
    "paper_filename": "micheal_2025_traige_wit_ai_rule_ot_framework_forrisk_and_benifits_for_mammography_automation.pdf",
    "paper_title": "Micheal 2025 Traige Wit Ai Rule Ot Framework Forrisk And Benifits For Mammography Automation",
    "chunk_index": 11,
    "total_chunks": 51,
    "text_content": "have been detected by a radiologist in standard practice if ruled out, adjusted for the additional cancer detections due to AI triage that would have missed in standard practice (i.e., \u201ccredit\u201d cancer cases that radiologists would have otherwise missed without AI triaging them). Another risk worth considering is that although radiologists are more likely to catch cancer cases they would have otherwise missed had AI not retained them, it is also likely that for the same reason, radiologists may a",
    "full_text_length": 47589,
    "chunk_length": 1366
  },
  {
    "chunk_id": 2115,
    "paper_filename": "micheal_2025_traige_wit_ai_rule_ot_framework_forrisk_and_benifits_for_mammography_automation.pdf",
    "paper_title": "Micheal 2025 Traige Wit Ai Rule Ot Framework Forrisk And Benifits For Mammography Automation",
    "chunk_index": 12,
    "total_chunks": 51,
    "text_content": "Simulation Methods To illustrate the trade-offs associated with different AI triage thresholds, we conducted a \u201csimulation\u201d using risk scores from a deep learning model with screening mammography. Study Sample. We conducted a single institution retrospective review of 114,229 bilateral 2D digital screening mammograms acquired between January 2006 and January 2023. Exams with histopathologically confirmed breast cancer within 12 months of the screening mammogram were considered positive. Exams wi",
    "full_text_length": 47589,
    "chunk_length": 1391
  },
  {
    "chunk_id": 2116,
    "paper_filename": "micheal_2025_traige_wit_ai_rule_ot_framework_forrisk_and_benifits_for_mammography_automation.pdf",
    "paper_title": "Micheal 2025 Traige Wit Ai Rule Ot Framework Forrisk And Benifits For Mammography Automation",
    "chunk_index": 13,
    "total_chunks": 51,
    "text_content": "( CRR): the percentage of screening mammograms that would have been read by a radiologist under standard care but were excluded from review due to AI-based triage. CRR=Total cases ruled out /Total cases. AI Negative predictive value ( AI-NPV ): probability of a patient not having breast cancer if ruled out. AI-NPV = TN / (FN + TN). AI Positive predictive value ( AI-PPV): probability of a patient having breast cancer given if retained. PPV = TP / (TP + FP). Gross AI False omission rate ( G-FOR ):",
    "full_text_length": 47589,
    "chunk_length": 1182
  },
  {
    "chunk_id": 2117,
    "paper_filename": "micheal_2025_traige_wit_ai_rule_ot_framework_forrisk_and_benifits_for_mammography_automation.pdf",
    "paper_title": "Micheal 2025 Traige Wit Ai Rule Ot Framework Forrisk And Benifits For Mammography Automation",
    "chunk_index": 14,
    "total_chunks": 51,
    "text_content": "10, 2025. ; https://doi.org/10.1101/2025.04.25.25326396doi: medRxiv preprint AI Net False omission rate ( N-FOR ): probability of a patient having breast cancer if ruled out and the radiologist would have caught it. AI Adjusted Net False omission rate ( AN-FOR ): probability of a patient having breast cancer if ruled out and the radiologist would have caught it (i.e. N-FOR), adjusted for breast cancer cases that AI retained but radiologists would have missed. AI False discovery rate ( AI-FDR ): ",
    "full_text_length": 47589,
    "chunk_length": 1272
  },
  {
    "chunk_id": 2118,
    "paper_filename": "micheal_2025_traige_wit_ai_rule_ot_framework_forrisk_and_benifits_for_mammography_automation.pdf",
    "paper_title": "Micheal 2025 Traige Wit Ai Rule Ot Framework Forrisk And Benifits For Mammography Automation",
    "chunk_index": 15,
    "total_chunks": 51,
    "text_content": "where sensitivities and specificities were estimated using the %ROCPLOT macro, and PPV, NPV, FDR, and FOR were calculated using Bayes\u2019 Theorem. The base rate of cancer was 0.76%. Results Approaches to Identifying Triage Threshold Data were simulated using two triage thresholds that can be generalized across practices. The first uses diagnostic performance\u2014Youden\u2019s J\u2014to define a threshold by optimizing the balance of sensitivity and specificity. The second defines a threshold using an outcome, in",
    "full_text_length": 47589,
    "chunk_length": 1326
  },
  {
    "chunk_id": 2119,
    "paper_filename": "micheal_2025_traige_wit_ai_rule_ot_framework_forrisk_and_benifits_for_mammography_automation.pdf",
    "paper_title": "Micheal 2025 Traige Wit Ai Rule Ot Framework Forrisk And Benifits For Mammography Automation",
    "chunk_index": 16,
    "total_chunks": 51,
    "text_content": "a license to display the preprint in perpetuity. (which was not certified by peer review)The copyright holder for this preprint this version posted June 10, 2025. ; https://doi.org/10.1101/2025.04.25.25326396doi: medRxiv preprint Identifying Threshold using Diagnostic Performance (Youden\u2019s J) For these data, we observed that the Youden\u2019s J value is a Mirai score of 0.20, achieving a sensitivity of 74% and a specificity of 75% (see Table 2 and 4). Given a local prevalence of 0.76%, this translate",
    "full_text_length": 47589,
    "chunk_length": 1232
  },
  {
    "chunk_id": 2120,
    "paper_filename": "micheal_2025_traige_wit_ai_rule_ot_framework_forrisk_and_benifits_for_mammography_automation.pdf",
    "paper_title": "Micheal 2025 Traige Wit Ai Rule Ot Framework Forrisk And Benifits For Mammography Automation",
    "chunk_index": 17,
    "total_chunks": 51,
    "text_content": "radiologists in standard of care while they recalled the remaining 141, thus achieving an N-FOR of 141/85,220 or 0.17%. Regarding the retained cases, AI retained 66 cases that radiologists missed. Assuming radiologists detect 10%, 30%, 50% or 70% of these cases in AI triage, the adjusted net number of missed cancers in AI triage would be reduced to by 7, 20, 33, or 46 to 134, 121, 108, or 95, respectively. This would correspond to Adjusted Net FOR values of 0.16%, 0.14%, 0.13%, and 0.11%, respec",
    "full_text_length": 47589,
    "chunk_length": 1211
  },
  {
    "chunk_id": 2121,
    "paper_filename": "micheal_2025_traige_wit_ai_rule_ot_framework_forrisk_and_benifits_for_mammography_automation.pdf",
    "paper_title": "Micheal 2025 Traige Wit Ai Rule Ot Framework Forrisk And Benifits For Mammography Automation",
    "chunk_index": 18,
    "total_chunks": 51,
    "text_content": "and 70%) that radiologists would have detected among those retained by AI triage (compared to standard of care), the rule-out threshold can be set by determining the caseload reduction rate where AN-FOR intersects a certain value (here 0). As discussed above, this threshold corresponds to no additional missed cancers overall (among both retained and ruled out cases) relative to standard practice. As illustrated in Figure 1 and Table 4 ( bold ), assuming radiologists detect an additional . CC-BY-",
    "full_text_length": 47589,
    "chunk_length": 1246
  },
  {
    "chunk_id": 2122,
    "paper_filename": "micheal_2025_traige_wit_ai_rule_ot_framework_forrisk_and_benifits_for_mammography_automation.pdf",
    "paper_title": "Micheal 2025 Traige Wit Ai Rule Ot Framework Forrisk And Benifits For Mammography Automation",
    "chunk_index": 19,
    "total_chunks": 51,
    "text_content": "a CRR of about 36%. If radiologists detect an additional 70% of missed cancers, a threshold of Mirai=0.09 would achieve an AN-FOR of 0, which would translate into a CRR of about 53%. These CRR values can then be used to examine the corresponding number of false positives. As seen in Figure 2, the AI-FDR was between about 98% and 99% for all thresholds considered, indicating that FDR was largely stable. Given that false positives are unlikely to vary significantly, mainly because of low cancer pr",
    "full_text_length": 47589,
    "chunk_length": 1195
  },
  {
    "chunk_id": 2123,
    "paper_filename": "micheal_2025_traige_wit_ai_rule_ot_framework_forrisk_and_benifits_for_mammography_automation.pdf",
    "paper_title": "Micheal 2025 Traige Wit Ai Rule Ot Framework Forrisk And Benifits For Mammography Automation",
    "chunk_index": 20,
    "total_chunks": 51,
    "text_content": "using an AI triage model compared to standard of care. At the 0.20 threshold, the G-FOR, N-FOR, and AN-FOR (30%) are 0.26%, 0.017%, and 0.14%, respectively. This corresponds to 223, 141, and 121 missed cancer cases for the benefit of reading 85,220 fewer cases with an FDR of 97.8%. In contrast, at the 0.05 threshold, the G-FOR, N-FOR, and AN-FOR (30%) are 0.12%, 0.07%, and 0.00% (rounded), corresponding to 49, 30, and 0 missed cancer cases for the benefit of reading 41,127 fewer cases with an FD",
    "full_text_length": 47589,
    "chunk_length": 1274
  },
  {
    "chunk_id": 2124,
    "paper_filename": "micheal_2025_traige_wit_ai_rule_ot_framework_forrisk_and_benifits_for_mammography_automation.pdf",
    "paper_title": "Micheal 2025 Traige Wit Ai Rule Ot Framework Forrisk And Benifits For Mammography Automation",
    "chunk_index": 21,
    "total_chunks": 51,
    "text_content": "author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)The copyright holder for this preprint this version posted June 10, 2025. ; https://doi.org/10.1101/2025.04.25.25326396doi: medRxiv preprint purpose of this framework is not to advocate for a specific threshold or risk-benefit ratio but rather to demonstrate how a risk-benefit ratio could be quantified to inform policy and clinical implementation of AI triage. All numer",
    "full_text_length": 47589,
    "chunk_length": 1357
  },
  {
    "chunk_id": 2125,
    "paper_filename": "micheal_2025_traige_wit_ai_rule_ot_framework_forrisk_and_benifits_for_mammography_automation.pdf",
    "paper_title": "Micheal 2025 Traige Wit Ai Rule Ot Framework Forrisk And Benifits For Mammography Automation",
    "chunk_index": 22,
    "total_chunks": 51,
    "text_content": "using historical data. This estimation not only accounts for the type of errors (i.e., false positive and false negative) but also the number of errors (i.e., false discovery and omission rates instead of false positive and negative rates) . While our simulation focused on the number of any missed cancers, the type (e.g. in situ versus invasive) and stages of cancers missed by AI could be incorporated to further assess the clinical significance of triage-related errors. What is more, we only eva",
    "full_text_length": 47589,
    "chunk_length": 1230
  },
  {
    "chunk_id": 2126,
    "paper_filename": "micheal_2025_traige_wit_ai_rule_ot_framework_forrisk_and_benifits_for_mammography_automation.pdf",
    "paper_title": "Micheal 2025 Traige Wit Ai Rule Ot Framework Forrisk And Benifits For Mammography Automation",
    "chunk_index": 23,
    "total_chunks": 51,
    "text_content": "relative to standard of care. Fan et al. also propose evaluating AI triage using PPV and NPV. Our approach builds upon Fan and colleagues in two key ways. Namely, Fan et al. do not account for key counterfactuals such as cancers that would have been missed by radiologists without triage and cancer only detected with triage because of changes in radiologist performance. 16 In addition, Fan et al. propose using expected utility (EU) to assess AI triage. However, this relies on baseline relative ut",
    "full_text_length": 47589,
    "chunk_length": 1312
  },
  {
    "chunk_id": 2127,
    "paper_filename": "micheal_2025_traige_wit_ai_rule_ot_framework_forrisk_and_benifits_for_mammography_automation.pdf",
    "paper_title": "Micheal 2025 Traige Wit Ai Rule Ot Framework Forrisk And Benifits For Mammography Automation",
    "chunk_index": 24,
    "total_chunks": 51,
    "text_content": "Along with a framework for determining a threshold for AI triage of screening mammograms, there are several important considerations that must be addressed before AI triage can be implemented in clinical practice. First, prospective validation of AI rule-out strategies is needed. This validation will be important for understanding how AI triage impacts radiologist performance in the retained cases, such as increased recalls and increased cancer detection to empirically determine AN-FOR (whereas ",
    "full_text_length": 47589,
    "chunk_length": 1365
  },
  {
    "chunk_id": 2128,
    "paper_filename": "micheal_2025_traige_wit_ai_rule_ot_framework_forrisk_and_benifits_for_mammography_automation.pdf",
    "paper_title": "Micheal 2025 Traige Wit Ai Rule Ot Framework Forrisk And Benifits For Mammography Automation",
    "chunk_index": 25,
    "total_chunks": 51,
    "text_content": "regulatory landscape to allow AI triage in clinical practice. Addressing these considerations is necessary for the implementation of AI triage. Conclusion We present a framework for quantifying AI triage thresholds based on errors and benefits. Such a framework can help translate the potential of AI into strategies that help alleviate the growing workload pressures and resource limitations in radiology. Table 1. Key Metrics and Definitions Metric Definition Caseload Reduction Rate (CRR) The perc",
    "full_text_length": 47589,
    "chunk_length": 1290
  },
  {
    "chunk_id": 2129,
    "paper_filename": "micheal_2025_traige_wit_ai_rule_ot_framework_forrisk_and_benifits_for_mammography_automation.pdf",
    "paper_title": "Micheal 2025 Traige Wit Ai Rule Ot Framework Forrisk And Benifits For Mammography Automation",
    "chunk_index": 26,
    "total_chunks": 51,
    "text_content": "the case was retained for radiologist review. AI Gross False Omission Rate (G-FOR) Probability of a patient having breast cancer if ruled out by AI. . CC-BY-ND 4.0 International license It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)The copyright holder for this preprint this version posted June 10, 2025. ; https://doi.org/10.1101/2025.04.25.25326396doi: medRxiv preprint Metric De",
    "full_text_length": 47589,
    "chunk_length": 1285
  },
  {
    "chunk_id": 2130,
    "paper_filename": "micheal_2025_traige_wit_ai_rule_ot_framework_forrisk_and_benifits_for_mammography_automation.pdf",
    "paper_title": "Micheal 2025 Traige Wit Ai Rule Ot Framework Forrisk And Benifits For Mammography Automation",
    "chunk_index": 27,
    "total_chunks": 51,
    "text_content": "Rates Using Youden\u2019s J Threshold All Cases Youden\u2019s J Breast Cancer Error Rates Threshold AI Decision No Yes Total \u2264.20 Rule Out 84,997 223 85,220 G-FOR: 223/85,220 >.20 Retain 28,368 641 29,009 AI-FDR: 28,368/29,009 Total 113,365 864 114,229 AI Rule Out Cases \u2264.20 Radiologist Recall No Yes Total No 79,194 82 79,276 Yes 5,803 141 5,944 Numerator for N-FOR: 141 84,997 223 85,220 Denominator for all FORs: 85,220 . CC-BY-ND 4.0 International license It is made available under a is the author/funder",
    "full_text_length": 47589,
    "chunk_length": 1196
  },
  {
    "chunk_id": 2131,
    "paper_filename": "micheal_2025_traige_wit_ai_rule_ot_framework_forrisk_and_benifits_for_mammography_automation.pdf",
    "paper_title": "Micheal 2025 Traige Wit Ai Rule Ot Framework Forrisk And Benifits For Mammography Automation",
    "chunk_index": 28,
    "total_chunks": 51,
    "text_content": "Key for Table 4 Threshold Breast Cancer Error Rates Decision No Yes Total \u2264.A Rule Out F- (C+F) C+E F G-FOR: (C+E)/F >.A Retain K- (H+K) H+J K AI-FDR: [K-(H+K)]/K Rule Out Decision No Yes Total Radiologist Recall No B C B+C Yes D E D+E Numerator for G-FOR: E . CC-BY-ND 4.0 International license It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)The copyright holder for this preprint t",
    "full_text_length": 47589,
    "chunk_length": 1199
  },
  {
    "chunk_id": 2132,
    "paper_filename": "micheal_2025_traige_wit_ai_rule_ot_framework_forrisk_and_benifits_for_mammography_automation.pdf",
    "paper_title": "Micheal 2025 Traige Wit Ai Rule Ot Framework Forrisk And Benifits For Mammography Automation",
    "chunk_index": 29,
    "total_chunks": 51,
    "text_content": "black line). G-FOR is Gross False Omission Rate (solid red). N-FOR is Net False Omission Rate (longest dash, bright blue). AN-FOR 10% (long dash, light blue), AN-FOR 30% (short dash, medium blue), . CC-BY-ND 4.0 International license It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)The copyright holder for this preprint this version posted June 10, 2025. ; https://doi.org/10.1101/20",
    "full_text_length": 47589,
    "chunk_length": 1285
  },
  {
    "chunk_id": 2133,
    "paper_filename": "micheal_2025_traige_wit_ai_rule_ot_framework_forrisk_and_benifits_for_mammography_automation.pdf",
    "paper_title": "Micheal 2025 Traige Wit Ai Rule Ot Framework Forrisk And Benifits For Mammography Automation",
    "chunk_index": 30,
    "total_chunks": 51,
    "text_content": "is False Discovery Rate (60% to 100%). Thick black line is the relationship between Caseload Reduction Rate and False Discovery Rate Youden refers to Youden\u2019s J (thin black line). AN-FOR 10% (long dash, light blue), AN-FOR 30% (short dash, medium blue), AN-FOR 50% (short dash, dark blue), AN-FOR 70% (shortest dash, grey blue) refer to the Adjusted Net . CC-BY-ND 4.0 International license It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint ",
    "full_text_length": 47589,
    "chunk_length": 1296
  },
  {
    "chunk_id": 2134,
    "paper_filename": "micheal_2025_traige_wit_ai_rule_ot_framework_forrisk_and_benifits_for_mammography_automation.pdf",
    "paper_title": "Micheal 2025 Traige Wit Ai Rule Ot Framework Forrisk And Benifits For Mammography Automation",
    "chunk_index": 31,
    "total_chunks": 51,
    "text_content": "on-call hours: dramatic increase in the past 15 years. Insights Imaging 11, 121 (2020). 2. Burns, J., Chung, Y., Rula, E.Y ., Duszak, R., Jr. & Rosenkrantz, A.B. Evolving Trainee Participation in Radiologists' Workload Using A National Medicare- Focused Analysis From 2008 to 2020. J Am Coll Radiol 22, 98-107 (2025). 3. Harry, E. , et al. Physician Task Load and the Risk of Burnout Among US Physicians in a National Survey. Jt Comm J Qual Patient Saf 47, 76-85 (2021). 4. Chetlen, A.L. , et al. Add",
    "full_text_length": 47589,
    "chunk_length": 1214
  },
  {
    "chunk_id": 2135,
    "paper_filename": "micheal_2025_traige_wit_ai_rule_ot_framework_forrisk_and_benifits_for_mammography_automation.pdf",
    "paper_title": "Micheal 2025 Traige Wit Ai Rule Ot Framework Forrisk And Benifits For Mammography Automation",
    "chunk_index": 32,
    "total_chunks": 51,
    "text_content": "82 (2023). 7. van Winkel, S.L. , et al. Impact of artificial intelligence support on accuracy and reading time in breast tomosynthesis image interpretation: a multi-reader multi-case study. Eur Radiol 31, 8682-8691 (2021). 8. Conant, E.F. , et al. Improving Accuracy and Efficiency with Concurrent Use of Artificial Intelligence for Digital Breast Tomosynthesis. Radiol Artif Intell 1, e180096 (2019). . CC-BY-ND 4.0 International license It is made available under a is the author/funder, who has gr",
    "full_text_length": 47589,
    "chunk_length": 1325
  },
  {
    "chunk_id": 2136,
    "paper_filename": "micheal_2025_traige_wit_ai_rule_ot_framework_forrisk_and_benifits_for_mammography_automation.pdf",
    "paper_title": "Micheal 2025 Traige Wit Ai Rule Ot Framework Forrisk And Benifits For Mammography Automation",
    "chunk_index": 33,
    "total_chunks": 51,
    "text_content": "Ansell, D. Recall and Cancer Detection Rates for Screening Mammography: Finding the Sweet Spot. AJR Am J Roentgenol 208, 208-213 (2017). 11. Larsen, M., Aglen, C.F., Hoff, S.R., Lund-Hanssen, H. & Hofvind, S. Possible strategies for use of artificial intelligence in screen-reading of mammograms, based on retrospective data from 122,969 screening examinations. Eur Radiol 32, 8238-8246 (2022). 12. Rodriguez-Ruiz, A. , et al. Can we reduce the workload of mammographic screening by automatic identif",
    "full_text_length": 47589,
    "chunk_length": 1374
  },
  {
    "chunk_id": 2137,
    "paper_filename": "micheal_2025_traige_wit_ai_rule_ot_framework_forrisk_and_benifits_for_mammography_automation.pdf",
    "paper_title": "Micheal 2025 Traige Wit Ai Rule Ot Framework Forrisk And Benifits For Mammography Automation",
    "chunk_index": 34,
    "total_chunks": 51,
    "text_content": "no significant findings in a primary health care setting in Oulu, Finland. arXiv (2022). 16. Kwok Lung Fan, Y .L.E.T., Weijie Chen, Craig K. Abbey, Frank W Samuelson. Use of Expected Utility (EU) to Evaluate Artificial Intelligence-Enabled Rule-Out Devices for Mammography Screening. arXiv (2024). 17. Obuchowski, N.A. & Bullen, J.A. Statistical considerations for testing an AI algorithm used for prescreening lung CT images. Contemp Clin Trials Commun 16, 100434 (2019). 18. Krupinski, E.A. Artific",
    "full_text_length": 47589,
    "chunk_length": 1382
  },
  {
    "chunk_id": 2138,
    "paper_filename": "micheal_2025_traige_wit_ai_rule_ot_framework_forrisk_and_benifits_for_mammography_automation.pdf",
    "paper_title": "Micheal 2025 Traige Wit Ai Rule Ot Framework Forrisk And Benifits For Mammography Automation",
    "chunk_index": 35,
    "total_chunks": 51,
    "text_content": "2025. ; https://doi.org/10.1101/2025.04.25.25326396doi: medRxiv preprint 20. L\u00e5ng, K., Hofvind, S., Rodr\u00edguez-Ruiz, A. & Andersson, I. Can artificial intelligence reduce the interval cancer rate in mammography screening? Eur Radiol 31, 5940-5947 (2021). 21. Yala, A., Schuster, T., Miles, R., Barzilay, R. & Lehman, C. A Deep Learning Model to Triage Screening Mammograms: A Simulation Study. Radiology 293, 38-46 (2019). 22. Dembrower, K. , et al. Effect of artificial intelligence-based triaging of",
    "full_text_length": 47589,
    "chunk_length": 1348
  },
  {
    "chunk_id": 2139,
    "paper_filename": "micheal_2025_traige_wit_ai_rule_ot_framework_forrisk_and_benifits_for_mammography_automation.pdf",
    "paper_title": "Micheal 2025 Traige Wit Ai Rule Ot Framework Forrisk And Benifits For Mammography Automation",
    "chunk_index": 36,
    "total_chunks": 51,
    "text_content": "et al. Can incorrect artificial intelligence (AI) results impact radiologists, and if so, what can we do about it? A multi-reader pilot study of lung cancer detection with chest radiography. Eur Radiol 33, 8263-8269 (2023). 26. Krupinski, E.A., Berbaum, K.S., Caldwell, R.T., Schartz, K.M. & Kim, J. Long radiology workdays reduce detection and accommodation accuracy. J Am Coll Radiol 7, 698-704 (2010). 27. Wolfe, J.M. , et al. Low target prevalence is a stubborn source of errors in visual search ",
    "full_text_length": 47589,
    "chunk_length": 1271
  },
  {
    "chunk_id": 2140,
    "paper_filename": "micheal_2025_traige_wit_ai_rule_ot_framework_forrisk_and_benifits_for_mammography_automation.pdf",
    "paper_title": "Micheal 2025 Traige Wit Ai Rule Ot Framework Forrisk And Benifits For Mammography Automation",
    "chunk_index": 37,
    "total_chunks": 51,
    "text_content": "J Clin Oncol 40, 1732-1740 (2022). 31. Yala, A. , et al. Toward robust mammography-based models for breast cancer risk. Sci Transl Med 13(2021). . CC-BY-ND 4.0 International license It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)The copyright holder for this preprint this version posted June 10, 2025. ; https://doi.org/10.1101/2025.04.25.25326396doi: medRxiv preprint . CC-BY-ND 4.",
    "full_text_length": 47589,
    "chunk_length": 1193
  },
  {
    "chunk_id": 2141,
    "paper_filename": "micheal_2025_traige_wit_ai_rule_ot_framework_forrisk_and_benifits_for_mammography_automation.pdf",
    "paper_title": "Micheal 2025 Traige Wit Ai Rule Ot Framework Forrisk And Benifits For Mammography Automation",
    "chunk_index": 38,
    "total_chunks": 51,
    "text_content": "% 10% 30% 50% 70% % % 0.01 104189 148 9176 716 114229 114229 99.24 0.02 10691 1 593 2 11287 93498 147 8583 714 102942 114229 0.03 0.02 -0.11 -0.37 -0.63 -0.89 99.16 9.88 0.03 22684 6 1345 11 24046 81505 142 7831 705 90183 114229 0.07 0.05 -0.01 -0.13 -0.25 -0.37 99.06 21.05 0.04 31603 13 1932 18 33566 72586 135 7244 698 80663 114229 0.09 0.05 0.01 -0.07 -0.15 -0.23 98.97 29.38 0.05 38634 19 2444 30 41127 65555 129 6732 686 73102 114229 0.12 0.07 0.04 -0.02 -0.08 -0.15 98.89 36.00 0.06 44185 27 2",
    "full_text_length": 47589,
    "chunk_length": 1022
  },
  {
    "chunk_id": 2142,
    "paper_filename": "micheal_2025_traige_wit_ai_rule_ot_framework_forrisk_and_benifits_for_mammography_automation.pdf",
    "paper_title": "Micheal 2025 Traige Wit Ai Rule Ot Framework Forrisk And Benifits For Mammography Automation",
    "chunk_index": 39,
    "total_chunks": 51,
    "text_content": "0.04 0.01 -0.03 98.67 49.57 0.09 56547 44 3837 65 60493 47642 104 5339 651 53736 114229 0.18 0.11 0.09 0.06 0.02 -0.01 98.59 52.96 0.10 59695 48 4109 75 63927 44494 100 5067 641 50302 114229 0.19 0.12 0.10 0.07 0.04 0.01 98.53 55.96 0.11 62487 51 4337 84 66959 41702 97 4839 632 47270 114229 0.20 0.13 0.11 0.08 0.05 0.02 98.46 58.62 0.12 65099 57 4541 93 69790 39090 91 4635 623 44439 114229 0.21 0.13 0.12 0.09 0.07 0.04 98.39 61.10 0.13 67432 62 4741 98 72333 36757 86 4435 618 41896 114229 0.22 0",
    "full_text_length": 47589,
    "chunk_length": 1018
  },
  {
    "chunk_id": 2143,
    "paper_filename": "micheal_2025_traige_wit_ai_rule_ot_framework_forrisk_and_benifits_for_mammography_automation.pdf",
    "paper_title": "Micheal 2025 Traige Wit Ai Rule Ot Framework Forrisk And Benifits For Mammography Automation",
    "chunk_index": 40,
    "total_chunks": 51,
    "text_content": "78748 30879 73 3930 599 35481 114229 0.24 0.15 0.14 0.12 0.10 0.08 98.11 68.94 0.17 74938 77 5387 125 80527 29251 71 3789 591 33702 114229 0.25 0.16 0.15 0.13 0.11 0.09 98.04 70.50 0.18 76506 80 5530 130 82246 27683 68 3646 586 31983 114229 0.26 0.16 0.15 0.13 0.12 0.10 97.96 72.00 0.19 77918 80 5657 135 83790 26271 68 3519 581 30439 114229 0.26 0.16 0.15 0.14 0.12 0.10 97.87 73.35 0.20 79194 82 5803 141 85220 24995 66 3373 575 29009 114229 0.26 0.17 0.16 0.14 0.13 0.11 97.79 74.60 0.21 80400 85",
    "full_text_length": 47589,
    "chunk_length": 1022
  },
  {
    "chunk_id": 2144,
    "paper_filename": "micheal_2025_traige_wit_ai_rule_ot_framework_forrisk_and_benifits_for_mammography_automation.pdf",
    "paper_title": "Micheal 2025 Traige Wit Ai Rule Ot Framework Forrisk And Benifits For Mammography Automation",
    "chunk_index": 41,
    "total_chunks": 51,
    "text_content": "0.16 0.15 0.14 97.58 77.98 0.24 83731 100 6250 167 90248 20458 48 2926 549 23981 114229 0.30 0.19 0.18 0.17 0.16 0.15 97.51 79.01 0.25 84690 102 6352 175 91319 19499 46 2824 541 22910 114229 0.30 0.19 0.19 0.18 0.17 0.16 97.44 79.94 0.26 85677 104 6446 179 92406 18512 44 2730 537 21823 114229 0.31 0.19 0.19 0.18 0.17 0.16 97.34 80.90 0.27 86525 106 6545 186 93362 17664 42 2631 530 20867 114229 0.31 0.20 0.19 0.19 0.18 0.17 97.26 81.73 . CC-BY-ND 4.0 International license It is made available und",
    "full_text_length": 47589,
    "chunk_length": 1125
  },
  {
    "chunk_id": 2145,
    "paper_filename": "micheal_2025_traige_wit_ai_rule_ot_framework_forrisk_and_benifits_for_mammography_automation.pdf",
    "paper_title": "Micheal 2025 Traige Wit Ai Rule Ot Framework Forrisk And Benifits For Mammography Automation",
    "chunk_index": 42,
    "total_chunks": 51,
    "text_content": "0.17 97.16 82.55 0.29 88174 109 6742 198 95223 16015 39 2434 518 19006 114229 0.32 0.21 0.20 0.20 0.19 0.18 97.07 83.36 0.30 88942 110 6822 205 96079 15247 38 2354 511 18150 114229 0.33 0.21 0.21 0.20 0.19 0.19 96.98 84.11 0.31 89608 111 6904 212 96835 14581 37 2272 504 17394 114229 0.33 0.22 0.22 0.21 0.20 0.19 96.89 84.77 0.32 90256 113 6992 217 97578 13933 35 2184 499 16651 114229 0.34 0.22 0.22 0.21 0.20 0.20 96.79 85.42 0.33 90865 113 7069 225 98272 13324 35 2107 491 15957 114229 0.34 0.23 ",
    "full_text_length": 47589,
    "chunk_length": 1032
  },
  {
    "chunk_id": 2146,
    "paper_filename": "micheal_2025_traige_wit_ai_rule_ot_framework_forrisk_and_benifits_for_mammography_automation.pdf",
    "paper_title": "Micheal 2025 Traige Wit Ai Rule Ot Framework Forrisk And Benifits For Mammography Automation",
    "chunk_index": 43,
    "total_chunks": 51,
    "text_content": "31 1896 473 13968 114229 0.36 0.24 0.24 0.23 0.23 0.22 96.39 87.77 0.37 93160 118 7342 248 100868 11029 30 1834 468 13361 114229 0.36 0.25 0.24 0.24 0.23 0.23 96.27 88.30 0.38 93678 119 7404 255 101456 10511 29 1772 461 12773 114229 0.37 0.25 0.25 0.24 0.24 0.23 96.16 88.82 0.39 94170 120 7462 259 102011 10019 28 1714 457 12218 114229 0.37 0.25 0.25 0.25 0.24 0.23 96.03 89.30 0.40 94655 123 7529 269 102576 9534 25 1647 447 11653 114229 0.38 0.26 0.26 0.25 0.25 0.25 95.95 89.80 0.41 95197 123 759",
    "full_text_length": 47589,
    "chunk_length": 1029
  },
  {
    "chunk_id": 2147,
    "paper_filename": "micheal_2025_traige_wit_ai_rule_ot_framework_forrisk_and_benifits_for_mammography_automation.pdf",
    "paper_title": "Micheal 2025 Traige Wit Ai Rule Ot Framework Forrisk And Benifits For Mammography Automation",
    "chunk_index": 44,
    "total_chunks": 51,
    "text_content": "0.26 95.43 91.29 0.44 96594 126 7791 294 104805 7595 22 1385 422 9424 114229 0.40 0.28 0.28 0.27 0.27 0.27 95.29 91.75 0.45 96997 127 7872 299 105295 7192 21 1304 417 8934 114229 0.40 0.28 0.28 0.28 0.27 0.27 95.10 92.18 0.46 97421 129 7926 310 105786 6768 19 1250 406 8443 114229 0.41 0.29 0.29 0.29 0.28 0.28 94.97 92.61 0.47 97800 129 7987 317 106233 6389 19 1189 399 7996 114229 0.42 0.30 0.30 0.29 0.29 0.29 94.77 93.00 0.48 98195 130 8032 323 106680 5994 18 1144 393 7549 114229 0.42 0.30 0.30 ",
    "full_text_length": 47589,
    "chunk_length": 1016
  },
  {
    "chunk_id": 2148,
    "paper_filename": "micheal_2025_traige_wit_ai_rule_ot_framework_forrisk_and_benifits_for_mammography_automation.pdf",
    "paper_title": "Micheal 2025 Traige Wit Ai Rule Ot Framework Forrisk And Benifits For Mammography Automation",
    "chunk_index": 45,
    "total_chunks": 51,
    "text_content": "16 994 363 6318 114229 0.45 0.33 0.33 0.32 0.32 0.32 94.00 94.47 0.52 99574 135 8223 362 108294 4615 13 953 354 5935 114229 0.46 0.33 0.33 0.33 0.33 0.33 93.82 94.80 0.53 99874 136 8285 367 108662 4315 12 891 349 5567 114229 0.46 0.34 0.34 0.33 0.33 0.33 93.52 95.13 0.54 100186 138 8338 374 109036 4003 10 838 342 5193 114229 0.47 0.34 0.34 0.34 0.34 0.34 93.22 95.45 0.55 100440 138 8381 375 109334 3749 10 795 341 4895 114229 0.47 0.34 0.34 0.34 0.34 0.34 92.83 95.71 0.56 100726 138 8429 389 1096",
    "full_text_length": 47589,
    "chunk_length": 1109
  },
  {
    "chunk_id": 2149,
    "paper_filename": "micheal_2025_traige_wit_ai_rule_ot_framework_forrisk_and_benifits_for_mammography_automation.pdf",
    "paper_title": "Micheal 2025 Traige Wit Ai Rule Ot Framework Forrisk And Benifits For Mammography Automation",
    "chunk_index": 46,
    "total_chunks": 51,
    "text_content": "0.36 91.74 96.60 . CC-BY-ND 4.0 International license It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)The copyright holder for this preprint this version posted June 10, 2025. ; https://doi.org/10.1101/2025.04.25.25326396doi: medRxiv preprint 0.59 101504 138 8555 412 110609 2685 10 621 304 3620 114229 0.50 0.37 0.37 0.37 0.37 0.37 91.33 96.83 0.60 101721 138 8596 419 110874 2468 10",
    "full_text_length": 47589,
    "chunk_length": 1112
  },
  {
    "chunk_id": 2150,
    "paper_filename": "micheal_2025_traige_wit_ai_rule_ot_framework_forrisk_and_benifits_for_mammography_automation.pdf",
    "paper_title": "Micheal 2025 Traige Wit Ai Rule Ot Framework Forrisk And Benifits For Mammography Automation",
    "chunk_index": 47,
    "total_chunks": 51,
    "text_content": "97.72 0.64 102525 139 8734 463 111861 1664 9 442 253 2368 114229 0.54 0.41 0.41 0.41 0.41 0.41 88.94 97.93 0.65 102681 139 8767 472 112059 1508 9 409 244 2170 114229 0.55 0.42 0.42 0.42 0.42 0.42 88.34 98.10 0.66 102847 140 8799 480 112266 1342 8 377 236 1963 114229 0.55 0.43 0.43 0.43 0.42 0.42 87.57 98.28 0.67 103014 141 8833 489 112477 1175 7 343 227 1752 114229 0.56 0.43 0.43 0.43 0.43 0.43 86.64 98.47 0.68 103153 142 8864 496 112655 1036 6 312 220 1574 114229 0.57 0.44 0.44 0.44 0.44 0.44 8",
    "full_text_length": 47589,
    "chunk_length": 1002
  },
  {
    "chunk_id": 2151,
    "paper_filename": "micheal_2025_traige_wit_ai_rule_ot_framework_forrisk_and_benifits_for_mammography_automation.pdf",
    "paper_title": "Micheal 2025 Traige Wit Ai Rule Ot Framework Forrisk And Benifits For Mammography Automation",
    "chunk_index": 48,
    "total_chunks": 51,
    "text_content": "193 1095 114229 0.59 0.46 0.46 0.46 0.46 0.46 82.10 99.04 0.72 103608 145 8996 535 113284 581 3 180 181 945 114229 0.60 0.47 0.47 0.47 0.47 0.47 80.53 99.17 0.73 103694 145 9018 541 113398 495 3 158 175 831 114229 0.60 0.48 0.48 0.48 0.48 0.48 78.58 99.27 0.74 103763 145 9043 553 113504 426 3 133 163 725 114229 0.61 0.49 0.49 0.49 0.49 0.49 77.10 99.37 0.75 103817 145 9053 565 113580 372 3 123 151 649 114229 0.63 0.50 0.50 0.50 0.50 0.50 76.27 99.43 0.76 103871 145 9068 580 113664 318 3 108 136 ",
    "full_text_length": 47589,
    "chunk_length": 984
  },
  {
    "chunk_id": 2152,
    "paper_filename": "micheal_2025_traige_wit_ai_rule_ot_framework_forrisk_and_benifits_for_mammography_automation.pdf",
    "paper_title": "Micheal 2025 Traige Wit Ai Rule Ot Framework Forrisk And Benifits For Mammography Automation",
    "chunk_index": 49,
    "total_chunks": 51,
    "text_content": "99.62 0.79 104003 145 9108 612 113868 186 3 68 104 361 114229 0.66 0.54 0.54 0.54 0.54 0.54 70.36 99.68 0.80 104025 145 9117 622 113909 164 3 59 94 320 114229 0.67 0.55 0.55 0.55 0.54 0.54 69.69 99.72 0.81 104063 146 9128 628 113965 126 2 48 88 264 114229 0.68 0.55 0.55 0.55 0.55 0.55 65.91 99.77 0.82 104081 146 9135 633 113995 108 2 41 83 234 114229 0.68 0.56 0.56 0.55 0.55 0.55 63.68 99.80 0.83 104098 146 9141 641 114026 91 2 35 75 203 114229 0.69 0.56 0.56 0.56 0.56 0.56 62.07 99.82 0.84 1041",
    "full_text_length": 47589,
    "chunk_length": 1024
  },
  {
    "chunk_id": 2153,
    "paper_filename": "micheal_2025_traige_wit_ai_rule_ot_framework_forrisk_and_benifits_for_mammography_automation.pdf",
    "paper_title": "Micheal 2025 Traige Wit Ai Rule Ot Framework Forrisk And Benifits For Mammography Automation",
    "chunk_index": 50,
    "total_chunks": 51,
    "text_content": "under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)The copyright holder for this preprint this version posted June 10, 2025. ; https://doi.org/10.1101/2025.04.25.25326396doi: medRxiv preprint",
    "full_text_length": 47589,
    "chunk_length": 283
  },
  {
    "chunk_id": 2154,
    "paper_filename": "ming_2024_multimodal_generative_aI_copilot_for_human_pathology.pdf",
    "paper_title": "Ming 2024 Multimodal Generative Ai Copilot For Human Pathology",
    "chunk_index": 0,
    "total_chunks": 92,
    "text_content": "466 | Nature | Vol 634 | 10 October 2024 ArticleA multimodal generative AI copilot for human pathology Ming Y . Lu1,2,3,4 ,11, Bowen Chen1,2,11, Drew F. K. Williamson1,2,3 ,11, Richard J. Chen1,2,3, Melissa Zhao1,2, Aaron K. Chow5, Kenji Ikemura1,2, Ahrong Kim1,6, Dimitra Pouli1,2, Ankush Patel7, Amr Soliman5, Chengkuan Chen1, Tong Ding1,8, Judy J. Wang1, Georg Gerber1, Ivy Liang1,8, Long Phi Le2, Anil V. Parwani5, Luca L. Weishaupt1,9 & Faisal Mahmood1,2,3,10 \u2709 Computational pathology1,2 has wi",
    "full_text_length": 93433,
    "chunk_length": 1460
  },
  {
    "chunk_id": 2155,
    "paper_filename": "ming_2024_multimodal_generative_aI_copilot_for_human_pathology.pdf",
    "paper_title": "Ming 2024 Multimodal Generative Ai Copilot For Human Pathology",
    "chunk_index": 1,
    "total_chunks": 92,
    "text_content": "fine-tuning the whole system on over 456,000 diverse visual- language instructions consisting of 999,202 question and answer turns. We compare PathChat with several multimodal vision-language AI assistants and GPT-4V, which powers the commercially available multimodal general-purpose AI assistant ChatGPT-4 (ref. 6 ). PathChat achieved state-of-the-art performance on multiple- choice diagnostic questions from cases with diverse tissue origins and disease models. Furthermore, using open-ended ques",
    "full_text_length": 93433,
    "chunk_length": 1603
  },
  {
    "chunk_id": 2156,
    "paper_filename": "ming_2024_multimodal_generative_aI_copilot_for_human_pathology.pdf",
    "paper_title": "Ming 2024 Multimodal Generative Ai Copilot For Human Pathology",
    "chunk_index": 2,
    "total_chunks": 92,
    "text_content": "intelligence (AI) research, increased accessibility of large datasets and substantial high-performance computing resources1,2,7. With varying degrees of success, researchers have leveraged deep learning to address a diverse range of tasks, including cancer subtyping8,9 and grading10,11, metasta - sis detection12, survival13\u201317 and response-to-treatment prediction18,19, tumour site of origin prediction20,21, mutation prediction and biomarker screening22\u201324, and more25. Moreover, general-purpose v",
    "full_text_length": 93433,
    "chunk_length": 1575
  },
  {
    "chunk_id": 2157,
    "paper_filename": "ming_2024_multimodal_generative_aI_copilot_for_human_pathology.pdf",
    "paper_title": "Ming 2024 Multimodal Generative Ai Copilot For Human Pathology",
    "chunk_index": 3,
    "total_chunks": 92,
    "text_content": "and end users. Notably, in general machine learning, representative works27,28 have demonstrated that large-scale vision-language representation learning can augment vision-only AI models with new capabilities, including zero-shot image recognition and text-to-image retrieval. Depending on the architectural design, training data and objectives, pretrained visual-language systems can often be fine-tuned for tailored tasks ranging from answering visual questions and image captioning to object dete",
    "full_text_length": 93433,
    "chunk_length": 1576
  },
  {
    "chunk_id": 2158,
    "paper_filename": "ming_2024_multimodal_generative_aI_copilot_for_human_pathology.pdf",
    "paper_title": "Ming 2024 Multimodal Generative Ai Copilot For Human Pathology",
    "chunk_index": 4,
    "total_chunks": 92,
    "text_content": "and https://doi.org/10.1038/s41586-024-07618-3 Received: 11 December 2023 Accepted: 28 May 2024 Published online: 12 June 2024 Open access Check for updates 1Department of Pathology, Brigham and Women\u2019s Hospital, Harvard Medical School, Boston, MA, USA. 2Department of Pathology, Massachusetts General Hospital, Harvard Medical School, Boston, MA, USA. 3Cancer Program, Broad Institute of Harvard and MIT, Cambridge, MA, USA. 4Electrical Engineering and Computer Science, Massachusetts Institute of T",
    "full_text_length": 93433,
    "chunk_length": 1475
  },
  {
    "chunk_id": 2159,
    "paper_filename": "ming_2024_multimodal_generative_aI_copilot_for_human_pathology.pdf",
    "paper_title": "Ming 2024 Multimodal Generative Ai Copilot For Human Pathology",
    "chunk_index": 5,
    "total_chunks": 92,
    "text_content": "Vol 634 | 10 October 2024 | 467pathology trainees alike, these models are not yet ready to serve as interactive companions (or copilots) that can follow diverse instruc - tions and coherently and accurately answer complex open-ended ques - tions posed in natural language. Following the rise of large language models (LLMs)44\u2013 47, rapid advances in multimodal LLMs (MLLMs)5,48,49 and the broader field of generative AI50 are poised to open a new frontier for computational pathology, one that emphasi",
    "full_text_length": 93433,
    "chunk_length": 1392
  },
  {
    "chunk_id": 2160,
    "paper_filename": "ming_2024_multimodal_generative_aI_copilot_for_human_pathology.pdf",
    "paper_title": "Ming 2024 Multimodal Generative Ai Copilot For Human Pathology",
    "chunk_index": 6,
    "total_chunks": 92,
    "text_content": "Although there have been attempts to investigate their performance on answering medicine-related queries, their capability to assist pro - fessionals and researchers in the highly specialized but important subfield of anatomic pathology remains relatively unexplored52\u201357. Yet, the potential applications of an interactive multimodal AI copilot for pathology are immense. The ability to understand and respond to com - plex queries in natural language could, in theory, enable such a copilot for path",
    "full_text_length": 93433,
    "chunk_length": 1346
  },
  {
    "chunk_id": 2161,
    "paper_filename": "ming_2024_multimodal_generative_aI_copilot_for_human_pathology.pdf",
    "paper_title": "Ming 2024 Multimodal Generative Ai Copilot For Human Pathology",
    "chunk_index": 7,
    "total_chunks": 92,
    "text_content": "If deemed reasonable, the user could then request helpful suggestions for ancillary testing and immunohistochemical (IHC) stains to narrow down the differential. Finally, the results of such tests could also be provided to the model, which would then make a final deduction and arrive at a diagnosis. In research, a multimodal AI copilot that can summarize the mor - phological features of large cohorts of histopathology images would potentially enable automated quantification and interpretation of",
    "full_text_length": 93433,
    "chunk_length": 1360
  },
  {
    "chunk_id": 2162,
    "paper_filename": "ming_2024_multimodal_generative_aI_copilot_for_human_pathology.pdf",
    "paper_title": "Ming 2024 Multimodal Generative Ai Copilot For Human Pathology",
    "chunk_index": 8,
    "total_chunks": 92,
    "text_content": "visual and natural language inputs, we began with UNI3, a state-of-the-art (SOTA) vision-only encoder pretrained on over 100 million histology image patches from over 100,000 slides using self-supervised learning. We performed further vision-language pretraining on the UNI encoder with 1.18 million pathology image and caption pairs to align its image representation space with that of pathology text34. The resulting vision encoder was subsequently connected to a 13-billion-parameter pretrained, L",
    "full_text_length": 93433,
    "chunk_length": 1344
  },
  {
    "chunk_id": 2163,
    "paper_filename": "ming_2024_multimodal_generative_aI_copilot_for_human_pathology.pdf",
    "paper_title": "Ming 2024 Multimodal Generative Ai Copilot For Human Pathology",
    "chunk_index": 9,
    "total_chunks": 92,
    "text_content": "and training of the Path - Chat model\u2019 in Methods, respectively, with further details summarized in Supplementary Tables 1\u20134. We demonstrate the capabilities of PathChat in various applica - tions including an analysis of pathology cases from diverse organ sites and practices (Figs. 2 and 3 ). Additionally, we curated a high-quality Case reports Educational notesImage captionsQ: A:Q: A: Q: A:Q: A:Free response Multiple choice Conversation DescriptionQ: A:Instruction dataset (n = 456,916) Q: A:Te",
    "full_text_length": 93433,
    "chunk_length": 1359
  },
  {
    "chunk_id": 2164,
    "paper_filename": "ming_2024_multimodal_generative_aI_copilot_for_human_pathology.pdf",
    "paper_title": "Ming 2024 Multimodal Generative Ai Copilot For Human Pathology",
    "chunk_index": 10,
    "total_chunks": 92,
    "text_content": "clear cell carcinomaUterine carcinosarcoma Uterine carcinosarcoma What is shown in this image? Upon examining the image\u2026 several key /f_indings are observed\u2026the most likely diagnosis is a\u2026What is the primary architectural pattern\u2026? The primary architectural patterns are\u2026 What type of epithelial cells are observed in this tumour? The epithelial cells observed in this tumour are described as\u2026\u2026Could you describe the key morphological features observed in Sertoli\u2013Leydig cell tumours (SLCT)\u2026? Sertoli",
    "full_text_length": 93433,
    "chunk_length": 1554
  },
  {
    "chunk_id": 2165,
    "paper_filename": "ming_2024_multimodal_generative_aI_copilot_for_human_pathology.pdf",
    "paper_title": "Ming 2024 Multimodal Generative Ai Copilot For Human Pathology",
    "chunk_index": 11,
    "total_chunks": 92,
    "text_content": "questions and short answers; see Extended Data Fig. 1 for complete examples) from diverse sources. b , To build an MLLM-based vision- language AI assistant that can reason over visual and natural language inputs, we began with a SOTA, vision-only, self-supervised, pretrained, foundation, encoder model, UNI and performed further vision-language pretraining analogous to CONCH. The resulting vision encoder was subsequently connected to a 13-billion-parameter, pretrained, Llama 2 LLM through a multi",
    "full_text_length": 93433,
    "chunk_length": 1367
  },
  {
    "chunk_id": 2166,
    "paper_filename": "ming_2024_multimodal_generative_aI_copilot_for_human_pathology.pdf",
    "paper_title": "Ming 2024 Multimodal Generative Ai Copilot For Human Pathology",
    "chunk_index": 12,
    "total_chunks": 92,
    "text_content": "for evaluating the performance of MLLMs in pathology, which we curated with expert supervision (see \u2018Benchmark for expert-curated pathol - ogy questions\u2019 in Methods for more details). We compare PathChat to both LLaVA5, a SOTA general-domain open-source MLLM, and LLaVA-Med53, which has been tailored to the biomedical domain. We also compare it with a SOTA commercial solution, ChatGPT-4 (powered by GPT-4V), despite our model being significantly smaller and cheaper to serve. Performance on multipl",
    "full_text_length": 93433,
    "chunk_length": 1363
  },
  {
    "chunk_id": 2167,
    "paper_filename": "ming_2024_multimodal_generative_aI_copilot_for_human_pathology.pdf",
    "paper_title": "Ming 2024 Multimodal Generative Ai Copilot For Human Pathology",
    "chunk_index": 13,
    "total_chunks": 92,
    "text_content": "11 different major pathology practices and organ sites (Supplementary Tables 5 and 6). For each organ system, the pathologist selected a set of ten possible answers that encompassed the correct answers for all questions within that organ system as well as other relatively common diagnoses within that organ system (Supplementary Table 7). For each question, we considered two evaluation strategies. In the first (image-only setting), the model was presented with only the image and the multiple-choi",
    "full_text_length": 93433,
    "chunk_length": 1301
  },
  {
    "chunk_id": 2168,
    "paper_filename": "ming_2024_multimodal_generative_aI_copilot_for_human_pathology.pdf",
    "paper_title": "Ming 2024 Multimodal Generative Ai Copilot For Human Pathology",
    "chunk_index": 14,
    "total_chunks": 92,
    "text_content": "an illustrative example of the complete model input in Fig. 2a. For all cases (denoted as \u2018Combined\u2019 in Fig. 2b), we compared PathChat against LLaVA 1.5, a SOTA general-purpose visual-language chatbot assistant, and LLaVA-Med, a specialized ver - sion of LLaVA fine-tuned for answering biomedical-related queries. For the subset of 52 cases derived from publicly available WSIs (denoted as PathQABench-Public), in addition to LLaVA 1.5 and LLaVA-Med, we also compared PathChat with GPT-4V, which powe",
    "full_text_length": 93433,
    "chunk_length": 1347
  },
  {
    "chunk_id": 2169,
    "paper_filename": "ming_2024_multimodal_generative_aI_copilot_for_human_pathology.pdf",
    "paper_title": "Ming 2024 Multimodal Generative Ai Copilot For Human Pathology",
    "chunk_index": 15,
    "total_chunks": 92,
    "text_content": "accuracy of 78.1% (+52.4% versus LLaVA 1.5 and +63.8% versus LLaVA-Med, P < 0.001 for both) on the full combined benchmark. In line with our expectation, the accuracy of PathChat improved to 89.5% (+39.0% versus LLaVA 1.5 and +60.9% versus LLaVA-Med, P < 0.001 for both) when useful clinical context was provided. Specifically, note that the addition of clinical context consistently improved the accuracy of PathChat for both the private in-house cases (PathQABench-Private, +11.3%) and the public T",
    "full_text_length": 93433,
    "chunk_length": 1457
  },
  {
    "chunk_id": 2170,
    "paper_filename": "ming_2024_multimodal_generative_aI_copilot_for_human_pathology.pdf",
    "paper_title": "Ming 2024 Multimodal Generative Ai Copilot For Human Pathology",
    "chunk_index": 16,
    "total_chunks": 92,
    "text_content": "histology images than when simply given such non-visual information in plain natural language without specialized data processing. Additionally, using PathQABench-Public, which contains cases only from the publicly available TCGA WSIs, we also compared our model Combined Combined with contextPathQABench -PublicPathQABench -Public with contextPathQABench -PrivatePathQABench -Private with context00.250.500.751.00AccuracyPathChat LLaVA-Med LLaVA v.1.5 GPT-4Va bA 63-year-old male presents with chron",
    "full_text_length": 93433,
    "chunk_length": 1535
  },
  {
    "chunk_id": 2171,
    "paper_filename": "ming_2024_multimodal_generative_aI_copilot_for_human_pathology.pdf",
    "paper_title": "Ming 2024 Multimodal Generative Ai Copilot For Human Pathology",
    "chunk_index": 17,
    "total_chunks": 92,
    "text_content": "example of a multiple-choice diagnostic question. The input always includes a salient ROI of an histology image selected by a board-certified anatomic pathologist and an instruction to select the most probable diagnosis from a set of possible choices. In the image + clinical context evaluation setting, which was designed to more closely mimic a real-world diagnostic workflow, relevant clinical context (designed by the pathologist, shown in blue) is provided together with the histology image and ",
    "full_text_length": 93433,
    "chunk_length": 1309
  },
  {
    "chunk_id": 2172,
    "paper_filename": "ming_2024_multimodal_generative_aI_copilot_for_human_pathology.pdf",
    "paper_title": "Ming 2024 Multimodal Generative Ai Copilot For Human Pathology",
    "chunk_index": 18,
    "total_chunks": 92,
    "text_content": "that we do not know the extent to which GPT-4V has been trained on histopathology-specific data from the internet, our use of manually curated ROIs from WSIs for evaluation helps to minimize the likelihood of data contamination and ensure a proper assessment of its performance on histopathol - ogy images. Note that guardrails appear to have been implemented into GPT-4V to prevent it from sometimes addressing queries that require an examination of medical images. In that case, it informs the user",
    "full_text_length": 93433,
    "chunk_length": 1269
  },
  {
    "chunk_id": 2173,
    "paper_filename": "ming_2024_multimodal_generative_aI_copilot_for_human_pathology.pdf",
    "paper_title": "Ming 2024 Multimodal Generative Ai Copilot For Human Pathology",
    "chunk_index": 19,
    "total_chunks": 92,
    "text_content": "52 questions for the image-only set - ting). An ultimately unsuccessful query was treated as incorrect as the response did not address the question. Although GPT-4V was more accurate than the open-source MLLMs when clinical context was provided, our domain-specific PathChat MLLM was consistently better in both evaluation settings (90.5% versus 63.5% by GPT-4V with clinical context, +26.9%; 78.8% versus 25% by GPT-4V for image-only, +53.8%; P < 0.001 for both). Although a part of this difference ",
    "full_text_length": 93433,
    "chunk_length": 1391
  },
  {
    "chunk_id": 2174,
    "paper_filename": "ming_2024_multimodal_generative_aI_copilot_for_human_pathology.pdf",
    "paper_title": "Ming 2024 Multimodal Generative Ai Copilot For Human Pathology",
    "chunk_index": 20,
    "total_chunks": 92,
    "text_content": "multiple-choice diagnostic questions, it is valuable to assess the ability of PathChat and other MLLMs to generate coherent, reasonable and clinically relevant responses to open-ended pathology-related inquir - ies (\u2018Benchmark for expert-curated pathology questions\u2019 in Methods). Based on cases from PathQABench-Public, a board-certified anatomic pathologist carefully curated open-ended questions targeting a broad spectrum of topics including microscopy image description, histologic grade and diff",
    "full_text_length": 93433,
    "chunk_length": 1412
  },
  {
    "chunk_id": 2175,
    "paper_filename": "ming_2024_multimodal_generative_aI_copilot_for_human_pathology.pdf",
    "paper_title": "Ming 2024 Multimodal Generative Ai Copilot For Human Pathology",
    "chunk_index": 21,
    "total_chunks": 92,
    "text_content": "of 7 pathologists Expert rankings 4 AI assistant modelsQ5: 1 > 3 = 4 > 2 Q5: 1 > 3 = 4 = 2 Q5: 3 > 1 > 4 > 2\u2026a Shuf/f_led and de-identi/f_ied responses ranked by each expert <Response 2> <Response 3> <Response 4>+ PathChat LLaVA-MedLLaVA v.1.5 GPT-4V 260 open-ended questions +Record of PathChat Win Tie Lose \u2026P1 P2 P7 Ancillary testing0.51.0Accuracy ClinicalDiagnosis Microscopy0.51.0Accuracy Fig. 3 | Open-response evaluation of PathChat and reader study from a panel of seven pathologists. a, Eval",
    "full_text_length": 93433,
    "chunk_length": 1275
  },
  {
    "chunk_id": 2176,
    "paper_filename": "ming_2024_multimodal_generative_aI_copilot_for_human_pathology.pdf",
    "paper_title": "Ming 2024 Multimodal Generative Ai Copilot For Human Pathology",
    "chunk_index": 22,
    "total_chunks": 92,
    "text_content": "details). Scale bar, 200 \u00b5m. b, Head-to-head records on open-ended questions for PathChat versus other MLLMs evaluated by seven pathologists independently. Win, PathChat was ranked higher than the model. Tie, PathChat tied with the model in terms of ranking. Lose: Said model was ranked higher than PathChat. Vertical bars represent median win rate (dark green) across all seven pathologists and median win + tie rate (light green). c , Accuracy of MLLMs on a subset ( n = 235 questions) of open-ende",
    "full_text_length": 93433,
    "chunk_length": 1255
  },
  {
    "chunk_id": 2177,
    "paper_filename": "ming_2024_multimodal_generative_aI_copilot_for_human_pathology.pdf",
    "paper_title": "Ming 2024 Multimodal Generative Ai Copilot For Human Pathology",
    "chunk_index": 23,
    "total_chunks": 92,
    "text_content": "the computed accuracy. 470 | Nature | Vol 634 | 10 October 2024 ArticleGiven the more subjective nature of evaluating responses to open-ended questions, our evaluation consisted of two components. First, seven expert pathologists each ranked (from best to worst, ties allowed) the responses from different models for all questions (Fig. 3a) based on their relevance to the question, correctness and whether it was supplemented with a correct explanation or reasoning in a suc - cinct manner (see \u2018MLL",
    "full_text_length": 93433,
    "chunk_length": 1300
  },
  {
    "chunk_id": 2178,
    "paper_filename": "ming_2024_multimodal_generative_aI_copilot_for_human_pathology.pdf",
    "paper_title": "Ming 2024 Multimodal Generative Ai Copilot For Human Pathology",
    "chunk_index": 24,
    "total_chunks": 92,
    "text_content": "wide range of expert judgement (including subjective human preference) on the responses. Overall, we found that PathChat produced on average more pref - erable, higher-ranked responses than all the other MLLMs tested. When considering head-to-head records (for example, PathChat ver- sus GPT-4V) for model ranking judged by a human expert, a \u2018win\u2019 for PathChat on a question equated to PathChat\u2019s response being ranked strictly higher than those of its counterparts. Similarly, a \u2018tie\u2019 for Path - Cha",
    "full_text_length": 93433,
    "chunk_length": 1205
  },
  {
    "chunk_id": 2179,
    "paper_filename": "ming_2024_multimodal_generative_aI_copilot_for_human_pathology.pdf",
    "paper_title": "Ming 2024 Multimodal Generative Ai Copilot For Human Pathology",
    "chunk_index": 25,
    "total_chunks": 92,
    "text_content": "in favour of PathChat compared to LLaVA 1.5 (median win rate of 67.7%, median lose rate of 11.2% and median tie rate of 21.5%) and LLaVA-Med (median win rate of 74.2%, median lose rate of 10.0% and median tie rate of 15.4%). Additionally, to establish a more objective metric for each model\u2019s accuracy on the open-ended questions, two board-certified patholo - gists independently reviewed responses for each question. They assigned a binary label of correct versus incorrect for each model (while re",
    "full_text_length": 93433,
    "chunk_length": 1242
  },
  {
    "chunk_id": 2180,
    "paper_filename": "ming_2024_multimodal_generative_aI_copilot_for_human_pathology.pdf",
    "paper_title": "Ming 2024 Multimodal Generative Ai Copilot For Human Pathology",
    "chunk_index": 26,
    "total_chunks": 92,
    "text_content": "on the subset of open-ended questions for which the pathologists were able to reach a consensus (Fig. 3c and Supplemen - tary Table 14), which corresponds to a 26.4% improvement (P < 0.001) compared to the accuracy of 52.3% achieved by the runner-up, GPT-4V. Compared to the publicly available general-purpose MLLM LLaVA 1.5 (accuracy of 29.8%) and the biomedicine-specialized MLLM LLaVA-Med (accuracy of 30.6%), the margin of improvement was even more sub - stantial, at +48.9% and +48.1%, respectiv",
    "full_text_length": 93433,
    "chunk_length": 1297
  },
  {
    "chunk_id": 2181,
    "paper_filename": "ming_2024_multimodal_generative_aI_copilot_for_human_pathology.pdf",
    "paper_title": "Ming 2024 Multimodal Generative Ai Copilot For Human Pathology",
    "chunk_index": 27,
    "total_chunks": 92,
    "text_content": "we analysed their performance for various subgroups of questions (described in Sup - plementary Tables 15 and 16 with examples provided in Extended Data Fig. 7). In particular, the microscopy category includes questions that test the ability of models to generate accurate and detailed morphologi - cal descriptions of histology microscopy images and assess clinically relevant features such as tumour differentiation and grade. Questions in the diagnosis category tested the ability of the models to",
    "full_text_length": 93433,
    "chunk_length": 1370
  },
  {
    "chunk_id": 2182,
    "paper_filename": "ming_2024_multimodal_generative_aI_copilot_for_human_pathology.pdf",
    "paper_title": "Ming 2024 Multimodal Generative Ai Copilot For Human Pathology",
    "chunk_index": 28,
    "total_chunks": 92,
    "text_content": "prognosis and treatment. Although GPT-4V was the runner-up to PathChat overall, PathChat\u2019s responses were especially superior to those of GPT-4V in the catego - ries that require examination of the histology image (microscopy and diagnosis), for which the accuracies on the consensus subset were 73.3% and 78.5% for PathChat respectively versus 22.8% and 31.6% for GPT-4V (Fig. 3d and Supplementary Tables 17\u201319). Similarly, the median head-to-head win rate against GPT-4V reached 70.6% and 71.3% on ",
    "full_text_length": 93433,
    "chunk_length": 1282
  },
  {
    "chunk_id": 2183,
    "paper_filename": "ming_2024_multimodal_generative_aI_copilot_for_human_pathology.pdf",
    "paper_title": "Ming 2024 Multimodal Generative Ai Copilot For Human Pathology",
    "chunk_index": 29,
    "total_chunks": 92,
    "text_content": "behind GPT-4V on clinical and ancil - lary testing, for which, for the consensus subset, PathChat achieved a respectable 80.3% accuracy on both categories compared to GPT-4V\u2019s higher scores of 88.5% and 89.5% on the two categories, respectively. Note that although PathChat convincingly outperformed GPT-4V in accuracy on the microscopy and diagnosis categories according to the consensus (P < 0.001 for both, n = 101 and 79, respectively), we did not find any statistical significance (P > 0.05) for",
    "full_text_length": 93433,
    "chunk_length": 1208
  },
  {
    "chunk_id": 2184,
    "paper_filename": "ming_2024_multimodal_generative_aI_copilot_for_human_pathology.pdf",
    "paper_title": "Ming 2024 Multimodal Generative Ai Copilot For Human Pathology",
    "chunk_index": 30,
    "total_chunks": 92,
    "text_content": "and in fact slightly more preferred by the panel of pathologists com - pared to GPT-4V (a median win rate of 44.1% and lose rate of 33.8% versus GPT-4V for clinical and a median win rate of 44.8% and lose rate of 35.6% for ancillary testing) on these same categories. Note that we included clinical and ancillary testing questions to com - prehensively assess the capabilities of AI assistant models to address pathology-related queries. However, these questions frequently do not require an actual e",
    "full_text_length": 93433,
    "chunk_length": 1238
  },
  {
    "chunk_id": 2185,
    "paper_filename": "ming_2024_multimodal_generative_aI_copilot_for_human_pathology.pdf",
    "paper_title": "Ming 2024 Multimodal Generative Ai Copilot For Human Pathology",
    "chunk_index": 31,
    "total_chunks": 92,
    "text_content": "adequately answer questions in these categories and that GPT-4V may, in particular, excel here, as it is presumably much larger and was trained on more extensive knowledge from the internet than open-source models and PathChat. As these queries can often readily be addressed through conventional means of querying, such as internet searches or consulting a reference manual, we focused on the microscopy and diagnosis categories as the main indicators for the utility of different models as vision-l",
    "full_text_length": 93433,
    "chunk_length": 1280
  },
  {
    "chunk_id": 2186,
    "paper_filename": "ming_2024_multimodal_generative_aI_copilot_for_human_pathology.pdf",
    "paper_title": "Ming 2024 Multimodal Generative Ai Copilot For Human Pathology",
    "chunk_index": 32,
    "total_chunks": 92,
    "text_content": "ability of LLMs to encode general clini - cal knowledge. Lastly, note that like our observation in the multiple-choice evalua- tion, of the 260 questions submitted to it, GPT-4V obviously refused to answer 38, presumably because of guardrails implemented within it. Nature | Vol 634 | 10 October 2024 | 471A maximum of three attempts were made for each question (see \u2018Evalu - ating GPT-4V\u2019 in Methods for more details). Consistent with our assess - ment of the other models, all GPT-4V responses, reg",
    "full_text_length": 93433,
    "chunk_length": 1293
  },
  {
    "chunk_id": 2187,
    "paper_filename": "ming_2024_multimodal_generative_aI_copilot_for_human_pathology.pdf",
    "paper_title": "Ming 2024 Multimodal Generative Ai Copilot For Human Pathology",
    "chunk_index": 33,
    "total_chunks": 92,
    "text_content": "of over 20% in accuracy (for example, 81.2% versus 59.9% for 202 questions with consensus, P < 0.001). Demonstration of PathChat on diverse use cases In addition to evaluating PathChat on answering multiple-choice and open-ended questions, we attempted to conceptualize and explore potential use cases for PathChat as an AI copilot specialized for pathology. We observed that PathChat can analyse and describe notable morphological details in histology images and that it can answer questions that re",
    "full_text_length": 93433,
    "chunk_length": 1388
  },
  {
    "chunk_id": 2188,
    "paper_filename": "ming_2024_multimodal_generative_aI_copilot_for_human_pathology.pdf",
    "paper_title": "Ming 2024 Multimodal Generative Ai Copilot For Human Pathology",
    "chunk_index": 34,
    "total_chunks": 92,
    "text_content": "models fine-tuned on relatively small datasets with a limited scope39\u201343. Additionally, the support for interactive multi-turn conversations enables PathChat to potentially serve as a consultant for human-in-the-loop differential diagnosis, in which an initial AI-assisted assessment can be followed up with further clinical context, namely workups that are subsequently performed to narrow the differential (Fig. 4e and Extended Data Fig. 10). This may be especially valuable in cases that involve m",
    "full_text_length": 93433,
    "chunk_length": 1305
  },
  {
    "chunk_id": 2189,
    "paper_filename": "ming_2024_multimodal_generative_aI_copilot_for_human_pathology.pdf",
    "paper_title": "Ming 2024 Multimodal Generative Ai Copilot For Human Pathology",
    "chunk_index": 35,
    "total_chunks": 92,
    "text_content": "and macrophages? CD20 is a marker commonly used to distinguish B lymphocytes from T lymphocytes. B lymphocytes are typically CD20 positive, while T lymphocytes are CD20 negative. Additionally, CD68 is a marker that can be used to identify macrophages. Macrophages are CD68 positive, while T lymphocytes are CD68 negative. Humanb PathChat HumanThis is an image of a lung core needle biopsy from a patient with a mass in their left lung. What are some diagnoses this could be and what IHC should I do t",
    "full_text_length": 93433,
    "chunk_length": 1276
  },
  {
    "chunk_id": 2190,
    "paper_filename": "ming_2024_multimodal_generative_aI_copilot_for_human_pathology.pdf",
    "paper_title": "Ming 2024 Multimodal Generative Ai Copilot For Human Pathology",
    "chunk_index": 36,
    "total_chunks": 92,
    "text_content": "squamous cell carcinoma. The expression pattern of these markers will help differentiate between these possibilities. I did TTF-1, p40, and synaptophysin and all stains were negative. What should I do next? The negative staining for TTF-1, p40, and synaptophysin makes it more challenging to determine the exact diagnosis. TTF-1 and p40 are commonly used markers for lung adenocarcinoma and squamous cell carcinoma, respectively. Synaptophysin is a marker for neuroendocrine tumours. The negative sta",
    "full_text_length": 93433,
    "chunk_length": 1319
  },
  {
    "chunk_id": 2191,
    "paper_filename": "ming_2024_multimodal_generative_aI_copilot_for_human_pathology.pdf",
    "paper_title": "Ming 2024 Multimodal Generative Ai Copilot For Human Pathology",
    "chunk_index": 37,
    "total_chunks": 92,
    "text_content": "etc.) can help identify the primary site of the tumour. It is also important to consider the clinical history and radiological /f_indings to guide the workup. If there is a known history of malignancy or suspicion of metastatic disease, further investigation into the primary site may be warranted.PathChat Human c HumanAssess the tissue morphology in this prostate resection. What Gleason patterns are present? The tissue morphology in this prostate resection reveals a combination of two distinct G",
    "full_text_length": 93433,
    "chunk_length": 1296
  },
  {
    "chunk_id": 2192,
    "paper_filename": "ming_2024_multimodal_generative_aI_copilot_for_human_pathology.pdf",
    "paper_title": "Ming 2024 Multimodal Generative Ai Copilot For Human Pathology",
    "chunk_index": 38,
    "total_chunks": 92,
    "text_content": "shows a tumour composed of spindle cells, which are elongated and have a fusiform shape. These cells are arranged in fascicles, which are bundles or groups of cells aligned in a parallel or whorled pattern. Additionally, there are scattered melanin- containing cells within the tumour. This tumour was found in the liver of a patient. Is it a primary tumour or a metastasis? Based on the information provided, this tumour is most likely a metastasis rather than a primary tumour. The presence of spin",
    "full_text_length": 93433,
    "chunk_length": 1237
  },
  {
    "chunk_id": 2193,
    "paper_filename": "ming_2024_multimodal_generative_aI_copilot_for_human_pathology.pdf",
    "paper_title": "Ming 2024 Multimodal Generative Ai Copilot For Human Pathology",
    "chunk_index": 39,
    "total_chunks": 92,
    "text_content": "tumour cells are arranged in a papillary pattern with multiple branching structures. The cells within the tumour are high-grade, indicating a high level of cellular atypia and aggressive behaviour. What do you think IHC for p53 would show in this case? Given the high-grade nature of this serous carcinoma, it is likely that the p53 immunohistochemistry (IHC) would show abnormal p53 protein expression. High-grade serous carcinomas often have TP53 mutations, which can result in aberrant p53 protein",
    "full_text_length": 93433,
    "chunk_length": 1358
  },
  {
    "chunk_id": 2194,
    "paper_filename": "ming_2024_multimodal_generative_aI_copilot_for_human_pathology.pdf",
    "paper_title": "Ming 2024 Multimodal Generative Ai Copilot For Human Pathology",
    "chunk_index": 40,
    "total_chunks": 92,
    "text_content": "of PathQABench. a , PathChat summarized key morphological features in an histology image. Based on the clinical context, it could reasonably infer the primary origin of the tumour. b , PathChat is familiar with different cell markers and can potentially help by guiding IHC interpretations. c , PathChat understands and can attempt to follow well-known guidelines on tumour grading, in this case, the Gleason grading system for prostate adenocarcinoma. d, PathChat can describe tumour tissue and cell",
    "full_text_length": 93433,
    "chunk_length": 1380
  },
  {
    "chunk_id": 2195,
    "paper_filename": "ming_2024_multimodal_generative_aI_copilot_for_human_pathology.pdf",
    "paper_title": "Ming 2024 Multimodal Generative Ai Copilot For Human Pathology",
    "chunk_index": 41,
    "total_chunks": 92,
    "text_content": "models based on image or genomics data. For histology images specifically, there has recently been growing interest in building foundational task-agnostic vision encoders pretrained with large num - bers of unlabelled images, which can provide robust feature embed - dings for diverse supervised and unsupervised downstream workflows. However, the explosive growth in generative AI technology and spe - cifically MLLMs, as exemplified by the likes of ChatGPT, has begun to open up a possible new fron",
    "full_text_length": 93433,
    "chunk_length": 1352
  },
  {
    "chunk_id": 2196,
    "paper_filename": "ming_2024_multimodal_generative_aI_copilot_for_human_pathology.pdf",
    "paper_title": "Ming 2024 Multimodal Generative Ai Copilot For Human Pathology",
    "chunk_index": 42,
    "total_chunks": 92,
    "text_content": "classification, captioning, retrieval, answering questions and more). For pathology specifically, such a model could, in theory, have appli - cations in a wide range of scenarios across education and research as well as human-in-the-loop clinical decision-making. In this work, we provide a proof of concept for building an AI copi - lot tailored to human pathology. We also provide, to the best of our knowledge, the most extensive evaluation of such technology for com - putational pathology by com",
    "full_text_length": 93433,
    "chunk_length": 1379
  },
  {
    "chunk_id": 2197,
    "paper_filename": "ming_2024_multimodal_generative_aI_copilot_for_human_pathology.pdf",
    "paper_title": "Ming 2024 Multimodal Generative Ai Copilot For Human Pathology",
    "chunk_index": 43,
    "total_chunks": 92,
    "text_content": "grade and differentiation, suggesting further IHC and molecular testing, and understanding the risk factors, prognosis and treatment of the underlying disease. We assessed these skills through a combination of multiple-choice diagnostic questions and open-ended questions coupled with human expert evaluation. In both evaluation settings, PathChat compared favourably to the current best-in-class commercial solution GPT-4V (presumably much larger and expensive to serve than PathChat) and substantia",
    "full_text_length": 93433,
    "chunk_length": 1417
  },
  {
    "chunk_id": 2198,
    "paper_filename": "ming_2024_multimodal_generative_aI_copilot_for_human_pathology.pdf",
    "paper_title": "Ming 2024 Multimodal Generative Ai Copilot For Human Pathology",
    "chunk_index": 44,
    "total_chunks": 92,
    "text_content": "halluci - nation in MLLM-based AI assistant models in general and also help them to capture certain nuances specific to pathology, such as when to request further contextual information or test results when it is not possible or is difficult to rule out certain morphologically similar diseases based on H&E histology alone or when to seek clarification on institutional-specific guidelines for diagnosis and treatment. For real-world deployment, improvement and validation are probably also warrante",
    "full_text_length": 93433,
    "chunk_length": 1388
  },
  {
    "chunk_id": 2199,
    "paper_filename": "ming_2024_multimodal_generative_aI_copilot_for_human_pathology.pdf",
    "paper_title": "Ming 2024 Multimodal Generative Ai Copilot For Human Pathology",
    "chunk_index": 45,
    "total_chunks": 92,
    "text_content": "Addition - ally, owing to their having been trained on retrospectively collected large datasets that inevitably contain outdated information, these models may reflect the scientific consensus of the past rather than that of today58. For example, as medical terminology and guidelines evolve, a model response that references the outdated term \u2018glioblas- toma multiforme\u2019 may result in factual inaccuracies. Besides continual training with fresh, up-to-date knowledge59, other research directions may ",
    "full_text_length": 93433,
    "chunk_length": 1436
  },
  {
    "chunk_id": 2200,
    "paper_filename": "ming_2024_multimodal_generative_aI_copilot_for_human_pathology.pdf",
    "paper_title": "Ming 2024 Multimodal Generative Ai Copilot For Human Pathology",
    "chunk_index": 46,
    "total_chunks": 92,
    "text_content": "with tools such as digital slide viewers or electronic health records. Online content Any methods, additional references, Nature Portfolio reporting summa - ries, source data, extended data, supplementary information, acknowl - edgements, peer review information; details of author contributions and competing interests; and statements of data and code availability are available at https://doi.org/10.1038/s41586-024-07618-3 . 1. Song, A. H. et al. Artificial intelligence for digital and computatio",
    "full_text_length": 93433,
    "chunk_length": 1409
  },
  {
    "chunk_id": 2201,
    "paper_filename": "ming_2024_multimodal_generative_aI_copilot_for_human_pathology.pdf",
    "paper_title": "Ming 2024 Multimodal Generative Ai Copilot For Human Pathology",
    "chunk_index": 47,
    "total_chunks": 92,
    "text_content": "(eds Oh, A. et al.) 34892\u201334916 (Curran Associates, 2023). 6. Josh, A. et al. GPT-4 technical report. Preprint at arxiv.org/abs/2303.08774 (2023). 7. Lipkova, J. et al. Artificial intelligence for multimodal data integration in oncology. Cancer Cell 40, 1095\u20131110 (2022). 8. Coudray, N. et al. Classification and mutation prediction from non\u2013small cell lung cancer histopathology images using deep learning. Nat. Med. 24, 1559\u20131567 (2018). 9. Lu, M. Y. et al. Data-efficient and weakly supervised com",
    "full_text_length": 93433,
    "chunk_length": 1352
  },
  {
    "chunk_id": 2202,
    "paper_filename": "ming_2024_multimodal_generative_aI_copilot_for_human_pathology.pdf",
    "paper_title": "Ming 2024 Multimodal Generative Ai Copilot For Human Pathology",
    "chunk_index": 48,
    "total_chunks": 92,
    "text_content": "in women with breast cancer. J. Am. Med. Assoc. 318, 2199\u20132210 (2017). 13. Beck, A. H. et al. Systematic analysis of breast cancer morphology uncovers stromal features associated with survival. Sci. Transl. Med. 3, 108ra113 (2011). 14. Chen, R. J. et al. Pan-cancer integrative histology-genomic analysis via multimodal deep learning. Cancer Cell 40, 865\u2013878 (2022). 15. Lee, Y. et al. Derivation of prognostic contextual histopathological features from whole- slide images of tumours via graph deep ",
    "full_text_length": 93433,
    "chunk_length": 1383
  },
  {
    "chunk_id": 2203,
    "paper_filename": "ming_2024_multimodal_generative_aI_copilot_for_human_pathology.pdf",
    "paper_title": "Ming 2024 Multimodal Generative Ai Copilot For Human Pathology",
    "chunk_index": 49,
    "total_chunks": 92,
    "text_content": "Artificial intelligence reveals features associated with breast cancer neoadjuvant chemotherapy responses from multi-stain histopathologic images. npj Precis. Oncol. 7, 14 (2023). 20. Lu, M. Y. et al. AI-based pathology predicts origins for cancers of unknown primary. Nature 594, 106\u2013110 (2021). 21. Tian, F. et al. Prediction of tumor origin in cancers of unknown primary origin with cytology-based deep learning. Nat. Med. 30, 1309\u20131319 (2024). 22. Kather, J. N. et al. Pan-cancer image-based dete",
    "full_text_length": 93433,
    "chunk_length": 1392
  },
  {
    "chunk_id": 2204,
    "paper_filename": "ming_2024_multimodal_generative_aI_copilot_for_human_pathology.pdf",
    "paper_title": "Ming 2024 Multimodal Generative Ai Copilot For Human Pathology",
    "chunk_index": 50,
    "total_chunks": 92,
    "text_content": "segmentation and classification. Med. Image Anal. 83, 102685 (2023). 26. Oquab, M. et al. DINOv2: learning robust visual features without supervision. Trans. Machine Learning Res., 1\u201331 (2024). 27. Radford, A. et al. Learning transferable visual models from natural language supervision. In Proc. International Conference on Machine Learning (eds Meila, M. & Zhang, T.) 8748\u20138763 (PMLR, 2021). 28. Lu, J. et al. ViLBERT: pretraining task-agnostic visiolinguistic representations for vision-and-langua",
    "full_text_length": 93433,
    "chunk_length": 1474
  },
  {
    "chunk_id": 2205,
    "paper_filename": "ming_2024_multimodal_generative_aI_copilot_for_human_pathology.pdf",
    "paper_title": "Ming 2024 Multimodal Generative Ai Copilot For Human Pathology",
    "chunk_index": 51,
    "total_chunks": 92,
    "text_content": "arxiv.org/abs/2303.00915 (2023). 32. Gamper, J. & Rajpoot, N. Multiple instance captioning: learning representations from histopathology textbooks and articles. In Proc. IEEE/CVF Conference on Computer Vision and Pattern Recognition 16549\u201316559 (IEEE, 2021). 33. Ikezogwo, W. et al. Quilt-1m: one million image-text pairs for histopathology. In Proc. Advances in Neural Information Processing Systems (eds Oh, A. et al.) 37995\u201338017 (Curran Associates, 2024). 34. Lu, M. Y. et al. A visual-language f",
    "full_text_length": 93433,
    "chunk_length": 1392
  },
  {
    "chunk_id": 2206,
    "paper_filename": "ming_2024_multimodal_generative_aI_copilot_for_human_pathology.pdf",
    "paper_title": "Ming 2024 Multimodal Generative Ai Copilot For Human Pathology",
    "chunk_index": 52,
    "total_chunks": 92,
    "text_content": "for Healthcare Conference (eds Lipton, Z. et al.) 2\u201325 (PMLR, 2022). 38. Boecking, B. et al. Making the most of text semantics to improve biomedical vision\u2013 language processing. In Proc. European Conference on Computer Vision (eds Avidan, S. et al.) 1\u201321 (Springer, 2022). 39. Zhang, H. et al. PathNarratives: data annotation for pathological human\u2013AI collaborative diagnosis. Front. Med. 9, 1070072 (2023). 40. Tsuneki, M. & Kanavati, F. Inference of captions from histopathological patches. In Proc",
    "full_text_length": 93433,
    "chunk_length": 1363
  },
  {
    "chunk_id": 2207,
    "paper_filename": "ming_2024_multimodal_generative_aI_copilot_for_human_pathology.pdf",
    "paper_title": "Ming 2024 Multimodal Generative Ai Copilot For Human Pathology",
    "chunk_index": 53,
    "total_chunks": 92,
    "text_content": "pathology images. In Proc. 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (eds Zong, C. et al.) 708\u2013718 (ACL, 2021). 44. Ouyang, L. et al. Training language models to follow instructions with human feedback. In Proc. Advances in Neural Information Processing Systems (eds Koyejo, S. et al.) 27730\u201327744 (Curran Associates, 2022). 45. Brown, T. et al. Language models are few-shot learners. In Proc. Adva",
    "full_text_length": 93433,
    "chunk_length": 1361
  },
  {
    "chunk_id": 2208,
    "paper_filename": "ming_2024_multimodal_generative_aI_copilot_for_human_pathology.pdf",
    "paper_title": "Ming 2024 Multimodal Generative Ai Copilot For Human Pathology",
    "chunk_index": 54,
    "total_chunks": 92,
    "text_content": "J.-B. et al. Flamingo: a visual language model for few-shot learning. In Proc. Advances in Neural Information Processing Systems (eds Koyejo, S. et al.) 23716\u201323736 (Curran Associates, 2022). 50. Moor, M. et al. Foundation models for generalist medical artificial intelligence. Nature 616, 259\u2013265 (2023). 51. Bubeck, S. et al. Sparks of artificial general intelligence: early experiments with GPT-4. Preprint at arxiv.org/abs/2303.12712 (2023). 52. Sun, Y. et al. PathAsst: a generative foundation A",
    "full_text_length": 93433,
    "chunk_length": 1386
  },
  {
    "chunk_id": 2209,
    "paper_filename": "ming_2024_multimodal_generative_aI_copilot_for_human_pathology.pdf",
    "paper_title": "Ming 2024 Multimodal Generative Ai Copilot For Human Pathology",
    "chunk_index": 55,
    "total_chunks": 92,
    "text_content": "al. Can GPT-4V (ision) serve medical applications? Case studies on GPT-4V for multimodal medical diagnosis. Preprint at arxiv.org/abs/2310.09909 (2023). 56. Oon, M. L. et al. Bridging bytes and biopsies: a comparative analysis of ChatGPT and histopathologists in pathology diagnosis and collaborative potential. Histopathology 84, 601\u2013613 (2023). 57. Seyfioglu, M. S. et al. Quilt-LLaVA: visual instruction tuning by extracting localized narratives from open-source histopathology videos.\u201d In Proc. I",
    "full_text_length": 93433,
    "chunk_length": 1484
  },
  {
    "chunk_id": 2210,
    "paper_filename": "ming_2024_multimodal_generative_aI_copilot_for_human_pathology.pdf",
    "paper_title": "Ming 2024 Multimodal Generative Ai Copilot For Human Pathology",
    "chunk_index": 56,
    "total_chunks": 92,
    "text_content": "Systems (eds Larochelle, H. et al.) 9459\u20139474 (Curran Associates, 2020). Publisher\u2019s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations. Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link",
    "full_text_length": 93433,
    "chunk_length": 1352
  },
  {
    "chunk_id": 2211,
    "paper_filename": "ming_2024_multimodal_generative_aI_copilot_for_human_pathology.pdf",
    "paper_title": "Ming 2024 Multimodal Generative Ai Copilot For Human Pathology",
    "chunk_index": 57,
    "total_chunks": 92,
    "text_content": "copy of this licence, visit http://creativecommons.org/licenses/by/4.0/. \u00a9 The Author(s) 2024 ArticleMethods Curation of the PathChat dataset We curated a dataset of 456,916 instructions with 999,202 question and answer turns, which was used to train PathChat to respond to pathology-specific queries. The instructions were roughly catego - rized as conversation (n = 132,563), description (n = 168,440), multiple choice (n = 42,445), free response (n = 21,686), text-only (n = 83,232) and guardrail ",
    "full_text_length": 93433,
    "chunk_length": 1358
  },
  {
    "chunk_id": 2212,
    "paper_filename": "ming_2024_multimodal_generative_aI_copilot_for_human_pathology.pdf",
    "paper_title": "Ming 2024 Multimodal Generative Ai Copilot For Human Pathology",
    "chunk_index": 58,
    "total_chunks": 92,
    "text_content": "dataset, which spanned image captions, educational articles from PubMed Open Access, pathology case reports and ROIs extracted from WSIs, which were sourced from several institutions. Data from TCGA were not used for training and were held out as part of our downstream evaluation. The data for each source were filtered individually to ensure quality and relevance for training a pathology-specific vision-language assis - tant. Examples of frequently used heuristics for filtering include the remov",
    "full_text_length": 93433,
    "chunk_length": 1293
  },
  {
    "chunk_id": 2213,
    "paper_filename": "ming_2024_multimodal_generative_aI_copilot_for_human_pathology.pdf",
    "paper_title": "Ming 2024 Multimodal Generative Ai Copilot For Human Pathology",
    "chunk_index": 59,
    "total_chunks": 92,
    "text_content": "image of a lung mass\u2019 but no image is provided, the model is expected to output the response: \u2018Sorry, I cannot assist you since you have not uploaded any image. \u2019 Additionally, when given an image not related to pathology (sampled from MS COCO; ref. 61), the model is trained to output: \u2018Sorry I can only assist you with queries related to pathology. \u2019 For some unstructured data formats, we prompted the open-source general-purpose LLMs46,62 to structure the original source text into a structured f",
    "full_text_length": 93433,
    "chunk_length": 1332
  },
  {
    "chunk_id": 2214,
    "paper_filename": "ming_2024_multimodal_generative_aI_copilot_for_human_pathology.pdf",
    "paper_title": "Ming 2024 Multimodal Generative Ai Copilot For Human Pathology",
    "chunk_index": 60,
    "total_chunks": 92,
    "text_content": "examining and interpreting visual information in high-resolution microscopy images (in conjunction with other clinical information) remains the cornerstone of the discipline and extends to many aspects of disease diagnosis and management in modern medicine. Inspired by LLaVA5,63, our MLLM, PathChat, consists of three key components: the vision encoder, the multimodal projector module and the LLM. The vision encoder is responsible for encoding an image from the original high-dimensional RGB pixel",
    "full_text_length": 93433,
    "chunk_length": 1321
  },
  {
    "chunk_id": 2215,
    "paper_filename": "ming_2024_multimodal_generative_aI_copilot_for_human_pathology.pdf",
    "paper_title": "Ming 2024 Multimodal Generative Ai Copilot For Human Pathology",
    "chunk_index": 61,
    "total_chunks": 92,
    "text_content": "sequence of input tokens, and predicts the desirable response through autoregres - sive next-word prediction. The response produced is finally decoded by the tokenizer back into natural language and presented to the end user.For the LLM, we adopted the 13-billion-parameter variant from the widely used Meta Llama 2 family46 of SOTA open-source LLMs, which is a decoder-only transformer-based autoregressive language model with 40 transformer layers, each with 40 attention heads, an embedding dimens",
    "full_text_length": 93433,
    "chunk_length": 1294
  },
  {
    "chunk_id": 2216,
    "paper_filename": "ming_2024_multimodal_generative_aI_copilot_for_human_pathology.pdf",
    "paper_title": "Ming 2024 Multimodal Generative Ai Copilot For Human Pathology",
    "chunk_index": 62,
    "total_chunks": 92,
    "text_content": "positional encoding to each token. The multimodal projector consists of an attention pooling layer followed by a two-layer multilayer perceptron. The attention pooling layer (also known as a perceiver resampler in some works49,64,65) uses a set of 128 learned latent queries and multi-headed cross-attention with 8 heads to reduce the last layer feature map of the encoder backbone into a fixed-length sequence of image tokens with an initial dimension of 768 to increase training and inference effic",
    "full_text_length": 93433,
    "chunk_length": 1302
  },
  {
    "chunk_id": 2217,
    "paper_filename": "ming_2024_multimodal_generative_aI_copilot_for_human_pathology.pdf",
    "paper_title": "Ming 2024 Multimodal Generative Ai Copilot For Human Pathology",
    "chunk_index": 63,
    "total_chunks": 92,
    "text_content": "vision-encoder backbone from UNI3, a SOTA vision-only self-supervised pretrained general-purpose encoder for H&E pathology and then fine-tuned the encoder backbone together with the attention pooling module on an expanded dataset of 1.18 paired images and captions from CONCH34 and the CoCa visual-language pretraining recipe66 (see Supplementary Table 1 for details of the hyperparameters). We followed the MLLM training recipe of LLaVA 1.5, which involves two stages of training. In the first, pret",
    "full_text_length": 93433,
    "chunk_length": 1335
  },
  {
    "chunk_id": 2218,
    "paper_filename": "ming_2024_multimodal_generative_aI_copilot_for_human_pathology.pdf",
    "paper_title": "Ming 2024 Multimodal Generative Ai Copilot For Human Pathology",
    "chunk_index": 64,
    "total_chunks": 92,
    "text_content": "the instruction fine-tuning stage, both the LLM and projector are trained end-to-end to generate responses to diverse instructions that include both natural language and visual inputs, as described in \u2018PathChat dataset curation\u2019 . Specifically, given an instruction Xinstruct , the reference answer Xans and the image Ximg, each represented as a sequence of tokenized inputs, we maximized the likelihood of each token in Xans, indexed by 1, ..., L, under the MLLM (viewed as an autoregressive languag",
    "full_text_length": 93433,
    "chunk_length": 1242
  },
  {
    "chunk_id": 2219,
    "paper_filename": "ming_2024_multimodal_generative_aI_copilot_for_human_pathology.pdf",
    "paper_title": "Ming 2024 Multimodal Generative Ai Copilot For Human Pathology",
    "chunk_index": 65,
    "total_chunks": 92,
    "text_content": "simply concatenate their respective image tokens, with the newline (\u2018\\n\u2019) token inserted between them as a separator, and treat the full sequence as Ximg. Both pretrain- ing and fine-tuning were performed using eight 80 GB NVIDIA A100 GPUs. We refer readers to Supplementary Tables 2 and 3 for details of the hyperparameters used in training. Benchmark for expert-curated pathology questions Evaluating powerful multimodal vision-language AI models in histopa - thology is an outstanding challenge, a",
    "full_text_length": 93433,
    "chunk_length": 1378
  },
  {
    "chunk_id": 2220,
    "paper_filename": "ming_2024_multimodal_generative_aI_copilot_for_human_pathology.pdf",
    "paper_title": "Ming 2024 Multimodal Generative Ai Copilot For Human Pathology",
    "chunk_index": 66,
    "total_chunks": 92,
    "text_content": "workflow used by PathVQA. Thus motivated, we curated a new high-quality quality-assessment benchmark suitable for evaluat- ing cutting-edge MLLMs for pathology, as described in detail below. To evaluate PathChat, we curated PathQABench using representa - tive high-resolution ROI images hand-selected by a board-certified pathologist from 105 H&E WSI cases using the open-source QuPath digital viewer67. These cases were withheld from all stages of training PathChat. Of the 105 image ROIs, 53 ROIs w",
    "full_text_length": 93433,
    "chunk_length": 1338
  },
  {
    "chunk_id": 2221,
    "paper_filename": "ming_2024_multimodal_generative_aI_copilot_for_human_pathology.pdf",
    "paper_title": "Ming 2024 Multimodal Generative Ai Copilot For Human Pathology",
    "chunk_index": 67,
    "total_chunks": 92,
    "text_content": "without any risk of violating institutional guidelines for handling patient data. Accordingly, the subset of questions based on private WSIs, referred to as PathQABench-Private, was used to evaluate only other publicly available MLLM solutions that we can run locally inside the hospital without transmitting the data to an external server. To select the ROIs, a board-certified pathologist manually reviewed WSIs related to each diagnosis and distilled a single ROI for each WSI wherein relevant mor",
    "full_text_length": 93433,
    "chunk_length": 1265
  },
  {
    "chunk_id": 2222,
    "paper_filename": "ming_2024_multimodal_generative_aI_copilot_for_human_pathology.pdf",
    "paper_title": "Ming 2024 Multimodal Generative Ai Copilot For Human Pathology",
    "chunk_index": 68,
    "total_chunks": 92,
    "text_content": "up the case. To accommodate the diversity of diagnoses included in our evaluation, the selected ROIs vary in magnification and dimen- sion. Across PathQABench, the selected magnifications of the ROIs ranged from \u00d73 to \u00d734.4 with a median of \u00d713.3. The widths varied from 859 to 2,388 px with a median of 1,201 px whereas the heights varied from 861 to 2,390 px with a median of 1,191 px. For each case, the pathologist wrote a short clinical summary based on the ground truth diagnosis, which include",
    "full_text_length": 93433,
    "chunk_length": 1266
  },
  {
    "chunk_id": 2223,
    "paper_filename": "ming_2024_multimodal_generative_aI_copilot_for_human_pathology.pdf",
    "paper_title": "Ming 2024 Multimodal Generative Ai Copilot For Human Pathology",
    "chunk_index": 69,
    "total_chunks": 92,
    "text_content": "of topics that include but are not limited to just diagnosis (Extended Data Fig. 7 and Supplementary Table 15). A total of 105 multiple-choice questions were created using the salient ROIs (one question per ROI). In the evaluation setting with multiple-choice questions, for each organ system, a board-certified pathologist selected a set of ten possible answers that encompassed the correct answers for all questions within that organ system as well as other relatively common diagnoses within that ",
    "full_text_length": 93433,
    "chunk_length": 1298
  },
  {
    "chunk_id": 2224,
    "paper_filename": "ming_2024_multimodal_generative_aI_copilot_for_human_pathology.pdf",
    "paper_title": "Ming 2024 Multimodal Generative Ai Copilot For Human Pathology",
    "chunk_index": 70,
    "total_chunks": 92,
    "text_content": "assessed based on its ability to accurately select the ground truth diagnosis from the set of possible options.In the evaluation setting for answering open-ended questions, we used the 52 cases from PathQABench-Public to curate five questions per case for a total of 260 questions. The questions were broadly cat- egorized as microscopy, diagnosis, clinical and ancillary testing, as described in Supplementary Table 15. The microscopy and diagnosis questions, in particular, focus on targeting diagn",
    "full_text_length": 93433,
    "chunk_length": 1313
  },
  {
    "chunk_id": 2225,
    "paper_filename": "ming_2024_multimodal_generative_aI_copilot_for_human_pathology.pdf",
    "paper_title": "Ming 2024 Multimodal Generative Ai Copilot For Human Pathology",
    "chunk_index": 71,
    "total_chunks": 92,
    "text_content": "for answering open-ended questions is specific to pathology, its size is substantially larger than the 140 questions used in an earlier work58 in which human experts evaluated the ability of LLMs to encode general clinical knowledge. MLLM evaluation We compared PathChat to the general-purpose SOTA MLLM LLaVA 1.5 (ref. 63) and to the biomedically focused MLLM LLaVA-Med53 using the full PathQABench dataset. We evaluated the performance of GPT-4V only on cases from PathQABench-Public. The precise p",
    "full_text_length": 93433,
    "chunk_length": 1273
  },
  {
    "chunk_id": 2226,
    "paper_filename": "ming_2024_multimodal_generative_aI_copilot_for_human_pathology.pdf",
    "paper_title": "Ming 2024 Multimodal Generative Ai Copilot For Human Pathology",
    "chunk_index": 72,
    "total_chunks": 92,
    "text_content": "the next section (\u2018Evalu - ating GPT-4V\u2019). For all models, the maximum length of each generated response was capped to 1,024 new tokens generated. For the multiple-choice questions, we observed that PathChat, LLaVA 1.5 and GPT-4V can output the predicted choice in a consistent and desir - able format (for example, \u2018 A\u2019 or \u2018 A. Lung adenocarcinoma\u2019), which can be directly used in our evaluation pipeline to compute the accuracy. How - ever, we found LLaVA-Med could not follow the instruction to an",
    "full_text_length": 93433,
    "chunk_length": 1271
  },
  {
    "chunk_id": 2227,
    "paper_filename": "ming_2024_multimodal_generative_aI_copilot_for_human_pathology.pdf",
    "paper_title": "Ming 2024 Multimodal Generative Ai Copilot For Human Pathology",
    "chunk_index": 73,
    "total_chunks": 92,
    "text_content": "who evalu - ated them by ranking them based on their human expertise. For each question, the order of the model responses was randomly shuffled and the pathologist was blinded as to which model produced which response. The responses were ranked based on, in order of importance: (1) following the prompt (whether the response correctly addressed the instruction), (2) completeness of the answer, (3) succinctness and (4) use of accepted pathology terminology. Ties of two (or more) responses were all",
    "full_text_length": 93433,
    "chunk_length": 1261
  },
  {
    "chunk_id": 2228,
    "paper_filename": "ming_2024_multimodal_generative_aI_copilot_for_human_pathology.pdf",
    "paper_title": "Ming 2024 Multimodal Generative Ai Copilot For Human Pathology",
    "chunk_index": 74,
    "total_chunks": 92,
    "text_content": "blinded to which model pro - duced which response. For questions with a single best answer (for example, \u2018What is the most likely diagnosis?\u2019), the responses were labelled as incorrect if the single best answer was not provided. For the open-ended questions (for example, \u2018What IHC stains would be useful in working up a glioblastoma?\u2019), responses were labelled as incorrect if any portion of the response was hallucinated or if the response did not answer the question at all. Correct and incorrect ",
    "full_text_length": 93433,
    "chunk_length": 1281
  },
  {
    "chunk_id": 2229,
    "paper_filename": "ming_2024_multimodal_generative_aI_copilot_for_human_pathology.pdf",
    "paper_title": "Ming 2024 Multimodal Generative Ai Copilot For Human Pathology",
    "chunk_index": 75,
    "total_chunks": 92,
    "text_content": "their assessments for the questions on which they disagreed originally. Following this discussion, they ultimately agreed completely on 235 of the 260 questions for all models. In the \u2018Performance on answering open-ended questions\u2019 section, we report the performance on this subset of questions where a consensus was reached (using the consensus as the ground truth) and report the performance according to each individual expert\u2019s assessment for all questions in Extended Data Fig. 6. Evaluating GPT",
    "full_text_length": 93433,
    "chunk_length": 1287
  },
  {
    "chunk_id": 2230,
    "paper_filename": "ming_2024_multimodal_generative_aI_copilot_for_human_pathology.pdf",
    "paper_title": "Ming 2024 Multimodal Generative Ai Copilot For Human Pathology",
    "chunk_index": 76,
    "total_chunks": 92,
    "text_content": "of the pathology image and that they should instead consult a trained medical professional. Queries for which GPT-4V obviously refused to address the given instructions were deemed \u2018unsuccessful\u2019 . In such instances, we made a maximum of two further resubmissions for the same query for up to a total of three attempts. Following this evaluation protocol, we recorded 28 out of 52 successful queries in the multiple-choice diagnostic assessment of PathQABench-Public cases when no further clinical co",
    "full_text_length": 93433,
    "chunk_length": 1327
  },
  {
    "chunk_id": 2231,
    "paper_filename": "ming_2024_multimodal_generative_aI_copilot_for_human_pathology.pdf",
    "paper_title": "Ming 2024 Multimodal Generative Ai Copilot For Human Pathology",
    "chunk_index": 77,
    "total_chunks": 92,
    "text_content": "for the other models (\u2018MLLM evaluation\u2019). A breakdown of successful queries by category is provided in Supplementary Table 39. Statistical analysis We used nonparametric bootstrapping ( n = 1,000 replicates) to esti - mate 95% confidence intervals for the reported metrics. Observed differences in performance for a pair of models were tested for statisti - cal significance using a two-sided paired permutation test (n = 1,000 permutations), with the null hypothesis being that there is no differenc",
    "full_text_length": 93433,
    "chunk_length": 1272
  },
  {
    "chunk_id": 2232,
    "paper_filename": "ming_2024_multimodal_generative_aI_copilot_for_human_pathology.pdf",
    "paper_title": "Ming 2024 Multimodal Generative Ai Copilot For Human Pathology",
    "chunk_index": 78,
    "total_chunks": 92,
    "text_content": "we used eight 80 GB NVIDIA A100 GPUs con - figured for multi-GPU training using the popular open-source deep learning framework PyTorch (v.2.0.1, CUDA 11.8). All inference jobs were performed using 24 GB NVIDIA 3090 GPUs. We used the implementa- tion of MLLM training and inference provided by LLaVA (v.1.1.3) and incorporated our own custom vision encoder and multimodal projec - tor implemented in Timm (v.0.9.2) and PyTorch. Pillow (v.10.1.0) was used for image processing. Flash Attention (v.2.3.",
    "full_text_length": 93433,
    "chunk_length": 1269
  },
  {
    "chunk_id": 2233,
    "paper_filename": "ming_2024_multimodal_generative_aI_copilot_for_human_pathology.pdf",
    "paper_title": "Ming 2024 Multimodal Generative Ai Copilot For Human Pathology",
    "chunk_index": 79,
    "total_chunks": 92,
    "text_content": "four-node distributed set-up (eight GPUs per node). The vision encoder used in PathChat was fine-tuned from UNI using a single node of eight 80 GB NVIDIA A100 GPUs for 21.5 h. Lastly, the combined system of PathChat (including the vision encoder, the multimodal projector and the LLM) were jointly trained for a total of 17 h and 18 min (includes both pretraining and fine-tuning) on a single node of eight 80 GB NVIDIA A100 GPUs to produce the final model. For inference, the PathChat model was run ",
    "full_text_length": 93433,
    "chunk_length": 1296
  },
  {
    "chunk_id": 2234,
    "paper_filename": "ming_2024_multimodal_generative_aI_copilot_for_human_pathology.pdf",
    "paper_title": "Ming 2024 Multimodal Generative Ai Copilot For Human Pathology",
    "chunk_index": 80,
    "total_chunks": 92,
    "text_content": "Further information on research design is available in the Nature Port - folio Reporting Summary linked to this article. Data availability The PubMed Central-OA dataset can be accessed from the National Institutes of Health (NIH) PubMed Central website ( https://www.ncbi. nlm.nih.gov/pmc/tools/openftlist/). The TCGA WSIs and associated clinical metadata are available from the NIH genomic data commons (https://portal.gdc.cancer.gov ). The curated PathQABench-Public benchmark is released for resea",
    "full_text_length": 93433,
    "chunk_length": 1455
  },
  {
    "chunk_id": 2235,
    "paper_filename": "ming_2024_multimodal_generative_aI_copilot_for_human_pathology.pdf",
    "paper_title": "Ming 2024 Multimodal Generative Ai Copilot For Human Pathology",
    "chunk_index": 81,
    "total_chunks": 92,
    "text_content": "transfer agreement between the institu - tions and will limit the utility of the data to non-commercial academic research purposes. The exact timeline will depend on the execution of such agreements. Please email all requests to the corresponding author (and also include M. Y .L., mlu16@bwh.harvard.edu). Code availability The code used to train PathChat has been made publicly available for non-commercial academic use and can be accessed here: https:// github.com/fedshyvana/pathology_mllm_trainin",
    "full_text_length": 93433,
    "chunk_length": 1397
  },
  {
    "chunk_id": 2236,
    "paper_filename": "ming_2024_multimodal_generative_aI_copilot_for_human_pathology.pdf",
    "paper_title": "Ming 2024 Multimodal Generative Ai Copilot For Human Pathology",
    "chunk_index": 82,
    "total_chunks": 92,
    "text_content": "context. In Proc. Computer Vision\u2013 ECCV 2014: 13th European Conference (eds Fleet, D. et al.) 740\u2013755 (Springer, 2014). 62. Bai, J. et al. Qwen technical report. Preprint at arxiv.org/abs/2309.16609 (2023). 63. Liu, H. et al. Improved baselines with visual instruction tuning. In Proc. IEEE/CVF Conf. on Computer Vision and Pattern Recognition, 26296\u201326306 (IEEE, 2024). 64. Zeng, Y. et al. What matters in training a GPT4-style language model with multimodal inputs? In Proc. 2024 Conf. of the North",
    "full_text_length": 93433,
    "chunk_length": 1403
  },
  {
    "chunk_id": 2237,
    "paper_filename": "ming_2024_multimodal_generative_aI_copilot_for_human_pathology.pdf",
    "paper_title": "Ming 2024 Multimodal Generative Ai Copilot For Human Pathology",
    "chunk_index": 83,
    "total_chunks": 92,
    "text_content": "digital pathology image analysis. Sci. Rep. 7, 16878 (2017). 68. Lu, M. Y. et al. Code for pathology MLLM training, version 0.1, April 2024. GitHub github. com/fedshyvana/pathology_mllm_training (2024). Acknowledgements This work was supported in part by the Brigham & Women\u2019s Hospital president\u2019s fund, Brigham & Women\u2019s Hospital and Massachusetts General Hospital Pathology. R.J.C. was supported by a graduate fellowship form the National Science Foundation. We thank T. Janicki, R., A. Ahmed and t",
    "full_text_length": 93433,
    "chunk_length": 1406
  },
  {
    "chunk_id": 2238,
    "paper_filename": "ming_2024_multimodal_generative_aI_copilot_for_human_pathology.pdf",
    "paper_title": "Ming 2024 Multimodal Generative Ai Copilot For Human Pathology",
    "chunk_index": 84,
    "total_chunks": 92,
    "text_content": "D.F.K.W., M.Z., A.K.C., R.J.C., K.I., A.K., D.P., A.P., F.M. and A.S. performed the experimental analysis and interpreted the results. R.J.C., K.I., G.G., I.L., T.D., L.P.L. and A.V.P. provided feedback on the analysis. M.Y.L., B.C., D.F.K.W. and F.M. prepared the manuscript with input from all co-authors. F.M. supervised the research. Competing interests A patent corresponding to this work has been filed by Mass General Brigham (Application 63/608,671). The tools, processes and models associate",
    "full_text_length": 93433,
    "chunk_length": 1480
  },
  {
    "chunk_id": 2239,
    "paper_filename": "ming_2024_multimodal_generative_aI_copilot_for_human_pathology.pdf",
    "paper_title": "Ming 2024 Multimodal Generative Ai Copilot For Human Pathology",
    "chunk_index": 85,
    "total_chunks": 92,
    "text_content": "at http://www.nature.com/reprints. Article Extended Data Fig. 1 | Examples of instructions for finetuning MLLM. An example of each of six different types of instructions to develop PathChat via instruction finetuning is illustrated. Bolded texts represent instructions provided to the model while italicized texts represent the reference outputs the model is expected to output during training. More details on dataset curation are provided in the PathChat dataset curation section of Methods . Scale",
    "full_text_length": 93433,
    "chunk_length": 1291
  },
  {
    "chunk_id": 2240,
    "paper_filename": "ming_2024_multimodal_generative_aI_copilot_for_human_pathology.pdf",
    "paper_title": "Ming 2024 Multimodal Generative Ai Copilot For Human Pathology",
    "chunk_index": 86,
    "total_chunks": 92,
    "text_content": "the context only setting, the clinical context is provided to the model but the image is not provided (see Fig. 2a for an example multiple choice question that contains the clinical context, the choices, and the image). On the flip side, in the image only setting, the clinical context is not provided, and the model is asked to infer the correct diagnosis from the possible choices based solely on the image. We observed that PathChat achieves maximum performance when both clinical context and the ",
    "full_text_length": 93433,
    "chunk_length": 1265
  },
  {
    "chunk_id": 2241,
    "paper_filename": "ming_2024_multimodal_generative_aI_copilot_for_human_pathology.pdf",
    "paper_title": "Ming 2024 Multimodal Generative Ai Copilot For Human Pathology",
    "chunk_index": 87,
    "total_chunks": 92,
    "text_content": "The other models give incorrect locations that the image is from, give an incorrect description of the image, or are so general as to be unhelpful. Scale bar is 200 \u00b5m. Extended Data Fig. 4 | Comparing model outputs on open-ended question answering, example 2. An example question in PathQABench-Public regarding glioblastoma for which the responses by all models were considered to be of roughly comparable quality by expert pathologists for all producing a reasonable and reasonably accurate respon",
    "full_text_length": 93433,
    "chunk_length": 1263
  },
  {
    "chunk_id": 2242,
    "paper_filename": "ming_2024_multimodal_generative_aI_copilot_for_human_pathology.pdf",
    "paper_title": "Ming 2024 Multimodal Generative Ai Copilot For Human Pathology",
    "chunk_index": 88,
    "total_chunks": 92,
    "text_content": "open response performance. a. Accuracy of MLLMs on open-ended questions ( n = 260) as evaluated by two pathologists. See Fig. 3c,d for accuracy on the subset of open-ended questions for which the two pathologists reached a consensus. See MLLM evaluation in Methods for details. b . Accuracy on different categories of questions as rated by two pathologists. Microscopy ( n = 109), Diagnosis ( n = 87), Clinical ( n = 68), Ancillary Testing ( n = 87). Each question may belong to more than one categor",
    "full_text_length": 93433,
    "chunk_length": 1263
  },
  {
    "chunk_id": 2243,
    "paper_filename": "ming_2024_multimodal_generative_aI_copilot_for_human_pathology.pdf",
    "paper_title": "Ming 2024 Multimodal Generative Ai Copilot For Human Pathology",
    "chunk_index": 89,
    "total_chunks": 92,
    "text_content": "sub-category based on the topics and skills that it aims to assess. The broad categories are \u201cMicroscopy\u201d, \u201cDiagnosis\u201d, \u201cClinical\u201d and \u201cAncillary testing\u201d. A detailed description of each category is included in Supplementary Data Table 15. Scale bars are 200 \u00b5m. Extended Data Fig. 8 | Performance on PathQABench open-ended questions stratified by broad categories. We analyze the head-to-head performance of PathChat against other MLLMs in each broad category of questions evaluated by 7 pathologist",
    "full_text_length": 93433,
    "chunk_length": 1349
  },
  {
    "chunk_id": 2244,
    "paper_filename": "ming_2024_multimodal_generative_aI_copilot_for_human_pathology.pdf",
    "paper_title": "Ming 2024 Multimodal Generative Ai Copilot For Human Pathology",
    "chunk_index": 90,
    "total_chunks": 92,
    "text_content": "open-ended questions stratified by sub-categories. We further analyze the head-to- head performance of PathChat against other MLLMs in each sub-category of questions evaluated by 7 pathologists independently. For each competing model (LLaVA 1.5, LLaVA-Med, GPT4V), we compute the win/tie/lose rate of PathChat against said model. Win (dark green): PathChat is ranked higher than the model; Tie (light green): PathChat is tied with the model in ranking; Lose (red): PathChat is ranked lower than the m",
    "full_text_length": 93433,
    "chunk_length": 1183
  },
  {
    "chunk_id": 2245,
    "paper_filename": "ming_2024_multimodal_generative_aI_copilot_for_human_pathology.pdf",
    "paper_title": "Ming 2024 Multimodal Generative Ai Copilot For Human Pathology",
    "chunk_index": 91,
    "total_chunks": 92,
    "text_content": "cervical cancers should be positive for CK7 and CK20 IHC when in fact, cervical cancers are usually positive for CK7 but negative for CK20. Scale bar is 200 \u00b5m.",
    "full_text_length": 93433,
    "chunk_length": 160
  },
  {
    "chunk_id": 2246,
    "paper_filename": "nada_2024_YOLO_based_CAD_framework_with_ViT_transformer_for_breast_mass_classiifcaiton_in_cesm_images.pdf",
    "paper_title": "Nada 2024 Yolo Based Cad Framework With Vit Transformer For Breast Mass Classiifcaiton In Cesm Images",
    "chunk_index": 0,
    "total_chunks": 91,
    "text_content": "ORIGINAL ARTICLE YOLO-based CAD framework with ViT transformer for breast mass detection and classification in CESM and FFDM images Nada M. Hassan1\u2022Safwat Hamad2,3\u2022Khaled Mahar4 Received: 7 December 2022 / Accepted: 7 December 2023 / Published online: 16 January 2024 /C211The Author(s) 2024 Abstract Breast cancer detection is considered a challenging task for the average experienced radiologist due to the variation of thelesions\u2019 size and shape, especially with the existence of high \ufb01bro-glandul",
    "full_text_length": 91280,
    "chunk_length": 1430
  },
  {
    "chunk_id": 2247,
    "paper_filename": "nada_2024_YOLO_based_CAD_framework_with_ViT_transformer_for_breast_mass_classiifcaiton_in_cesm_images.pdf",
    "paper_title": "Nada 2024 Yolo Based Cad Framework With Vit Transformer For Breast Mass Classiifcaiton In Cesm Images",
    "chunk_index": 1,
    "total_chunks": 91,
    "text_content": "paper proposes a fully automated CAD framework based on YOLOv4 network and ViTtransformers for mass detection and classi\ufb01cation of Contrast Enhanced Spectral Mammography (CESM) images. CESM is an evolution type of Full Field Digital Mammography (FFDM) images that provides enhanced visualization for breast tissues. Different experiments were conducted to evaluate the proposed framework on two different datasets that areINbreast and CDD-CESM that provides both FFDM and CESM images. The model achie",
    "full_text_length": 91280,
    "chunk_length": 1447
  },
  {
    "chunk_id": 2248,
    "paper_filename": "nada_2024_YOLO_based_CAD_framework_with_ViT_transformer_for_breast_mass_classiifcaiton_in_cesm_images.pdf",
    "paper_title": "Nada 2024 Yolo Based Cad Framework With Vit Transformer For Breast Mass Classiifcaiton In Cesm Images",
    "chunk_index": 2,
    "total_chunks": 91,
    "text_content": "especially with the highly dense breast tissues. Keywords CESM /C1Vision transformer /C1Mass detection /C1Mass classi\ufb01cation Abbreviations ACR-BIRADS American College of Radiology Breast Imaging Reporting and Data System ACS American Cancer Society AUC Area Under CurveCAD Computer-Aided (Diagnoses/Detection) CBIS-DDSM Curated Breast Imaging Subset of DDSM CC Carnio CaudalCDD-CESM Categorized Digital Database for Low energy and Subtracted Contrast Enhanced Spectral Mammography CE Contrast Enhance",
    "full_text_length": 91280,
    "chunk_length": 1661
  },
  {
    "chunk_id": 2249,
    "paper_filename": "nada_2024_YOLO_based_CAD_framework_with_ViT_transformer_for_breast_mass_classiifcaiton_in_cesm_images.pdf",
    "paper_title": "Nada 2024 Yolo Based Cad Framework With Vit Transformer For Breast Mass Classiifcaiton In Cesm Images",
    "chunk_index": 3,
    "total_chunks": 91,
    "text_content": "Arab Academy for Science and Technology, Alexandria, Egypt 123Neural Computing and Applications (2024) 36:6467\u20136496 https://doi.org/10.1007/s00521-023-09364-5 (0123456789().,-volV) (0123456789(). ,- volV) FNR False Positive Rate FP False Positive FPR False Positive RateFRCN Full Resolution Convolutional Network GLCM Gray Level Co-occurrence Matrix IoU Intersection over UnionLWT Lifting Wavelet Transform mAP Mean Average Precision MIAS Ammographic Image Analysis SocietyMLO Medio Lateral Oblique M",
    "full_text_length": 91280,
    "chunk_length": 1396
  },
  {
    "chunk_id": 2250,
    "paper_filename": "nada_2024_YOLO_based_CAD_framework_with_ViT_transformer_for_breast_mass_classiifcaiton_in_cesm_images.pdf",
    "paper_title": "Nada 2024 Yolo Based Cad Framework With Vit Transformer For Breast Mass Classiifcaiton In Cesm Images",
    "chunk_index": 4,
    "total_chunks": 91,
    "text_content": "ones. The American Cancer Society (ACS) published an estimation for the breast cancer cases among females in the USA only (2022), and it stated that about 290,560 newcases would be diagnosed with breast cancer and 43,780 would die. Most cases mainly occur between 45 and 62 years old [ 1]. The substantial support for awareness about the risks of breast cancer helped a lot in the early diagnosis of this disease. This happens through encourag- ing regular check-ups, which can be done via differenta",
    "full_text_length": 91280,
    "chunk_length": 1303
  },
  {
    "chunk_id": 2251,
    "paper_filename": "nada_2024_YOLO_based_CAD_framework_with_ViT_transformer_for_breast_mass_classiifcaiton_in_cesm_images.pdf",
    "paper_title": "Nada 2024 Yolo Based Cad Framework With Vit Transformer For Breast Mass Classiifcaiton In Cesm Images",
    "chunk_index": 5,
    "total_chunks": 91,
    "text_content": "Various abnormalitiescan be detected through mammogram interpretation; these abnormalities can be classi\ufb01ed into masses and calci\ufb01ca- tions. Those abnormalities are diagnosed as benign or malignant according to their appearance and morphologicalfeatures such as shape and pattern. Benign masses have an oval or circular shape with well-de\ufb01ned edges, while malignant masses look like it has spikes out from theircenter. The accurate interpretation of the mammogram leads to precise diagnosis; Radiolog",
    "full_text_length": 91280,
    "chunk_length": 1343
  },
  {
    "chunk_id": 2252,
    "paper_filename": "nada_2024_YOLO_based_CAD_framework_with_ViT_transformer_for_breast_mass_classiifcaiton_in_cesm_images.pdf",
    "paper_title": "Nada 2024 Yolo Based Cad Framework With Vit Transformer For Breast Mass Classiifcaiton In Cesm Images",
    "chunk_index": 6,
    "total_chunks": 91,
    "text_content": "made these systems work moreconsistently, especially now there are a variety of neural network architectures that can be used in different ways depending on the data type in order to get the most out ofthem. For example, CNN models \ufb01t more the medical imaging tasks [ 3], while RNN [ 4,5] models seem to be more effective with the sequential data such as genetic data[6]. Therefore, the data type is critical in determining which architecture will be used to design the CAD system. This work mainly f",
    "full_text_length": 91280,
    "chunk_length": 1281
  },
  {
    "chunk_id": 2253,
    "paper_filename": "nada_2024_YOLO_based_CAD_framework_with_ViT_transformer_for_breast_mass_classiifcaiton_in_cesm_images.pdf",
    "paper_title": "Nada 2024 Yolo Based Cad Framework With Vit Transformer For Breast Mass Classiifcaiton In Cesm Images",
    "chunk_index": 7,
    "total_chunks": 91,
    "text_content": "such as shape, mass size variation ranging from too small to very large, and thenature of the breast tissues that sometimes mask the mas- ses, especially with the highly dense breast tissues. Mammograms were generated using systems based on phosphorescent screen-\ufb01lm technology until the US Food and Drug Administration approved the Full Field Digital Mammography (FFDM) systems. The FFDM systems havesigni\ufb01cantly improved the quality of mammographic ima- ges and the sensitivity of breast cancer det",
    "full_text_length": 91280,
    "chunk_length": 1331
  },
  {
    "chunk_id": 2254,
    "paper_filename": "nada_2024_YOLO_based_CAD_framework_with_ViT_transformer_for_breast_mass_classiifcaiton_in_cesm_images.pdf",
    "paper_title": "Nada 2024 Yolo Based Cad Framework With Vit Transformer For Breast Mass Classiifcaiton In Cesm Images",
    "chunk_index": 8,
    "total_chunks": 91,
    "text_content": "most of the \ufb01bro-glandular tissues have the same image gray levels of the lesions. This can lower the sensitivity in detecting tumors when breastdensity increases [ 8]. Recently, Contrast Enhanced Spec- tral Mammography (CESM) was introduced in 2011 as a new image technique for mammogram screening. CESMprovides improved visualization for mammographic ima- ges by combining low and high-energy breast images. Figure 1illustrates the difference between the FFDM and CESM images and how the CESM can p",
    "full_text_length": 91280,
    "chunk_length": 1375
  },
  {
    "chunk_id": 2255,
    "paper_filename": "nada_2024_YOLO_based_CAD_framework_with_ViT_transformer_for_breast_mass_classiifcaiton_in_cesm_images.pdf",
    "paper_title": "Nada 2024 Yolo Based Cad Framework With Vit Transformer For Breast Mass Classiifcaiton In Cesm Images",
    "chunk_index": 9,
    "total_chunks": 91,
    "text_content": "with the use of transfer learningconcept. As deep learning has great capability to auto- matically extract the deep features from the mammograms with no need for hand-crafted features, in addition to that,transfer learning reduces the computational cost and the need of large datasets for training. Convolutional Neural Networks (CNN) ruled the detection and classi\ufb01cation tasks in medical imaging diagnosis through the last few years in the most of these studies, relying on the idea of the dependen",
    "full_text_length": 91280,
    "chunk_length": 1337
  },
  {
    "chunk_id": 2256,
    "paper_filename": "nada_2024_YOLO_based_CAD_framework_with_ViT_transformer_for_breast_mass_classiifcaiton_in_cesm_images.pdf",
    "paper_title": "Nada 2024 Yolo Based Cad Framework With Vit Transformer For Breast Mass Classiifcaiton In Cesm Images",
    "chunk_index": 10,
    "total_chunks": 91,
    "text_content": "performance compared to CNNs; vision transformers arebuilt on the attention mechanism, which focuses on the local and global spatial features [ 11]. Few studies adopted the ViT transformers in medical imaging tasks [ 12\u201314] especially in breast cancer diagnosis. Furthermore, few works proposed a fully automated model for detecting andclassifying the masses. Based on the aforementioned points, this work proposes a fully automated framework in an end-to-end training fashion for mass detection andc",
    "full_text_length": 91280,
    "chunk_length": 1310
  },
  {
    "chunk_id": 2257,
    "paper_filename": "nada_2024_YOLO_based_CAD_framework_with_ViT_transformer_for_breast_mass_classiifcaiton_in_cesm_images.pdf",
    "paper_title": "Nada 2024 Yolo Based Cad Framework With Vit Transformer For Breast Mass Classiifcaiton In Cesm Images",
    "chunk_index": 11,
    "total_chunks": 91,
    "text_content": "points: 1. Integrate the YOLOv4 with ViT transformers for mass detection and classi\ufb01cation, to utilize the capability of ViT in learning the local and global spatial features inthe mass classi\ufb01cation instead of the CNN models. 2. To the best of our knowledge, this is the \ufb01rst fully automated CAD framework for mass detection and Fig. 1 aLow-energy FFDM image shows negative \ufb01ndings; brecombination CESM image that clearly shows the existenceof mass [ 8]Neural Computing and Applications (2024) 36:64",
    "full_text_length": 91280,
    "chunk_length": 1342
  },
  {
    "chunk_id": 2258,
    "paper_filename": "nada_2024_YOLO_based_CAD_framework_with_ViT_transformer_for_breast_mass_classiifcaiton_in_cesm_images.pdf",
    "paper_title": "Nada 2024 Yolo Based Cad Framework With Vit Transformer For Breast Mass Classiifcaiton In Cesm Images",
    "chunk_index": 12,
    "total_chunks": 91,
    "text_content": "CNN models at mass classi\ufb01cation in mammograms. 6. Utilize the newly introduced CDD-CESM dataset [ 15] for mass detection and classi\ufb01cation to evaluate the performance of the proposed model. This paper is organized as follows: Sect. 2presents the literature survey, while Sect. 3demonstrates the methods and materials employed in this work. Section 4shows the experimental design and results, and Sect. 5discusses the results. Finally, Sect. 6presents the conclusion, while Sect. 7discusses the advan",
    "full_text_length": 91280,
    "chunk_length": 1405
  },
  {
    "chunk_id": 2259,
    "paper_filename": "nada_2024_YOLO_based_CAD_framework_with_ViT_transformer_for_breast_mass_classiifcaiton_in_cesm_images.pdf",
    "paper_title": "Nada 2024 Yolo Based Cad Framework With Vit Transformer For Breast Mass Classiifcaiton In Cesm Images",
    "chunk_index": 13,
    "total_chunks": 91,
    "text_content": "Detection can be described as the process of localizing the abnormal area or spots within the mammogram images, while segmentation mainly targets the pixel-by-pixel annotation of the abnormal \ufb01ndings; \ufb01nally, the classi\ufb01-cation is considered as the process of classifying the \ufb01nd- ings into (Normal/Abnormal) or (Benign/Malignant). Some studies focused on observing the morphological features(texture, color, brightness, etc.) in their works to extract the ROIs and then classify them into benign or ",
    "full_text_length": 91280,
    "chunk_length": 1436
  },
  {
    "chunk_id": 2260,
    "paper_filename": "nada_2024_YOLO_based_CAD_framework_with_ViT_transformer_for_breast_mass_classiifcaiton_in_cesm_images.pdf",
    "paper_title": "Nada 2024 Yolo Based Cad Framework With Vit Transformer For Breast Mass Classiifcaiton In Cesm Images",
    "chunk_index": 14,
    "total_chunks": 91,
    "text_content": "the most discriminant features [ 21]. Furthermore, feature-based techniques are often designed to capture speci\ufb01c characteristics or patterns through the training phase. Accordingly, this may not generalize wellto unseen data, especially since those techniques struggle with large and high-dimensional feature space datasets; and this can lead to more computational overhead. On the other side, with the appearance of deep learning, CNNs replaced the traditional hand engineering approa-ches for feat",
    "full_text_length": 91280,
    "chunk_length": 1445
  },
  {
    "chunk_id": 2261,
    "paper_filename": "nada_2024_YOLO_based_CAD_framework_with_ViT_transformer_for_breast_mass_classiifcaiton_in_cesm_images.pdf",
    "paper_title": "Nada 2024 Yolo Based Cad Framework With Vit Transformer For Breast Mass Classiifcaiton In Cesm Images",
    "chunk_index": 15,
    "total_chunks": 91,
    "text_content": "the problems of feature-based techniques. Automating the feature extraction process through deep learning saves thetime and effort needed to extract hand-crafted features. The deep learning techniques capture the features at multiple levels of abstraction, and this hierarchal representationallows more informative features to be extracted. More- over, transfer learning reduces the computational cost of learning the features from scratch; as the pre-trainedmodels can be used as a feature extractor",
    "full_text_length": 91280,
    "chunk_length": 1374
  },
  {
    "chunk_id": 2262,
    "paper_filename": "nada_2024_YOLO_based_CAD_framework_with_ViT_transformer_for_breast_mass_classiifcaiton_in_cesm_images.pdf",
    "paper_title": "Nada 2024 Yolo Based Cad Framework With Vit Transformer For Breast Mass Classiifcaiton In Cesm Images",
    "chunk_index": 16,
    "total_chunks": 91,
    "text_content": "discriminant features about masses and calci-\ufb01cations; and accordingly, this may enhance the results. In [26], the authors also proposed a CAD system for whole mammogram classi\ufb01cation based on deep CNNs forfeature extraction and SVM for classi\ufb01cation. They con- ducted their experiments on two different datasets, MIAS and INbreast, that is composed of FFDM images; theirapproach achieved an accuracy of 97.93% and 96.64%, respectively. In 2015, a giant leap occurred in object detection tech- niques",
    "full_text_length": 91280,
    "chunk_length": 1353
  },
  {
    "chunk_id": 2263,
    "paper_filename": "nada_2024_YOLO_based_CAD_framework_with_ViT_transformer_for_breast_mass_classiifcaiton_in_cesm_images.pdf",
    "paper_title": "Nada 2024 Yolo Based Cad Framework With Vit Transformer For Breast Mass Classiifcaiton In Cesm Images",
    "chunk_index": 17,
    "total_chunks": 91,
    "text_content": "they introduced a YOLO-based CAD system that achieved an accuracy of 85.2% for detection. In[29], they proposed a CAD system for mass detection, segmentation, and classi\ufb01cation; they utilized YOLO for detection, then segmented the detected masses using Full Resolution Convolutional Network (FRCN) and AlexNet architecture-based classi\ufb01er for classi\ufb01cation. Theirapproach achieved a detection accuracy of 97.2%, seg- mentation accuracy of 92.97%, and classi\ufb01cation accuracy of 95.3%. However, their m",
    "full_text_length": 91280,
    "chunk_length": 1393
  },
  {
    "chunk_id": 2264,
    "paper_filename": "nada_2024_YOLO_based_CAD_framework_with_ViT_transformer_for_breast_mass_classiifcaiton_in_cesm_images.pdf",
    "paper_title": "Nada 2024 Yolo Based Cad Framework With Vit Transformer For Breast Mass Classiifcaiton In Cesm Images",
    "chunk_index": 18,
    "total_chunks": 91,
    "text_content": "has high inference rate/ image relative to the other recent similar studies. Also, Faster-RCNN [ 31] was one of the object detection models that showed promising performance in some stud- ies. In [ 32], Ribli et al. also utilized the two-shot detector Faster-RCNN in their developed system for mass detection and classi\ufb01cation. Their system detected 90% of the malignant masses with a classi\ufb01cation accuracy of 95%.Agarwal et al. [ 33] also proposed a Faster-RCNN-based model for mass detection; the ",
    "full_text_length": 91280,
    "chunk_length": 1285
  },
  {
    "chunk_id": 2265,
    "paper_filename": "nada_2024_YOLO_based_CAD_framework_with_ViT_transformer_for_breast_mass_classiifcaiton_in_cesm_images.pdf",
    "paper_title": "Nada 2024 Yolo Based Cad Framework With Vit Transformer For Breast Mass Classiifcaiton In Cesm Images",
    "chunk_index": 19,
    "total_chunks": 91,
    "text_content": "increase the size of the dataset. This augmentation technique enhanced their results; however, it has more computational costrather than the traditional augmentation techniques. The model attained 0.495 False Positive Rate (FPR)/image for INbreast and 0.599 FPR/image for DDSM.Shen et al. [ 36] proposed a framework for mass detec- tion with an attempt to automate the process of mass annotation in mammographic images. The model mainly depended on adversarial learning; the experiments weredone over",
    "full_text_length": 91280,
    "chunk_length": 1323
  },
  {
    "chunk_id": 2266,
    "paper_filename": "nada_2024_YOLO_based_CAD_framework_with_ViT_transformer_for_breast_mass_classiifcaiton_in_cesm_images.pdf",
    "paper_title": "Nada 2024 Yolo Based Cad Framework With Vit Transformer For Breast Mass Classiifcaiton In Cesm Images",
    "chunk_index": 20,
    "total_chunks": 91,
    "text_content": "this type of image has replaced the SFM images in recent years;however, one of the problems still exists is the masked masses in cases with high breast density. In FFDM, the gray levels of the glandular tissues have similar values as the masses. Accordingly, this makes the mass hide within the dense tissues and decreases the visi- bility of the masses. CESM is a relatively new techniquefor obtaining mammographic images; this technique depends on getting a new mammographic image by sub- tracting ",
    "full_text_length": 91280,
    "chunk_length": 1259
  },
  {
    "chunk_id": 2267,
    "paper_filename": "nada_2024_YOLO_based_CAD_framework_with_ViT_transformer_for_breast_mass_classiifcaiton_in_cesm_images.pdf",
    "paper_title": "Nada 2024 Yolo Based Cad Framework With Vit Transformer For Breast Mass Classiifcaiton In Cesm Images",
    "chunk_index": 21,
    "total_chunks": 91,
    "text_content": "of 97.2%; they used the CDD-CESM dataset in their experiments. Other studies [ 39\u201341] introduced dif- ferent classi\ufb01cation models based on deep learning and conventional machine learning for CESM images. From Table 1, it can be noticeable that most of the proposed work was done mainly on the FFDM images, few studies only have been proposed CAD systems for CESM. There is not enough evaluation of the performance of CAD systemson CESM images, speci\ufb01cally in mass detection. Moreover, based on the pr",
    "full_text_length": 91280,
    "chunk_length": 1335
  },
  {
    "chunk_id": 2268,
    "paper_filename": "nada_2024_YOLO_based_CAD_framework_with_ViT_transformer_for_breast_mass_classiifcaiton_in_cesm_images.pdf",
    "paper_title": "Nada 2024 Yolo Based Cad Framework With Vit Transformer For Breast Mass Classiifcaiton In Cesm Images",
    "chunk_index": 22,
    "total_chunks": 91,
    "text_content": "and the relationship between image pixels in mass classi\ufb01cation.Consequently, this work proposes a fully automated framework for mass detection and classi\ufb01cation in CESMimages by integrating the power of YOLO with the ViT transformer. Furthermore, the performance of the model was also evaluated on FFDM images. Also, the workTable 1 Summary of recent studies for both deep learning and feature-based techniques Reference Methodology Feature extractionDataset Image typeDetection Classi\ufb01cation Result",
    "full_text_length": 91280,
    "chunk_length": 1476
  },
  {
    "chunk_id": 2269,
    "paper_filename": "nada_2024_YOLO_based_CAD_framework_with_ViT_transformer_for_breast_mass_classiifcaiton_in_cesm_images.pdf",
    "paper_title": "Nada 2024 Yolo Based Cad Framework With Vit Transformer For Breast Mass Classiifcaiton In Cesm Images",
    "chunk_index": 23,
    "total_chunks": 91,
    "text_content": "AlexNet-based CNN lv?SVMDeep learningDDSM CBIS-DDSMFM \u2014 4 Classi\ufb01cation Acc: 80.5%, 87.2% [38] Multi-feature deep- information bottleneckDeep learningCDD-CESM CESM \u2014 4 Classi\ufb01cation Acc: 97.2% [39] Multilayer Perceptron (MLP) Feature- basedPrivate dataset CESM \u2014 4 Classi\ufb01cation acc: 84.8% [41] AlexNet and RawNet Deep learningPrivate datasetCESM \u2014 4 Classi\ufb01cation sensitivity: 100% Classi\ufb01cation speci\ufb01city: 66% [42] Three different features extraction methods (Statistical ?GLCM features, two metho",
    "full_text_length": 91280,
    "chunk_length": 1541
  },
  {
    "chunk_id": 2270,
    "paper_filename": "nada_2024_YOLO_based_CAD_framework_with_ViT_transformer_for_breast_mass_classiifcaiton_in_cesm_images.pdf",
    "paper_title": "Nada 2024 Yolo Based Cad Framework With Vit Transformer For Breast Mass Classiifcaiton In Cesm Images",
    "chunk_index": 24,
    "total_chunks": 91,
    "text_content": "summarizes recent studies in FFDM and CESM images formass detection and/or (mass/whole image) classi\ufb01cation. 3 Methods and materials This work proposes a fully automated framework for mass detection and classi\ufb01cation in end-to-end training strategy in CESM and FFDM images; deep learning models areadopted in both phases. The proposed CAD system can be divided into three parts, as shown in Fig. 2; pre-processing, detection, and classi\ufb01cation. The pre-processing steps wereinspired by the technique ",
    "full_text_length": 91280,
    "chunk_length": 1296
  },
  {
    "chunk_id": 2271,
    "paper_filename": "nada_2024_YOLO_based_CAD_framework_with_ViT_transformer_for_breast_mass_classiifcaiton_in_cesm_images.pdf",
    "paper_title": "Nada 2024 Yolo Based Cad Framework With Vit Transformer For Breast Mass Classiifcaiton In Cesm Images",
    "chunk_index": 25,
    "total_chunks": 91,
    "text_content": "of the images. Gaussian \ufb01lter is a linear \ufb01lter with a symmetric kernel with an odd size that passes through each pixel in the image. The values inside the kernels arecalculated as shown in Eq. 1, where ( x,y) represents the pixel coordinate, ris the standard deviation of the Gaus- sian distribution. Fig. 2 Flow diagram for the steps of the proposedframework for mass detection and classi\ufb01cationNeural Computing and Applications (2024) 36:6467\u20136496 6473 123 Gx ;y\u00f0\u00de \u00bc1 2pr2e/C0x2\u00fey2 2r2 \u00f01\u00de Otsu th",
    "full_text_length": 91280,
    "chunk_length": 1236
  },
  {
    "chunk_id": 2272,
    "paper_filename": "nada_2024_YOLO_based_CAD_framework_with_ViT_transformer_for_breast_mass_classiifcaiton_in_cesm_images.pdf",
    "paper_title": "Nada 2024 Yolo Based Cad Framework With Vit Transformer For Breast Mass Classiifcaiton In Cesm Images",
    "chunk_index": 26,
    "total_chunks": 91,
    "text_content": "rather than the whole image. This algorithm is used to enhance the contrast of the medical images to improve thevisual appearance of the mammogram. The clip limit (CL) is an essential parameter for CLAHE, as this parameter controls the image\u2019s brightness level. In the pre-processing phase, two clip limits were used, as shown in Fig. 2. 3.2 Detection phase At this phase, You Look Only Once (YOLO) model has been selected for mass detection. YOLO is a well-known object detection architecture known ",
    "full_text_length": 91280,
    "chunk_length": 1257
  },
  {
    "chunk_id": 2273,
    "paper_filename": "nada_2024_YOLO_based_CAD_framework_with_ViT_transformer_for_breast_mass_classiifcaiton_in_cesm_images.pdf",
    "paper_title": "Nada 2024 Yolo Based Cad Framework With Vit Transformer For Breast Mass Classiifcaiton In Cesm Images",
    "chunk_index": 27,
    "total_chunks": 91,
    "text_content": "YOLO. There are different of\ufb01cial versions of YOLO, which are YOLOv1, YOLOv2 [ 47], YOLOv3 [ 48], YOLOv4 [ 49], YOLOv7 [ 50], and YOLOv8 [ 51]. Different studies [7,52,53] show that YOLOv4 performs better than the other elder versions, YOLOv1, YOLOv2, and YOLOv3. The improvements that were introduced in YOLOv4enhanced the accuracy and detection time. However, the recent versions, YOLOv7 and YOLOv8, were introduced with some improvements to enhance the trade-off betweenaccuracy and time. 3.2.1 YO",
    "full_text_length": 91280,
    "chunk_length": 1338
  },
  {
    "chunk_id": 2274,
    "paper_filename": "nada_2024_YOLO_based_CAD_framework_with_ViT_transformer_for_breast_mass_classiifcaiton_in_cesm_images.pdf",
    "paper_title": "Nada 2024 Yolo Based Cad Framework With Vit Transformer For Breast Mass Classiifcaiton In Cesm Images",
    "chunk_index": 28,
    "total_chunks": 91,
    "text_content": "classi\ufb01cation6474 Neural Computing and Applications (2024) 36:6467\u20136496 123 YOLOv7 and YOLOv8 regarding mAP, recall, and preci- sion; however, YOLOv7 showed competitive results on theINbreast dataset. On the other side, YOLOv7 and YOLOv8 provide faster performance than YOLOv4, as shown in Table 2. Based on the conducted experiments, YOLOv7 strug- gles in small mass detection, especially with crowded mammographic scenes, whether false mass detection ormissed mass detection, especially with CESM-C",
    "full_text_length": 91280,
    "chunk_length": 1355
  },
  {
    "chunk_id": 2275,
    "paper_filename": "nada_2024_YOLO_based_CAD_framework_with_ViT_transformer_for_breast_mass_classiifcaiton_in_cesm_images.pdf",
    "paper_title": "Nada 2024 Yolo Based Cad Framework With Vit Transformer For Breast Mass Classiifcaiton In Cesm Images",
    "chunk_index": 29,
    "total_chunks": 91,
    "text_content": "mechanism [ 55]. Also, the model is Fig. 4 YOLO v4 Architecture [ 33] Table 2 Detection results of YOLOv4, YOLOv7, and YOLOv8 on INbreast andCESM-CDD datasetsDataset Model mAP(%)@ IoU = 0.5 Recall (%) Precision (%) Inference/image INbreast YOLOv4 98.96% 100 92 0.0892 s YOLOv7 97.60% 95.60 95.70 0.0377 sYOLOv8 89.3% 85.7 86.8 0.0545 s CE-CDD YOLOv4 81.52% 79 77 0.0833 s YOLOv7 61.70% 64.10 66.30 0.0469 sYOLOv8 50.60% 54.80 58.50 0.0578 s DM-CDD YOLOv4 71.65% 71 68 0.0144 s YOLOv7 58.10% 54.10 67.",
    "full_text_length": 91280,
    "chunk_length": 1234
  },
  {
    "chunk_id": 2276,
    "paper_filename": "nada_2024_YOLO_based_CAD_framework_with_ViT_transformer_for_breast_mass_classiifcaiton_in_cesm_images.pdf",
    "paper_title": "Nada 2024 Yolo Based Cad Framework With Vit Transformer For Breast Mass Classiifcaiton In Cesm Images",
    "chunk_index": 30,
    "total_chunks": 91,
    "text_content": "probability of the overlapping between the small-size objects and other size objects that may partiallyblock its appearance. Due to the consequences of the above reasons, at the detection phase, YOLOv4 was used to mainly detect the masses existing in the mammograms. It splits the input mammographic image into grid cells (s 9s) cells; if the mass falls within the cell, it is considered responsible for detecting this mass. A \ufb01xed number of bounding boxes is predicted for each cell with their con\ufb01d",
    "full_text_length": 91280,
    "chunk_length": 1304
  },
  {
    "chunk_id": 2277,
    "paper_filename": "nada_2024_YOLO_based_CAD_framework_with_ViT_transformer_for_breast_mass_classiifcaiton_in_cesm_images.pdf",
    "paper_title": "Nada 2024 Yolo Based Cad Framework With Vit Transformer For Breast Mass Classiifcaiton In Cesm Images",
    "chunk_index": 31,
    "total_chunks": 91,
    "text_content": "the dataset and the resolution of the input image;accordingly, the K-Means clustering algorithm was used for that in all conducted experiments. The anchors were generated in these experiments for each dataset separatelyand based on the different resolutions used. 3.3 Classification phase 3.3.1 Vision transformers The transformer is considered a de facto architecture for Natural Language Processing (NLP) tasks. The trans- formers generally are built on the self-attention mechanismthat allows the ",
    "full_text_length": 91280,
    "chunk_length": 1382
  },
  {
    "chunk_id": 2278,
    "paper_filename": "nada_2024_YOLO_based_CAD_framework_with_ViT_transformer_for_breast_mass_classiifcaiton_in_cesm_images.pdf",
    "paper_title": "Nada 2024 Yolo Based Cad Framework With Vit Transformer For Breast Mass Classiifcaiton In Cesm Images",
    "chunk_index": 32,
    "total_chunks": 91,
    "text_content": "introduced the ViT transformer. After that, many were proposed, such as DeiT (Data Ef\ufb01cient Transformer) [ 58], Swin\u2013Transformer[59], and ConvNeXt [ 60]. Furthermore, different studies recently utilized transformers in various medical imaging tasks such as classi\ufb01cation, segmentation, and detection. In [61], the authors proposed a model for predicting breast tumor malignancy using a convNeXt transformer over ultrasound images. Van et al. [ 62] also introduced a model that utilized transformers t",
    "full_text_length": 91280,
    "chunk_length": 1362
  },
  {
    "chunk_id": 2279,
    "paper_filename": "nada_2024_YOLO_based_CAD_framework_with_ViT_transformer_for_breast_mass_classiifcaiton_in_cesm_images.pdf",
    "paper_title": "Nada 2024 Yolo Based Cad Framework With Vit Transformer For Breast Mass Classiifcaiton In Cesm Images",
    "chunk_index": 33,
    "total_chunks": 91,
    "text_content": "the local texture, which represents a local subset of pixels from the image, and that enforces the network to ignore theglobal context of the features as the network fails to encode the relative position of the features; accordingly, different studies were proposed recently to overcome this problemby utilizing attention mechanisms and pyramid networks. Transformers are one of the recent architectures that built on the concept of the self-attention mechanism. Thesigni\ufb01cant advantage of transforme",
    "full_text_length": 91280,
    "chunk_length": 1342
  },
  {
    "chunk_id": 2280,
    "paper_filename": "nada_2024_YOLO_based_CAD_framework_with_ViT_transformer_for_breast_mass_classiifcaiton_in_cesm_images.pdf",
    "paper_title": "Nada 2024 Yolo Based Cad Framework With Vit Transformer For Breast Mass Classiifcaiton In Cesm Images",
    "chunk_index": 34,
    "total_chunks": 91,
    "text_content": "ViT Transformer is used for classi\ufb01cation in this work; as shown in Fig. 5, at the classi\ufb01cation phase, there are three main components of the ViT network; patchembedding, feature extraction using stacked transformer encoders, and the classi\ufb01cation head that is built with Multilayer Perceptron (MLP). The mammography image is reshaped into a sequence of patches. Then, the patches are \ufb02attened and mapped to dimensions with linear projection, as a constant latent6476 Neural Computing and Applicatio",
    "full_text_length": 91280,
    "chunk_length": 1275
  },
  {
    "chunk_id": 2281,
    "paper_filename": "nada_2024_YOLO_based_CAD_framework_with_ViT_transformer_for_breast_mass_classiifcaiton_in_cesm_images.pdf",
    "paper_title": "Nada 2024 Yolo Based Cad Framework With Vit Transformer For Breast Mass Classiifcaiton In Cesm Images",
    "chunk_index": 35,
    "total_chunks": 91,
    "text_content": "the input sequence length (number of patches) that will be fed to the transformer after applying linear projection according to Eq. 2. z0\u00bcxclass ;x1 pE;x2 pE;/C1/C1/C1 ;xN pEhi \u00feEpos; E2RP2/C1C\u00f0\u00de /C2D;Epos2RN\u00fe1 \u00f0\u00de /C2 D\u00f02\u00de At the patch embedding phase and after splitting the image into n2patches of shape ( P,P,C), the \ufb02attened patches are multiplied by a trainable embedding tensor which is in a shape of ( P2/C1C;d) to learn how to project each patch linearly into d dimension (where dis a constan",
    "full_text_length": 91280,
    "chunk_length": 1259
  },
  {
    "chunk_id": 2282,
    "paper_filename": "nada_2024_YOLO_based_CAD_framework_with_ViT_transformer_for_breast_mass_classiifcaiton_in_cesm_images.pdf",
    "paper_title": "Nada 2024 Yolo Based Cad Framework With Vit Transformer For Breast Mass Classiifcaiton In Cesm Images",
    "chunk_index": 36,
    "total_chunks": 91,
    "text_content": "prediction of the class label if it is a benign mass or a malignant mass.3.4 Transfer learning This work adopted the transfer learning concept due to the lack of publicly available mammograms and to minimize the training time. Pre-trained weights for different net-works were used to initialize the parameters of these net- works through the training phase. The YOLO network was \ufb01ne-tuned for detection, and the pre-trained weights on the COCO dataset were trans- ferred to initialize the network wei",
    "full_text_length": 91280,
    "chunk_length": 1238
  },
  {
    "chunk_id": 2283,
    "paper_filename": "nada_2024_YOLO_based_CAD_framework_with_ViT_transformer_for_breast_mass_classiifcaiton_in_cesm_images.pdf",
    "paper_title": "Nada 2024 Yolo Based Cad Framework With Vit Transformer For Breast Mass Classiifcaiton In Cesm Images",
    "chunk_index": 37,
    "total_chunks": 91,
    "text_content": "used ViT transformer; this model was pre-trained on ImageNet21K. The input image size ofthe model is 224 9224, and the input images in this model are divided into 16 916 patches. The Adam optimizer was used through training. A linear layer was added on the topof the pre-trained encoder to downstream the model for the mass classi\ufb01cation task. Additionally, the MLP head was modi\ufb01ed to just generate two outputs that are malignantand benign. Furthermore, for the other classi\ufb01cation models, some modi",
    "full_text_length": 91280,
    "chunk_length": 1258
  },
  {
    "chunk_id": 2284,
    "paper_filename": "nada_2024_YOLO_based_CAD_framework_with_ViT_transformer_for_breast_mass_classiifcaiton_in_cesm_images.pdf",
    "paper_title": "Nada 2024 Yolo Based Cad Framework With Vit Transformer For Breast Mass Classiifcaiton In Cesm Images",
    "chunk_index": 38,
    "total_chunks": 91,
    "text_content": "was also modi\ufb01ed to output malignant or benign. ConvNeXt architecture was adopted in the experiments to explore the potential of this model, as the architecture of this model integrates the ConvNeXt based on ResNet50 with the design of the training approaches of the vision transformer, the network was pre-trained on ImageNet22K, with an input image size of 224 9224. ResNet50 was modi\ufb01ed; the last dense layer was excluded from the feature extraction layers. Moreover, the layers of ResNet were fro",
    "full_text_length": 91280,
    "chunk_length": 1243
  },
  {
    "chunk_id": 2285,
    "paper_filename": "nada_2024_YOLO_based_CAD_framework_with_ViT_transformer_for_breast_mass_classiifcaiton_in_cesm_images.pdf",
    "paper_title": "Nada 2024 Yolo Based Cad Framework With Vit Transformer For Breast Mass Classiifcaiton In Cesm Images",
    "chunk_index": 39,
    "total_chunks": 91,
    "text_content": "\ufb01nal tested model was Inception v3, used in the classi\ufb01cation phase; the model took input images of size 2249224. The base layers were set to be not trainable, and the last layer was also replaced with a fully connectedlayer to \ufb01t the mass classi\ufb01cation task. For ResNet50, VGG16, and Inception v3, the Adam optimizer was used with binary cross-entropy loss function; also, the threemodels were pre-trained on ImageNet. 4 Experimental design and results The experiments were conducted through three m",
    "full_text_length": 91280,
    "chunk_length": 1316
  },
  {
    "chunk_id": 2286,
    "paper_filename": "nada_2024_YOLO_based_CAD_framework_with_ViT_transformer_for_breast_mass_classiifcaiton_in_cesm_images.pdf",
    "paper_title": "Nada 2024 Yolo Based Cad Framework With Vit Transformer For Breast Mass Classiifcaiton In Cesm Images",
    "chunk_index": 40,
    "total_chunks": 91,
    "text_content": "the performance of the transformer-based modelsversus the CNN-based models on mammographic diagnosing. 4.1 Dataset This work used two datasets: INbreast for FFDM images and CDD-CESM for both FFDM and CESM images. Each dataset was split into training, validation, and testing setsusing the splitting ratio of 70%\u201310% and 20%, respec- tively. During the splitting process, the images for the same patients were included in the same set to prevent the results from being biased. The same sets were used ",
    "full_text_length": 91280,
    "chunk_length": 1255
  },
  {
    "chunk_id": 2287,
    "paper_filename": "nada_2024_YOLO_based_CAD_framework_with_ViT_transformer_for_breast_mass_classiifcaiton_in_cesm_images.pdf",
    "paper_title": "Nada 2024 Yolo Based Cad Framework With Vit Transformer For Breast Mass Classiifcaiton In Cesm Images",
    "chunk_index": 41,
    "total_chunks": 91,
    "text_content": "diagnosed with benign and malignant masses, and those are included in this work\u2019s experiments. The masses were classi\ufb01ed into benign andmalignant based on the BI-RADS score that is provided with the dataset, where 1, 2, and 3 are considered benign; on the other hand, 3, 4, and 5 are counted as malignant. Themass annotations were extracted from the XML \ufb01les that are provided with the dataset, as those annotations were converted to be in the accepted form for YOLO annotation.As each image should b",
    "full_text_length": 91280,
    "chunk_length": 1229
  },
  {
    "chunk_id": 2288,
    "paper_filename": "nada_2024_YOLO_based_CAD_framework_with_ViT_transformer_for_breast_mass_classiifcaiton_in_cesm_images.pdf",
    "paper_title": "Nada 2024 Yolo Based Cad Framework With Vit Transformer For Breast Mass Classiifcaiton In Cesm Images",
    "chunk_index": 42,
    "total_chunks": 91,
    "text_content": "both detection and classi\ufb01cation. 4.1.2 CDD-CESM CDD-CESM is a newly introduced publicly available dataset that provides images in two types FDDM and CESM. It is the \ufb01rst publicly available dataset that contains CESM images. The dataset includes 2006 images dividedinto 1003 low-energy images (FDDM) and 1003 subtracted images representing the (CESM). The dataset has MLO and CC view images; the images were acquired using twodifferent scanners that are G.E. Healthcare Senographe DS Table 3 INbreast",
    "full_text_length": 91280,
    "chunk_length": 1304
  },
  {
    "chunk_id": 2289,
    "paper_filename": "nada_2024_YOLO_based_CAD_framework_with_ViT_transformer_for_breast_mass_classiifcaiton_in_cesm_images.pdf",
    "paper_title": "Nada 2024 Yolo Based Cad Framework With Vit Transformer For Breast Mass Classiifcaiton In Cesm Images",
    "chunk_index": 43,
    "total_chunks": 91,
    "text_content": "are provided according to the ACR-BIRADS lexicon. The images areavailable in JPEG format with a CSV \ufb01le for the annota- tions; the medical reports are also attached to the dataset. The dataset includes 310 FFDM images with masses and 333 CESM images with masses; in the conducted experi- ments, we used 310 images for the experiments on FFDMand CESM. Table 4shows the data distribution of CESM in the training, validation, and testing sets. The corre- sponding images (cases) to those used in the CES",
    "full_text_length": 91280,
    "chunk_length": 1237
  },
  {
    "chunk_id": 2290,
    "paper_filename": "nada_2024_YOLO_based_CAD_framework_with_ViT_transformer_for_breast_mass_classiifcaiton_in_cesm_images.pdf",
    "paper_title": "Nada 2024 Yolo Based Cad Framework With Vit Transformer For Breast Mass Classiifcaiton In Cesm Images",
    "chunk_index": 44,
    "total_chunks": 91,
    "text_content": "to get X min,Xmax,Ymin, and Ymaxfrom those lists, and those coordinates were used as coordinates for obtaining the bounding boxes. For the circle annotations, the dataset provides center x (cx), center y(cy), and the radius r;S oE q s .( 3) and ( 4) were used to get, x1, and, y1of the box; to get the width and the height the r multiplied by 2 then the width were added tox1to get, x2and height to y1to get y2. x1\u00bccx/C0r \u00f03\u00de y1\u00bccy/C0r \u00f04\u00de To get the bounding box of the ellipse segmentation annotati",
    "full_text_length": 91280,
    "chunk_length": 1209
  },
  {
    "chunk_id": 2291,
    "paper_filename": "nada_2024_YOLO_based_CAD_framework_with_ViT_transformer_for_breast_mass_classiifcaiton_in_cesm_images.pdf",
    "paper_title": "Nada 2024 Yolo Based Cad Framework With Vit Transformer For Breast Mass Classiifcaiton In Cesm Images",
    "chunk_index": 45,
    "total_chunks": 91,
    "text_content": "dataset over training, validation, and testing sets that have been used for both detection and classi\ufb01cation. 4.2 Implementation environment The experiments were conducted on a single machine with NVIDIA GeForce RTX3080Ti GPU with 12 vRAM, Intel/C210Core i7-11700 k processor with 3.200 GHz fre- quency, and 32 GB RAM. C ?? , python 3.8, and TensorFlow were used to implement the proposed system on Windows 10 operating system. 4.3 Implementation set-up The proposed framework was implemented over di",
    "full_text_length": 91280,
    "chunk_length": 1321
  },
  {
    "chunk_id": 2292,
    "paper_filename": "nada_2024_YOLO_based_CAD_framework_with_ViT_transformer_for_breast_mass_classiifcaiton_in_cesm_images.pdf",
    "paper_title": "Nada 2024 Yolo Based Cad Framework With Vit Transformer For Breast Mass Classiifcaiton In Cesm Images",
    "chunk_index": 46,
    "total_chunks": 91,
    "text_content": "(0\u2013255).Then CLAHE was used at two different clip limits (1 and 2), and the generated images were mixed up with the normalized image to reconstruct new colored images. Thisstep was done for both datasets to obtain new images with enhanced visibility for the mammographic images in addition to the original images in the training phase. Theimages were augmented to increase the number of images in the training phase, especially with the existence of imbalanced distribution between malignant and beni",
    "full_text_length": 91280,
    "chunk_length": 1376
  },
  {
    "chunk_id": 2293,
    "paper_filename": "nada_2024_YOLO_based_CAD_framework_with_ViT_transformer_for_breast_mass_classiifcaiton_in_cesm_images.pdf",
    "paper_title": "Nada 2024 Yolo Based Cad Framework With Vit Transformer For Breast Mass Classiifcaiton In Cesm Images",
    "chunk_index": 47,
    "total_chunks": 91,
    "text_content": "and INbreast.Table 4 CDD-CESM dataset splitting distribution for experiments Training Validation Testing Benign 50 9 13 Malignant 173 16 49Total 223 25 62Neural Computing and Applications (2024) 36:6467\u20136496 6479 123 Algorithm 1 Pre-processing CDD- CESM mammograms algorithm6480 Neural Computing and Applications (2024) 36:6467\u20136496 123 Algorithm 2 Pre-processing INbreast mammograms algorithm 4.3.2 Mass detection The YOLOv4 is used in this work for the detection task; Darknet with CSP53 is used as",
    "full_text_length": 91280,
    "chunk_length": 1305
  },
  {
    "chunk_id": 2294,
    "paper_filename": "nada_2024_YOLO_based_CAD_framework_with_ViT_transformer_for_breast_mass_classiifcaiton_in_cesm_images.pdf",
    "paper_title": "Nada 2024 Yolo Based Cad Framework With Vit Transformer For Breast Mass Classiifcaiton In Cesm Images",
    "chunk_index": 48,
    "total_chunks": 91,
    "text_content": "(416 9416), (640 9640) Nom. of anchors 9Batches 64Subdivisions 16 for 416 image size, 64 for 640 image sizeNumber of classes 1, 2Learning rate (LR) 0.001Scales 0.1, 0.1Steps 3200, 3600Max. batches 4000No of \ufb01lters (18), (21)Neural Computing and Applications (2024) 36:6467\u20136496 6481 123 for two input sizes (416 9416) and (640 9640). More- over, the model performance was evaluated by two dif- ferent scenarios; the \ufb01rst one was to detect the existence of the masses regardless of their type (benign/",
    "full_text_length": 91280,
    "chunk_length": 1197
  },
  {
    "chunk_id": 2295,
    "paper_filename": "nada_2024_YOLO_based_CAD_framework_with_ViT_transformer_for_breast_mass_classiifcaiton_in_cesm_images.pdf",
    "paper_title": "Nada 2024 Yolo Based Cad Framework With Vit Transformer For Breast Mass Classiifcaiton In Cesm Images",
    "chunk_index": 49,
    "total_chunks": 91,
    "text_content": "on different experiments. Two scales, 0.1 and 0.1,were used to change the learning rate at two different steps that are 3200 and 3600. Finally, the max batches were set up to be 4000 as recommended in [ 27] (max batches = number of classes 92000). Also, K-means clustering was used to select the anchors\u2019 sizes based on the dataset; nine anchors were used for those experiments.4.3.3 Mass classification Six experiments were conducted for classi\ufb01cation with the same sets for training, validation, an",
    "full_text_length": 91280,
    "chunk_length": 1311
  },
  {
    "chunk_id": 2296,
    "paper_filename": "nada_2024_YOLO_based_CAD_framework_with_ViT_transformer_for_breast_mass_classiifcaiton_in_cesm_images.pdf",
    "paper_title": "Nada 2024 Yolo Based Cad Framework With Vit Transformer For Breast Mass Classiifcaiton In Cesm Images",
    "chunk_index": 50,
    "total_chunks": 91,
    "text_content": "with binary cross-entropy loss function, and all the models were modi\ufb01ed to transfer their pre-trained weights to \ufb01t the mass classi\ufb01cation task. Further- more, weight decay was used for all transformer models to avoid the over\ufb01tting problem. Algorithm 3 shows thealgorithmic steps for the mass detection and classi\ufb01cation process.6482 Neural Computing and Applications (2024) 36:6467\u20136496 123 Algorithm 3 Mass detection and classi\ufb01cation algorithm 4.4 Evaluation metrics Different evaluation metrics",
    "full_text_length": 91280,
    "chunk_length": 1380
  },
  {
    "chunk_id": 2297,
    "paper_filename": "nada_2024_YOLO_based_CAD_framework_with_ViT_transformer_for_breast_mass_classiifcaiton_in_cesm_images.pdf",
    "paper_title": "Nada 2024 Yolo Based Cad Framework With Vit Transformer For Breast Mass Classiifcaiton In Cesm Images",
    "chunk_index": 51,
    "total_chunks": 91,
    "text_content": "overlapping between the predicted bounding box andNeural Computing and Applications (2024) 36:6467\u20136496 6483 123 the ground truth at a speci\ufb01c threshold; the threshold used in the proposed model is 0.5. For detection, only TP, FP, and FN were de\ufb01ned as the detected mass is considered to be TP if the IoU [= 0.5 and considered to be FP if the IoU is \\0.5. If the model fails to detect an existing mass, this is counted as FN. The mAP is also calculated to estimate the mean of all average precisions ",
    "full_text_length": 91280,
    "chunk_length": 1133
  },
  {
    "chunk_id": 2298,
    "paper_filename": "nada_2024_YOLO_based_CAD_framework_with_ViT_transformer_for_breast_mass_classiifcaiton_in_cesm_images.pdf",
    "paper_title": "Nada 2024 Yolo Based Cad Framework With Vit Transformer For Breast Mass Classiifcaiton In Cesm Images",
    "chunk_index": 52,
    "total_chunks": 91,
    "text_content": "(sensitivity) and FNR were cal- culated as shown in Eqs. ( 11) and ( 13). The dataset sets suffer from an imbalanced class prob- lem, as shown from the data distribution in Tables 3and4 in Sects. 4.1.1 and4.1.2 . Accordingly, F1-score was cal- culated as it is used as one of the useful evaluation matricesin this case, as shown in Eq. ( 12). Recall senstivity =TPR \u00f0\u00de \u00bcTP TP\u00feFN\u00f011\u00de F1/C0score \u00bc2/C2precision /C2recall precision \u00ferecall\u00f012\u00de FNR \u00bcFN TP\u00feFN\u00f013\u00de Different classi\ufb01cation metrics were used",
    "full_text_length": 91280,
    "chunk_length": 1265
  },
  {
    "chunk_id": 2299,
    "paper_filename": "nada_2024_YOLO_based_CAD_framework_with_ViT_transformer_for_breast_mass_classiifcaiton_in_cesm_images.pdf",
    "paper_title": "Nada 2024 Yolo Based Cad Framework With Vit Transformer For Breast Mass Classiifcaiton In Cesm Images",
    "chunk_index": 53,
    "total_chunks": 91,
    "text_content": "False Positive (FP): this represents the number of themisclassi\ufb01ed masses as benign, and they are malignant. False Negative (FN): this represents the number of the misclassi\ufb01ed masses as malignant, and they are benign. Accuracy \u00bc TP\u00feTN TN\u00feTP\u00feFP\u00feFN/C2100 \u00f014\u00de Moreover, speci\ufb01city was calculated as shown in Eq. ( 15), also FPR and TPR were calculated to plot the ROC curve and calculate the AUC score. The ROC curverepresents the trade-off between FPR and TPR; the higher the AUC score, the better th",
    "full_text_length": 91280,
    "chunk_length": 1290
  },
  {
    "chunk_id": 2300,
    "paper_filename": "nada_2024_YOLO_based_CAD_framework_with_ViT_transformer_for_breast_mass_classiifcaiton_in_cesm_images.pdf",
    "paper_title": "Nada 2024 Yolo Based Cad Framework With Vit Transformer For Breast Mass Classiifcaiton In Cesm Images",
    "chunk_index": 54,
    "total_chunks": 91,
    "text_content": "CESM, respectively, at different resolution input sizes. Moreover, Fig. 6presents false detection on a mammographic image from the INbreast dataset. At the same time, Fig. 7illustrates detection results on some images on FFDM vs. its correspondingCESM images from the CDD-CESM dataset. Tables 6and7show that the performance of YOLO is better at detecting the existence of masses regardless itstype. Also, Tables 7and8show that the higher input image size improves the detection accuracy as the best m",
    "full_text_length": 91280,
    "chunk_length": 1253
  },
  {
    "chunk_id": 2301,
    "paper_filename": "nada_2024_YOLO_based_CAD_framework_with_ViT_transformer_for_breast_mass_classiifcaiton_in_cesm_images.pdf",
    "paper_title": "Nada 2024 Yolo Based Cad Framework With Vit Transformer For Breast Mass Classiifcaiton In Cesm Images",
    "chunk_index": 55,
    "total_chunks": 91,
    "text_content": "case with patient_id(22,614,236), while the model detected two masses. One of them is truly detected with con\ufb01dence score of 94% and the other one is a false detected mass. For further demonstration of the impact of mass detec- tion on CESM, Fig. 7illustrates the effectiveness of using CESM in enhancing the sensitivity of mass detection ratherthan FFDM images. The \ufb01gure provides samples of images from CDD-CESM dataset in forms of CESM and their corresponding DM to illustrate the performance of t",
    "full_text_length": 91280,
    "chunk_length": 1322
  },
  {
    "chunk_id": 2302,
    "paper_filename": "nada_2024_YOLO_based_CAD_framework_with_ViT_transformer_for_breast_mass_classiifcaiton_in_cesm_images.pdf",
    "paper_title": "Nada 2024 Yolo Based Cad Framework With Vit Transformer For Breast Mass Classiifcaiton In Cesm Images",
    "chunk_index": 56,
    "total_chunks": 91,
    "text_content": "the results on eachdataset; INbreast, CE-CESM, and CDD-CESM, respectively. Table 9shows that ViT transformer outperforms the other networks, speci\ufb01cally the CNN-based ones. It can beTable 6 Detection results on INbreast for Benign masses vs. Malignant masses in terms of mAP, F1-score, TP, FP, FN, recall, and precision Dataset Input image sizemAP(%)@ IoU = 0.5F1-score (%)TP FP FN Recall (%)Precision (%)B-AP (%)M-AP (%)TP-FP (B)TP-FP (M) INbreast 416 9416 84.43% 86 19 3 3 86 86 75.00 93.86 3\u20132 16\u2013",
    "full_text_length": 91280,
    "chunk_length": 1192
  },
  {
    "chunk_id": 2303,
    "paper_filename": "nada_2024_YOLO_based_CAD_framework_with_ViT_transformer_for_breast_mass_classiifcaiton_in_cesm_images.pdf",
    "paper_title": "Nada 2024 Yolo Based Cad Framework With Vit Transformer For Breast Mass Classiifcaiton In Cesm Images",
    "chunk_index": 57,
    "total_chunks": 91,
    "text_content": "Trial 3 640 9640 Without normal images98.96% 96 23 20 100 92 0 The best results that were obtained during the experiments are indicated by bold Table 8 Detection results for CDD-CESM for FDDM and CESM images at different input image sizes in terms of mAP, F1-score, TP, F.P., F.N., recall, precision, and FNR # Input image size Image type mAP(%)@IoU = 0.5 F1-score (%) TP FP F.N Recall (%) Precision (%) FNR Trial 1 416 9416 DM 70.27% 72 73 24 33 69 75 0.311 Trial 2 640 9640 DM 71.65% 69 75 34 31 71",
    "full_text_length": 91280,
    "chunk_length": 1098
  },
  {
    "chunk_id": 2304,
    "paper_filename": "nada_2024_YOLO_based_CAD_framework_with_ViT_transformer_for_breast_mass_classiifcaiton_in_cesm_images.pdf",
    "paper_title": "Nada 2024 Yolo Based Cad Framework With Vit Transformer For Breast Mass Classiifcaiton In Cesm Images",
    "chunk_index": 58,
    "total_chunks": 91,
    "text_content": "on the left, followed by its corresponding ground truth and then its detection result (The purple box indicates the detected mass while the red box indicates the ground truth)Neural Computing and Applications (2024) 36:6467\u20136496 6485 123 noticed that model achieved competitive accuracy scores on the testing set with 95.65% in INbreast, 97.61% in CESM, and 80% in DM. Also, it provides the highest AUCscore among the other experimented networks, it achieved 88%, 90%, and 70% in INbreast CESM and DM",
    "full_text_length": 91280,
    "chunk_length": 1277
  },
  {
    "chunk_id": 2305,
    "paper_filename": "nada_2024_YOLO_based_CAD_framework_with_ViT_transformer_for_breast_mass_classiifcaiton_in_cesm_images.pdf",
    "paper_title": "Nada 2024 Yolo Based Cad Framework With Vit Transformer For Breast Mass Classiifcaiton In Cesm Images",
    "chunk_index": 59,
    "total_chunks": 91,
    "text_content": "samples of benign masses. Fig. 7 Detection results for CESM images with their corresponding ground truth on the left. While on the right the same images in FFDM with its detection results and their corresponding ground truth. (Purple box indicates to the detected mass while red box indicates to the ground truth)6486 Neural Computing and Applications (2024) 36:6467\u20136496 123 4.6.1 INbreast mass classification results in terms of ROC and confusion matrix See Figs. 8,9here.Table 9 Mass classi\ufb01cation",
    "full_text_length": 91280,
    "chunk_length": 1301
  },
  {
    "chunk_id": 2306,
    "paper_filename": "nada_2024_YOLO_based_CAD_framework_with_ViT_transformer_for_breast_mass_classiifcaiton_in_cesm_images.pdf",
    "paper_title": "Nada 2024 Yolo Based Cad Framework With Vit Transformer For Breast Mass Classiifcaiton In Cesm Images",
    "chunk_index": 60,
    "total_chunks": 91,
    "text_content": "42.85 86.96 94.73 0 81.81 87.80 47 CE-CDD VIT transformer 95.55 97.61 100 80 97.36 98.66 90 SWIN- Transformer95.87 94.04 98.64 60 94.80 96.68 79 ConvNeXt 96.34 94.04 97.29 70 82.81 85.48 84 ReseNet50 93.33 88.09 100 10 89.15 94.26 50 VGG16 89.36 92.85 97.29 60 94.73 95.99 79 Inception v3 92.70 84.52 94.59 10 88.60 91.50 52 DM- CDDVIT transformer 72.83 80.00 86.66 53.33 88.13 87.39 70 SWIN- Transformer76.41 77.33 83.33 53.33 87.71 85.47 68 ConvNeXt 71.69 76.00 88.33 26.66 82.81 85.48 57 ReseNet50",
    "full_text_length": 91280,
    "chunk_length": 1232
  },
  {
    "chunk_id": 2307,
    "paper_filename": "nada_2024_YOLO_based_CAD_framework_with_ViT_transformer_for_breast_mass_classiifcaiton_in_cesm_images.pdf",
    "paper_title": "Nada 2024 Yolo Based Cad Framework With Vit Transformer For Breast Mass Classiifcaiton In Cesm Images",
    "chunk_index": 61,
    "total_chunks": 91,
    "text_content": "matrix See Figs. 10,11here. Fig. 9 Confusion Matrix (CM) for the classi\ufb01cation models on INbreast Fig. 10 ROC curve for the classi\ufb01cation models on CE images from CDD-CESM6488 Neural Computing and Applications (2024) 36:6467\u20136496 123 4.6.3 DM-CDD mass classification results in terms of ROC and confusion matrix See Figs. 12,13here. Fig. 11 Confusion Matrix (CM) for the classi\ufb01cation models on CE images from CDD-CESM Fig. 12 ROC curve for the classi\ufb01cation models on DM images in CDD-CESMNeural Com",
    "full_text_length": 91280,
    "chunk_length": 1349
  },
  {
    "chunk_id": 2308,
    "paper_filename": "nada_2024_YOLO_based_CAD_framework_with_ViT_transformer_for_breast_mass_classiifcaiton_in_cesm_images.pdf",
    "paper_title": "Nada 2024 Yolo Based Cad Framework With Vit Transformer For Breast Mass Classiifcaiton In Cesm Images",
    "chunk_index": 62,
    "total_chunks": 91,
    "text_content": "images. At the classi\ufb01cation phase, the classi\ufb01cation network of YOLO was replaced by different Networks. Furthermore, theexperiments aimed to explore the potential of the trans- formers regarding the CNN-based models that were used for mass classi\ufb01cation. 5.1 Mass detection Four experiments are conducted on INbreast. The \ufb01rst two experiments were done to evaluate the model in two cases; the \ufb01rst was to detect the benign and malignant masses, while the other aimed to detect the existence of the ",
    "full_text_length": 91280,
    "chunk_length": 1210
  },
  {
    "chunk_id": 2309,
    "paper_filename": "nada_2024_YOLO_based_CAD_framework_with_ViT_transformer_for_breast_mass_classiifcaiton_in_cesm_images.pdf",
    "paper_title": "Nada 2024 Yolo Based Cad Framework With Vit Transformer For Breast Mass Classiifcaiton In Cesm Images",
    "chunk_index": 63,
    "total_chunks": 91,
    "text_content": "size affected the model\u2019s performance as the higher resolution showed better mAP. The experiments are done for an input image size of 416 9416 and 640 9640; the proposed model in Trial 3 detected all 23 existing masses in the testing set; however, there were 2 FP masses. This model achieved 98.96% mAP with a sensitivity of 100%. In Trial 1, themodel achieved mAP of 97.78%, as the model failed to detect one of the existing masses, and there was one falsely detected mass. Higher resolution means l",
    "full_text_length": 91280,
    "chunk_length": 1193
  },
  {
    "chunk_id": 2310,
    "paper_filename": "nada_2024_YOLO_based_CAD_framework_with_ViT_transformer_for_breast_mass_classiifcaiton_in_cesm_images.pdf",
    "paper_title": "Nada 2024 Yolo Based Cad Framework With Vit Transformer For Breast Mass Classiifcaiton In Cesm Images",
    "chunk_index": 64,
    "total_chunks": 91,
    "text_content": "resolution needs more time through training as this affects the batch size andsubdivision values to allow enough memory through training. The model in Trial 2 from Table 7was trained with some normal images within the training set; it showed lower mean Average Precision than in Trial 1. However, Fig. 13 Confusion Matrix (CM) for the classi\ufb01cation models on DM images in CDD-CESM6490 Neural Computing and Applications (2024) 36:6467\u20136496 123 the model succeeded in detecting 22 masses out of the 23 ",
    "full_text_length": 91280,
    "chunk_length": 1269
  },
  {
    "chunk_id": 2311,
    "paper_filename": "nada_2024_YOLO_based_CAD_framework_with_ViT_transformer_for_breast_mass_classiifcaiton_in_cesm_images.pdf",
    "paper_title": "Nada 2024 Yolo Based Cad Framework With Vit Transformer For Breast Mass Classiifcaiton In Cesm Images",
    "chunk_index": 65,
    "total_chunks": 91,
    "text_content": "work, we conducted experi- ments to explore the potential of using those images in detecting the masses. Moreover, the experiments evaluatethe detection in contrast-enhanced mammography and its equivalent digital mammography for the same dataset. The results of Table 8showed that mass detection in CESM has promising results and outperformed the model\u2019s perfor- mance on the FFDM images. In Table 8, Trials 2 and 4 showed that the mAP of the detection model was improvedby 3.4%\u20139.8% on the CESM imag",
    "full_text_length": 91280,
    "chunk_length": 1307
  },
  {
    "chunk_id": 2312,
    "paper_filename": "nada_2024_YOLO_based_CAD_framework_with_ViT_transformer_for_breast_mass_classiifcaiton_in_cesm_images.pdf",
    "paper_title": "Nada 2024 Yolo Based Cad Framework With Vit Transformer For Breast Mass Classiifcaiton In Cesm Images",
    "chunk_index": 66,
    "total_chunks": 91,
    "text_content": "accordingly, this helped the model to learn more discriminant features during the training phase. Theseimprovements can be demonstrated from even the trials that were done on the images of size 416 9416. 5.2 Mass classification For classi\ufb01cation, six models were experimented on each dataset. From Table 9, it can be deduced that the vision transformer (ViT) outperformed the other classi\ufb01ers in classifying the detected masses. As shown in Table 9, the vision transformers achieved the highest accur",
    "full_text_length": 91280,
    "chunk_length": 1245
  },
  {
    "chunk_id": 2313,
    "paper_filename": "nada_2024_YOLO_based_CAD_framework_with_ViT_transformer_for_breast_mass_classiifcaiton_in_cesm_images.pdf",
    "paper_title": "Nada 2024 Yolo Based Cad Framework With Vit Transformer For Breast Mass Classiifcaiton In Cesm Images",
    "chunk_index": 67,
    "total_chunks": 91,
    "text_content": "The ViT achieved 80% on DM images, but its performance is still considered the best among the other classi\ufb01ers regarding the ROC and AUCvalues. The model achieved the highest score of 70%, as shown in Fig. 12. This means that the model candifferentiate between benign and malignant masses more than the other classi\ufb01ers. On the other hand, ResNet50 achieved an AUC of 57% and sensitivity of 100%, which means that the model is useless because it considered mostof the cases as malignant. It showed ba",
    "full_text_length": 91280,
    "chunk_length": 1256
  },
  {
    "chunk_id": 2314,
    "paper_filename": "nada_2024_YOLO_based_CAD_framework_with_ViT_transformer_for_breast_mass_classiifcaiton_in_cesm_images.pdf",
    "paper_title": "Nada 2024 Yolo Based Cad Framework With Vit Transformer For Breast Mass Classiifcaiton In Cesm Images",
    "chunk_index": 68,
    "total_chunks": 91,
    "text_content": "Thetransformer interprets the images as a matrix of patches, not a matrix of pixels, and this allowed the model to pre- serve long global relationships between the patches andobtain more semantic information rather than CNN-based models. However, the SWIN transformer provides some improvements on the regular vision transformer (ViT); itshowed lower performance than the ViT. The architecture of ViT is mainly based on observing the relationships between each patch (image token) and all of the rest",
    "full_text_length": 91280,
    "chunk_length": 1234
  },
  {
    "chunk_id": 2315,
    "paper_filename": "nada_2024_YOLO_based_CAD_framework_with_ViT_transformer_for_breast_mass_classiifcaiton_in_cesm_images.pdf",
    "paper_title": "Nada 2024 Yolo Based Cad Framework With Vit Transformer For Breast Mass Classiifcaiton In Cesm Images",
    "chunk_index": 69,
    "total_chunks": 91,
    "text_content": "mammographic images. Accordingly, the relationship between each patch and all other patches for the massmatters; it is not only about the relation of the patch and its neighboring patches of the same window. And so, this can clarify why ViT outperformed the SWIN in the conductedexperiments. Maybe the approach of SWIN can increase the computation ef\ufb01ciency of the model; however, this was not so in\ufb02uential in these experiments as the images werenot with high resolution, especially with the fact th",
    "full_text_length": 91280,
    "chunk_length": 1268
  },
  {
    "chunk_id": 2316,
    "paper_filename": "nada_2024_YOLO_based_CAD_framework_with_ViT_transformer_for_breast_mass_classiifcaiton_in_cesm_images.pdf",
    "paper_title": "Nada 2024 Yolo Based Cad Framework With Vit Transformer For Breast Mass Classiifcaiton In Cesm Images",
    "chunk_index": 70,
    "total_chunks": 91,
    "text_content": "ViT and SIWN. However, it showed better performance than ResNet50. From Fig. 9, it can be deduced that only one benign mass was misclassi\ufb01ed with ViT in INbreast Dataset; the model succeeded in classifying all the malignant masses. Also, from Fig. 11for CE images, the proposed model rightly predicted all the malignant masses. Only two benignNeural Computing and Applications (2024) 36:6467\u20136496 6491 123 masses were misclassi\ufb01ed; Fig. 14shows the misclassi\ufb01ed masses in INbreast and CE images from ",
    "full_text_length": 91280,
    "chunk_length": 1308
  },
  {
    "chunk_id": 2317,
    "paper_filename": "nada_2024_YOLO_based_CAD_framework_with_ViT_transformer_for_breast_mass_classiifcaiton_in_cesm_images.pdf",
    "paper_title": "Nada 2024 Yolo Based Cad Framework With Vit Transformer For Breast Mass Classiifcaiton In Cesm Images",
    "chunk_index": 71,
    "total_chunks": 91,
    "text_content": "problem, but those techniques do not pro-vide too much realistic transformation for the images. And this can be considered one of the limitations facing breast cancer CAD systems. ViT transformers utilize the idea of parallel processing, making the transformers provide more computational ef\ufb01- ciency than CNN-based models. Table 10shows that the inference time that the proposed framework took per image is less than the time YOLOv4 took before replacing the classi\ufb01cation layers of YOLO with the Vi",
    "full_text_length": 91280,
    "chunk_length": 1325
  },
  {
    "chunk_id": 2318,
    "paper_filename": "nada_2024_YOLO_based_CAD_framework_with_ViT_transformer_for_breast_mass_classiifcaiton_in_cesm_images.pdf",
    "paper_title": "Nada 2024 Yolo Based Cad Framework With Vit Transformer For Breast Mass Classiifcaiton In Cesm Images",
    "chunk_index": 72,
    "total_chunks": 91,
    "text_content": "proposed model outperformed the proposed model by [ 15] in terms of F1-score for mass detection in CE images by almost 5%; however, it achieved the same score as DM images.6 Conclusion Vision transformers are considerably revolutionizingcomputer vision tasks, especially image classi\ufb01cation.Utilizing the power of transformers in medical image interpretation can help in enhancing the performance of CAD systems. This work proposed a novel framework formass detection and classi\ufb01cation based on integ",
    "full_text_length": 91280,
    "chunk_length": 1365
  },
  {
    "chunk_id": 2319,
    "paper_filename": "nada_2024_YOLO_based_CAD_framework_with_ViT_transformer_for_breast_mass_classiifcaiton_in_cesm_images.pdf",
    "paper_title": "Nada 2024 Yolo Based Cad Framework With Vit Transformer For Breast Mass Classiifcaiton In Cesm Images",
    "chunk_index": 73,
    "total_chunks": 91,
    "text_content": "in the experiments. The conducted experimentsshowed that the CESM images could improve the CAD system\u2019s performance at both detection and classi\ufb01cation levels, as they showed better results than FFDM images.The proposed model achieved detection accuracy of 98.96% and 81.52%; moreover, it achieved a classi\ufb01cation accuracy of 95.65% and 97.61% for INbreast and CESM,respectively. The experiments also showed that the image size affected the detection results speci\ufb01cally for the CDD-CESM dataset as t",
    "full_text_length": 91280,
    "chunk_length": 1323
  },
  {
    "chunk_id": 2320,
    "paper_filename": "nada_2024_YOLO_based_CAD_framework_with_ViT_transformer_for_breast_mass_classiifcaiton_in_cesm_images.pdf",
    "paper_title": "Nada 2024 Yolo Based Cad Framework With Vit Transformer For Breast Mass Classiifcaiton In Cesm Images",
    "chunk_index": 74,
    "total_chunks": 91,
    "text_content": "results compared to the other experimented models; it shows the best AUC score for INbreast and CDD-CESM datasets. Vit achieved AUCscores of 88%, 90%, 70%, and F1-score of 97.43%, Fig. 14 Misclassi\ufb01ed masses aINbreast b,cCE-CDD (truth: Benign, prediction: Malignant) Table 10 Inference time per image for the proposed framework Method Inference time/image ( s) YOLOv4 ?ViT transformer 0.0378 YOLOv4 0.06526492 Neural Computing and Applications (2024) 36:6467\u20136496 123 98.66%, 87.39% for INbreast, CE-",
    "full_text_length": 91280,
    "chunk_length": 1421
  },
  {
    "chunk_id": 2321,
    "paper_filename": "nada_2024_YOLO_based_CAD_framework_with_ViT_transformer_for_breast_mass_classiifcaiton_in_cesm_images.pdf",
    "paper_title": "Nada 2024 Yolo Based Cad Framework With Vit Transformer For Breast Mass Classiifcaiton In Cesm Images",
    "chunk_index": 75,
    "total_chunks": 91,
    "text_content": "be considered in future work; the available datasets suffer from the imbal-ance class problem even with using augmentation tech- niques as they did not provide natural transformations. Furthermore, the size of the publicly available datasets isrelatively small. Moreover, the proposed model\u2019s compu- tational time can be considered another limitation that can be enhanced in the future regarding the performance of therecent versions of YOLO. Based on the results, this work can be extended in dif- f",
    "full_text_length": 91280,
    "chunk_length": 1450
  },
  {
    "chunk_id": 2322,
    "paper_filename": "nada_2024_YOLO_based_CAD_framework_with_ViT_transformer_for_breast_mass_classiifcaiton_in_cesm_images.pdf",
    "paper_title": "Nada 2024 Yolo Based Cad Framework With Vit Transformer For Breast Mass Classiifcaiton In Cesm Images",
    "chunk_index": 76,
    "total_chunks": 91,
    "text_content": "ability to use them for detection or segmentation. Author contribution NMH was involved in conceptualization, methodology, software, data curation, writing\u2014original draft. SH helped in conceptualization, methodology, writing\u2014reviewing andediting, supervision. Khaled Mahar contributed to conceptualization,validation, writing\u2014reviewing and editing, supervision. Funding Open access funding provided by The Science, Technology & Innovation Funding Authority (STDF) in cooperation with TheEgyptian Know",
    "full_text_length": 91280,
    "chunk_length": 1698
  },
  {
    "chunk_id": 2323,
    "paper_filename": "nada_2024_YOLO_based_CAD_framework_with_ViT_transformer_for_breast_mass_classiifcaiton_in_cesm_images.pdf",
    "paper_title": "Nada 2024 Yolo Based Cad Framework With Vit Transformer For Breast Mass Classiifcaiton In Cesm Images",
    "chunk_index": 77,
    "total_chunks": 91,
    "text_content": "YOLO ?InceptionResNetv2 INbreast 70\u201310\u201320 97.27 97.50 Proposed modelYOLOv4 ?ViT transformer INbreast 70\u201310\u201320 98.96 95.65 CE-CESM 81.52 97.61DM-CESM 71.65 80.00 Table 12 Comparison of detection and classi\ufb01cation results between proposed work and related studies on CDD-CESM Reference Method Dataset Segmentation/detection (F1-score) Classi\ufb01cation accuracy [15] EffecientNetB0 ?GradCAM CDD-CESM DM: 72% _ C.E.: 73% Proposed model YOLOv4 ?ViT transformer INbreast 96% 95.65% CE-CESM 78% 97.61%DM-CESM 7",
    "full_text_length": 91280,
    "chunk_length": 1440
  },
  {
    "chunk_id": 2324,
    "paper_filename": "nada_2024_YOLO_based_CAD_framework_with_ViT_transformer_for_breast_mass_classiifcaiton_in_cesm_images.pdf",
    "paper_title": "Nada 2024 Yolo Based Cad Framework With Vit Transformer For Breast Mass Classiifcaiton In Cesm Images",
    "chunk_index": 78,
    "total_chunks": 91,
    "text_content": "were made. The images or other third party material in thisarticle are included in the article\u2019s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article\u2019s Creative Commons licence and your intendeduse is not permitted by statutory regulation or exceeds the permitteduse, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons. org/licenses/by/4.0/ .",
    "full_text_length": 91280,
    "chunk_length": 1479
  },
  {
    "chunk_id": 2325,
    "paper_filename": "nada_2024_YOLO_based_CAD_framework_with_ViT_transformer_for_breast_mass_classiifcaiton_in_cesm_images.pdf",
    "paper_title": "Nada 2024 Yolo Based Cad Framework With Vit Transformer For Breast Mass Classiifcaiton In Cesm Images",
    "chunk_index": 79,
    "total_chunks": 91,
    "text_content": "Kumar R (2023) Memory recurrent elman neural network-based identi\ufb01cation of time-delayed nonlinear dynamical system. IEEE Trans Syst Man Cybern Syst 53:753\u2013762. https://doi.org/10. 1109/TSMC.2022.3186610 5. Sherstinsky A (2020) Fundamentals of recurrent neural network (RNN) and long short-term memory (LSTM) network. Phys DNonlinear Phenom 404:132306 6. Nasser M, Yusof UK (2023) Deep learning based methods for breast cancer diagnosis: a systematic review and future direction. Diagnostics 13:161. ",
    "full_text_length": 91280,
    "chunk_length": 1635
  },
  {
    "chunk_id": 2326,
    "paper_filename": "nada_2024_YOLO_based_CAD_framework_with_ViT_transformer_for_breast_mass_classiifcaiton_in_cesm_images.pdf",
    "paper_title": "Nada 2024 Yolo Based Cad Framework With Vit Transformer For Breast Mass Classiifcaiton In Cesm Images",
    "chunk_index": 80,
    "total_chunks": 91,
    "text_content": "(2022) Mammogram breast cancer CAD systems for mass detection and classi\ufb01cation: a review. Multimed Tools Appl 81:20043\u201320075. https://doi.org/ 10.1007/S11042-022-12332-1/FIGURES/5 11. Raghu M, Unterthiner T, Kornblith S et al (2021) Do vision transformers see like convolutional neural networks? Neural Inf Process Syst 34:12116\u20131212812. He K, Gan C, Li Z et al (2022) Transformers in medical image analysis: a review. Intell Med. https://doi.org/10.1016/J.IMED. 2022.07.002 13. Ghe\ufb02ati B, Rivaz H (",
    "full_text_length": 91280,
    "chunk_length": 1598
  },
  {
    "chunk_id": 2327,
    "paper_filename": "nada_2024_YOLO_based_CAD_framework_with_ViT_transformer_for_breast_mass_classiifcaiton_in_cesm_images.pdf",
    "paper_title": "Nada 2024 Yolo Based Cad Framework With Vit Transformer For Breast Mass Classiifcaiton In Cesm Images",
    "chunk_index": 81,
    "total_chunks": 91,
    "text_content": "ERE, Zwiggelaar R (2018) Classi\ufb01cation of micro-calci\ufb01cation in mammograms using scalable linear Fisher discriminant analysis. Med Biol Eng Comput 56:1475\u20131485.https://doi.org/10.1007/S11517-017-1774-Z/TABLES/2 17. Punitha S, Amuthan A, Joseph KS (2018) Benign and malignant breast cancer segmentation using optimized region growingtechnique. Future Comput Inform J 3:348\u2013358. https://doi.org/10. 1016/J.FCIJ.2018.10.005 18. Mughal B, Sharif M, Muhammad N (2017) Bi-model processing for early detecti",
    "full_text_length": 91280,
    "chunk_length": 1593
  },
  {
    "chunk_id": 2328,
    "paper_filename": "nada_2024_YOLO_based_CAD_framework_with_ViT_transformer_for_breast_mass_classiifcaiton_in_cesm_images.pdf",
    "paper_title": "Nada 2024 Yolo Based Cad Framework With Vit Transformer For Breast Mass Classiifcaiton In Cesm Images",
    "chunk_index": 82,
    "total_chunks": 91,
    "text_content": "classi\ufb01cationusing machine learning. Appl Sci (Switzerland) 12:7404. https:// doi.org/10.3390/app12157404 22. Dara S, Tumma P (2018) Feature extraction by using deep learning: a survey. In: Proceedings of the 2nd international con-ference on electronics, communication and aerospace technology, ICECA 2018, pp 1795\u20131801. https://doi.org/10.1109/ICECA. 2018.8474912 23. LeCun Y, Bengio Y, Hinton G (2015) Deep learning. Nature 521:436\u2013444. https://doi.org/10.1038/nature14539 24. Bengio Y, Courville A",
    "full_text_length": 91280,
    "chunk_length": 1567
  },
  {
    "chunk_id": 2329,
    "paper_filename": "nada_2024_YOLO_based_CAD_framework_with_ViT_transformer_for_breast_mass_classiifcaiton_in_cesm_images.pdf",
    "paper_title": "Nada 2024 Yolo Based Cad Framework With Vit Transformer For Breast Mass Classiifcaiton In Cesm Images",
    "chunk_index": 83,
    "total_chunks": 91,
    "text_content": "JM et al (2017) Detection and classi\ufb01cation of the breast abnormalities in digital mammograms via regional convolutional neural network. In: Annual interna-tional conference IEEE engineering medicine and biology soci-ety, pp 1230\u20131233. https://doi.org/10.1109/EMBC.2017.8037053 29. Al-antari MA, Al-masni MA, Kim TS (2020) Deep learning computer-aided diagnosis for breast lesion in digital6494 Neural Computing and Applications (2024) 36:6467\u20136496 123 mammogram. Adv Exp Med Biol 1213:59\u201372. https:/",
    "full_text_length": 91280,
    "chunk_length": 1550
  },
  {
    "chunk_id": 2330,
    "paper_filename": "nada_2024_YOLO_based_CAD_framework_with_ViT_transformer_for_breast_mass_classiifcaiton_in_cesm_images.pdf",
    "paper_title": "Nada 2024 Yolo Based Cad Framework With Vit Transformer For Breast Mass Classiifcaiton In Cesm Images",
    "chunk_index": 84,
    "total_chunks": 91,
    "text_content": "MH et al (2020) Deep learning for mass detection in full \ufb01eld digital mammograms. Comput Biol Med121:103774. https://doi.org/10.1016/J.COMPBIOMED.2020. 103774 34. Cao H, Pu S, Tan W, Tong J (2021) Breast mass detection in digital mammography based on anchor-free architecture. Comput Methods Programs Biomed 205:106033. https://doi.org/10.1016/ J.CMPB.2021.106033 35. Zhu C, He Y, Savvides M (2019) Feature selective anchor-free module for single-shot object detection 36. Shen R, Yao J, Yan K et al ",
    "full_text_length": 91280,
    "chunk_length": 1570
  },
  {
    "chunk_id": 2331,
    "paper_filename": "nada_2024_YOLO_based_CAD_framework_with_ViT_transformer_for_breast_mass_classiifcaiton_in_cesm_images.pdf",
    "paper_title": "Nada 2024 Yolo Based Cad Framework With Vit Transformer For Breast Mass Classiifcaiton In Cesm Images",
    "chunk_index": 85,
    "total_chunks": 91,
    "text_content": "B, Aghaei F et al (2018) Classi\ufb01cation of breast masses using a computer-aided diagnosis scheme of contrast enhanced digital mammograms. Ann Biomed Eng 46:1419\u20131431. https://doi.org/10.1007/s10439-018-2044-4 40. Gao F, Wu T, Li J et al (2018) SD-CNN: a shallow-deep CNN for improved breast cancer diagnosis. Comput Med Imaging Graph 70:53\u201362. https://doi.org/10.1016/J.COMPMEDIMAG.2018.09. 004 41. Perek S, Kiryati N, Zimmerman-Moreno G et al (2019) Classi- \ufb01cation of contrast-enhanced spectral mamm",
    "full_text_length": 91280,
    "chunk_length": 1601
  },
  {
    "chunk_id": 2332,
    "paper_filename": "nada_2024_YOLO_based_CAD_framework_with_ViT_transformer_for_breast_mass_classiifcaiton_in_cesm_images.pdf",
    "paper_title": "Nada 2024 Yolo Based Cad Framework With Vit Transformer For Breast Mass Classiifcaiton In Cesm Images",
    "chunk_index": 86,
    "total_chunks": 91,
    "text_content": "gray-level his- tograms. IEEE Trans Syst Man Cybern SMC 9:62\u201366. https://doi. org/10.1109/TSMC.1979.4310076 46. Pizer SM, Amburn EP, Austin JD et al (1987) Adaptive his- togram equalization and its variations. Comput Vis Graph ImageProcess 39:355\u2013368. https://doi.org/10.1016/S0734- 189X(87)80186-X47. Redmon J, Farhadi A (2016) YOLO9000: better, faster, stronger. In: Proceedings\u201330th IEEE conference on computer vision and pattern recognition, CVPR 2017-January, pp 6517\u20136525. https:// doi.org/10.1",
    "full_text_length": 91280,
    "chunk_length": 1471
  },
  {
    "chunk_id": 2333,
    "paper_filename": "nada_2024_YOLO_based_CAD_framework_with_ViT_transformer_for_breast_mass_classiifcaiton_in_cesm_images.pdf",
    "paper_title": "Nada 2024 Yolo Based Cad Framework With Vit Transformer For Breast Mass Classiifcaiton In Cesm Images",
    "chunk_index": 87,
    "total_chunks": 91,
    "text_content": "(2021) Performance bench- marking of YOLO architectures for vehicle license plate detec-tion from real-time videos captured by a mobile robot. Sorbonne University, Paris 54. Zhou S, Cai K, Feng Y et al (2023) An accurate detection model of Takifugu rubripes using an improved YOLO-V7 network.J Mar Sci Eng 11:1051. https://doi.org/10.3390/jmse11051051 55. Tian Z, Shen C, Chen H, He T (2019) FCOS: fully convolutional one-stage object detection 56. Lou H, Duan X, Guo J et al (2023) DC-YOLOv8: small-",
    "full_text_length": 91280,
    "chunk_length": 1379
  },
  {
    "chunk_id": 2334,
    "paper_filename": "nada_2024_YOLO_based_CAD_framework_with_ViT_transformer_for_breast_mass_classiifcaiton_in_cesm_images.pdf",
    "paper_title": "Nada 2024 Yolo Based Cad Framework With Vit Transformer For Breast Mass Classiifcaiton In Cesm Images",
    "chunk_index": 88,
    "total_chunks": 91,
    "text_content": "Proceedings of the IEEE international conference on computer vision 9992\u201310002.https://doi.org/10.48550/arxiv.2103.14030 60. Liu Z, Mao H, Wu C-Y et al (2022) A ConvNet for the 2020s 61. Hassanien MA, Singh VK, Puig D, Abdel-Nasser M (2022) Predicting breast tumor malignancy using deep ConvNeXtradiomics and quality-based score pooling in ultrasoundsequences. Diagnostics (Basel) 12:1053. https://doi.org/10.3390/ DIAGNOSTICS12051053 62. van Tulder G, Tong Y, Marchiori E (2021) Multi-view analysis ",
    "full_text_length": 91280,
    "chunk_length": 1494
  },
  {
    "chunk_id": 2335,
    "paper_filename": "nada_2024_YOLO_based_CAD_framework_with_ViT_transformer_for_breast_mass_classiifcaiton_in_cesm_images.pdf",
    "paper_title": "Nada 2024 Yolo Based Cad Framework With Vit Transformer For Breast Mass Classiifcaiton In Cesm Images",
    "chunk_index": 89,
    "total_chunks": 91,
    "text_content": "model for mammography mass detection using mosaic and recon- structed multichannel images. Lecture notes in computer science (including subseries lecture notes in arti\ufb01cial intelligence andlecture notes in bioinformatics). Springer, Cham, pp 544\u2013559 66. Al-antari MA, Al-masni MA, Choi MT et al (2018) A fully integrated computer-aided diagnosis system for digital X-ray mammograms via deep learning detection, segmentation, andclassi\ufb01cation. Int J Med Inform 117:44\u201354. https://doi.org/10. 1016/J.IJ",
    "full_text_length": 91280,
    "chunk_length": 1369
  },
  {
    "chunk_id": 2336,
    "paper_filename": "nada_2024_YOLO_based_CAD_framework_with_ViT_transformer_for_breast_mass_classiifcaiton_in_cesm_images.pdf",
    "paper_title": "Nada 2024 Yolo Based Cad Framework With Vit Transformer For Breast Mass Classiifcaiton In Cesm Images",
    "chunk_index": 90,
    "total_chunks": 91,
    "text_content": "remains neutral with regard to jurisdictional claims in published maps and institutional af\ufb01liations.6496 Neural Computing and Applications (2024) 36:6467\u20136496",
    "full_text_length": 91280,
    "chunk_length": 159
  },
  {
    "chunk_id": 2337,
    "paper_filename": "nazannin_2024_Efficient_labeling_of_french_mammography_reports.pdf",
    "paper_title": "Nazannin 2024 Efficient Labeling Of French Mammography Reports",
    "chunk_index": 0,
    "total_chunks": 42,
    "text_content": "Efficient labeling of french mammogram reports with MammoBERT Nazanin Dehghani\uf02a, Vera Saliba-Colombani, Aur\u00e9lien Chick, Morgane Heng, Gr\u00e9gory Operto & Pierre Fillard Recent advances in deep learning and natural language processing (NLP) have broadened opportunities for automatic text processing in the medical field. Howe ver, the development of models for low-resource languages like French is challenged by limited datasets, often due to legal restrictions. Large-scale training of medical imaging",
    "full_text_length": 44459,
    "chunk_length": 1469
  },
  {
    "chunk_id": 2338,
    "paper_filename": "nazannin_2024_Efficient_labeling_of_french_mammography_reports.pdf",
    "paper_title": "Nazannin 2024 Efficient Labeling Of French Mammography Reports",
    "chunk_index": 1,
    "total_chunks": 42,
    "text_content": "fine-tuned on a limited dataset of radiologist annotations. Then, it underwent training on annotations generated by a rule-based labeler. Our findings reveal that our final model, MammoBERT, significantly outperforms the rule-based labeler while simultaneously reducing the necessity for radiologist annotations during training. This research not only advances the state of the art in medical image report labeling but also offers an efficient and effective solution for large-scale medical imaging m",
    "full_text_length": 44459,
    "chunk_length": 1438
  },
  {
    "chunk_id": 2339,
    "paper_filename": "nazannin_2024_Efficient_labeling_of_french_mammography_reports.pdf",
    "paper_title": "Nazannin 2024 Efficient Labeling Of French Mammography Reports",
    "chunk_index": 2,
    "total_chunks": 42,
    "text_content": "learning have opened new avenues for developing more accurate and reliable models. These models can diagnose and treat breast cancer. However, training these models, especially in medical imaging, requires large labeled datasets. Given the high cost of radiologist annotation for radiology images, deep learning models for radiology image interpretation often rely on labels automatically extracted from accompanying reports, as noted by2. Mammography reports provide detailed insights into mammogram",
    "full_text_length": 44459,
    "chunk_length": 1438
  },
  {
    "chunk_id": 2340,
    "paper_filename": "nazannin_2024_Efficient_labeling_of_french_mammography_reports.pdf",
    "paper_title": "Nazannin 2024 Efficient Labeling Of French Mammography Reports",
    "chunk_index": 3,
    "total_chunks": 42,
    "text_content": "can change the tissue texture. This makes such images unsuitable for training breast cancer detection models on screening mammograms. This task is challenging because there are many invasive treatment methods. These include lumpectomy, mastectomy, surgical biopsy, radiotherapy, and cosmetic surgeries such as breast reduction and symmetrization, all of which alter breast tissue. Moreover, the surgical history is often mentioned sporadically in reports, or not at all. It may also include surgeries",
    "full_text_length": 44459,
    "chunk_length": 1490
  },
  {
    "chunk_id": 2341,
    "paper_filename": "nazannin_2024_Efficient_labeling_of_french_mammography_reports.pdf",
    "paper_title": "Nazannin 2024 Efficient Labeling Of French Mammography Reports",
    "chunk_index": 4,
    "total_chunks": 42,
    "text_content": "Imp. Reille, 75014 Paris, France. \uf02aemail: ndehghani@therapixel.com OPEN Scientific Reports | (2024) 14:24842 1 | https://doi.org/10.1038/s41598-024-76369-y www.nature.com/scientificreports existing rule-based labelers, which remain state-of-the-art for many medical tasks. In this paper, we introduce a simple method that leverages the strengths of both rule-based labels and radiologist annotations. This approach, shown in Fig. 1, begins with a pretrained BERT-based model by5, initially trained on",
    "full_text_length": 44459,
    "chunk_length": 1467
  },
  {
    "chunk_id": 2342,
    "paper_filename": "nazannin_2024_Efficient_labeling_of_french_mammography_reports.pdf",
    "paper_title": "Nazannin 2024 Efficient Labeling Of French Mammography Reports",
    "chunk_index": 5,
    "total_chunks": 42,
    "text_content": "to capture straightforward, well-defined patterns, while the machine learning model was employed to manage more complex cases that required understanding context and handling linguistic variability. This combination allowed us to leverage the precision of rules where applicable while relying on the adaptability of machine learning for more ambiguous cases. Our method, named MammoBERT, applies to the task of extracting surgery prior information from mammography reports. We demonstrate that MammoB",
    "full_text_length": 44459,
    "chunk_length": 1505
  },
  {
    "chunk_id": 2343,
    "paper_filename": "nazannin_2024_Efficient_labeling_of_french_mammography_reports.pdf",
    "paper_title": "Nazannin 2024 Efficient Labeling Of French Mammography Reports",
    "chunk_index": 6,
    "total_chunks": 42,
    "text_content": "and grammatical rules. Notably, tools like NegEx11 and its extension, NegBio2, CheXpert labeler12, have played an important role in this domain, using regular expressions and universal dependencies for report labeling and detecting negations and uncertainties in radiology findings. Deep learning revolutionized radiology report analysis, shifting from rule-based to data-driven approaches. Studies by9 and13 demonstrate the use of deep learning models, including CNNs with GloVe embeddings and netwo",
    "full_text_length": 44459,
    "chunk_length": 1529
  },
  {
    "chunk_id": 2344,
    "paper_filename": "nazannin_2024_Efficient_labeling_of_french_mammography_reports.pdf",
    "paper_title": "Nazannin 2024 Efficient Labeling Of French Mammography Reports",
    "chunk_index": 7,
    "total_chunks": 42,
    "text_content": "number of radiologist- labeled reports. These studies have demonstrated notable improvements over simpler models, showing that Transformer-based architectures effectively capture the complexities of medical language. In line with these advancements, specialized models have been developed to enhance the processing capabilities for non-English languages. CamemBERT by5 has effectively adapted the BERT architecture for processing French across various NLP tasks, and DrBERT18 is specifically develope",
    "full_text_length": 44459,
    "chunk_length": 1447
  },
  {
    "chunk_id": 2345,
    "paper_filename": "nazannin_2024_Efficient_labeling_of_french_mammography_reports.pdf",
    "paper_title": "Nazannin 2024 Efficient Labeling Of French Mammography Reports",
    "chunk_index": 8,
    "total_chunks": 42,
    "text_content": "initially trained on a small dataset of radiologist annotations ( \u223c2k). In Phase 2, the model undergoes fine-tuning through an active learning loop, which integrates a combination of manual annotations ( \u223c2k) selected via uncertainty sampling, and a larger set of automatic rule-based labels ( \u223c40k) acquired through agreement sampling. Scientific Reports | (2024) 14:24842 2 | https://doi.org/10.1038/s41598-024-76369-ywww.nature.com/scientificreports/ In this context, our work introduces a BERT-ba",
    "full_text_length": 44459,
    "chunk_length": 1472
  },
  {
    "chunk_id": 2346,
    "paper_filename": "nazannin_2024_Efficient_labeling_of_french_mammography_reports.pdf",
    "paper_title": "Nazannin 2024 Efficient Labeling Of French Mammography Reports",
    "chunk_index": 9,
    "total_chunks": 42,
    "text_content": "We obtained a comprehensive dataset comprising Digital Imaging and Communications in Medicine (DICOM) images and mammography reports from 397,100 studies. The studies were conducted on 226,295 patients with benign or malignant breast who underwent mammography examination between January 2008 and January 2021. The data were sourced from eight Radiology and Medical Imaging institutions in France. The institutions perform screening mammograms. For model training, data from seven institutions were u",
    "full_text_length": 44459,
    "chunk_length": 1335
  },
  {
    "chunk_id": 2347,
    "paper_filename": "nazannin_2024_Efficient_labeling_of_french_mammography_reports.pdf",
    "paper_title": "Nazannin 2024 Efficient Labeling Of French Mammography Reports",
    "chunk_index": 10,
    "total_chunks": 42,
    "text_content": "serving as an unseen test set to assess the final model\u2019s generalization capabilities. Including data Institutes NO_SURGERY LEFT_SURGERY RIGHT_SURGERY BOTH_SURGERY Total zeta 13,914 (79.1%) 1404 (7.9%) 1345 (7.6%) 908 (5.1%) 17,571 chi 12,108 (79.0%) 1209 (7.8%) 1178 (7.6%) 829 (5.4%) 15,324 beta 26,674 (65.6%) 5567 (13.6%) 5557 (13.6%) 2851 (7.0%) 40,649 delta 85,046 (83.4%) 5935 (5.8%) 5549 (5.4%) 5428 (5.3%) 101,958 omicron 25,724 (80.7%) 2391 (7.5%) 2293 (7.1%) 1440 (4.5%) 31,848 Table 3 . C",
    "full_text_length": 44459,
    "chunk_length": 1440
  },
  {
    "chunk_id": 2348,
    "paper_filename": "nazannin_2024_Efficient_labeling_of_french_mammography_reports.pdf",
    "paper_title": "Nazannin 2024 Efficient Labeling Of French Mammography Reports",
    "chunk_index": 11,
    "total_chunks": 42,
    "text_content": "0.879 0.872 0.793 0.806 0.799 0.88 EXP_HYB_Label 0.898 0.924 0.913 0.872 0.896 0.887 0.90 EXP_HYB_REFINED_Label 0.956 0.967 0.961 0.938 0.958 0.948 0.91 EXP_HYB_REFINED_Label_DrBERT 0.927 0.952 0.942 0.902 0.936 0.915 0.89 EXP_HYB_REFINED_Label_AUG 0.990 0.991 0.991 0.978 0.981 0.979 0.94 EXP_HYB_REFINED_Label_AUG_ORG 0.988 0.989 0.988 0.982 0.985 0.983 0.97 Table 2 . Average performance metrics of our models in terms of Precision (P), Recall (R) and F1 score with 95% confidence intervals. *Perf",
    "full_text_length": 44459,
    "chunk_length": 1485
  },
  {
    "chunk_id": 2349,
    "paper_filename": "nazannin_2024_Efficient_labeling_of_french_mammography_reports.pdf",
    "paper_title": "Nazannin 2024 Efficient Labeling Of French Mammography Reports",
    "chunk_index": 12,
    "total_chunks": 42,
    "text_content": "obtained from the rule-based method, initial labels provided by radiologists, labels from the extended hybrid method, and labels from an unseen institute provided by radiologists. Scientific Reports | (2024) 14:24842 3 | https://doi.org/10.1038/s41598-024-76369-ywww.nature.com/scientificreports/ from multiple centers makes our approach more generalizable and robust. Data statistics and class distribution of the data is detailed in Table 1. Ethical and legal considerations After acquiring DICOM i",
    "full_text_length": 44459,
    "chunk_length": 1509
  },
  {
    "chunk_id": 2350,
    "paper_filename": "nazannin_2024_Efficient_labeling_of_french_mammography_reports.pdf",
    "paper_title": "Nazannin 2024 Efficient Labeling Of French Mammography Reports",
    "chunk_index": 13,
    "total_chunks": 42,
    "text_content": "privacy. To further protect patient data, we developed a secure annotation system for radiologists. Access to this system was tightly controlled via a Virtual Private Network (VPN), ensuring that radiologists could only access and annotate data through this secure environment. This system ensured that data never left the secure server, and only authorized personnel were able to interact with the data. Model architecture We employed a two-level sub-classification approach for learning surgery pri",
    "full_text_length": 44459,
    "chunk_length": 1372
  },
  {
    "chunk_id": 2351,
    "paper_filename": "nazannin_2024_Efficient_labeling_of_french_mammography_reports.pdf",
    "paper_title": "Nazannin 2024 Efficient Labeling Of French Mammography Reports",
    "chunk_index": 14,
    "total_chunks": 42,
    "text_content": "model based on a modified version of the BERT-base transformer architecture15, incorporating a sequence classification head on top. The mammography report texts are tokenized, and the maximum number of tokens in each input sequence is constrained to 512. Labels Rule-based labels In our study, we implemented a rule-based method based on regular expressions to extract surgery prior information from the text of mammography reports. This approach involved designing specific patterns to identify whet",
    "full_text_length": 44459,
    "chunk_length": 1354
  },
  {
    "chunk_id": 2352,
    "paper_filename": "nazannin_2024_Efficient_labeling_of_french_mammography_reports.pdf",
    "paper_title": "Nazannin 2024 Efficient Labeling Of French Mammography Reports",
    "chunk_index": 15,
    "total_chunks": 42,
    "text_content": "extracted the rule- based labels for all the reports in our dataset (see Table 1). However, the primary challenge of this approach is Fig. 2 . Schematic Representation of the Two-Level Sub-Classification Model. The first level involves the Surgery_Presence model, which performs binary classification to determine the presence or absence of prior surgery. The second level, applicable only to cases with prior surgery history, involves the Surgery_Laterality model, which further classifies the surge",
    "full_text_length": 44459,
    "chunk_length": 1384
  },
  {
    "chunk_id": 2353,
    "paper_filename": "nazannin_2024_Efficient_labeling_of_french_mammography_reports.pdf",
    "paper_title": "Nazannin 2024 Efficient Labeling Of French Mammography Reports",
    "chunk_index": 16,
    "total_chunks": 42,
    "text_content": "P: Precision, R: Recall, F1: macro-F1 Score. Scientific Reports | (2024) 14:24842 4 | https://doi.org/10.1038/s41598-024-76369-ywww.nature.com/scientificreports/ the time-consuming nature of the pattern design process. Additionally, labels can not generalize beyond those patterns. This means enhancing it requires constant pattern updates for each institution or the creation of new patterns for specific institutions. As shown in Table 2, this method achieves suboptimal performance on the unseen i",
    "full_text_length": 44459,
    "chunk_length": 1428
  },
  {
    "chunk_id": 2354,
    "paper_filename": "nazannin_2024_Efficient_labeling_of_french_mammography_reports.pdf",
    "paper_title": "Nazannin 2024 Efficient Labeling Of French Mammography Reports",
    "chunk_index": 17,
    "total_chunks": 42,
    "text_content": "task. The initial level focuses on the surgery presence task. The second level addresses the surgery laterality task. Extended hybrid labels The labels generated by radiologists show high quality. However, their production comes at a considerable cost. On the other hand, the rule-based method offers a low-cost alternative for label production. However, it has limitations in terms of quality and generalization. On the other hand, a substantial dataset of high-quality labels is crucial to effectiv",
    "full_text_length": 44459,
    "chunk_length": 1320
  },
  {
    "chunk_id": 2355,
    "paper_filename": "nazannin_2024_Efficient_labeling_of_french_mammography_reports.pdf",
    "paper_title": "Nazannin 2024 Efficient Labeling Of French Mammography Reports",
    "chunk_index": 18,
    "total_chunks": 42,
    "text_content": "focused on instances where the model\u2019s predictions aligned with the rule-based labels and were made with high confidence. This strategy aimed to reinforce the model\u2019s existing confidence and equip it with extensive language knowledge of mammography reports. At the time of writing, there is no pre-trained lan - guage model specifically for mammography reports. Thus, we employed a general domain pre-trained French language model. By augmenting the training set with an additional \u223c60k samples ident",
    "full_text_length": 44459,
    "chunk_length": 1337
  },
  {
    "chunk_id": 2356,
    "paper_filename": "nazannin_2024_Efficient_labeling_of_french_mammography_reports.pdf",
    "paper_title": "Nazannin 2024 Efficient Labeling Of French Mammography Reports",
    "chunk_index": 19,
    "total_chunks": 42,
    "text_content": "conflicting cases were then annotated by radiologists. We ensured diversity in our dataset by selecting \u223c2k additional samples uniformly at random from eight distinct institutions. It method targeted the model\u2019s weaknesses, making the learning process more robust and accurate. It also enhanced the variety of the training data.To ensure the highest quality of our dataset, we implemented an iterative process of label extension, which combines automatic methods with manual refinements by radiologis",
    "full_text_length": 44459,
    "chunk_length": 1390
  },
  {
    "chunk_id": 2357,
    "paper_filename": "nazannin_2024_Efficient_labeling_of_french_mammography_reports.pdf",
    "paper_title": "Nazannin 2024 Efficient Labeling Of French Mammography Reports",
    "chunk_index": 20,
    "total_chunks": 42,
    "text_content": "Training procedure For all our models, we fine-tune all layers of the CamemBERT model for 4 epochs. The Surgery_Presence model takes the text of the mammography report as input. It assigns the text the class label NO_SURGERY or YES_SURGERY . SURGERY_LEFT, SURGERY_RIGHT and SURGERY_BOTH are considered YES_SURGERY labels. This model is trained using binary cross-entropy loss with class weight and Adam optimization with a learning rate of 2e\u22125. We used scikit-learn\u2019s compute_class_weight with the \u201c",
    "full_text_length": 44459,
    "chunk_length": 1368
  },
  {
    "chunk_id": 2358,
    "paper_filename": "nazannin_2024_Efficient_labeling_of_french_mammography_reports.pdf",
    "paper_title": "Nazannin 2024 Efficient Labeling Of French Mammography Reports",
    "chunk_index": 21,
    "total_chunks": 42,
    "text_content": "number of epochs to 8 due to improved performance observed on the validation set. The Surgery_Laterality model is trained using categorical cross- entropy loss, considering class weights to address the slight imbalance in the class distribution. Throughout the training process, we periodically assess the model on the development set and save the checkpoint corresponding to the highest performance. All models undergo training using 2 NVIDIA Tesla P100 SXM2 16GB GPUs with a batch size of 32. Evalu",
    "full_text_length": 44459,
    "chunk_length": 1367
  },
  {
    "chunk_id": 2359,
    "paper_filename": "nazannin_2024_Efficient_labeling_of_french_mammography_reports.pdf",
    "paper_title": "Nazannin 2024 Efficient Labeling Of French Mammography Reports",
    "chunk_index": 22,
    "total_chunks": 42,
    "text_content": "radiologists to annotate this sub - Scientific Reports | (2024) 14:24842 5 | https://doi.org/10.1038/s41598-024-76369-ywww.nature.com/scientificreports/ set, and the performance of all experiments is reported based on this radiologist-annotated set. Outputs of both models on the this set are combined to get the final prediction of surgery prior information among 4-classes of No_Surgery, Left_Surgery, Right_Surgery, Both_Surgery and evaluation in terms of accuracy is reported. Experiments Rule-ba",
    "full_text_length": 44459,
    "chunk_length": 1367
  },
  {
    "chunk_id": 2360,
    "paper_filename": "nazannin_2024_Efficient_labeling_of_french_mammography_reports.pdf",
    "paper_title": "Nazannin 2024 Efficient Labeling Of French Mammography Reports",
    "chunk_index": 23,
    "total_chunks": 42,
    "text_content": "ensures fairness and relevance in the development of the rule-based model. As shown in Table 2, EXP_Rule_ Label achieves an accuracy of 0.82 on unseen data. Training models solely on rule-based labels is impractical due to their limited quality and reliability. These labels, created from simple hand-crafted rules often fail to reflect the complexity of real data, resulting in potential biases and unreliable outcomes. A neural network trained on such labels would likely replicate these inaccuraci",
    "full_text_length": 44459,
    "chunk_length": 1354
  },
  {
    "chunk_id": 2361,
    "paper_filename": "nazannin_2024_Efficient_labeling_of_french_mammography_reports.pdf",
    "paper_title": "Nazannin 2024 Efficient Labeling Of French Mammography Reports",
    "chunk_index": 24,
    "total_chunks": 42,
    "text_content": "0.84 for the Surgery_Presence model and an average F1 of 0.76 for the Surgery_Laterality model. Although the labels were prepared by radiologists, the small size of the training set limited the learning capacity of the model. GAN-CamemBERT: semi-supervised with adversarial learning We conducted EXP_GAN_CamemBERT using the GAN-BERT model, as proposed by22, which combines Generative Adversarial Networks (GANs) with BERT for text classification. We selected this model as a state- of-the-art baselin",
    "full_text_length": 44459,
    "chunk_length": 1375
  },
  {
    "chunk_id": 2362,
    "paper_filename": "nazannin_2024_Efficient_labeling_of_french_mammography_reports.pdf",
    "paper_title": "Nazannin 2024 Efficient Labeling Of French Mammography Reports",
    "chunk_index": 25,
    "total_chunks": 42,
    "text_content": "the supervised model using only CamemBERT, yet it still does not achieve better performance than the EXP_HYB_Label model. Notably, GAN-BERT has shown more improvements in previous studies when applied to shorter text, such as single sentences or tweets22. Training on extended hybrid labels The EXP_HYB_Label is obtained by training the model on the hybrid of radiologists and rule-based labels described in section \u201c Labels \u201d , and fine-tuning of all weights. Performance metrics are reported in ter",
    "full_text_length": 44459,
    "chunk_length": 1301
  },
  {
    "chunk_id": 2363,
    "paper_filename": "nazannin_2024_Efficient_labeling_of_french_mammography_reports.pdf",
    "paper_title": "Nazannin 2024 Efficient Labeling Of French Mammography Reports",
    "chunk_index": 26,
    "total_chunks": 42,
    "text_content": "performance of the baselines. These results confirm that the devised semi-automated proposed method effectively increased the size of the training data. It preserved label quality and achieved notable diversity and coverage. In the pro - cess of making these hybrid labels, a large portion was added automatically, which may contain noise. After we got the results of the EXP_HYB_Label, we gave the misclassification in each fold to radiologists for review. Then, we updated the refined labels in the",
    "full_text_length": 44459,
    "chunk_length": 1312
  },
  {
    "chunk_id": 2364,
    "paper_filename": "nazannin_2024_Efficient_labeling_of_french_mammography_reports.pdf",
    "paper_title": "Nazannin 2024 Efficient Labeling Of French Mammography Reports",
    "chunk_index": 27,
    "total_chunks": 42,
    "text_content": "data. DrBERT18 is the language model for French Biomedical applications. It is based on the RoBERTa architecture and pretrained on the French Biomedical corpus NACHOS. We initialized the weights of EXP_HYB_REFINED_Label_DrBERT by adopting weights from DrBERT instead of CamemBERT. We used the same training procedure as EXP_HYB_Label. \u2022 Results , EXP_HYB_REFINED_Label_DrBERT as shown in Table 2, achieves an average F1 of 0.94 for the Surgery_Presence model and an average F1 of 0.91 for the Surgery",
    "full_text_length": 44459,
    "chunk_length": 1450
  },
  {
    "chunk_id": 2365,
    "paper_filename": "nazannin_2024_Efficient_labeling_of_french_mammography_reports.pdf",
    "paper_title": "Nazannin 2024 Efficient Labeling Of French Mammography Reports",
    "chunk_index": 28,
    "total_chunks": 42,
    "text_content": "of surgery but with a grandmother who underwent breast surgery: \u201cHistory of breast surgery in a paternal grandmother around the age of 65\u201d . To address the model\u2019s difficulty in correctly classifying reports with family-related surgery mentions, we augmented the training data by creating 1,000 synthetic samples. These samples were drawn from reports originally labeled as NO_SURGERY . We manually inserted references to family members\u2019 surgeries into these reports while maintaining the NO_SURGERY ",
    "full_text_length": 44459,
    "chunk_length": 1349
  },
  {
    "chunk_id": 2366,
    "paper_filename": "nazannin_2024_Efficient_labeling_of_french_mammography_reports.pdf",
    "paper_title": "Nazannin 2024 Efficient Labeling Of French Mammography Reports",
    "chunk_index": 29,
    "total_chunks": 42,
    "text_content": "other body organs During the examination of misclassified samples, we identified an additional source of error wherein mammography reports contained histories of surgeries on other organs of the body. As demonstrated in the example, \u201cHistory of ovarian cancer treated by hysterectomy in 2012\u201d , this might lead to model confusion. To address this issue, we compiled a list of other body organs. During the pre-processing stage, we systematically excluded sentences mentioning these organs from the te",
    "full_text_length": 44459,
    "chunk_length": 1348
  },
  {
    "chunk_id": 2367,
    "paper_filename": "nazannin_2024_Efficient_labeling_of_french_mammography_reports.pdf",
    "paper_title": "Nazannin 2024 Efficient Labeling Of French Mammography Reports",
    "chunk_index": 30,
    "total_chunks": 42,
    "text_content": "data from each source. Subsequently, we computed the class distribution over the full dataset of each source. The analysis of class distribution in Table 3 reveals consistent patterns in patients\u2019 surgical history across five different institutes that conduct patient screenings. This consistency validates the model\u2019s robustness. It indicates that the model could be effectively generalized to other institutes in the future. The distinct distribution observed in the beta institute\u2019s data can be at",
    "full_text_length": 44459,
    "chunk_length": 1402
  },
  {
    "chunk_id": 2368,
    "paper_filename": "nazannin_2024_Efficient_labeling_of_french_mammography_reports.pdf",
    "paper_title": "Nazannin 2024 Efficient Labeling Of French Mammography Reports",
    "chunk_index": 31,
    "total_chunks": 42,
    "text_content": "the model\u2019s generalizability and robustness, we run an experiment using a modified cross- validation method. Since \u2019Omicron\u2019 is also a screening institution, we implemented 3-fold cross-validation where each fold corresponds to a specific screening institution (\u2019zeta\u2019 , \u2019 chi\u2019 , \u2019 delta\u2019) being used as the test set. The performance results of this cross-validation are added in Table 4, showing how well the model performs across different institutions, providing insights into its robustness and g",
    "full_text_length": 44459,
    "chunk_length": 1408
  },
  {
    "chunk_id": 2369,
    "paper_filename": "nazannin_2024_Efficient_labeling_of_french_mammography_reports.pdf",
    "paper_title": "Nazannin 2024 Efficient Labeling Of French Mammography Reports",
    "chunk_index": 32,
    "total_chunks": 42,
    "text_content": "board-certified radiologist determined the ground truth radiologist labels. Using more radiologists could provide a more accurate comparison to the radiologist benchmark. Fourth, we do test performance on a dataset from an institution unseen in training. Additional datasets across institutions could further establish the model\u2019s ability to generalize. Conclusion and future work In this study, we developed a new method for mammography report labeling. We integrated radiologist annotations with ex",
    "full_text_length": 44459,
    "chunk_length": 1503
  },
  {
    "chunk_id": 2370,
    "paper_filename": "nazannin_2024_Efficient_labeling_of_french_mammography_reports.pdf",
    "paper_title": "Nazannin 2024 Efficient Labeling Of French Mammography Reports",
    "chunk_index": 33,
    "total_chunks": 42,
    "text_content": "which are costly and time-consuming to produce. Scientific Reports | (2024) 14:24842 7 | https://doi.org/10.1038/s41598-024-76369-ywww.nature.com/scientificreports/ Our extensive experiments revealed two major sources of errors. The first was confusion between patients\u2019 and their relatives\u2019 medical histories. The second was the misinterpretation of surgeries on other body organs. We effectively addressed these by applying data augmentation for the former and preprocessing data to exclude irrelev",
    "full_text_length": 44459,
    "chunk_length": 1503
  },
  {
    "chunk_id": 2371,
    "paper_filename": "nazannin_2024_Efficient_labeling_of_french_mammography_reports.pdf",
    "paper_title": "Nazannin 2024 Efficient Labeling Of French Mammography Reports",
    "chunk_index": 34,
    "total_chunks": 42,
    "text_content": "the impact of image integration on the model\u2019s stability and accuracy could provide further insights into optimizing performance and be a valuable direction for future research. Data availability The datasets used and analysed within this study are not publicly available due to legal restrictions but are avail - able from the corresponding author on reasonable request. Code availability The source code as well as the pre-trained models will be available on github. Received: 22 April 2024; Accept",
    "full_text_length": 44459,
    "chunk_length": 1285
  },
  {
    "chunk_id": 2372,
    "paper_filename": "nazannin_2024_Efficient_labeling_of_french_mammography_reports.pdf",
    "paper_title": "Nazannin 2024 Efficient Labeling Of French Mammography Reports",
    "chunk_index": 35,
    "total_chunks": 42,
    "text_content": "I. et al. Supervised and unsupervised language modelling in chest X-ray radiological reports. PLoS One 15, e0229963 (2020). 4. Wood, D. A., Lynch, J., Kafiabadi, S., Guilhem, E., Al Busaidi, A., Montvila, A., Varsavsky, T., Siddiqui, J., Gadapa, N., Townend, M. et al. Automated labelling using an attention model for radiology reports of mri scans (alarm). In Medical Imaging with Deep Learning, PMLR , pp. 811\u2013826 (2020). 5. Martin, L., Muller, B., Ortiz Suarez, P . J., Dupont, Y ., Romary, L., de",
    "full_text_length": 44459,
    "chunk_length": 1207
  },
  {
    "chunk_id": 2373,
    "paper_filename": "nazannin_2024_Efficient_labeling_of_french_mammography_reports.pdf",
    "paper_title": "Nazannin 2024 Efficient Labeling Of French Mammography Reports",
    "chunk_index": 36,
    "total_chunks": 42,
    "text_content": "imaging reports for pediatric traumatic brain injury. Acad. Emerg. Med. 23, 171\u2013178 (2016). 8. Hassanpour, S., Langlotz, C. P ., Amrhein, T. J., Befera, N. T. & Lungren, M. P . Performance of a machine learning classifier of knee mri reports in two large academic radiology practices: a tool to estimate diagnostic yield. Am. J. Roentgenol. , 750\u2013753 (2017). 9. Chen, M. C. et al. Deep learning to classify radiology free-text reports. Radiology 286, 845\u2013852 (2018). 10. Bozkurt, S., Alkim, E., Baner",
    "full_text_length": 44459,
    "chunk_length": 1242
  },
  {
    "chunk_id": 2374,
    "paper_filename": "nazannin_2024_Efficient_labeling_of_french_mammography_reports.pdf",
    "paper_title": "Nazannin 2024 Efficient Labeling Of French Mammography Reports",
    "chunk_index": 37,
    "total_chunks": 42,
    "text_content": "et al. Chexpert: A large chest radiograph dataset with uncertainty labels and expert comparison. In Proceedings of the AAAI Conference on Artificial Intelligence , vol. 33, pp. 590\u2013597 (2019). 13. Bustos, A., Pertusa, A., Salinas, J.-M. & De La Iglesia-Vaya, M. Padchest: A large chest X-ray image dataset with multi-label annotated reports. Med. Image Anal. 66, 101797 (2020). 14. Chapman, B. E., Lee, S., Kang, H. P . & Chapman, W . W . Document-level classification of CT pulmonary angiography rep",
    "full_text_length": 44459,
    "chunk_length": 1265
  },
  {
    "chunk_id": 2375,
    "paper_filename": "nazannin_2024_Efficient_labeling_of_french_mammography_reports.pdf",
    "paper_title": "Nazannin 2024 Efficient Labeling Of French Mammography Reports",
    "chunk_index": 38,
    "total_chunks": 42,
    "text_content": "Syst. 32 (2019). 17. Lee, J. et al. Biobert: a pre-trained biomedical language representation model for biomedical text mining. Bioinformatics 36, 1234\u2013 1240 (2020). 18. Labrak, Y ., Bazoge, A., Dufour, R., Rouvier, M., Morin, E., Daille, B. & Gourraud, P .-A. DrBERT: A Robust Pre-trained Model in French for Biomedical and Clinical domains. In Proceedings of the 61th Annual Meeting of the Association for Computational Linguistics (ACL \u201923), Long Paper (Association for Computational Linguistics, ",
    "full_text_length": 44459,
    "chunk_length": 1359
  },
  {
    "chunk_id": 2376,
    "paper_filename": "nazannin_2024_Efficient_labeling_of_french_mammography_reports.pdf",
    "paper_title": "Nazannin 2024 Efficient Labeling Of French Mammography Reports",
    "chunk_index": 39,
    "total_chunks": 42,
    "text_content": "& Huang, X. How to fine-tune bert for text classification?. In Chinese Computational Linguistics: 18th China National Conference, CCL 2019, Kunming, China, October 18\u201320, 2019, Proceedings 18 , 194\u2013206 (Springer, 2019). 22. Croce, D., Castellucci, G. & Basili, R. GAN-BERT: Generative adversarial learning for robust text classification with a bunch of labeled examples. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguis",
    "full_text_length": 44459,
    "chunk_length": 1613
  },
  {
    "chunk_id": 2377,
    "paper_filename": "nazannin_2024_Efficient_labeling_of_french_mammography_reports.pdf",
    "paper_title": "Nazannin 2024 Efficient Labeling Of French Mammography Reports",
    "chunk_index": 40,
    "total_chunks": 42,
    "text_content": "(2019). Scientific Reports | (2024) 14:24842 8 | https://doi.org/10.1038/s41598-024-76369-ywww.nature.com/scientificreports/ Author contributions N.D. designed and developed the models, carried out the experiments and wrote the manuscript. V .S. processed the experimental data and analysed the results. A.C. managed the team, verified the experiments and analysed the results. M.H. processed and analysed the data. G.O. processed and analysed the data. P .F. supervised the project. All authors disc",
    "full_text_length": 44459,
    "chunk_length": 1484
  },
  {
    "chunk_id": 2378,
    "paper_filename": "nazannin_2024_Efficient_labeling_of_french_mammography_reports.pdf",
    "paper_title": "Nazannin 2024 Efficient Labeling Of French Mammography Reports",
    "chunk_index": 41,
    "total_chunks": 42,
    "text_content": "medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if you modified the licensed material. Y ou do not have permission under this licence to share adapted material derived from this article or parts of it. The images or other third party material in this article are included in the article\u2019s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material ",
    "full_text_length": 44459,
    "chunk_length": 950
  },
  {
    "chunk_id": 2379,
    "paper_filename": "nazish_2024_efficient_deep_learning_approaches_for_automated_tumor_detection_classification_and_localization_in_experimental_microwave_breast_Imaging_data.pdf",
    "paper_title": "Nazish 2024 Efficient Deep Learning Approaches For Automated Tumor Detection Classification And Localization In Experimental Microwave Breast Imaging Data",
    "chunk_index": 0,
    "total_chunks": 21,
    "text_content": "Efficient Deep Learning Approaches for Automated Tumor Detection, Classification, and Localization in Experimental Microwave Breast Imaging Data Nazish Khalid\u2217, Muhammad Hashir\u2217, Nasir Mahmood\u2020, Muhammad Asad\u2217, Muhammad A. Rehman\u2020, Muhammad Q. Mehmood\u2217, Muhammad Zubair\u2020and Yehia Massoud\u2020 \u2217Department of Electrical Engineering, Information Technology University of the Punjab, Lahore, Pakistan \u2020Innovative Technologies Laboratories (ITL), King Abdullah University of Science and Technology (KAUST), S",
    "full_text_length": 20600,
    "chunk_length": 1598
  },
  {
    "chunk_id": 2380,
    "paper_filename": "nazish_2024_efficient_deep_learning_approaches_for_automated_tumor_detection_classification_and_localization_in_experimental_microwave_breast_Imaging_data.pdf",
    "paper_title": "Nazish 2024 Efficient Deep Learning Approaches For Automated Tumor Detection Classification And Localization In Experimental Microwave Breast Imaging Data",
    "chunk_index": 1,
    "total_chunks": 21,
    "text_content": "Also, we present a deep learning framework that outperforms state- of-the-art microwave imaging methods and ML algorithms for tumor detection, localization, and characterization. The proposed framework gives promising results using our BMI system\u2019s mea- sured reflection coefficients ( S11). This work shows the potential advantages of applying cutting-edge deep learning algorithms in practical BMI systems. Index Terms \u2014Microwave Imaging (MWI), Deep learning (DL), Machine learning (ML), Convolutio",
    "full_text_length": 20600,
    "chunk_length": 1383
  },
  {
    "chunk_id": 2381,
    "paper_filename": "nazish_2024_efficient_deep_learning_approaches_for_automated_tumor_detection_classification_and_localization_in_experimental_microwave_breast_Imaging_data.pdf",
    "paper_title": "Nazish 2024 Efficient Deep Learning Approaches For Automated Tumor Detection Classification And Localization In Experimental Microwave Breast Imaging Data",
    "chunk_index": 2,
    "total_chunks": 21,
    "text_content": "being developed as a complementary tool with the standard ones. In the last decade, various Tomography- based and Radar-based techniques have been proposed and ongoing effort is being done to improve the outcomes. Breast Microwave Imaging (BMI) works by bombarding the microwaves on the breast, and the reflected signals from the healthy and affected areas help detect cancerous cells. Microwaves scatter differently in a tumor than healthy breast tissue due to the difference in their electrical pro",
    "full_text_length": 20600,
    "chunk_length": 1333
  },
  {
    "chunk_id": 2382,
    "paper_filename": "nazish_2024_efficient_deep_learning_approaches_for_automated_tumor_detection_classification_and_localization_in_experimental_microwave_breast_Imaging_data.pdf",
    "paper_title": "Nazish 2024 Efficient Deep Learning Approaches For Automated Tumor Detection Classification And Localization In Experimental Microwave Breast Imaging Data",
    "chunk_index": 3,
    "total_chunks": 21,
    "text_content": "drawn to AI as it forges a path in several fields. Many machine learning (ML) techniques have already been applied to diagnose diseases using X- rays, mammograms, and CT scans. These techniques are used to classify or diagnose particular diseases. The researchers employ mammogram images and X-rays for the diagnosis. However, only a tiny amount of research has been done using AI approaches to directly identify, localize, or classify breast cancer from scattering parameters without constructing im",
    "full_text_length": 20600,
    "chunk_length": 1347
  },
  {
    "chunk_id": 2383,
    "paper_filename": "nazish_2024_efficient_deep_learning_approaches_for_automated_tumor_detection_classification_and_localization_in_experimental_microwave_breast_Imaging_data.pdf",
    "paper_title": "Nazish 2024 Efficient Deep Learning Approaches For Automated Tumor Detection Classification And Localization In Experimental Microwave Breast Imaging Data",
    "chunk_index": 4,
    "total_chunks": 21,
    "text_content": "collect the data-set. Moreover, we created novel DL frameworks for the detection, localization, and characterization of different tumor sizes and phantom compositions directly from scatter- ing parameters captured by the BreastCare dataset and also tested our architecture on the University of Manitoba Breast Microwave Imaging Data-set (UM-BMID) [2]. The salient contributions of this paper are as following: 1) To scrutinize the subtlety of the BreastCare dataset, an open-access BMI data-set is pr",
    "full_text_length": 20600,
    "chunk_length": 1384
  },
  {
    "chunk_id": 2384,
    "paper_filename": "nazish_2024_efficient_deep_learning_approaches_for_automated_tumor_detection_classification_and_localization_in_experimental_microwave_breast_Imaging_data.pdf",
    "paper_title": "Nazish 2024 Efficient Deep Learning Approaches For Automated Tumor Detection Classification And Localization In Experimental Microwave Breast Imaging Data",
    "chunk_index": 5,
    "total_chunks": 21,
    "text_content": "Authorized licensed use limited to: University of Glasgow. Downloaded on March 11,2025 at 19:03:22 UTC from IEEE Xplore. Restrictions apply. 3) The novel deep learning frameworks are designed to detect, and localize tumors in the x,y, and z-axis, and characterize tumors based on size. To the best of our knowledge, the developed architecture is state-of-the-art in this domain, it outperforms the existing techniques and image reconstruction algorithms in false negative cases. II. S TATE -OF-THE-AR",
    "full_text_length": 20600,
    "chunk_length": 1355
  },
  {
    "chunk_id": 2385,
    "paper_filename": "nazish_2024_efficient_deep_learning_approaches_for_automated_tumor_detection_classification_and_localization_in_experimental_microwave_breast_Imaging_data.pdf",
    "paper_title": "Nazish 2024 Efficient Deep Learning Approaches For Automated Tumor Detection Classification And Localization In Experimental Microwave Breast Imaging Data",
    "chunk_index": 6,
    "total_chunks": 21,
    "text_content": "an intelligent classification system to identify breasts with lesions. These algorithms include nearest neighbor (NN), multi-layer perceptron (MLP), neural network, and Support Vector Machine (SVM). The findings are thoroughly investigated, and statistical measurements are employed to validate them. SVM was found to be the best with 98% accuracy. Reimer et al. [2] provided data from 1257 phantom scans in his data-set UM-BMID and used logistic regression to get 85.4%, diagnostic accuracy. Hamza e",
    "full_text_length": 20600,
    "chunk_length": 1308
  },
  {
    "chunk_id": 2386,
    "paper_filename": "nazish_2024_efficient_deep_learning_approaches_for_automated_tumor_detection_classification_and_localization_in_experimental_microwave_breast_Imaging_data.pdf",
    "paper_title": "Nazish 2024 Efficient Deep Learning Approaches For Automated Tumor Detection Classification And Localization In Experimental Microwave Breast Imaging Data",
    "chunk_index": 7,
    "total_chunks": 21,
    "text_content": "[7] proposed a system for DL that uses deep neural network (DNN) with convolutional layers to make it easier to detect, locate, and characterize tumours using measurements of scattering parameters and metadata information. III. B REAST MICROWAVE IMAGING SYSTEM The hardware setup consists of three main parts: antenna, phantom and autonomous rotational setup calibrated with the vector network analyzer (VNA). The image reconstruction algorithm we used with our mono-static design is based on Interfe",
    "full_text_length": 20600,
    "chunk_length": 1301
  },
  {
    "chunk_id": 2387,
    "paper_filename": "nazish_2024_efficient_deep_learning_approaches_for_automated_tumor_detection_classification_and_localization_in_experimental_microwave_breast_Imaging_data.pdf",
    "paper_title": "Nazish 2024 Efficient Deep Learning Approaches For Automated Tumor Detection Classification And Localization In Experimental Microwave Breast Imaging Data",
    "chunk_index": 8,
    "total_chunks": 21,
    "text_content": "antenna was designed according to 1-3cm tumor size. (a) Pre-clinical BMI system developed by our group (b) 3-D printed realistic breast phantom (left) with inclusion (center) and tumor detection using IMUSIC MWI algorithm(right) Fig. 1: BMI System and Results using MWI Algorithm The formation of a realistic phantom is a key factor for accurate analysis. Different compositions of Triton and water have been proposed by Duchene et al. [10] and Massa et al. [11], which have proved to be more robust ",
    "full_text_length": 20600,
    "chunk_length": 1250
  },
  {
    "chunk_id": 2388,
    "paper_filename": "nazish_2024_efficient_deep_learning_approaches_for_automated_tumor_detection_classification_and_localization_in_experimental_microwave_breast_Imaging_data.pdf",
    "paper_title": "Nazish 2024 Efficient Deep Learning Approaches For Automated Tumor Detection Classification And Localization In Experimental Microwave Breast Imaging Data",
    "chunk_index": 9,
    "total_chunks": 21,
    "text_content": "which transmits and receives signals through our low-cost, portable Pocket VNA. The signals are being recorded at a bandwidth of 2-4 GHz with 40 frequency points. The motor-based antenna arm rotates the antenna at equidistant angles of 8\u25e6hence taking 45 measure- ments for each rotation. The process takes 5 minutes for a complete scan. Figure 1a represents our designed hardware of preclinical BMI. B. Data-set Collection The position of the tumor was recorded for training the DL algorithm with our",
    "full_text_length": 20600,
    "chunk_length": 1267
  },
  {
    "chunk_id": 2389,
    "paper_filename": "nazish_2024_efficient_deep_learning_approaches_for_automated_tumor_detection_classification_and_localization_in_experimental_microwave_breast_Imaging_data.pdf",
    "paper_title": "Nazish 2024 Efficient Deep Learning Approaches For Automated Tumor Detection Classification And Localization In Experimental Microwave Breast Imaging Data",
    "chunk_index": 10,
    "total_chunks": 21,
    "text_content": "applied to get the images to validate the accuracy of the data collected. S-parameters along with the metadata files are provided for features. These metadata files include tumor size, position, BI-RADS class type and location. We have included different tumor sizes and phantom compositions to cater all major BI-RADS densities to diversify data-set. Two-third of the scans are with tumors Authorized licensed use limited to: University of Glasgow. Downloaded on March 11,2025 at 19:03:22 UTC from I",
    "full_text_length": 20600,
    "chunk_length": 1232
  },
  {
    "chunk_id": 2390,
    "paper_filename": "nazish_2024_efficient_deep_learning_approaches_for_automated_tumor_detection_classification_and_localization_in_experimental_microwave_breast_Imaging_data.pdf",
    "paper_title": "Nazish 2024 Efficient Deep Learning Approaches For Automated Tumor Detection Classification And Localization In Experimental Microwave Breast Imaging Data",
    "chunk_index": 11,
    "total_chunks": 21,
    "text_content": "tumor sizes 1, 1.2, 2, 2.5, and 3 cm. The S11parameters are converted from the frequency domain to the time domain using inverse Fourier Transform for feature extraction and lowering of computational time. IV. T UMOR DETECTION , LOCALIZATION ,AND CHARACTERIZATION USING NOVEL DL F RAMEWORKS DL is a subset of ML, which processes data according to a predetermined logical framework to uncover correlations and patterns; therefore, the trends move toward state-of-the-art DL. DL, also known as DNN, emp",
    "full_text_length": 20600,
    "chunk_length": 1346
  },
  {
    "chunk_id": 2391,
    "paper_filename": "nazish_2024_efficient_deep_learning_approaches_for_automated_tumor_detection_classification_and_localization_in_experimental_microwave_breast_Imaging_data.pdf",
    "paper_title": "Nazish 2024 Efficient Deep Learning Approaches For Automated Tumor Detection Classification And Localization In Experimental Microwave Breast Imaging Data",
    "chunk_index": 12,
    "total_chunks": 21,
    "text_content": "whole working of the frameworks. A. Tumor detection using novel DL framework Using shortcut connections, we proposed the framework inspired by the residual networks (ResNet). By enabling the gradient to pass through this additional shortcut path, ResNet\u2019sskip connections address the issue of vanishing gradients in deep neural networks (DNN). These connections also assist the model by enabling it to learn identity functions, which guarantees that the higher layer will perform at least as well as ",
    "full_text_length": 20600,
    "chunk_length": 1292
  },
  {
    "chunk_id": 2392,
    "paper_filename": "nazish_2024_efficient_deep_learning_approaches_for_automated_tumor_detection_classification_and_localization_in_experimental_microwave_breast_Imaging_data.pdf",
    "paper_title": "Nazish 2024 Efficient Deep Learning Approaches For Automated Tumor Detection Classification And Localization In Experimental Microwave Breast Imaging Data",
    "chunk_index": 13,
    "total_chunks": 21,
    "text_content": "of four ResNet layers. All these block outputs are concatenated and passed through our designed dense layer network inspired by the VGG network. The framework is first trained and tested by using the BreastCare data-set. The framework used only S11 parameter and was evaluated using the classification task met- rics like F1-Score. The framework obtained F1-score for the detection of 0.973 on the BreastCare data-set. Our framework is further evaluated on UM-BMID second-generation scan data-set and",
    "full_text_length": 20600,
    "chunk_length": 1254
  },
  {
    "chunk_id": 2393,
    "paper_filename": "nazish_2024_efficient_deep_learning_approaches_for_automated_tumor_detection_classification_and_localization_in_experimental_microwave_breast_Imaging_data.pdf",
    "paper_title": "Nazish 2024 Efficient Deep Learning Approaches For Automated Tumor Detection Classification And Localization In Experimental Microwave Breast Imaging Data",
    "chunk_index": 14,
    "total_chunks": 21,
    "text_content": "for image localization. The framework consists of two con- volutional layers and a dense layer network influenced by the VGG network. The framework is evaluated on the BreastCare data-set. The data-set consists of feature matrices comprised of a matrix of 40x45 for each scan, and data consist of 900 Fig. 2: The illustration of the whole framework working for Detection, Localization in terms of coordinates x,y,z, and Characterization of tumor in terms of size. Authorized licensed use limited to: ",
    "full_text_length": 20600,
    "chunk_length": 1150
  },
  {
    "chunk_id": 2394,
    "paper_filename": "nazish_2024_efficient_deep_learning_approaches_for_automated_tumor_detection_classification_and_localization_in_experimental_microwave_breast_Imaging_data.pdf",
    "paper_title": "Nazish 2024 Efficient Deep Learning Approaches For Automated Tumor Detection Classification And Localization In Experimental Microwave Breast Imaging Data",
    "chunk_index": 15,
    "total_chunks": 21,
    "text_content": "- SVM poly 92 92.7 92 92.9 - - SVM Gaussian 93 96.7 85.7 94 - - SVM sigmoid 56 69 20 66 - - SVM Linear 93 94.5 90 93.9 - - LDA 81 85.4 70 85.9 - - DL Framework Detection 97 95 94.5 97.3 - - Localization - - - - 30 84 Characterization 97 96 96.5 97 - - such scans making it the same as the image numeric data and can apply image localization problem. The data-set is divided randomly between train, validation, and test split. Seventy percent of data is used for training and validating the model, whi",
    "full_text_length": 20600,
    "chunk_length": 1069
  },
  {
    "chunk_id": 2395,
    "paper_filename": "nazish_2024_efficient_deep_learning_approaches_for_automated_tumor_detection_classification_and_localization_in_experimental_microwave_breast_Imaging_data.pdf",
    "paper_title": "Nazish 2024 Efficient Deep Learning Approaches For Automated Tumor Detection Classification And Localization In Experimental Microwave Breast Imaging Data",
    "chunk_index": 16,
    "total_chunks": 21,
    "text_content": "a Mean Squared Error (MSE) during training of 0.13 and an R2 score of 0.98; during the testing phase, the R2 score was 0.84. The MSE was 0.30, which shows the good working of our architecture to predict, the x,y, and zaxis of tumor location on the BreastCare data-set. C. Tumor Characterization using novel DL framework The framework characterises tumor size directly from the S11parameter. It consists of three convolutional layers using different filters, kernel sizes, and a dense layer network. T",
    "full_text_length": 20600,
    "chunk_length": 1270
  },
  {
    "chunk_id": 2396,
    "paper_filename": "nazish_2024_efficient_deep_learning_approaches_for_automated_tumor_detection_classification_and_localization_in_experimental_microwave_breast_Imaging_data.pdf",
    "paper_title": "Nazish 2024 Efficient Deep Learning Approaches For Automated Tumor Detection Classification And Localization In Experimental Microwave Breast Imaging Data",
    "chunk_index": 17,
    "total_chunks": 21,
    "text_content": "KNN, NB, CART, and SVM with kernels Polynomial (Poly), Gaus- sian, Sigmoid, and Linear Discriminant Analysis (LDA). Table I shows the complete comparison of the algorithm\u2019s perfor- mance based on evaluation metrics with our designed DL frameworks.V. S UMMARY We designed a portable breast microwave imaging system prototype, and the data-set is collected using realistic breast phantoms. Our novel DL frameworks outperform different state-of-the-art ML algorithms in terms of tumour localization, det",
    "full_text_length": 20600,
    "chunk_length": 1315
  },
  {
    "chunk_id": 2397,
    "paper_filename": "nazish_2024_efficient_deep_learning_approaches_for_automated_tumor_detection_classification_and_localization_in_experimental_microwave_breast_Imaging_data.pdf",
    "paper_title": "Nazish 2024 Efficient Deep Learning Approaches For Automated Tumor Detection Classification And Localization In Experimental Microwave Breast Imaging Data",
    "chunk_index": 18,
    "total_chunks": 21,
    "text_content": "diagnosis of various life- threatening diseases. REFERENCES [1] M. Arnold, E. Morgan, H. Rumgay, A. Mafra, D. Singh, M. Laversanne, J. Vignat, J. R. Gralow, F. Cardoso, S. Siesling et al. , \u201cCurrent and future burden of breast cancer: Global statistics for 2020 and 2040,\u201d The Breast , vol. 66, pp. 15\u201323, 2022. [2] T. Reimer, J. Krenkevich, and S. Pistorius, \u201cAn open-access experimental dataset for breast microwave imaging,\u201d in 2020 14th European Confer- ence on Antennas and Propagation (EuCAP) .",
    "full_text_length": 20600,
    "chunk_length": 1226
  },
  {
    "chunk_id": 2398,
    "paper_filename": "nazish_2024_efficient_deep_learning_approaches_for_automated_tumor_detection_classification_and_localization_in_experimental_microwave_breast_Imaging_data.pdf",
    "paper_title": "Nazish 2024 Efficient Deep Learning Approaches For Automated Tumor Detection Classification And Localization In Experimental Microwave Breast Imaging Data",
    "chunk_index": 19,
    "total_chunks": 21,
    "text_content": "approaches for automated lesion detection in microwave breast imaging clinical data,\u201d Scientific reports , vol. 9, no. 1, pp. 1\u201312, 2019. [5] H. Sami, M. Sagheer, K. Riaz, M. Q. Mehmood, and M. Zubair, \u201cMachine learning-based approaches for breast cancer detection in microwave imaging,\u201d in 2021 IEEE USNC-URSI Radio Science Meeting (Joint with AP-S Symposium) , 2021, pp. 72\u201373. [6] R. C. Conceic \u00b8 \u02dcao, H. Medeiros, D. M. Godinho, M. O\u2019Halloran, D. Rodriguez-Herrera, D. Flores-Tapia, and S. Pistor",
    "full_text_length": 20600,
    "chunk_length": 1266
  },
  {
    "chunk_id": 2399,
    "paper_filename": "nazish_2024_efficient_deep_learning_approaches_for_automated_tumor_detection_classification_and_localization_in_experimental_microwave_breast_Imaging_data.pdf",
    "paper_title": "Nazish 2024 Efficient Deep Learning Approaches For Automated Tumor Detection Classification And Localization In Experimental Microwave Breast Imaging Data",
    "chunk_index": 20,
    "total_chunks": 21,
    "text_content": "R. Solimene, A. Cuccaro, D. Gaetano, J. Browne, and M. Ammann, \u201cBreast cancer detection using interferometric music: Experimental and numerical assessment,\u201d Medical Physics , vol. 41, 10 2014. [9] H. Sami, M. Sagheer, M. A. Altaf, J. Iqbal, and M. Zubair, \u201cDevel- opment of prototype microwave radar-based imaging system for breast cancer detection,\u201d Journal of Engineering and Applied Sciences , vol. 39, 2020. [10] N. Joachimowicz, C. Conessa, T. Henriksson, and B. Duch \u02c6ene, \u201cBreast phantoms for ",
    "full_text_length": 20600,
    "chunk_length": 1008
  },
  {
    "chunk_id": 2400,
    "paper_filename": "neurIPS_2023_visual-instruction_tuning.pdf",
    "paper_title": "Neurips 2023 Visual-Instruction Tuning",
    "chunk_index": 0,
    "total_chunks": 91,
    "text_content": "Visual Instruction Tuning Haotian Liu1\u0003, Chunyuan Li2\u0003, Qingyang Wu3, Yong Jae Lee1 1University of Wisconsin\u2013Madison2Microsoft Research3Columbia University https://llava-vl.github.io Abstract Instruction tuning large language models (LLMs) using machine-generated instruction-following data has been shown to improve zero-shot capabilities on new tasks, but the idea is less explored in the multimodal \ufb01eld. We present the \ufb01rst attempt to use language-only GPT-4 to generate multimodal language-image",
    "full_text_length": 90529,
    "chunk_length": 1489
  },
  {
    "chunk_id": 2401,
    "paper_filename": "neurIPS_2023_visual-instruction_tuning.pdf",
    "paper_title": "Neurips 2023 Visual-Instruction Tuning",
    "chunk_index": 1,
    "total_chunks": 91,
    "text_content": "with GPT-4 on a synthetic multimodal instruction-following dataset. When \ufb01ne-tuned on Science QA, the synergy of LLaV A and GPT-4 achieves a new state-of-the-art accuracy of 92.53%. We make GPT-4 generated visual instruction tuning data, our model, and code publicly available. 1 Introduction Humans interact with the world through many channels such as vision and language, as each individual channel has a unique advantage in representing and communicating certain concepts, and thus facilitates a ",
    "full_text_length": 90529,
    "chunk_length": 1367
  },
  {
    "chunk_id": 2402,
    "paper_filename": "neurIPS_2023_visual-instruction_tuning.pdf",
    "paper_title": "Neurips 2023 Visual-Instruction Tuning",
    "chunk_index": 2,
    "total_chunks": 91,
    "text_content": "and captioning [ 49,27], as well as visual generation and editing [ 41,42,55,15,43,29]. We refer readers to the Computer Vision in the Wild reading list for a more up-to-date literature compilation [ 12]. In this line of work, each task is solved independently by one single large vision model, with the task instruction implicitly considered in the model design. Further, language is only utilized to describe the image content. While this allows language to play an important role in mapping visual",
    "full_text_length": 90529,
    "chunk_length": 1264
  },
  {
    "chunk_id": 2403,
    "paper_filename": "neurIPS_2023_visual-instruction_tuning.pdf",
    "paper_title": "Neurips 2023 Visual-Instruction Tuning",
    "chunk_index": 3,
    "total_chunks": 91,
    "text_content": "to switch to the task of interest to solve it. For example, the recent success of ChatGPT [ 34] and GPT-4 [ 35] have demonstrated the power of aligned LLMs in following human instructions, and have stimulated tremendous interest in developing open-source LLMs. Among them, LLaMA [ 48] is an open- source LLM that matches the performance of GPT-3. Alpaca [ 47], Vicuna [ 9], GPT-4-LLM [ 37] 37th Conference on Neural Information Processing Systems (NeurIPS 2023). utilize various machine-generated hig",
    "full_text_length": 90529,
    "chunk_length": 1390
  },
  {
    "chunk_id": 2404,
    "paper_filename": "neurIPS_2023_visual-instruction_tuning.pdf",
    "paper_title": "Neurips 2023 Visual-Instruction Tuning",
    "chunk_index": 4,
    "total_chunks": 91,
    "text_content": "lack of vision-language instruction-following data. We present a data reformation perspective and pipeline to convert image-text pairs into an appropriate instruction-following format, using ChatGPT/GPT-4. \u2022Large multimodal models . We develop a large multimodal model (LMM), by connecting the open-set visual encoder of CLIP [ 39] with the language decoder Vicuna [ 9], and \ufb01ne-tuning end-to-end on our generated instructional vision-language data. Our empirical study validates the effectiveness of",
    "full_text_length": 90529,
    "chunk_length": 1471
  },
  {
    "chunk_id": 2405,
    "paper_filename": "neurIPS_2023_visual-instruction_tuning.pdf",
    "paper_title": "Neurips 2023 Visual-Instruction Tuning",
    "chunk_index": 5,
    "total_chunks": 91,
    "text_content": "Related Work Multimodal Instruction-following Agents. In computer vision, existing works that build instruction-following agents can be broadly categorized into two classes: (i)End-to-end trained models, which are separately explored for each speci\ufb01c research topic. For example, the vision- language navigation task [ 3,19] and Habitat [ 46] require the embodied AI agent to follow natural language instructions and take a sequence of actions to complete goals in visual environments. In the image e",
    "full_text_length": 90529,
    "chunk_length": 1304
  },
  {
    "chunk_id": 2406,
    "paper_filename": "neurIPS_2023_visual-instruction_tuning.pdf",
    "paper_title": "Neurips 2023 Visual-Instruction Tuning",
    "chunk_index": 6,
    "total_chunks": 91,
    "text_content": "for multiple tasks. Instruction Tuning. In the natural language processing (NLP) community, to enable LLMs such as GPT-3 [ 7], T5 [ 40], PaLM [ 10], and OPT [ 59] to follow natural language instructions and complete real-world tasks, researchers have explored methods for LLM instruction-tuning [ 36,51,50], leading to instruction-tuned counterparts such as InstructGPT [ 36]/ChatGPT [ 34], FLAN-T5 [ 11], FLAN-PaLM [ 11], and OPT-IML [ 22], respectively. It turns out that this simple approach can e",
    "full_text_length": 90529,
    "chunk_length": 1242
  },
  {
    "chunk_id": 2407,
    "paper_filename": "neurIPS_2023_visual-instruction_tuning.pdf",
    "paper_title": "Neurips 2023 Visual-Instruction Tuning",
    "chunk_index": 7,
    "total_chunks": 91,
    "text_content": "on image- text pairs include BLIP-2 [ 27], FROMAGe [ 24], and KOSMOS-1 [ 20]. PaLM-E [ 13] is an LMM for embodied AI. Based on the recent \u201cbest\u201d open-source LLM LLaMA, OpenFlamingo [ 5] and LLaMA-Adapter [ 58] are open-source efforts that enable LLaMA to use image inputs, paving the way to build open-source multimodal LLMs. While these models present promising task transfer generalization performance, they are not explicitly tuned with vision-language instruction data, and their performance in m",
    "full_text_length": 90529,
    "chunk_length": 1228
  },
  {
    "chunk_id": 2408,
    "paper_filename": "neurIPS_2023_visual-instruction_tuning.pdf",
    "paper_title": "Neurips 2023 Visual-Instruction Tuning",
    "chunk_index": 8,
    "total_chunks": 91,
    "text_content": "vehicle with various luggage. Luggage surrounds a vehicle in an underground parking area People try to \ufb01t all of their luggage in an SUV . The sport utility vehicle is parked in the public garage, being packed for a trip Some people with luggage near a van that is transporting it. Context type 2: Boxes person: [0.681, 0.242, 0.774, 0.694], backpack: [0.384, 0.696, 0.485, 0.914], suitcase: ...<omitted> Response type 1: conversation Question: What type of vehicle is featured in the image? Answer: ",
    "full_text_length": 90529,
    "chunk_length": 1207
  },
  {
    "chunk_id": 2409,
    "paper_filename": "neurIPS_2023_visual-instruction_tuning.pdf",
    "paper_title": "Neurips 2023 Visual-Instruction Tuning",
    "chunk_index": 9,
    "total_chunks": 91,
    "text_content": "pack their luggage into the SUV for a trip. ...<omitted> Response type 3: complex reasoning Question: What challenges do these people face? Answer: In the image, a group of people is standing outside a black SUV in a parking area, surrounded by various pieces of luggage, including suitcases and backpacks. They are facing the challenge of \ufb01tting all their luggage into the black SUV . There are multiple suitcases and backpacks to be packed, which suggests that the group has a signi\ufb01cant amount of ",
    "full_text_length": 90529,
    "chunk_length": 1209
  },
  {
    "chunk_id": 2410,
    "paper_filename": "neurIPS_2023_visual-instruction_tuning.pdf",
    "paper_title": "Neurips 2023 Visual-Instruction Tuning",
    "chunk_index": 10,
    "total_chunks": 91,
    "text_content": "surge in the amount of public multimodal data such as image-text pairs, ranging from CC [ 8] to LAION [ 44]. However, when it comes to multimodal instruction- following data, the available amount is limited, partially because the process for creating such data is time-consuming and less well-de\ufb01ned when human crowd-scouring is considered. Inspired by the success of recent GPT models in text-annotation tasks [ 17], we propose to leverage ChatGPT/GPT-4 for multimodal instruction-following data col",
    "full_text_length": 90529,
    "chunk_length": 1300
  },
  {
    "chunk_id": 2411,
    "paper_filename": "neurIPS_2023_visual-instruction_tuning.pdf",
    "paper_title": "Neurips 2023 Visual-Instruction Tuning",
    "chunk_index": 11,
    "total_chunks": 91,
    "text_content": "simple expanded version lacks diversity and in-depth reasoning in both the instructions and responses. To mitigate this issue, we leverage language-only GPT-4 or ChatGPT as the strong teacher (both accept only text as input), to create instruction-following data involving visual content. Speci\ufb01cally, in order to encode an image into its visual features to prompt a text-only GPT, we use two types of symbolic representations: (i)Captions typically describe the visual scene from various perspective",
    "full_text_length": 90529,
    "chunk_length": 1252
  },
  {
    "chunk_id": 2412,
    "paper_filename": "neurIPS_2023_visual-instruction_tuning.pdf",
    "paper_title": "Neurips 2023 Visual-Instruction Tuning",
    "chunk_index": 12,
    "total_chunks": 91,
    "text_content": "a few examples. They are the only human annotations we have during data collection, and are used as seed examples in in-context-learning to query GPT-4. \u2022Conversation . We design a conversation between the assistant and a person asking questions about this photo. The answers are in a tone as if the assistant is seeing the image and answering the question. A diverse set of questions are asked about the visual content of the image, including the object types, counting the objects, object actions, ",
    "full_text_length": 90529,
    "chunk_length": 1256
  },
  {
    "chunk_id": 2413,
    "paper_filename": "neurIPS_2023_visual-instruction_tuning.pdf",
    "paper_title": "Neurips 2023 Visual-Instruction Tuning",
    "chunk_index": 13,
    "total_chunks": 91,
    "text_content": "question from the list to ask GPT-4 to generate the detailed description. \u2022Complex reasoning . The above two types focus on the visual content itself, based on which we further create in-depth reasoning questions. The answers typically require a step-by-step reasoning process by following rigorous logic. We collect 158K unique language-image instruction-following samples in total, including 58K in conversations, 23K in detailed description, and 77k in complex reasoning, respectively. We ablated ",
    "full_text_length": 90529,
    "chunk_length": 5524
  },
  {
    "chunk_id": 2414,
    "paper_filename": "neurIPS_2023_visual-instruction_tuning.pdf",
    "paper_title": "Neurips 2023 Visual-Instruction Tuning",
    "chunk_index": 14,
    "total_chunks": 91,
    "text_content": "among publicly available checkpoints [47, 9, 37]. Vision Encoder<latexit sha1_base64=\"nmaulJAcZ9L9s1EtmepKU/wnbmw=\">AAAB7XicbVDLSgNBEOz1GeMr6tHLYBA8hV0R9Rj04jGCeUASwuykNxkzO7PMzAphyT948aCIV//Hm3/jJNmDJhY0FFXddHeFieDG+v63t7K6tr6xWdgqbu/s7u2XDg4bRqWaYZ0poXQrpAYFl1i33ApsJRppHApshqPbqd98Qm24kg92nGA3pgPJI86odVKjE0ZZc9Irlf2KPwNZJkFOypCj1it9dfqKpTFKywQ1ph34ie1mVFvOBE6KndRgQtmIDrDtqKQxmm42u3ZCTp3SJ5HSrqQlM/X3REZjY8Zx6Dpjaodm0ZuK/3nt1EbX3YzLJLUo2XxRlApiFZm+TvpcI7Ni7AhlmrtbCRtSTZl1ARVdCMHiy8ukcV4JLivB/UW5",
    "full_text_length": 90529,
    "chunk_length": 5487
  },
  {
    "chunk_id": 2415,
    "paper_filename": "neurIPS_2023_visual-instruction_tuning.pdf",
    "paper_title": "Neurips 2023 Visual-Instruction Tuning",
    "chunk_index": 15,
    "total_chunks": 91,
    "text_content": "considered, such as gated cross-attention in Flamingo [ 2] and Q-former in BLIP-2 [ 27]. We leave exploring possibly more effective and sophisticated architecture designs for LLaV A as future work. 4.2 Training For each image Xv, we generate multi-turn conversation data (X1 q;X1 a;\u0001\u0001\u0001;XT q;XT a), where Tis the total number of turns. We organize them as a sequence, by treating all answers as the assistant\u2019s response, and the instruction Xt instruct at the t-th turn as: Xt instruct =\u001aRandomly choo",
    "full_text_length": 90529,
    "chunk_length": 1258
  },
  {
    "chunk_id": 2416,
    "paper_filename": "neurIPS_2023_visual-instruction_tuning.pdf",
    "paper_title": "Neurips 2023 Visual-Instruction Tuning",
    "chunk_index": 16,
    "total_chunks": 91,
    "text_content": "input sequence used to train the model. Only two conversation turns are illustrated here; in practice, the number of turns varies based on the instruction-following data. In our current implementation, we follow Vicuna-v0 [ 9] to set the system message Xsystem-message and we set <STOP> =###. The model is trained to predict the assistant answers and where to stop, and thus only green sequence/tokens are used to compute the loss in the auto-regressive model. Speci\ufb01cally, for a sequence of length L",
    "full_text_length": 90529,
    "chunk_length": 1309
  },
  {
    "chunk_id": 2417,
    "paper_filename": "neurIPS_2023_visual-instruction_tuning.pdf",
    "paper_title": "Neurips 2023 Visual-Instruction Tuning",
    "chunk_index": 17,
    "total_chunks": 91,
    "text_content": "omit Xsystem-message and all previous <STOP> for better readability. For LLaV A model training, we consider a two-stage instruction-tuning procedure. Stage 1: Pre-training for Feature Alignment. To strike a balance between concept coverage and training ef\ufb01ciency, we \ufb01lter CC3M to 595K image-text pairs. Please see Appendix for details of the \ufb01ltering process. These pairs are converted to the instruction-following data using the naive expansion method describe in Section 3. Each sample can be trea",
    "full_text_length": 90529,
    "chunk_length": 1303
  },
  {
    "chunk_id": 2418,
    "paper_filename": "neurIPS_2023_visual-instruction_tuning.pdf",
    "paper_title": "Neurips 2023 Visual-Instruction Tuning",
    "chunk_index": 18,
    "total_chunks": 91,
    "text_content": "pre-trained LLM word embedding. This stage can be understood as training a compatible visual tokenizer for the frozen LLM. Stage 2: Fine-tuning End-to-End. We always keep the visual encoder weights frozen, and con- tinue to update both the pre-trained weights of the projection layer and LLM in LLaV A; i.e., the trainable parameters are \u0012=fW; gin (3). We consider two speci\ufb01c use case scenarios: \u2022Multimodal Chatbot . We develop a Chatbot by \ufb01ne-tuning on the 158K language-image instruction-followi",
    "full_text_length": 90529,
    "chunk_length": 1263
  },
  {
    "chunk_id": 2419,
    "paper_filename": "neurIPS_2023_visual-instruction_tuning.pdf",
    "paper_title": "Neurips 2023 Visual-Instruction Tuning",
    "chunk_index": 19,
    "total_chunks": 91,
    "text_content": "the reasoning process in natural language and selects the answer among multiple choices. For training in (2), we organize the data as a single turn conversation, the question & context as Xinstruct , and reasoning & answer as Xa. 5 Experiments We assess the performance of LLaV A in instruction-following and visual reasoning capabilities with two primary experimental settings: multimodal chatbot and the ScienceQA dataset, respectively. We train all models with 8 \u0002A100s, following Vicuna\u2019s hyperpa",
    "full_text_length": 90529,
    "chunk_length": 1271
  },
  {
    "chunk_id": 2420,
    "paper_filename": "neurIPS_2023_visual-instruction_tuning.pdf",
    "paper_title": "Neurips 2023 Visual-Instruction Tuning",
    "chunk_index": 20,
    "total_chunks": 91,
    "text_content": "and to study how well LLaV A is able to digest visual inputs and exhibit instruction-following 5 Visual input example, Extreme Ironing: Source: https://www.barnorama.com/wp-content/uploads/2016/12/03-Confusing-Pictures.jpg User What is unusual about this image? LLaV A The unusual aspect of this image is a man ironing clothes on the back of a minivan or van. This is not a typical place to perform this activity, as one would usually iron clothes in a more stationary and safe location, such as a ho",
    "full_text_length": 90529,
    "chunk_length": 1196
  },
  {
    "chunk_id": 2421,
    "paper_filename": "neurIPS_2023_visual-instruction_tuning.pdf",
    "paper_title": "Neurips 2023 Visual-Instruction Tuning",
    "chunk_index": 21,
    "total_chunks": 91,
    "text_content": "image depicts a man standing on top of a yellow SUV in a busy city street. He is holding a portable ladder, seemingly ironing clothes while standing on the vehicle. Around the scene, there are other cars, a traf\ufb01c light, and a \ufb01re hydrant. Additionally, there are two other people in the vicinity, one close to the man on the car and another further away. The man on the ladder appears to be attracting attention due to his unconventional choice of ironing his clothes on top of a moving car. The cit",
    "full_text_length": 90529,
    "chunk_length": 1081
  },
  {
    "chunk_id": 2422,
    "paper_filename": "neurIPS_2023_visual-instruction_tuning.pdf",
    "paper_title": "Neurips 2023 Visual-Instruction Tuning",
    "chunk_index": 22,
    "total_chunks": 91,
    "text_content": "man is sitting on the back of a yellow cab User What is unusual about this image? OpenFlamingo The man is drying his clothes on the hood of his car. Table 3: Example prompt from GPT-4 paper [ 35] to compare visual reasoning and chat capabilities. Compared to BLIP-2 [ 27] and OpenFlamingo [ 5], LLaV A accurately follows the user\u2019s instructions, instead of simply describing the scene. LLaV A offers a more comprehensive response than GPT-4. Even when merely asked to describe the image, LLaV A ident",
    "full_text_length": 90529,
    "chunk_length": 1236
  },
  {
    "chunk_id": 2423,
    "paper_filename": "neurIPS_2023_visual-instruction_tuning.pdf",
    "paper_title": "Neurips 2023 Visual-Instruction Tuning",
    "chunk_index": 23,
    "total_chunks": 91,
    "text_content": "trained with a small multimodal instruction-following dataset (\u001880K unique images), it demonstrates quite similar reasoning results with multimodal GPT-4 on these examples. Note that while these images are out-of-domain for LLaV A, LLaV A is still able to understand the scenes and follow the question instruction to provide a reasonable response. In contrast, BLIP-2 and OpenFlamingo focus on describing the image, instead of following the user instruction to answer in an appropriate manner. Quanti",
    "full_text_length": 90529,
    "chunk_length": 1343
  },
  {
    "chunk_id": 2424,
    "paper_filename": "neurIPS_2023_visual-instruction_tuning.pdf",
    "paper_title": "Neurips 2023 Visual-Instruction Tuning",
    "chunk_index": 24,
    "total_chunks": 91,
    "text_content": ", we create a reference prediction based on the question and theground-truth textual descriptions, using the text-only GPT-4. After obtaining the responses from 6 Conversation Detail description Complex reasoning All Full data 83.1 75.3 96.5 85.1 Detail + Complex 81.5 (-1.6) 73.3 (-2.0) 90.8 (-5.7) 81.9 (-3.2) Conv + 5% Detail + 10% Complex 81.0 (-2.1) 68.4 (-7.1) 91.5 (-5.0) 80.5 (-4.4) Conversation 76.5 (-6.6) 59.8 (-16.2) 84.9 (-12.4) 73.8 (-11.3) No Instruction Tuning 22.0 (-61.1) 24.0 (-51.",
    "full_text_length": 90529,
    "chunk_length": 1313
  },
  {
    "chunk_id": 2425,
    "paper_filename": "neurIPS_2023_visual-instruction_tuning.pdf",
    "paper_title": "Neurips 2023 Visual-Instruction Tuning",
    "chunk_index": 25,
    "total_chunks": 91,
    "text_content": "OpenFlamingo [5] 19.3\u00060.5 19.0\u00060.5 19.1\u00060.7 19.1 \u00060.4 BLIP-2 [27] 54.6\u00061.4 29.1\u00061.2 32.9\u00060.7 38.1 \u00061.0 LLaV A 57.3\u00061.9 52.5\u00066.3 81.7\u00061.8 67.3 \u00062.0 LLaV Ay58.8\u00060.6 49.2\u00060.8 81.4\u00060.3 66.7 \u00060.3 Table 5: Instruction-following capability comparison using relative scores on LLaV A-Bench (In-the- Wild). The results are reported in the format of mean\u0006std. For the \ufb01rst three rows, we report three inference runs. LLaV A performs signi\ufb01cantly better than others.yFor a given set of LLaV A decoding sequences",
    "full_text_length": 90529,
    "chunk_length": 1316
  },
  {
    "chunk_id": 2426,
    "paper_filename": "neurIPS_2023_visual-instruction_tuning.pdf",
    "paper_title": "Neurips 2023 Visual-Instruction Tuning",
    "chunk_index": 26,
    "total_chunks": 91,
    "text_content": "is also asked to provide a comprehensive explanation for the evaluation, for us to better understand the models. We report relative scores w.r.t. the text-only GPT-4 model that uses the textural ground truth description as visual input. We create two benchmarks to evaluate the model\u2019s performance. LLaVA-Bench (COCO). We randomly select 30 images from COCO-Val-2014, and for each image, we generate three types of questions (conversation, detailed description, complex reasoning) using the proposed ",
    "full_text_length": 90529,
    "chunk_length": 1416
  },
  {
    "chunk_id": 2427,
    "paper_filename": "neurIPS_2023_visual-instruction_tuning.pdf",
    "paper_title": "Neurips 2023 Visual-Instruction Tuning",
    "chunk_index": 27,
    "total_chunks": 91,
    "text_content": "improvement of the model\u2019s overall capability by 7 points. Furthermore, it also improves the model\u2019s performance on conversational questions, suggesting that improvements in reasoning capabilities complement conversational abilities. Finally, we show that having all three types of data yields the best performance at 85.1%. LLaVA-Bench (In-the-Wild). To evaluate the model\u2019s capability in more challenging tasks and generalizability to novel domains, we collect a diverse set of 24 images with 60 qu",
    "full_text_length": 90529,
    "chunk_length": 1371
  },
  {
    "chunk_id": 2428,
    "paper_filename": "neurIPS_2023_visual-instruction_tuning.pdf",
    "paper_title": "Neurips 2023 Visual-Instruction Tuning",
    "chunk_index": 28,
    "total_chunks": 91,
    "text_content": "overall score of 67.3%. Limitations. This LLaV A-Bench (In-the-Wild) is designed to be challenging and to reveal a model\u2019s weaknesses. We provide two examples with associated captions and questions in Table 6. For the ramen example (left), to correctly answer the name of the restaurant, it requires the model to have a large knowledge coverage and multilingual understanding capability; to correctly describe the side dishes, the model may need to retrieve relevant multimodal information from Inter",
    "full_text_length": 90529,
    "chunk_length": 1261
  },
  {
    "chunk_id": 2429,
    "paper_filename": "neurIPS_2023_visual-instruction_tuning.pdf",
    "paper_title": "Neurips 2023 Visual-Instruction Tuning",
    "chunk_index": 29,
    "total_chunks": 91,
    "text_content": "a spoon is placed in the center. The ramen is seasoned with chili sauce, chopped scallions, and served with two pieces of chashu. Chopsticks are placed to the right of the bowl, still in their paper wrap, not yet opened. The ramen is also served with nori on the left. On top, from left to right, the fol- lowing sides are served: a bowl of or- ange spice (possibly garlic sauce), a plate of smoke-\ufb02avored stewed pork with chopped scallions, and a cup of matcha green tea.An open refrigerator \ufb01lled w",
    "full_text_length": 90529,
    "chunk_length": 1088
  },
  {
    "chunk_id": 2430,
    "paper_filename": "neurIPS_2023_visual-instruction_tuning.pdf",
    "paper_title": "Neurips 2023 Visual-Instruction Tuning",
    "chunk_index": 30,
    "total_chunks": 91,
    "text_content": "is an unidenti\ufb01ed plastic bag placed on it. Towards the back, there is a carton of milk. In the right part of the compartment, towards the front, there is a box of blueberries with three yogurts stacked on top. The large bottle of yogurt is Fage non-fat yogurt, and one of the smaller cups is Fage blueberry yogurt. The brand and \ufb02avor of the other smaller cup are unknown. Towards the back, there is a container with an unknown content. Question 1 What\u2019s the name of the restaurant? What is the bran",
    "full_text_length": 90529,
    "chunk_length": 1199
  },
  {
    "chunk_id": 2431,
    "paper_filename": "neurIPS_2023_visual-instruction_tuning.pdf",
    "paper_title": "Neurips 2023 Visual-Instruction Tuning",
    "chunk_index": 31,
    "total_chunks": 91,
    "text_content": "contains only yogurt andstrawberries. This indicates that, at times, LLaV A perceives the image as a \u201cbag of patches\u201d, failing to grasp the complex semantics within the image. We hope LLaV A serves as a solid baseline on the benchmarks, on which our \ufb01ndings can inspire future work in developing more capable LMMs. 5.2 ScienceQA ScienceQA [ 33] contains 21k multimodal multiple choice questions with rich domain diversity across 3 subjects, 26 topics, 127 categories, and 379 skills. The benchmark da",
    "full_text_length": 90529,
    "chunk_length": 1223
  },
  {
    "chunk_id": 2432,
    "paper_filename": "neurIPS_2023_visual-instruction_tuning.pdf",
    "paper_title": "Neurips 2023 Visual-Instruction Tuning",
    "chunk_index": 32,
    "total_chunks": 91,
    "text_content": "we use the visual features before the last layer, ask the model to \ufb01rst predict reasons and then the answer, and train it for 12 epochs. It yields 90.92% accuracy, which is quite close to the SoTA 91.68%. To explore the limit of LLMs, we also prompt GPT-4 using 2-shot in-context-learning and achieve 82.69% accuracy, which is a 7.52% absolute gain compared with 75.17% from GPT-3.5. For a substantial number of questions, we note that GPT-4 fails simply because it reports that there is insuf\ufb01cient ",
    "full_text_length": 90529,
    "chunk_length": 1184
  },
  {
    "chunk_id": 2433,
    "paper_filename": "neurIPS_2023_visual-instruction_tuning.pdf",
    "paper_title": "Neurips 2023 Visual-Instruction Tuning",
    "chunk_index": 33,
    "total_chunks": 91,
    "text_content": "prompt GPT-4 again, asking it to provide its own \ufb01nal answer based on the question and two outcomes. The spirit is similar with CoT, but with the external knowledge from the other model. Surprisingly, this scheme is able to provide consistent improvement over all question classes, and achieves a new SoTA accuracy of 92.53%. Interestingly, 8 MethodSubject Context Modality GradeAverageNAT SOC LAN TXT IMG NO G1-6 G7-12 Representative & SoTA methods with numbers reported in the literature Human [33]",
    "full_text_length": 90529,
    "chunk_length": 1249
  },
  {
    "chunk_id": 2434,
    "paper_filename": "neurIPS_2023_visual-instruction_tuning.pdf",
    "paper_title": "Neurips 2023 Visual-Instruction Tuning",
    "chunk_index": 34,
    "total_chunks": 91,
    "text_content": "own experiment runs GPT-4y84.06 73.45 87.36 81.87 70.75 90.73 84.69 79.10 82.69 LLaV A 90.36 95.95 88.00 89.49 88.00 90.66 90.93 90.90 90.92 LLaV A+GPT-4y(complement) 90.36 95.50 88.55 89.05 87.80 91.08 92.22 88.73 90.97 LLaV A+GPT-4y(judge) 91.56 96.74 91.09 90.62 88.99 93.52 92.73 92.16 92.53 Table 7: Accuracy (%) on Science QA dataset. Question categories: NAT = natural science, SOC = social science, LAN = language science, TXT = text context, IMG = image context, NO = no context, G1-6 = grad",
    "full_text_length": 90529,
    "chunk_length": 1218
  },
  {
    "chunk_id": 2435,
    "paper_filename": "neurIPS_2023_visual-instruction_tuning.pdf",
    "paper_title": "Neurips 2023 Visual-Instruction Tuning",
    "chunk_index": 35,
    "total_chunks": 91,
    "text_content": "answer. The GPT-4 judge can identify such cases and correct some of the errors that LLaV A makes. See the example in Appendix. To the best of our knowledge, this is the \ufb01rst time that GPT-4 is used for model ensembling. We hope this \ufb01nding can encourage future research to explore more effective methods to leverage LLMs for model ensembling. Visual features Before Last Best variant 90.92 89.96 (-0.96) Predict answer \ufb01rst - 89.77 (-1.15) Training from scratch 85.81 (-5.11) - 7B model size 89.84 (-",
    "full_text_length": 90529,
    "chunk_length": 1205
  },
  {
    "chunk_id": 2436,
    "paper_filename": "neurIPS_2023_visual-instruction_tuning.pdf",
    "paper_title": "Neurips 2023 Visual-Instruction Tuning",
    "chunk_index": 36,
    "total_chunks": 91,
    "text_content": "last layer features may focus more on global and abstract image properties compared to the layer before it, which can focus more on localized properties that are useful for under- standing speci\ufb01c image details. (ii)Chain-of-thought . To decide the order between the answer and reasoning process in the model prediction, we run both variants and observe that answer-\ufb01rst reports the best number 89.77% accuracy in 12 epochs, while reasoning-\ufb01rst can quickly reach 89.77% accuracy in 6 epochs, but no ",
    "full_text_length": 90529,
    "chunk_length": 1332
  },
  {
    "chunk_id": 2437,
    "paper_filename": "neurIPS_2023_visual-instruction_tuning.pdf",
    "paper_title": "Neurips 2023 Visual-Instruction Tuning",
    "chunk_index": 37,
    "total_chunks": 91,
    "text_content": "the vast pre-trained knowledge. (iv)Model size. We keep all con\ufb01gurations the same as our best 13B model, and train a 7B model. This yields 89.84% accuracy, which is 1.08% lower than 90.92%, demonstrating the importance of model scale. 6 Conclusion This paper demonstrated the effectiveness of visual instruction tuning. We presented an automatic pipeline to create language-image instruction-following data, based on which we train LLaV A, a multimodal model to follow human intent to complete visua",
    "full_text_length": 90529,
    "chunk_length": 1324
  },
  {
    "chunk_id": 2438,
    "paper_filename": "neurIPS_2023_visual-instruction_tuning.pdf",
    "paper_title": "Neurips 2023 Visual-Instruction Tuning",
    "chunk_index": 38,
    "total_chunks": 91,
    "text_content": "hope our work can inspire future research on building more capable multimodal models. Acknowledgements. We thank Baolin Peng and Pan Lu for valuable discussions on instruction- tuning language models and Science QA, respectively. We thank the LLaMA team for giving us access to their models, and open-source projects, including Alpaca and Vicuna. This work was supported in part by NSF CAREER IIS2150012, and Institute of Information & communications Technology Planning & Evaluation(IITP) grants fun",
    "full_text_length": 90529,
    "chunk_length": 1455
  },
  {
    "chunk_id": 2439,
    "paper_filename": "neurIPS_2023_visual-instruction_tuning.pdf",
    "paper_title": "Neurips 2023 Visual-Instruction Tuning",
    "chunk_index": 39,
    "total_chunks": 91,
    "text_content": "Anderson, Qi Wu, Damien Teney, Jake Bruce, Mark Johnson, Niko S\u00fcnderhauf, Ian Reid, Stephen Gould, and Anton Van Den Hengel. Vision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments. In Proceedings of the IEEE conference on computer vision and pattern recognition , 2018. 2 [4]Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Jones, Nicholas Joseph, Ben Mann, Nova DasSarma, et al. A general language assistant as a",
    "full_text_length": 90529,
    "chunk_length": 1440
  },
  {
    "chunk_id": 2440,
    "paper_filename": "neurIPS_2023_visual-instruction_tuning.pdf",
    "paper_title": "Neurips 2023 Visual-Instruction Tuning",
    "chunk_index": 40,
    "total_chunks": 91,
    "text_content": "D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems , 33:1877\u20131901, 2020. 2 [8]Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12m: Pushing web-scale image-text pre-training to recognize long-tail visual concepts. In CVPR , 2021. 3 [9]Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, ",
    "full_text_length": 90529,
    "chunk_length": 1415
  },
  {
    "chunk_id": 2441,
    "paper_filename": "neurIPS_2023_visual-instruction_tuning.pdf",
    "paper_title": "Neurips 2023 Visual-Instruction Tuning",
    "chunk_index": 41,
    "total_chunks": 91,
    "text_content": "Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-\ufb01netuned language models. arXiv preprint arXiv:2210.11416 , 2022. 2 [12] CVinW. Computer vision in the wild. https://github.com/ Computer-Vision-in-the-Wild/CVinW_Readings , 2022. 1 [13] Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. PaLM-E: An embodied multimodal language model. arXiv preprint arXiv:2303.03378 , 2023",
    "full_text_length": 90529,
    "chunk_length": 1499
  },
  {
    "chunk_id": 2442,
    "paper_filename": "neurIPS_2023_visual-instruction_tuning.pdf",
    "paper_title": "Neurips 2023 Visual-Instruction Tuning",
    "chunk_index": 42,
    "total_chunks": 91,
    "text_content": "trends. Foundations and Trends \u00aein Computer Graphics and Vision , 2022. 1 [17] Fabrizio Gilardi, Meysam Alizadeh, and Ma\u00ebl Kubli. Chatgpt outperforms crowd-workers for text-annotation tasks. arXiv preprint arXiv:2303.15056 , 2023. 3 [18] Tanmay Gupta and Aniruddha Kembhavi. Visual programming: Compositional visual reasoning without training. arXiv preprint arXiv:2211.11559 , 2022. 2 [19] Weituo Hao, Chunyuan Li, Xiujun Li, Lawrence Carin, and Jianfeng Gao. Towards learning a generic agent for vi",
    "full_text_length": 90529,
    "chunk_length": 1380
  },
  {
    "chunk_id": 2443,
    "paper_filename": "neurIPS_2023_visual-instruction_tuning.pdf",
    "paper_title": "Neurips 2023 Visual-Instruction Tuning",
    "chunk_index": 43,
    "total_chunks": 91,
    "text_content": "If you use this software, please cite it as below. 1 [22] Srinivasan Iyer, Xi Victoria Lin, Ramakanth Pasunuru, Todor Mihaylov, D\u00e1niel Simig, Ping Yu, Kurt Shuster, Tianlu Wang, Qing Liu, Punit Singh Koura, et al. Opt-iml: Scaling language model instruction meta learning through the lens of generalization. arXiv preprint arXiv:2212.12017 , 2022. 2 [23] Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie, Bharath Hariharan, and Ser-Nam Lim. Visual prompt tuning. In ECCV , 2022.",
    "full_text_length": 90529,
    "chunk_length": 1316
  },
  {
    "chunk_id": 2444,
    "paper_filename": "neurIPS_2023_visual-instruction_tuning.pdf",
    "paper_title": "Neurips 2023 Visual-Instruction Tuning",
    "chunk_index": 44,
    "total_chunks": 91,
    "text_content": "A bench- mark and toolkit for evaluating language-augmented visual models. In NeurIPS Track on Datasets and Benchmarks , 2022. 1 [27] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language- image pre-training with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597 , 2023. 1, 2, 4, 6, 7 [28] Liunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jianwei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang, Lu Yuan, Lei Zhang, Jenq-Neng Hwang, et al. ",
    "full_text_length": 90529,
    "chunk_length": 1295
  },
  {
    "chunk_id": 2445,
    "paper_filename": "neurIPS_2023_visual-instruction_tuning.pdf",
    "paper_title": "Neurips 2023 Visual-Instruction Tuning",
    "chunk_index": 45,
    "total_chunks": 91,
    "text_content": "Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning, 2023. 10, 14 [32] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. arXiv preprint arXiv:2303.05499 , 2023. 1 [33] Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explai",
    "full_text_length": 90529,
    "chunk_length": 1329
  },
  {
    "chunk_id": 2446,
    "paper_filename": "neurIPS_2023_visual-instruction_tuning.pdf",
    "paper_title": "Neurips 2023 Visual-Instruction Tuning",
    "chunk_index": 46,
    "total_chunks": 91,
    "text_content": "in Neural Information Processing Systems , 35:27730\u201327744, 2022. 2 [37] Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. Instruction tuning with GPT-4. arXiv preprint arXiv:2304.03277 , 2023. 1, 4 [38] Hieu Pham, Zihang Dai, Golnaz Ghiasi, Kenji Kawaguchi, Hanxiao Liu, Adams Wei Yu, Jiahui Yu, Yi-Ting Chen, Minh-Thang Luong, Yonghui Wu, et al. Combined scaling for open-vocabulary image classi\ufb01cation. arXiv preprint arXiv: 2111.10050 , 2021. 1 [39] Alec Radford, Jong Wook ",
    "full_text_length": 90529,
    "chunk_length": 1367
  },
  {
    "chunk_id": 2447,
    "paper_filename": "neurIPS_2023_visual-instruction_tuning.pdf",
    "paper_title": "Neurips 2023 Visual-Instruction Tuning",
    "chunk_index": 47,
    "total_chunks": 91,
    "text_content": "[41] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. ArXiv , abs/2204.06125, 2022. 1 [42] Robin Rombach, A. Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High- resolution image synthesis with latent diffusion models. CVPR , pages 10674\u201310685, 2022. 1 [43] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L. Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, Seye",
    "full_text_length": 90529,
    "chunk_length": 1438
  },
  {
    "chunk_id": 2448,
    "paper_filename": "neurIPS_2023_visual-instruction_tuning.pdf",
    "paper_title": "Neurips 2023 Visual-Instruction Tuning",
    "chunk_index": 48,
    "total_chunks": 91,
    "text_content": "Vipergpt: Visual inference via python execution for reasoning. arXiv preprint arXiv:2303.08128 , 2023. 2 [46] Andrew Szot, Alex Clegg, Eric Undersander, Erik Wijmans, Yili Zhao, John Turner, Noah Maestre, Mustafa Mukadam, Devendra Chaplot, Oleksandr Maksymets, Aaron Gokaslan, Vladimir V ondrus, Sameer Dharur, Franziska Meier, Wojciech Galuba, Angel Chang, Zsolt Kira, Vladlen Koltun, Jitendra Malik, Manolis Savva, and Dhruv Batra. Habitat 2.0: Training home assistants to rearrange their habitat. ",
    "full_text_length": 90529,
    "chunk_length": 1450
  },
  {
    "chunk_id": 2449,
    "paper_filename": "neurIPS_2023_visual-instruction_tuning.pdf",
    "paper_title": "Neurips 2023 Visual-Instruction Tuning",
    "chunk_index": 49,
    "total_chunks": 91,
    "text_content": "Zhengyuan Yang, Xiaowei Hu, Linjie Li, Kevin Lin, Zhe Gan, Zicheng Liu, Ce Liu, and Lijuan Wang. Git: A generative image-to-text transformer for vision and language. arXiv preprint arXiv:2205.14100 , 2022. 1 [50] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-instruct: Aligning language model with self generated instruc- tions. arXiv preprint arXiv:2212.10560 , 2022. 2 [51] Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi,",
    "full_text_length": 90529,
    "chunk_length": 1394
  },
  {
    "chunk_id": 2450,
    "paper_filename": "neurIPS_2023_visual-instruction_tuning.pdf",
    "paper_title": "Neurips 2023 Visual-Instruction Tuning",
    "chunk_index": 50,
    "total_chunks": 91,
    "text_content": "contrastive learning in image-text-label space. CVPR , 2022. 1 [54] Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu, Michael Zeng, and Lijuan Wang. Mm-react: Prompting chatgpt for multimodal reasoning and action. arXiv preprint arXiv:2303.11381 , 2023. 2 [55] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, Benton C. Hutchinson, Wei Han, Zarana Parekh",
    "full_text_length": 90529,
    "chunk_length": 1346
  },
  {
    "chunk_id": 2451,
    "paper_filename": "neurIPS_2023_visual-instruction_tuning.pdf",
    "paper_title": "Neurips 2023 Visual-Instruction Tuning",
    "chunk_index": 51,
    "total_chunks": 91,
    "text_content": "Zhang. A simple framework for open-vocabulary segmentation and detection. arXiv preprint arXiv:2303.08131 , 2023. 1 [58] Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, Peng Gao, and Yu Qiao. Llama-adapter: Ef\ufb01cient \ufb01ne-tuning of language models with zero-init attention. arXiv preprint arXiv:2303.16199 , 2023. 2, 8, 9 [59] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin,",
    "full_text_length": 90529,
    "chunk_length": 1335
  },
  {
    "chunk_id": 2452,
    "paper_filename": "neurIPS_2023_visual-instruction_tuning.pdf",
    "paper_title": "Neurips 2023 Visual-Instruction Tuning",
    "chunk_index": 52,
    "total_chunks": 91,
    "text_content": "on Computer Vision and Pattern Recognition , pages 16793\u201316803, 2022. 1 [62] Xueyan Zou, Zi-Yi Dou, Jianwei Yang, Zhe Gan, Linjie Li, Chunyuan Li, Xiyang Dai, Harkirat Behl, Jianfeng Wang, Lu Yuan, et al. Generalized decoding for pixel, image, and language. arXiv preprint arXiv:2212.11270 , 2022. 1, 2 13 A Broader Impact The broader impact of LLaV A, a general-purpose visual assistant, has potential bene\ufb01ts and risks associated with its deployment and release. Some considerations are unique to L",
    "full_text_length": 90529,
    "chunk_length": 1244
  },
  {
    "chunk_id": 2453,
    "paper_filename": "neurIPS_2023_visual-instruction_tuning.pdf",
    "paper_title": "Neurips 2023 Visual-Instruction Tuning",
    "chunk_index": 53,
    "total_chunks": 91,
    "text_content": "two pre- cautionary measures for LLaV A: (1) OpenAI Filter API for user input text to prevent harmful or inappropriate text instructions from being processed by the model, and (2) NSFW Filter for uploaded user images to detect and block Not Safe For Work (NSFW) content or any other potentially harmful image inputs. Hallucination. Similar to LLMs, LLaV A might generate outputs that aren\u2019t grounded in facts or input data. This raises concerns about inferences made, especially in critical applicati",
    "full_text_length": 90529,
    "chunk_length": 1276
  },
  {
    "chunk_id": 2454,
    "paper_filename": "neurIPS_2023_visual-instruction_tuning.pdf",
    "paper_title": "Neurips 2023 Visual-Instruction Tuning",
    "chunk_index": 54,
    "total_chunks": 91,
    "text_content": "the pretraining dataset or increasing the model size, e.g., to a larger LLaMA version like the 65B model. Evaluation complexities. Assessing the performance of LLaV A is challenging as it involves both language and visual tasks. Our evaluation benchmark covers several aspects, including accuracy, concept coverage, reasoning ability, and creativity. However, additional aspects need consideration, such as the degree of visual content hallucination and \ufb01ne-grained understanding of visual content. W",
    "full_text_length": 90529,
    "chunk_length": 1383
  },
  {
    "chunk_id": 2455,
    "paper_filename": "neurIPS_2023_visual-instruction_tuning.pdf",
    "paper_title": "Neurips 2023 Visual-Instruction Tuning",
    "chunk_index": 55,
    "total_chunks": 91,
    "text_content": "the development of new applications and research directions, ultimately contributing to the progress and responsible deployment of foundation models in vision-language tasks. B More Results We present more qualitative results of LLaV A to analyze its emergent behaviors and observed weaknesses. For more quantitative results of LLaV A on academic benchmarks, please refer to the improved baselines with visual instruction tuning [ 31]. In Table 9, LLaV A demonstrates a similar behavior as GPT-4 in a",
    "full_text_length": 90529,
    "chunk_length": 1206
  },
  {
    "chunk_id": 2456,
    "paper_filename": "neurIPS_2023_visual-instruction_tuning.pdf",
    "paper_title": "Neurips 2023 Visual-Instruction Tuning",
    "chunk_index": 56,
    "total_chunks": 91,
    "text_content": "textual knowledge from the pretrained LLM, as demonstrated in Fig. 4 and Fig. 5. One interesting emergent behavior of LLaV A is that it is able to understand visual contents that are not covered in the training. For example, in Fig. 6, it is able to recognize Elon Musk both in a headshot and in a humorous meme where he is dressed as a doge, even though Elon Musk never appears in the training data for either the visual feature alignment or visual instruction tuning stages of 14 LLaV A. LLaV A als",
    "full_text_length": 90529,
    "chunk_length": 1229
  },
  {
    "chunk_id": 2457,
    "paper_filename": "neurIPS_2023_visual-instruction_tuning.pdf",
    "paper_title": "Neurips 2023 Visual-Instruction Tuning",
    "chunk_index": 57,
    "total_chunks": 91,
    "text_content": "that enable LLaV A to demonstrate such generalization abilities. This will pave the way towards building better LMMs, including enhancing robustness, reducing biases, and improving the alignment and the scope of the learned vision-language representations. Visual input example, Chicken Nugget Map: User Can you explain this meme in detail? LLaV A The meme in the image is a creative and humorous take on food, with a focus on chicken nuggets, as the center of the universe. The meme begins with the ",
    "full_text_length": 90529,
    "chunk_length": 1195
  },
  {
    "chunk_id": 2458,
    "paper_filename": "neurIPS_2023_visual-instruction_tuning.pdf",
    "paper_title": "Neurips 2023 Visual-Instruction Tuning",
    "chunk_index": 58,
    "total_chunks": 91,
    "text_content": "versions of different places. The punchline of the meme appears in smaller text at the bottom, reading \"I mean, it\u2019s not the real Earth, but how beautiful it is all is.\" This lighthearted and entertaining meme combines food and humor, using the image of chicken nuggets to create a funny and imaginative representation of the world. User Can you explain this meme? GPT-4 [35] This meme is a joke that combines two unrelated things: pictures of the earth from space and chicken nuggets. The text of th",
    "full_text_length": 90529,
    "chunk_length": 1125
  },
  {
    "chunk_id": 2459,
    "paper_filename": "neurIPS_2023_visual-instruction_tuning.pdf",
    "paper_title": "Neurips 2023 Visual-Instruction Tuning",
    "chunk_index": 59,
    "total_chunks": 91,
    "text_content": "image is actually something mundane and silly. User Can you explain this meme in detail? BLIP-2 sometimes i just look at pictures of the earth from space and marvel how beautiful it is User Can you explain this meme in detail? OpenFlamingo It\u2019s a picture of a chicken nugget on the International Space Station. Table 9: Example prompt comparing LLaV A, GPT-4, BLIP-2, and OpenFlamingo\u2019s visual reasoning capabilities in understanding the humor. BLIP-2 and OpenFlamingo fail to follow the user\u2019s instr",
    "full_text_length": 90529,
    "chunk_length": 1321
  },
  {
    "chunk_id": 2460,
    "paper_filename": "neurIPS_2023_visual-instruction_tuning.pdf",
    "paper_title": "Neurips 2023 Visual-Instruction Tuning",
    "chunk_index": 60,
    "total_chunks": 91,
    "text_content": "button { background -color: blue; color: white; border: none; padding: 10px 20px; font -size: 18px; cursor: pointer; } </style> </head> <body> <div class=\"container\"> <h1>My Joke Website</h1> <pid=\"punchline\" >Welcome to my joke website. Push the button to reveal the punchline!</p> <button onclick=\" revealPunchline ()\">Push me!</button> </div> <script> function revealPunchline () { document.getElementById (\"punchline\"). innerHTML = \"The punchline is: 'Why was the math book sad? Because it had to",
    "full_text_length": 90529,
    "chunk_length": 1297
  },
  {
    "chunk_id": 2461,
    "paper_filename": "neurIPS_2023_visual-instruction_tuning.pdf",
    "paper_title": "Neurips 2023 Visual-Instruction Tuning",
    "chunk_index": 61,
    "total_chunks": 91,
    "text_content": "LLaV A\u2019s output, such as splitting the joke and punchline into two rows, and only revealing the punchline upon button click, to better re\ufb02ect the user\u2019s intent. 16 With the variety of food items stored in the refrigerator, you can prepare several meals. For example, you can create a fruit salad using the strawberries, blueberries, and carrots. Additionally, you can make a delicious smoothie using the fruits and vegetables, whic h would be a healthy and nutritious snack or breakfast option. The m",
    "full_text_length": 90529,
    "chunk_length": 1162
  },
  {
    "chunk_id": 2462,
    "paper_filename": "neurIPS_2023_visual-instruction_tuning.pdf",
    "paper_title": "Neurips 2023 Visual-Instruction Tuning",
    "chunk_index": 62,
    "total_chunks": 91,
    "text_content": "cook with these? Certainly! Here's a simple fruit salad recipe using the items in your refrigerator: Ingredients: -2 cups fresh strawberries, sliced -1 cup fresh blueberries -1 large carrot, grated -Juice of half a lemon -A handful of chopped fresh parsley or mint -A few ice cubes -Optional: honey or maple syrup (adjust to taste) Instructions: 1. In a large bowl, combine the sliced strawberries, blueberries, and carrot. Gently toss them together to mix. 2. Drizzle the juice of half a lemon over ",
    "full_text_length": 90529,
    "chunk_length": 1128
  },
  {
    "chunk_id": 2463,
    "paper_filename": "neurIPS_2023_visual-instruction_tuning.pdf",
    "paper_title": "Neurips 2023 Visual-Instruction Tuning",
    "chunk_index": 63,
    "total_chunks": 91,
    "text_content": "least 30 minutes or up to a few hours. This allows the flavors to me ld and creates a more refreshing salad. 6. Serve the fruit salad in cups or bowls, garnished with a sprinkle of fresh parsley or mint, if desired. This fruit salad recipe is perfect for a healthy snack or side dish, and with the variety of ingredients available in your re frigerator, it should be easy to put together quickly.I would like to do the fruit salad. Show me the recipe. LLaVA LLaVA User User When visiting this peacefu",
    "full_text_length": 90529,
    "chunk_length": 1126
  },
  {
    "chunk_id": 2464,
    "paper_filename": "neurIPS_2023_visual-instruction_tuning.pdf",
    "paper_title": "Neurips 2023 Visual-Instruction Tuning",
    "chunk_index": 64,
    "total_chunks": 91,
    "text_content": "of the structural integrity of the pier; avoid o verloading it with too many people or excessive weight, as this could potentially damage the pier or cause it to collapse. Additionally, be aware of the surrounding wildlife and respect their space; do not harm or disturb them in any way. Finally, if you decide to go fishing, ensure that y ou adhere to any local fishing regulations and use appropriate fishing techniques to keep the lake ecosystem healthy and maintain the balance of the food chain.",
    "full_text_length": 90529,
    "chunk_length": 1155
  },
  {
    "chunk_id": 2465,
    "paper_filename": "neurIPS_2023_visual-instruction_tuning.pdf",
    "paper_title": "Neurips 2023 Visual-Instruction Tuning",
    "chunk_index": 65,
    "total_chunks": 91,
    "text_content": "a deep connection with nature, taking a moment to soak in the peaceful atmosphere. One of the highlights of my trip was undoubtedly the fishing experience. The area is teeming with life, and I was thrilled to land a sizeable catch, despite the modest size of the lake. The joy of fishing in such a beautiful setting was truly rewarding, and it allowe d me to take a break from my busy life and truly immerse myself in the tranquility of nature. While exploring the surrounding area, I came across a f",
    "full_text_length": 90529,
    "chunk_length": 1154
  },
  {
    "chunk_id": 2466,
    "paper_filename": "neurIPS_2023_visual-instruction_tuning.pdf",
    "paper_title": "Neurips 2023 Visual-Instruction Tuning",
    "chunk_index": 66,
    "total_chunks": 91,
    "text_content": "the gentle sounds of water and wildlife. If you're looking for a serene escape from the hustle and bustle of everyday life, t hispier overlooking the lake is the perfect place to find solace and reconnect with nature.Compose an engaging travel blog post about my trip to this place, highlighting the beautiful sceneries, joyful experiences, and include possible anecdotes. User UserLLaVA LLaVA Figure 3: LLaV A is capable of recognizing the visual content following the user\u2019s intent, without directl",
    "full_text_length": 90529,
    "chunk_length": 1197
  },
  {
    "chunk_id": 2467,
    "paper_filename": "neurIPS_2023_visual-instruction_tuning.pdf",
    "paper_title": "Neurips 2023 Visual-Instruction Tuning",
    "chunk_index": 67,
    "total_chunks": 91,
    "text_content": "as the characters Jack and Rose from the popular movie Titanic. In the movie, their romance blossoms aboard the fictional ocean liner Titanic, with the pair ultimately sharing a pa ssionate kiss as the ship sinks into the ocean. The ending of the movie is known for its emotional and heartfelt scenes, where the characters are forced to say goodbye to each other as they are separated by the sinking ship. The image evokes a sense of romance and nostalgia, paying tr ibute to the iconic love story po",
    "full_text_length": 90529,
    "chunk_length": 1141
  },
  {
    "chunk_id": 2468,
    "paper_filename": "neurIPS_2023_visual-instruction_tuning.pdf",
    "paper_title": "Neurips 2023 Visual-Instruction Tuning",
    "chunk_index": 68,
    "total_chunks": 91,
    "text_content": "it could be a reference to the famous artist Da Vinci, know n for his artistic works, including the Mona Lisa. The painting is likely a creative and comical take on the traditional portrait style, replacing a hu man figure with a dog dressed in a woman's clothing, making it an interesting and amusing visual piece.Do you know who drew this painting? The painting depicts a woman, commonly believed to be Mona Lisa, the famous artwork by Leonardo da Vinci. It is a portrait pa inting that showcases t",
    "full_text_length": 90529,
    "chunk_length": 1140
  },
  {
    "chunk_id": 2469,
    "paper_filename": "neurIPS_2023_visual-instruction_tuning.pdf",
    "paper_title": "Neurips 2023 Visual-Instruction Tuning",
    "chunk_index": 69,
    "total_chunks": 91,
    "text_content": "Start a new conversation, and the history is cleared.User UserLLaVA LLaVA Figure 5: LLaV A recognizes the famous art work, Mona Lisa, by Leonardo da Vinci. When we start a new conversation, it also explains the humourous artwork created on the web, mimicking the Mona Lisa. 18 The name of the man in the photo is Elon Musk.What is the name of the man in the photo? The name of the man in the photo is Elon Musk.What is the name of the man in the photo? Start a new conversation, and the history is cl",
    "full_text_length": 90529,
    "chunk_length": 1124
  },
  {
    "chunk_id": 2470,
    "paper_filename": "neurIPS_2023_visual-instruction_tuning.pdf",
    "paper_title": "Neurips 2023 Visual-Instruction Tuning",
    "chunk_index": 70,
    "total_chunks": 91,
    "text_content": "because Elon Musk never appears in the training data for either the visual feature alignment or visual instruction tuning stages of LLaV A, which indicates that the base language model generalizes to unseen visual concepts. Question: Which material is this rocking chair made of? Options: (A) wood (B) silk LLaVA answer: LECTURE: A material is a type of matter. Wood, glass, metal, and plastic are common materials. Some objects are made of just one material. Most nails are made of metal. Other obje",
    "full_text_length": 90529,
    "chunk_length": 1170
  },
  {
    "chunk_id": 2471,
    "paper_filename": "neurIPS_2023_visual-instruction_tuning.pdf",
    "paper_title": "Neurips 2023 Visual-Instruction Tuning",
    "chunk_index": 71,
    "total_chunks": 91,
    "text_content": "materials that can support the weight of a person and withstand the rocking motion. Wood is a common material used for making rocking chairs due to its strength, durability, and natural appearance. GPT-4 (text-only) judge: Since there is no context or image to refer to, I cannot independently analyze the rocking chair in question. However, I can try to analyze the reasoning given by the assistants. Assistant 1, who claims to have seen the image, mentions that the legs of the rocking chair are ma",
    "full_text_length": 90529,
    "chunk_length": 1181
  },
  {
    "chunk_id": 2472,
    "paper_filename": "neurIPS_2023_visual-instruction_tuning.pdf",
    "paper_title": "Neurips 2023 Visual-Instruction Tuning",
    "chunk_index": 72,
    "total_chunks": 91,
    "text_content": "padding made of fabrics, such as cotton or polyester. Assistant 2 argues that rocking chairs are typically made of sturdy materials, and wood is a common material used for making rocking chairs due to its strength, durability, and natural appearance. This reasoning stands true and is based on common knowledge. Given the information available, the most plausible answer is that the rocking chair is made of wood, as suggested by Assistant 1. The answer is A. Table 10: One example on how the text-on",
    "full_text_length": 90529,
    "chunk_length": 1137
  },
  {
    "chunk_id": 2473,
    "paper_filename": "neurIPS_2023_visual-instruction_tuning.pdf",
    "paper_title": "Neurips 2023 Visual-Instruction Tuning",
    "chunk_index": 73,
    "total_chunks": 91,
    "text_content": "2e-5 and a batch size of 32. Following Vicuna, we use the Adam optimizer with no weight decay and a cosine learning rate with a warmup ratio of 3%. During \ufb01netuning, FSDP (Full Shard Data Parallel) and gradient checkpointing is used to save GPU memory, and of\ufb02oading is not used. BF16 and TF32 are enabled to achieve a balance between speed and precision. We train all models with 8 \u0002A100s. Pretraining on CC-595K completes within 4 hours. Finetuning on Instruct-158K completes within 10 hours. Finet",
    "full_text_length": 90529,
    "chunk_length": 1255
  },
  {
    "chunk_id": 2474,
    "paper_filename": "neurIPS_2023_visual-instruction_tuning.pdf",
    "paper_title": "Neurips 2023 Visual-Instruction Tuning",
    "chunk_index": 74,
    "total_chunks": 91,
    "text_content": "the model checkpoints after compression is 25GB, which exceeds the 5GB limit of GitHub LFS (Large File Storage). We\u2019ll release the checkpoint to the public, or upon request with reviewers for this submission. E Data Instructions for brief image description. The list of instructions used to brie\ufb02y describe the image content are shown in Table 11. They present the same meaning with natural language variance. \u2022 \"Describe the image concisely.\" \u2022 \"Provide a brief description of the given image.\" \u2022 \"O",
    "full_text_length": 90529,
    "chunk_length": 1236
  },
  {
    "chunk_id": 2475,
    "paper_filename": "neurIPS_2023_visual-instruction_tuning.pdf",
    "paper_title": "Neurips 2023 Visual-Instruction Tuning",
    "chunk_index": 75,
    "total_chunks": 91,
    "text_content": "but informative summary of the picture.\" \u2022 \"Create a compact narrative representing the image presented.\" Table 11: The list of instructions for brief image description. Instructions for detailed image description. The list of instructions used to describe the image content in detail are shown in Table 12. They present the same meaning with natural language variance. CC3M. We extract noun-phrases using Spacy for each caption over the whole CC3M dataset, and count the frequency of each unique nou",
    "full_text_length": 90529,
    "chunk_length": 1235
  },
  {
    "chunk_id": 2476,
    "paper_filename": "neurIPS_2023_visual-instruction_tuning.pdf",
    "paper_title": "Neurips 2023 Visual-Instruction Tuning",
    "chunk_index": 76,
    "total_chunks": 91,
    "text_content": "the image before you\" \u2022 \"Clarify the contents of the displayed image with great detail\" \u2022 \"Characterize the image using a well-detailed description\" \u2022 \"Break down the elements of the image in a detailed manner\" \u2022 \"Walk through the important details of the image\" \u2022 \"Portray the image with a rich, descriptive narrative\" \u2022 \"Narrate the contents of the image with precision\" \u2022 \"Analyze the image in a comprehensive and detailed manner\" \u2022 \"Illustrate the image through a descriptive explanation\" \u2022 \"Exam",
    "full_text_length": 90529,
    "chunk_length": 1219
  },
  {
    "chunk_id": 2477,
    "paper_filename": "neurIPS_2023_visual-instruction_tuning.pdf",
    "paper_title": "Neurips 2023 Visual-Instruction Tuning",
    "chunk_index": 77,
    "total_chunks": 91,
    "text_content": "of all its captions. This results in around 595K image-text pairs. The comparison of noun-phrase statistics before and after \ufb01ltering CC3M is shown in Figure 7. The \ufb01ltered dataset shows a good coverage of concepts whose frequency is higher from 3, but with a smaller number of image-text pairs. 0 10000 20000 30000 40000 50000 Unique noun-phrases (ordered by frequency in the descending order)101103105FrequencyCC3M: 108182 CC3M (Filtered): 31423 Figure 7: Comparison of noun-phrase statistics befor",
    "full_text_length": 90529,
    "chunk_length": 1263
  },
  {
    "chunk_id": 2478,
    "paper_filename": "neurIPS_2023_visual-instruction_tuning.pdf",
    "paper_title": "Neurips 2023 Visual-Instruction Tuning",
    "chunk_index": 78,
    "total_chunks": 91,
    "text_content": "Design a conversation between you and a person asking about this photo. The answers should be in a tone that a visual AI assistant is seeing the image and answering the question. Ask diverse questions and give corresponding answers. Include questions asking about the visual content of the image, including the object types, counting the objects, object actions, object locations, relative positions between objects , etc. Only include questions that have de\ufb01nite answers: (1) one can see the content",
    "full_text_length": 90529,
    "chunk_length": 1412
  },
  {
    "chunk_id": 2479,
    "paper_filename": "neurIPS_2023_visual-instruction_tuning.pdf",
    "paper_title": "Neurips 2023 Visual-Instruction Tuning",
    "chunk_index": 79,
    "total_chunks": 91,
    "text_content": "etc. Again, do not ask about uncertain details. Provide detailed answers when answering complex questions. For example, give detailed examples or reasoning steps to make the content more convincing and well-organized. You can include multiple paragraphs if necessary.\"\"\"} ] for sample in fewshot_samples: messages.append({\"role\":\"user\", \"content\":sample[\u2018context\u2019]}) messages.append({\"role\":\"assistant\", \"content\":sample[\u2018response\u2019]} ) messages.append({\"role\":\"user\", \"content\":\u2018\\n\u2019.join(query)}) Tab",
    "full_text_length": 90529,
    "chunk_length": 1463
  },
  {
    "chunk_id": 2480,
    "paper_filename": "neurIPS_2023_visual-instruction_tuning.pdf",
    "paper_title": "Neurips 2023 Visual-Instruction Tuning",
    "chunk_index": 80,
    "total_chunks": 91,
    "text_content": "Captions A group of people standing outside of a black vehicle with various luggage. Luggage surrounds a vehicle in an underground parking area People try to \ufb01t all of their luggage in an SUV . The sport utility vehicle is parked in the public garage, being packed for a trip Some people with luggage near a van that is transporting it. Context type 2: Boxes person: [0.681, 0.242, 0.774, 0.694], person: [0.63, 0.222, 0.686, 0.516], person: [0.444, 0.233, 0.487, 0.34], backpack: [0.384, 0.696, 0.48",
    "full_text_length": 90529,
    "chunk_length": 1239
  },
  {
    "chunk_id": 2481,
    "paper_filename": "neurIPS_2023_visual-instruction_tuning.pdf",
    "paper_title": "Neurips 2023 Visual-Instruction Tuning",
    "chunk_index": 81,
    "total_chunks": 91,
    "text_content": "Where is the vehicle parked? Answer: The vehicle is parked in an underground parking area, likely in a public garage. Question: What are the people in the image doing? Answer: The people are trying to \ufb01t all of their luggage into the SUV , likely preparing for a trip. Response type 2: detailed description The image is an underground parking area with a black sport utility vehicle (SUV) parked. There are three people in the scene, with one person standing closer to the left side of the vehicle, a",
    "full_text_length": 90529,
    "chunk_length": 1101
  },
  {
    "chunk_id": 2482,
    "paper_filename": "neurIPS_2023_visual-instruction_tuning.pdf",
    "paper_title": "Neurips 2023 Visual-Instruction Tuning",
    "chunk_index": 82,
    "total_chunks": 91,
    "text_content": "Additionally, there are two suitcases, one on the right side of the car and another further away near the center of the parking area. A bicycle can also be seen on the left side of the vehicle. Other cars are parked around the main SUV , with one car positioned behind it and slightly to the left, another behind and slightly to the right, and the third car further behind on the right side. Response type 3: complex reasoning Question: What challenges do these people face? Answer: In the image, a g",
    "full_text_length": 90529,
    "chunk_length": 1170
  },
  {
    "chunk_id": 2483,
    "paper_filename": "neurIPS_2023_visual-instruction_tuning.pdf",
    "paper_title": "Neurips 2023 Visual-Instruction Tuning",
    "chunk_index": 83,
    "total_chunks": 91,
    "text_content": "accommodate. They might have to strategize and arrange the luggage ef\ufb01ciently to ensure that everything \ufb01ts properly into the vehicle. Additionally, they need to consider the comfort of the passengers and visibility while driving, so the placement of the luggage must not obstruct the driver\u2019s view or make the passengers uncomfortable during the trip. Table 14: One example to illustrate the instruction-following data. The top block shows the contexts such as captions and boxes used to prompt GPT,",
    "full_text_length": 90529,
    "chunk_length": 1135
  },
  {
    "chunk_id": 2484,
    "paper_filename": "neurIPS_2023_visual-instruction_tuning.pdf",
    "paper_title": "Neurips 2023 Visual-Instruction Tuning",
    "chunk_index": 84,
    "total_chunks": 91,
    "text_content": "winter with snow. A hotel for dogs in the snow in winter. Conversation Question: What color is the \ufb01re hydrant in the image? === Answer: The \ufb01re hydrant in the image is red. === Question: Is the \ufb01re hydrant fully covered by snow or partially visible? === Answer: The \ufb01re hydrant is partially visible, with a signi\ufb01cant portion of it covered by the deep snow. === Question: What is the condition of the sidewalk near the \ufb01re hydrant? === Answer: The sidewalk near the \ufb01re hydrant has been recently plo",
    "full_text_length": 90529,
    "chunk_length": 1194
  },
  {
    "chunk_id": 2485,
    "paper_filename": "neurIPS_2023_visual-instruction_tuning.pdf",
    "paper_title": "Neurips 2023 Visual-Instruction Tuning",
    "chunk_index": 85,
    "total_chunks": 91,
    "text_content": "which can lead to various dif\ufb01culties and disruptions for its residents. For instance, the deep snow covering the \ufb01re hydrant could potentially hinder its accessibility in case of emergencies, which poses safety concerns. The recently plowed sidewalk implies that the city\u2019s maintenance crews have been actively working to keep the walkways clear and safe for pedestrians, but the sheer amount of snowfall might make it dif\ufb01cult to keep up with the continuous snow removal efforts. Furthermore, such ",
    "full_text_length": 90529,
    "chunk_length": 1311
  },
  {
    "chunk_id": 2486,
    "paper_filename": "neurIPS_2023_visual-instruction_tuning.pdf",
    "paper_title": "Neurips 2023 Visual-Instruction Tuning",
    "chunk_index": 86,
    "total_chunks": 91,
    "text_content": "the red \ufb01re hydrant deep in the snow and the recently plowed sidewalk suggest that the city has faced a particularly severe winter season, with substantial snowfall that has likely caused various challenges and disruptions for its residents and infrastructure. Table 15: One example used in in-context-learning to construct visual conversation data. 24 Captions A man is skiing in the open snow covered hills A skier is making his way into the snow. A skier on the foothills of a huge mountain range.",
    "full_text_length": 90529,
    "chunk_length": 1159
  },
  {
    "chunk_id": 2487,
    "paper_filename": "neurIPS_2023_visual-instruction_tuning.pdf",
    "paper_title": "Neurips 2023 Visual-Instruction Tuning",
    "chunk_index": 87,
    "total_chunks": 91,
    "text_content": "=== Question: Are there any other people or skiers visible in the image? === Answer: No, there is only a single person skiing in the image, and they are surrounded by an area with a lot of bushes. === Question: What kind of skiing is the person doing? Cross-country or downhill? === Answer: Based on what I can see in the photo, it is not possible to de\ufb01nitively determine whether the skier is doing cross-country or downhill skiing. However, given the presence of a trail sign and the fact that they",
    "full_text_length": 90529,
    "chunk_length": 1225
  },
  {
    "chunk_id": 2488,
    "paper_filename": "neurIPS_2023_visual-instruction_tuning.pdf",
    "paper_title": "Neurips 2023 Visual-Instruction Tuning",
    "chunk_index": 88,
    "total_chunks": 91,
    "text_content": "downhill skiing are two distinct types of skiing, each with its own techniques, equipment, and terrain preferences. 1. Cross-Country Skiing: Also known as Nordic skiing, cross-country skiing involves traversing relatively \ufb02at terrain or gentle slopes using skis and poles. It is a popular recreational and competitive sport that focuses on endurance, balance, and aerobic \ufb01tness. Cross-country skiers use their own locomotion to move across snow-covered terrain, rather than relying on ski lifts or o",
    "full_text_length": 90529,
    "chunk_length": 1314
  },
  {
    "chunk_id": 2489,
    "paper_filename": "neurIPS_2023_visual-instruction_tuning.pdf",
    "paper_title": "Neurips 2023 Visual-Instruction Tuning",
    "chunk_index": 89,
    "total_chunks": 91,
    "text_content": "faster than the classic technique. Cross-country ski equipment consists of lightweight skis, boots, bindings, and poles. The boots are more \ufb02exible than those used in downhill skiing, allowing for better ankle movement and control. 2. Downhill Skiing: Also known as alpine skiing, downhill skiing involves descending slopes at high speeds using skis and poles for balance and control. This sport is more focused on speed, technique, and navigating challenging terrain, including steep slopes, moguls,",
    "full_text_length": 90529,
    "chunk_length": 1366
  },
  {
    "chunk_id": 2490,
    "paper_filename": "neurIPS_2023_visual-instruction_tuning.pdf",
    "paper_title": "Neurips 2023 Visual-Instruction Tuning",
    "chunk_index": 90,
    "total_chunks": 91,
    "text_content": "that involves traveling across \ufb02at or gently sloping terrain, while downhill skiing is focused on speed and technique as skiers navigate steeper slopes and challenging terrain. Both sports require specialized equipment and techniques, but they offer different experiences and challenges to participants. Table 16: One example used in in-context-learning to construct visual conversation data. 25",
    "full_text_length": 90529,
    "chunk_length": 395
  },
  {
    "chunk_id": 2491,
    "paper_filename": "nicoloas_2025_a_multitask_conversational_vision_language_model_for_radiology.pdf",
    "paper_title": "Nicoloas 2025 A Multitask Conversational Vision Language Model For Radiology",
    "chunk_index": 0,
    "total_chunks": 84,
    "text_content": "RadVLM: A Multitask Conversational Vision-Language Model for Radiology Nicolas Deperrois1\u2217, Hidetoshi Matsuo2, Samuel Ruip\u00b4 erez-Campillo3, Moritz Vandenhirtz3, Sonia Laguna3, Alain Ryser3, Koji Fujimoto4, Mizuho Nishio2, Thomas M. Sutter3, Julia E. Vogt3, Jonas Kluckert1,5, Thomas Frauenfelder5, Christian Bl\u00a8 uthgen5, Farhad Nooralahzadeh1\u2020, and Michael Krauthammer1\u2020 1Department of Quantitative Biomedicine, University of Zurich, Zurich, Switzerland 2Department of Radiology, Kobe University, Kob",
    "full_text_length": 84679,
    "chunk_length": 1582
  },
  {
    "chunk_id": 2492,
    "paper_filename": "nicoloas_2025_a_multitask_conversational_vision_language_model_for_radiology.pdf",
    "paper_title": "Nicoloas 2025 A Multitask Conversational Vision Language Model For Radiology",
    "chunk_index": 1,
    "total_chunks": 84,
    "text_content": "model designed for CXR interpretation. To this end, we curate a large-scale in- struction dataset comprising over 1 million image-instruction pairs containing both single-turn tasks \u2013 such as report generation, abnormality classification, and visual grounding \u2013 and multi-turn, multi-task conver- sational interactions. After fine-tuning RadVLM on this instruction dataset, we evaluate it across different tasks along with re-implemented baseline VLMs. Our results show that RadVLM achieves state-of-",
    "full_text_length": 84679,
    "chunk_length": 1486
  },
  {
    "chunk_id": 2493,
    "paper_filename": "nicoloas_2025_a_multitask_conversational_vision_language_model_for_radiology.pdf",
    "paper_title": "Nicoloas 2025 A Multitask Conversational Vision Language Model For Radiology",
    "chunk_index": 2,
    "total_chunks": 84,
    "text_content": "to be the most frequently used medical imaging modality worldwide due to their convenience and cost-effectiveness (Akhter et al., 2023). Chest X-ray (CXR) remains the most commonly performed radiological exam globally, particularly important for diagnosing and monitoring thoracic conditions such as pneumonia, heart failure, and lung cancer (C \u00b8all\u0131 et al., 2021). Problematically, the growing volume of CXRs and other imaging studies in recent years have lead to a reduction in the time available f",
    "full_text_length": 84679,
    "chunk_length": 1362
  },
  {
    "chunk_id": 2494,
    "paper_filename": "nicoloas_2025_a_multitask_conversational_vision_language_model_for_radiology.pdf",
    "paper_title": "Nicoloas 2025 A Multitask Conversational Vision Language Model For Radiology",
    "chunk_index": 3,
    "total_chunks": 84,
    "text_content": "In recent years, various deep learning models have shown promise in clinical applications, such as the detection of conditions like COVID-19 pneumonia (Nishio et al., 2020) or pulmonary nodules (Homayounieh et al., 2021). Another extensively studied task is the automated generation of free text reports from CXR images using transformer-based architectures (Nooralahzadeh et al., 2021; Yang et al., 2023; Hyland et al., 2023; Chaves et al., 2024). These models can provide preliminary drafts summari",
    "full_text_length": 84679,
    "chunk_length": 1434
  },
  {
    "chunk_id": 2495,
    "paper_filename": "nicoloas_2025_a_multitask_conversational_vision_language_model_for_radiology.pdf",
    "paper_title": "Nicoloas 2025 A Multitask Conversational Vision Language Model For Radiology",
    "chunk_index": 4,
    "total_chunks": 84,
    "text_content": "In addition, physicians should be allowed to formulate their queries flexibly and in any order, potentially within a multi-turn, conversational interaction with the assistant (Tu et al., 2024). Recently, significant advancements in the field of multimodal artificial intelligence (AI) have enabled the development of models such as GPT-4 Vision (GPT4-V, OpenAI, 2024) and Claude (Anthropic, 2024), which have the ability to describe and converse about images with increasing reliability. The training",
    "full_text_length": 84679,
    "chunk_length": 1373
  },
  {
    "chunk_id": 2496,
    "paper_filename": "nicoloas_2025_a_multitask_conversational_vision_language_model_for_radiology.pdf",
    "paper_title": "Nicoloas 2025 A Multitask Conversational Vision Language Model For Radiology",
    "chunk_index": 5,
    "total_chunks": 84,
    "text_content": "CXR interpretation. In this direction, models such as CheXagent (Chen et al., 2024), RaDialog (Pellegrini et al., 2023), or MAIRA-2 (Bannur et al., 2024) were developed, extending beyond report generation to tasks such as observation grounding and visual question answering, covering a larger part of the clinical workflow. However, their capacity to handle diverse and complex user queries, or to respond accurately to multiple prompts within an arbitrary conversational framework, remains limited. ",
    "full_text_length": 84679,
    "chunk_length": 1484
  },
  {
    "chunk_id": 2497,
    "paper_filename": "nicoloas_2025_a_multitask_conversational_vision_language_model_for_radiology.pdf",
    "paper_title": "Nicoloas 2025 A Multitask Conversational Vision Language Model For Radiology",
    "chunk_index": 6,
    "total_chunks": 84,
    "text_content": "of single-turn image-instruction pairs for different tasks and image-conversation pairs designed for more flexible and multi-turn interactions. We then fine-tune a vision-language architecture (Li et al., 2024) on this instruction dataset, naming the resulting model RadVLM, and develop an evaluation pipeline to assess its performance across multiple tasks, systematically comparing it to state-of-the-art generalist and CXR-specific foundation models. Our results show that, despite its relatively ",
    "full_text_length": 84679,
    "chunk_length": 1520
  },
  {
    "chunk_id": 2498,
    "paper_filename": "nicoloas_2025_a_multitask_conversational_vision_language_model_for_radiology.pdf",
    "paper_title": "Nicoloas 2025 A Multitask Conversational Vision Language Model For Radiology",
    "chunk_index": 7,
    "total_chunks": 84,
    "text_content": "clinical workflows. \u2022We design and train RadVLM, a multitask conversational foundation model designed to assist physicians in CXR analysis that solely relies on visual information \u2013 avoiding the need for providing additional metadata. \u2022We employ an evaluation pipeline, re-implementing existing models for comparison and ensuring repro- ducibility of results. \u2022We evaluate RadVLM systematically across multiple tasks, demonstrating competitive performance against state-of-the-art vision-language mod",
    "full_text_length": 84679,
    "chunk_length": 1500
  },
  {
    "chunk_id": 2499,
    "paper_filename": "nicoloas_2025_a_multitask_conversational_vision_language_model_for_radiology.pdf",
    "paper_title": "Nicoloas 2025 A Multitask Conversational Vision Language Model For Radiology",
    "chunk_index": 8,
    "total_chunks": 84,
    "text_content": "ChatGPT, are largely attributed to the instruction-tuning process (Wei et al., 2021; Ouyang et al., 2022). This process commonly involves fine-tuning a pre-trained model on a labeled dataset of diverse instruction-following tasks, ensuring the model can generalize to diverse user instructions in a zero-shot setting. Instruction-following datasets generally consist of instruction-output pairs and/or multi-turn dialogues (Zheng et al., 2023a) mimicking real-life interaction between users and AI as",
    "full_text_length": 84679,
    "chunk_length": 1384
  },
  {
    "chunk_id": 2500,
    "paper_filename": "nicoloas_2025_a_multitask_conversational_vision_language_model_for_radiology.pdf",
    "paper_title": "Nicoloas 2025 A Multitask Conversational Vision Language Model For Radiology",
    "chunk_index": 9,
    "total_chunks": 84,
    "text_content": "parallel, open research efforts have led to the development of vision-language models such as LLaVA (Liu et al., 2023) and BLIP-2 (Li et al., 2023b), which introduced effective training strategies for visual instruction tuning. These approaches have in- spired the development of vision-language models (VLMs) such as LLaVA-OneVision (Li et al., 2024), Idefics3 (Lauren\u00b8 con et al., 2024), Qwen2-VL (Wang et al., 2024), and Llama-3.2 Vision (Dubey et al., 2024). Similar to text-based LLMs, the instr",
    "full_text_length": 84679,
    "chunk_length": 1376
  },
  {
    "chunk_id": 2501,
    "paper_filename": "nicoloas_2025_a_multitask_conversational_vision_language_model_for_radiology.pdf",
    "paper_title": "Nicoloas 2025 A Multitask Conversational Vision Language Model For Radiology",
    "chunk_index": 10,
    "total_chunks": 84,
    "text_content": "performance across a range of mul- timodal medical tasks, including medical visual question answering (VQA), report generation, summarization. In parallel, open source models such as LLaVA-Med (Li et al., 2023a) have been developed following similar training strategies as LLaVA (Liu et al., 2023), leveraging biomedical datasets from PubMed (NIH, nd) to design instruction prompts and muti-turn conversations. Among medical applications, CXR interpretation remains a key area of interest. Early AI-d",
    "full_text_length": 84679,
    "chunk_length": 1438
  },
  {
    "chunk_id": 2502,
    "paper_filename": "nicoloas_2025_a_multitask_conversational_vision_language_model_for_radiology.pdf",
    "paper_title": "Nicoloas 2025 A Multitask Conversational Vision Language Model For Radiology",
    "chunk_index": 11,
    "total_chunks": 84,
    "text_content": "models lack conversational capabilities. Other approaches, such as Wolf (Kang et al., 2024), RaDialog (Pellegrini et al., 2023), and M4CXR (Chen et al., 2024), incorporate conversational features but are constrained by predefined response templates, limiting their adaptability in real-world interactions. In this work, we introduce a model that integrates multiple CXR interpretation tasks while enabling flexible, multi-turn dialogue, bridging the gap between task-specific AI models and interactiv",
    "full_text_length": 84679,
    "chunk_length": 1422
  },
  {
    "chunk_id": 2503,
    "paper_filename": "nicoloas_2025_a_multitask_conversational_vision_language_model_for_radiology.pdf",
    "paper_title": "Nicoloas 2025 A Multitask Conversational Vision Language Model For Radiology",
    "chunk_index": 12,
    "total_chunks": 84,
    "text_content": "in the form of a single Q&A designed for predefined tasks (single instructions) or of a multi-turn exchange (conversations). The composition of this instruction dataset is detailed below and summarized in Table 1. 3.1.1 Free-text report generation In alignment with existing CXR models, we aim to generate clinically coherent radiology reports from CXR images. To achieve this, we collect public datasets containing CXRs paired with anonymized free-text reports. Radiology reports often compare to an",
    "full_text_length": 84679,
    "chunk_length": 1292
  },
  {
    "chunk_id": 2504,
    "paper_filename": "nicoloas_2025_a_multitask_conversational_vision_language_model_for_radiology.pdf",
    "paper_title": "Nicoloas 2025 A Multitask Conversational Vision Language Model For Radiology",
    "chunk_index": 13,
    "total_chunks": 84,
    "text_content": "leverage two public datasets for the report generation task: MIMIC-CXR (Johnson et al., 2019), which contains 377,110 CXR images paired with free-text radiology reports describing findings and patient history. After filtering, we retain 232,344 image-text pairs in the training set and 3,282 in the test set. CheXpert-Plus (Chambon et al., 2024), which features 223,228 image-report pairs from 187,711 studies. Applying the same filtering process as for MIMIC-CXR results in a dataset of 178,368 imag",
    "full_text_length": 84679,
    "chunk_length": 1324
  },
  {
    "chunk_id": 2505,
    "paper_filename": "nicoloas_2025_a_multitask_conversational_vision_language_model_for_radiology.pdf",
    "paper_title": "Nicoloas 2025 A Multitask Conversational Vision Language Model For Radiology",
    "chunk_index": 14,
    "total_chunks": 84,
    "text_content": "there is one abnormality identified: pneu- mothorax. c. Visual grounding: Anatomy/Abnormality Indicate the position of the left apical zone . The left apical zone is situated at [0.57, 0.14, 0.85, 0.3] in the image. d. Visual grounding: Phrase Mark the area where you observe: marked elon- gation and increased caliber of the thoracic aorta . The area specified is at coordinates [0.39, 0.3, 0.66, 0.79]. Figure 1: Examples of single instructions for different tasks. We design three main types of in",
    "full_text_length": 84679,
    "chunk_length": 1322
  },
  {
    "chunk_id": 2506,
    "paper_filename": "nicoloas_2025_a_multitask_conversational_vision_language_model_for_radiology.pdf",
    "paper_title": "Nicoloas 2025 A Multitask Conversational Vision Language Model For Radiology",
    "chunk_index": 15,
    "total_chunks": 84,
    "text_content": "the user asks the assistant to generate a report for a given CXR, and the assistant responds with the filtered report corresponding to that CXR (Figure 1a). 3.1.2 Abnormality classification Another essential task an AI-assistant should be capable of is to identify the presence of abnormalities on a CXR. While simpler than generating detailed, unstructured observations, this functionality serves as a quick and helpful overview for physicians, highlighting key observations before they dive into a ",
    "full_text_length": 84679,
    "chunk_length": 1316
  },
  {
    "chunk_id": 2507,
    "paper_filename": "nicoloas_2025_a_multitask_conversational_vision_language_model_for_radiology.pdf",
    "paper_title": "Nicoloas 2025 A Multitask Conversational Vision Language Model For Radiology",
    "chunk_index": 16,
    "total_chunks": 84,
    "text_content": "use 191,027 image-labels pairs from CheXpert (Irvin et al., 2019) and 237,912 pairs from MIMIC-CXR (Table 1). The instructions are designed such that the user asks for the abnormalities present on the CXR and the assistant answers by providing the list of abnormalities (Figure 1b). 3.1.3 Visual grounding Detecting the location of specific anatomical regions or pathologies on a CXR is an important task for AI assistants. In addition to providing a textual description of the image, they should be ",
    "full_text_length": 84679,
    "chunk_length": 1269
  },
  {
    "chunk_id": 2508,
    "paper_filename": "nicoloas_2025_a_multitask_conversational_vision_language_model_for_radiology.pdf",
    "paper_title": "Nicoloas 2025 A Multitask Conversational Vision Language Model For Radiology",
    "chunk_index": 17,
    "total_chunks": 84,
    "text_content": "input images are pre-processed to a unique size by the vision encoder, we normalize these coordinates to the original dimension to obtain floating values between 0 and 1, similarly as in You et al. (2023); Park et al. (2024); Zhang et al. (2024a). 4 Task Dataset source Image-instruction pairs (#) Evaluation (#) Report GenerationMIMIC-CXR 232,344 \u00d71 3,282 CheXpert-Plus 178,368 \u00d71 - Abnormality classif.MIMIC-CXR 237,912 \u00d71 518 CheXpert 191,027 \u00d71 - Anatomical grounding Chest Imagenome 80,000 \u00d71 2,",
    "full_text_length": 84679,
    "chunk_length": 1460
  },
  {
    "chunk_id": 2509,
    "paper_filename": "nicoloas_2025_a_multitask_conversational_vision_language_model_for_radiology.pdf",
    "paper_title": "Nicoloas 2025 A Multitask Conversational Vision Language Model For Radiology",
    "chunk_index": 18,
    "total_chunks": 84,
    "text_content": "corresponding number of image-instruction pairs are listed, with smaller datasets balanced by varying the frequency of instruction occurrences. The following datasets, with fine-grained X-ray information, were collected to design visual grounding in- structions: Chest Imagenome (Wu et al., 2021), derived from MIMIC-CXR, provides additional annotations to frontal X- ray images, in particular, bounding box coordinates for 29 anatomical regions. In our training data, we randomly select one region p",
    "full_text_length": 84679,
    "chunk_length": 1397
  },
  {
    "chunk_id": 2510,
    "paper_filename": "nicoloas_2025_a_multitask_conversational_vision_language_model_for_radiology.pdf",
    "paper_title": "Nicoloas 2025 A Multitask Conversational Vision Language Model For Radiology",
    "chunk_index": 19,
    "total_chunks": 84,
    "text_content": "abnormality (also following Figure 1c) and ii) abnormality detection, asking the location of all abnormalities, if any (Table 1). MS-CXR (Boecking et al., 2022) provides image-sentence pairs of bounding boxes and corresponding phrases, complementing MIMIC-CXR. PadChest-GR (Castro et al., 2024) also contains grounded sentences derived from the PadChest dataset (Bustos et al., 2020). From the last two datasets, we construct the \u2018phrase grounding\u2019 task, where a user asks about the location of a spe",
    "full_text_length": 84679,
    "chunk_length": 1343
  },
  {
    "chunk_id": 2511,
    "paper_filename": "nicoloas_2025_a_multitask_conversational_vision_language_model_for_radiology.pdf",
    "paper_title": "Nicoloas 2025 A Multitask Conversational Vision Language Model For Radiology",
    "chunk_index": 20,
    "total_chunks": 84,
    "text_content": "such as asking about the characteristics of a specific organ (lungs, heart), the orientation of the X-ray, or the definition of certain medical terms. More importantly, interactions can be decomposed over several Q&A rounds, with sometimes a question referring to previous answers (e.g., asking about the location of a specific observation from the previous answer). The model should thus be tuned to sequentially connect visual concepts (textual observations, presence or absence of abnormalities, f",
    "full_text_length": 84679,
    "chunk_length": 1315
  },
  {
    "chunk_id": 2512,
    "paper_filename": "nicoloas_2025_a_multitask_conversational_vision_language_model_for_radiology.pdf",
    "paper_title": "Nicoloas 2025 A Multitask Conversational Vision Language Model For Radiology",
    "chunk_index": 21,
    "total_chunks": 84,
    "text_content": "describe what you see in the chest X- ray? Sure, I see a chest X-ray taken from the front view, known as AP (anterior-posterior). The heart ap- pears to be borderline enlarged, but otherwise, the mediastinal and hilar regions look normal. There are no signs of fluid around the lungs or collapsed lung areas. Is there anything unusual in the lung area? The lungs mostly look clear, but there is some lin- ear scarring or atelectasis in the right lung base. Apart from that, the lung fields do not sho",
    "full_text_length": 84679,
    "chunk_length": 1164
  },
  {
    "chunk_id": 2513,
    "paper_filename": "nicoloas_2025_a_multitask_conversational_vision_language_model_for_radiology.pdf",
    "paper_title": "Nicoloas 2025 A Multitask Conversational Vision Language Model For Radiology",
    "chunk_index": 22,
    "total_chunks": 84,
    "text_content": "signs of pleural effusion, which is fluid accumulation around the lungs. Is the heart looking normal in size and shape? The heart size appears borderline enlarged, which means it is slightly bigger than expected, but not significantly abnormal in shape.b. Sample conversation with grounding Can you tell me what you observe in this chest X-ray? The X-ray shows that the cardiac silhouette is en- larged. There is also suspicion of mild left basal atelectasis. Where is the cardiac silhouette enlargem",
    "full_text_length": 84679,
    "chunk_length": 1265
  },
  {
    "chunk_id": 2514,
    "paper_filename": "nicoloas_2025_a_multitask_conversational_vision_language_model_for_radiology.pdf",
    "paper_title": "Nicoloas 2025 A Multitask Conversational Vision Language Model For Radiology",
    "chunk_index": 23,
    "total_chunks": 84,
    "text_content": "the partial collapse or incom- plete inflation of the lung. In this X-ray, mild left basal atelectasis is suspected at the base of the left lung. Figure 2: Example of LLM-generated conversations within the instruction dataset . LLM-generated user-assistant interactions designed for instruction tuning in RadVLM, covering both standard conversations and grounded responses. (a) Standard conversation : The assistant responds to user queries based on textual attributes extracted from the CXR (e.g., r",
    "full_text_length": 84679,
    "chunk_length": 1419
  },
  {
    "chunk_id": 2515,
    "paper_filename": "nicoloas_2025_a_multitask_conversational_vision_language_model_for_radiology.pdf",
    "paper_title": "Nicoloas 2025 A Multitask Conversational Vision Language Model For Radiology",
    "chunk_index": 24,
    "total_chunks": 84,
    "text_content": "assistant, along with detailed CXR information \u2013 including the radiology report, abnormality labels, bounding box coordinates, view, and gender (see prompt in Appendix- Figure 1). Importantly, by leveraging the provided CXR information, the assistant is designed to respond as if it had direct visual access to the image (Figure 2). Following this process, we generate in total 83k image-conversation pairs, including 80k standard conver- sations (Figure 2a) and 3k \u201cgrounded conversations\u201d (Figure 2",
    "full_text_length": 84679,
    "chunk_length": 1385
  },
  {
    "chunk_id": 2516,
    "paper_filename": "nicoloas_2025_a_multitask_conversational_vision_language_model_for_radiology.pdf",
    "paper_title": "Nicoloas 2025 A Multitask Conversational Vision Language Model For Radiology",
    "chunk_index": 25,
    "total_chunks": 84,
    "text_content": "1). 6 a. Report generation Draft a concise report for this image. Small right pleural e\u21b5usion is present. Minimalright lower lobe atelectasis is likely. The right lungis otherwise clear. No appreciable pneumothorax isseen. Biapical apical margin thickening, left worsethan right is seen. The left lung is well expandedand clear. The heart size is normal. Hemi arthro-plasty is seen on the right shoulder.b. Abnormality classi\ufb01cation Are there identi\ufb01able abnormalities on thisChest X-ray? Yes, there ",
    "full_text_length": 84679,
    "chunk_length": 1441
  },
  {
    "chunk_id": 2517,
    "paper_filename": "nicoloas_2025_a_multitask_conversational_vision_language_model_for_radiology.pdf",
    "paper_title": "Nicoloas 2025 A Multitask Conversational Vision Language Model For Radiology",
    "chunk_index": 26,
    "total_chunks": 84,
    "text_content": "pairs(e.g., MIMIC-CXR), we design Q&A towards report generation (a). For datasets containing abnormality labels (e.g.,CheXpert), the instructions are designed to perform multi-class classi\ufb01cation (b). When bounding boxes are available,we design visual grounding instructions, where the assistant provides the bounding box coordinates to display them onthe input image (c,d).each of 14 possible abnormalities is present, absent, or uncertain. In our setup, we only consider frontal imagesand consider ",
    "full_text_length": 84679,
    "chunk_length": 1478
  },
  {
    "chunk_id": 2518,
    "paper_filename": "nicoloas_2025_a_multitask_conversational_vision_language_model_for_radiology.pdf",
    "paper_title": "Nicoloas 2025 A Multitask Conversational Vision Language Model For Radiology",
    "chunk_index": 27,
    "total_chunks": 84,
    "text_content": "located. This is usually done via the prediction of bounding box coordinates of top-left and bottom-rightcorners [x1,y1,x2,y2]. While classical object detectors (Ren et al.,2016;Redmon,2016) tackle this task byleveraging specialized architecture and learning rules, we embark it with the other text-based task via next-tokenprediction (Equation (1)), formatting coordinates into text, enclosed between square brackets. As input imagesare processed to a unique size by the vision encoder, we normalize",
    "full_text_length": 84679,
    "chunk_length": 1511
  },
  {
    "chunk_id": 2519,
    "paper_filename": "nicoloas_2025_a_multitask_conversational_vision_language_model_for_radiology.pdf",
    "paper_title": "Nicoloas 2025 A Multitask Conversational Vision Language Model For Radiology",
    "chunk_index": 28,
    "total_chunks": 84,
    "text_content": "18,000 frontal images, each manually annotated by 3 di\u21b5erentradiologists. To merge their annotations, we pre-processed them by fusing bounding boxes of the samepathology using weighted box fusion (Solovyev et al.,2021), similarly asM\u00a8 uller et al.(2024). From thisdataset, we design two types of tasks: i) abnormality grounding, questioning the location of a speci\ufb01cabnormality (also followingFigure 1c).4pneumothoraxAre\u2026<stop>LossVision encoderLLM\u2026 Adapter Figure 3: Instruction fine-tuning of the v",
    "full_text_length": 84679,
    "chunk_length": 1412
  },
  {
    "chunk_id": 2520,
    "paper_filename": "nicoloas_2025_a_multitask_conversational_vision_language_model_for_radiology.pdf",
    "paper_title": "Nicoloas 2025 A Multitask Conversational Vision Language Model For Radiology",
    "chunk_index": 29,
    "total_chunks": 84,
    "text_content": "originally pretrained and instruction-tuned on image- text datasets in the general domain. We also follow the Higher AnyRes strategy (Chai et al., 2022) by encoding multiple patches of the input image in different resolutions (in addition to the full image) and feeding the concatenated output representation from the vision encoder to the language model. Chai et al. (2022) showed that the scaling in image resolution is successful in the general domain. We fine-tune the LLaVA-OneVision-7B architec",
    "full_text_length": 84679,
    "chunk_length": 1339
  },
  {
    "chunk_id": 2521,
    "paper_filename": "nicoloas_2025_a_multitask_conversational_vision_language_model_for_radiology.pdf",
    "paper_title": "Nicoloas 2025 A Multitask Conversational Vision Language Model For Radiology",
    "chunk_index": 30,
    "total_chunks": 84,
    "text_content": "Following recent trends in visual instruction tuning (Li et al., 2024; Lauren\u00b8 con et al., 2024), the whole architecture is trained, using a learning rate of 2e-6 in the vision encoder weights and 1e-5 in the 2-layer MLP and language model weights. RadVLM is trained over 1 epoch of the instruction dataset using full-fine-tuning. We use 128 GH GPUs, each with 96GB of memory (Fusco et al., 2024) for approximately 12 hours. For the ablation studies, we follow the same fine-tuning procedure but limi",
    "full_text_length": 84679,
    "chunk_length": 1313
  },
  {
    "chunk_id": 2522,
    "paper_filename": "nicoloas_2025_a_multitask_conversational_vision_language_model_for_radiology.pdf",
    "paper_title": "Nicoloas 2025 A Multitask Conversational Vision Language Model For Radiology",
    "chunk_index": 31,
    "total_chunks": 84,
    "text_content": "individual tasks from our instruction dataset. This pipeline leverages existing metrics for report generation, abnormality classification and visual grounding and creates novel evaluation tasks to assess the model\u2019s perfor- mance in conversational abilities. 4.1.1 Report generation To evaluate generated reports, we employ both lexical and radiology-specific metrics. Lexical metrics particu- larly quantify word overlap between generated and ground truth reports, among which we report BertScore, a",
    "full_text_length": 84679,
    "chunk_length": 1407
  },
  {
    "chunk_id": 2523,
    "paper_filename": "nicoloas_2025_a_multitask_conversational_vision_language_model_for_radiology.pdf",
    "paper_title": "Nicoloas 2025 A Multitask Conversational Vision Language Model For Radiology",
    "chunk_index": 32,
    "total_chunks": 84,
    "text_content": "Yu et al., 2023a): computing this metric requires to map each generated and ground truth reports to a structured graph, named RadGraph (Jain et al., 2021), containing radiology- specific entities (anatomy or observations) and the relations between them (\u201csuggestive of\u201d, \u201clocated at\u201d). The RadGraph F1 score (Delbrouck et al., 2022) computes the overlap in inferred structured graphs (extracted entities and relations) from both generated and ground truth reports. In our study, we use the recently r",
    "full_text_length": 84679,
    "chunk_length": 1357
  },
  {
    "chunk_id": 2524,
    "paper_filename": "nicoloas_2025_a_multitask_conversational_vision_language_model_for_radiology.pdf",
    "paper_title": "Nicoloas 2025 A Multitask Conversational Vision Language Model For Radiology",
    "chunk_index": 33,
    "total_chunks": 84,
    "text_content": "expert preferences as compared to other LLM-based evaluations (e.g., GPT-4). 4.1.2 Abnormality classification We use the test split of the CheXpert dataset and prompt the model by asking to list the abnormalities present on the CXR. The mentioned abnormalities in the model\u2019s answer are extracted via key-word matching and compared to the ground truth list. We calculate the F1 score for the 14 abnormalities and report the macro- averaged F1-score over all categories. 4.1.3 Visual grounding We asse",
    "full_text_length": 84679,
    "chunk_length": 1288
  },
  {
    "chunk_id": 2525,
    "paper_filename": "nicoloas_2025_a_multitask_conversational_vision_language_model_for_radiology.pdf",
    "paper_title": "Nicoloas 2025 A Multitask Conversational Vision Language Model For Radiology",
    "chunk_index": 34,
    "total_chunks": 84,
    "text_content": "threshold of 0.5. At this setting, we evaluate both precision and recall for predicted boxes that overlap with ground truth by at least 50%. 4.1.4 Multi-turn evaluation within conversational interactions Evaluating conversational aspects of a model is essential to assess the utility and performance of an AI assistant in a multi-turn setting. We designed an LLM-based evaluation method carefully crafted for conversations, following the \u201cLLM-as-judge\u201d scheme (Zheng et al., 2023b) previously adopted",
    "full_text_length": 84679,
    "chunk_length": 1335
  },
  {
    "chunk_id": 2526,
    "paper_filename": "nicoloas_2025_a_multitask_conversational_vision_language_model_for_radiology.pdf",
    "paper_title": "Nicoloas 2025 A Multitask Conversational Vision Language Model For Radiology",
    "chunk_index": 35,
    "total_chunks": 84,
    "text_content": "and 523 conversations without (Table 1). 8 During the evaluation process, we provide the CXR image to the VLM and sequentially ask the questions from this dataset. After collecting the VLM\u2019s answer to each question, we prompt GPT-4o (text-only) with the CXR information (report, list of abnormalities, etc.), the expected answer per question (derived from the ground truth information), and the VLM-generated answer (see Appendix-Figure 2 for the prompt). At the end of the prompt, GPT-4o is asked to",
    "full_text_length": 84679,
    "chunk_length": 1210
  },
  {
    "chunk_id": 2527,
    "paper_filename": "nicoloas_2025_a_multitask_conversational_vision_language_model_for_radiology.pdf",
    "paper_title": "Nicoloas 2025 A Multitask Conversational Vision Language Model For Radiology",
    "chunk_index": 36,
    "total_chunks": 84,
    "text_content": "\u2713 \u2717 MAIRA-2 13B \u2713 \u2717 \u2713 \u2717 RadVLM 7B \u2713 \u2713 \u2713 \u2713 Table 2: Functional capabilities of RadVLM and baseline models. Comparison of RadVLM with baseline models in terms of parameter size and task coverage across report generation, abnormality classification, visual grounding and conversational interactions. To provide insights into the performance of RadVLM in multiple tasks, we compare the instruction-tuned RadVLM to existing and publicly available state-of-the-art VLMs (from 3B to 13B, Table 2), adjusting",
    "full_text_length": 84679,
    "chunk_length": 1281
  },
  {
    "chunk_id": 2528,
    "paper_filename": "nicoloas_2025_a_multitask_conversational_vision_language_model_for_radiology.pdf",
    "paper_title": "Nicoloas 2025 A Multitask Conversational Vision Language Model For Radiology",
    "chunk_index": 37,
    "total_chunks": 84,
    "text_content": "(possessing conversation skills), and MAIRA-2 (Bannur et al., 2024), specialized in report generation with visual grounding. We summarize the tasks on which each model is evaluated on in Table 2, and provide links to the original repositories from which the model weights were obtained in Appendix-Table 1. 4.3 Results 4.3.1 Report generation Firstly, we evaluate RadVLM on a core radiology task: generating a textual report from a frontal CXR. As our evaluation set, we use a filtered version of the",
    "full_text_length": 84679,
    "chunk_length": 1326
  },
  {
    "chunk_id": 2529,
    "paper_filename": "nicoloas_2025_a_multitask_conversational_vision_language_model_for_radiology.pdf",
    "paper_title": "Nicoloas 2025 A Multitask Conversational Vision Language Model For Radiology",
    "chunk_index": 38,
    "total_chunks": 84,
    "text_content": "originally evaluated under different conditions (sometimes benefiting from extra inputs like prior images/reports or patient details or omitting certain metrics such as GREEN). To ensure that our setup remains consistent across all of them, we apply the same evaluation pipeline (test set and metrics). For non-CXR specific VLMs, we observe a poor performance in both lexical and clinical metrics, with an unexpected improved performance for the generalist model (LLaVA-OneVision) over the medical on",
    "full_text_length": 84679,
    "chunk_length": 1363
  },
  {
    "chunk_id": 2530,
    "paper_filename": "nicoloas_2025_a_multitask_conversational_vision_language_model_for_radiology.pdf",
    "paper_title": "Nicoloas 2025 A Multitask Conversational Vision Language Model For Radiology",
    "chunk_index": 39,
    "total_chunks": 84,
    "text_content": "metrics. This validates our approach, even though our training methodology was not specifically designed for the report generation task. 4.3.2 Abnormality classification In this section, we assess each model\u2019s ability to predict which abnormalities are visible on the CXR. We use the manually curated CheXpert test set, prompt RadVLM to list any observed abnormalities, and compare them against the ground truth. As for the report generation task, we adapt the prompt for each compared 9 NLG Metrics ",
    "full_text_length": 84679,
    "chunk_length": 1436
  },
  {
    "chunk_id": 2531,
    "paper_filename": "nicoloas_2025_a_multitask_conversational_vision_language_model_for_radiology.pdf",
    "paper_title": "Nicoloas 2025 A Multitask Conversational Vision Language Model For Radiology",
    "chunk_index": 40,
    "total_chunks": 84,
    "text_content": "while underlined values represent the second-best performance across models. atelectasis cardiomegaly consolidationedema enlarged cardiom.fracture lung lesionlung opacity pleural e\ufb00usionpleural otherpneumonia pneumothoraxsupport devices020406080100F1 Score (%)RaDialog CheXagent RadVLM Figure 4: F1 scores for abnormality classification across different models. Classification performance ofRadVLM (grey) ,RaDialog (yellow) , and CheXagent (blue) . Bars represent the F1 scores for indi- vidual patho",
    "full_text_length": 84679,
    "chunk_length": 1517
  },
  {
    "chunk_id": 2532,
    "paper_filename": "nicoloas_2025_a_multitask_conversational_vision_language_model_for_radiology.pdf",
    "paper_title": "Nicoloas 2025 A Multitask Conversational Vision Language Model For Radiology",
    "chunk_index": 41,
    "total_chunks": 84,
    "text_content": "reflected in its higher macro-F1 score. In particular, it shows improved classification for crucial pathologies such as atelectasis, edema, fracture, lung lesion, lung opacity, pneumonia, pleural effusion, and pneumothorax. While CheXagent showed strong classification results under binary or restricted-label evaluations (Chen et al., 2024), it struggles when tasked with identifying all categories at once (see Figure 4). Overall, these results indicate that RadVLM performs equally or better than ",
    "full_text_length": 84679,
    "chunk_length": 1365
  },
  {
    "chunk_id": 2533,
    "paper_filename": "nicoloas_2025_a_multitask_conversational_vision_language_model_for_radiology.pdf",
    "paper_title": "Nicoloas 2025 A Multitask Conversational Vision Language Model For Radiology",
    "chunk_index": 42,
    "total_chunks": 84,
    "text_content": "pinpoint exactly where the abnormality appears on the image. 10 a. Anatomical grounding Where is the [region] located on the image? The [region] is located at [coordinates] on the im- age. cardiac silhouette cavoatrial junction right hilar structures left lung right lung left upper abdomen upper mediastinum aortic arch b. Abnormality grounding Locate the [abnormality] on the CXR. The [abnormality] is positioned at the coordinates [coordinates] on the CXR. cardiomegaly calcification pleural thick",
    "full_text_length": 84679,
    "chunk_length": 1456
  },
  {
    "chunk_id": 2534,
    "paper_filename": "nicoloas_2025_a_multitask_conversational_vision_language_model_for_radiology.pdf",
    "paper_title": "Nicoloas 2025 A Multitask Conversational Vision Language Model For Radiology",
    "chunk_index": 43,
    "total_chunks": 84,
    "text_content": "Visual grounding performance measured in mAP (%) . Mean average precision (mAP) scores for anatomical grounding, abnormality grounding, and phrase grounding tasks across different models. Bounding box coordinates are extracted from each model\u2019s output, and mAP is computed using an IoU threshold of 50%. Bold values indicate the highest performance, while underlined values represent the second-best performance. We report performance metrics for the three main grounding tasks RadVLM was trained on:",
    "full_text_length": 84679,
    "chunk_length": 1373
  },
  {
    "chunk_id": 2535,
    "paper_filename": "nicoloas_2025_a_multitask_conversational_vision_language_model_for_radiology.pdf",
    "paper_title": "Nicoloas 2025 A Multitask Conversational Vision Language Model For Radiology",
    "chunk_index": 44,
    "total_chunks": 84,
    "text_content": "capable of predicting bounding box coordi- nates when provided with input text (see corresponding model card in Appendix-Table 1). After retrieving each model\u2019s instruction template for generating bounding box coordinates, we evaluated both CheXagent and MAIRA-2 on all three grounding tasks performed by RadVLM. Our results show that RadVLM performs well at localizing anatomical regions (e.g., \u201cright lung\u201d, \u201caortic arch\u201d, illustrated in Figure 5a), achieving a mAP of 85.3 %, by far surpassing the",
    "full_text_length": 84679,
    "chunk_length": 1334
  },
  {
    "chunk_id": 2536,
    "paper_filename": "nicoloas_2025_a_multitask_conversational_vision_language_model_for_radiology.pdf",
    "paper_title": "Nicoloas 2025 A Multitask Conversational Vision Language Model For Radiology",
    "chunk_index": 45,
    "total_chunks": 84,
    "text_content": "achieves best performance (Table 4). For the phrase grounding task, while MAIRA-2 and CheXagent demonstrate great performance, RadVLM surpasses them with a mAP of 82.8% (Table 4), presumably benefiting from the newly released PadChest-GR dataset (Castro et al., 2024) used for training. Overall, these results show that our instruction tuning strategy for visual grounding (covering three essential tasks), combined to a modern VLM backbone, offers a promising avenue to help clinicians localize anat",
    "full_text_length": 84679,
    "chunk_length": 1449
  },
  {
    "chunk_id": 2537,
    "paper_filename": "nicoloas_2025_a_multitask_conversational_vision_language_model_for_radiology.pdf",
    "paper_title": "Nicoloas 2025 A Multitask Conversational Vision Language Model For Radiology",
    "chunk_index": 46,
    "total_chunks": 84,
    "text_content": "from image-based attributes (e.g., reports, bounding boxes, and view type). Scores are reported for standard conversations and grounded conversations, where models must provide spatially aware responses. Bold values indicate the best performance, while underlined values represent the second-best performance. 4.3.4 Conversational abilities Our model can already handle multiple advanced tasks for CXR interpretation. However, we have only tested its ability to follow instructions in a single-turn, ",
    "full_text_length": 84679,
    "chunk_length": 1377
  },
  {
    "chunk_id": 2538,
    "paper_filename": "nicoloas_2025_a_multitask_conversational_vision_language_model_for_radiology.pdf",
    "paper_title": "Nicoloas 2025 A Multitask Conversational Vision Language Model For Radiology",
    "chunk_index": 47,
    "total_chunks": 84,
    "text_content": "available grounded phrases, and additional details. As mentioned in Section 4.1.4, we created two held-out evaluation sets using GPT-4o: one without grounded questions and one focused on grounded questions derived from the MS-CXR test set (Table 1). We run the evaluation on both test sets and report the average score across all samples. We also evaluate the other baseline models that possess conversational abilities, thoroughly respecting their prompting template within a multi-turn setting. The",
    "full_text_length": 84679,
    "chunk_length": 1353
  },
  {
    "chunk_id": 2539,
    "paper_filename": "nicoloas_2025_a_multitask_conversational_vision_language_model_for_radiology.pdf",
    "paper_title": "Nicoloas 2025 A Multitask Conversational Vision Language Model For Radiology",
    "chunk_index": 48,
    "total_chunks": 84,
    "text_content": "The gap becomes even more pronounced in grounded scenarios, where RadVLM maintains strong performance (6.55/10) while others drop even more. Notably, this suggests that even the limited number of grounded conversations included (Table 1) was sufficient to equip RadVLM with robust grounding capabilities in a multi-turn setting. 4.4 Ablation studies In this section, we investigate a fundamental question in LLM fine-tuning: do models perform individual tasks more effectively when they are trained s",
    "full_text_length": 84679,
    "chunk_length": 1404
  },
  {
    "chunk_id": 2540,
    "paper_filename": "nicoloas_2025_a_multitask_conversational_vision_language_model_for_radiology.pdf",
    "paper_title": "Nicoloas 2025 A Multitask Conversational Vision Language Model For Radiology",
    "chunk_index": 49,
    "total_chunks": 84,
    "text_content": "with smaller training sets (e.g., phrase grounding; see Table 1). We attribute this to the complementary nature of the various grounding tasks, which collectively improve the model\u2019s ability to localize visual elements (including overlapping categories, such as \u201cabnormalities\u201d also mentioned in \u201cphrases\u201d) and prevent overfitting. A similar trend emerges in the conversation scores, where RadVLM outperforms a model trained solely on the conversation dataset. While this ablated model still performs",
    "full_text_length": 84679,
    "chunk_length": 1296
  },
  {
    "chunk_id": 2541,
    "paper_filename": "nicoloas_2025_a_multitask_conversational_vision_language_model_for_radiology.pdf",
    "paper_title": "Nicoloas 2025 A Multitask Conversational Vision Language Model For Radiology",
    "chunk_index": 50,
    "total_chunks": 84,
    "text_content": "information textual reports contain over other tasks, which might suffice for the model to generalize over new instances. 12 What type of view is the X-ray taken in? The X-ray is taken in an anteroposterior (AP) view. This Chest X-ray is taken in an anteroposterior (AP) view. This image was taken from the AP view. RaDialog The X-ray is taken in a posteroanterior (PA) view.LLaVA- Med Can you tell me if there\u2019s anything unusual with the lung bases? Yes, there are heterogeneous opacities observed a",
    "full_text_length": 84679,
    "chunk_length": 1176
  },
  {
    "chunk_id": 2542,
    "paper_filename": "nicoloas_2025_a_multitask_conversational_vision_language_model_for_radiology.pdf",
    "paper_title": "Nicoloas 2025 A Multitask Conversational Vision Language Model For Radiology",
    "chunk_index": 51,
    "total_chunks": 84,
    "text_content": "in the lung bases. Bilateral infiltrates refer to the presence of abnormal substances or fluid in both lungs, which can be seen as areas of increased density or opacity on the X-ray image (...)LLaVA- Med Is there any evidence of a pneumothorax in this image? Yes, a small right apical pneumothorax is present. Yes, there is a small right apical pneumothorax, which means there is air in the pleural space on the right side. No, there is no evidence of a pneumothorax. The chest x-ray image shows no s",
    "full_text_length": 84679,
    "chunk_length": 1159
  },
  {
    "chunk_id": 2543,
    "paper_filename": "nicoloas_2025_a_multitask_conversational_vision_language_model_for_radiology.pdf",
    "paper_title": "Nicoloas 2025 A Multitask Conversational Vision Language Model For Radiology",
    "chunk_index": 52,
    "total_chunks": 84,
    "text_content": "right apical region with bounding box coordinates [0.14, 0.10, 0.45, 0.19]. The pneumothorax is located at coordinates [0.11, 0.10, 0.43, 0.27] on the image. The pneumothorax is located in the right lung. RaDialog The pneumothorax is located in the right lung base, as indicated by the arrow in the image. LLaVA- Med Is there any type of support device visible on the X-ray? Yes, a right pigtail pleural catheter is in place. Yes, there is a right pigtail pleural catheter visible on the X-ray. No, t",
    "full_text_length": 84679,
    "chunk_length": 1282
  },
  {
    "chunk_id": 2544,
    "paper_filename": "nicoloas_2025_a_multitask_conversational_vision_language_model_for_radiology.pdf",
    "paper_title": "Nicoloas 2025 A Multitask Conversational Vision Language Model For Radiology",
    "chunk_index": 53,
    "total_chunks": 84,
    "text_content": "RadVLM (grey bubble), RaDialog (yellow bubble), and LLaVA-Med (purple bubble). 13 Training tasksEvaluation tasks Report generationClassificationAnatomical groundingAbnormality groundingPhrase groundingConversation (standard/ grounded) RadGraph/GREEN F1 (macro) (%) mAP (%) mAP (%) mAP (%) GPT4o-Score Report generation 17.7 / 25.4 - - - - - Classification - 41.5 - - - - Anatomical grounding - - 83.0 - - - Abnormality grounding - - - 27.1 - - Phrase grounding - - - - 64.6 - Conversation - - - - - 6",
    "full_text_length": 84679,
    "chunk_length": 1259
  },
  {
    "chunk_id": 2545,
    "paper_filename": "nicoloas_2025_a_multitask_conversational_vision_language_model_for_radiology.pdf",
    "paper_title": "Nicoloas 2025 A Multitask Conversational Vision Language Model For Radiology",
    "chunk_index": 54,
    "total_chunks": 84,
    "text_content": "and can interact with clinicians in a natural, multi-turn conversation. In this study, inspired by advances in both general and CXR- specific vision-language models, we explored the potential of constructing an assistant that not only aims for strong performance in individual tasks but also for handling them in a multi-turn, interactive setting. To achieve this, we built an instruction dataset that integrates both stand-alone tasks and multi-task dialogues, then fine- tuned a VLM backbone on it.",
    "full_text_length": 84679,
    "chunk_length": 1405
  },
  {
    "chunk_id": 2546,
    "paper_filename": "nicoloas_2025_a_multitask_conversational_vision_language_model_for_radiology.pdf",
    "paper_title": "Nicoloas 2025 A Multitask Conversational Vision Language Model For Radiology",
    "chunk_index": 55,
    "total_chunks": 84,
    "text_content": "within the same context, we enhance both reproducibility and reliability. This standardized approach aligns with initiatives such as the ReXrank leaderboard for CXR report generation (Zhang et al., 2024b), which similarly evaluates state-of-the-art models using a consistent protocol. Ultimately, these efforts provide a clearer understanding of each model\u2019s true capabilities and help researchers identify the most promising directions for future work. Another key takeaway is the flexibility of VLM",
    "full_text_length": 84679,
    "chunk_length": 1379
  },
  {
    "chunk_id": 2547,
    "paper_filename": "nicoloas_2025_a_multitask_conversational_vision_language_model_for_radiology.pdf",
    "paper_title": "Nicoloas 2025 A Multitask Conversational Vision Language Model For Radiology",
    "chunk_index": 56,
    "total_chunks": 84,
    "text_content": "data formats can now be tokenized and processed through a common next-token generation paradigm. Our ablation studies further show that while each task differs in purpose, they collectively enhance the model\u2019s overall understanding of a CXR image, an effect likely amplified by training them together. Even though multi-agent approaches emerge in the medical setting (Schmidgall et al., 2024), these results support the value of joint training for developing comprehensive, single-agent solutions. Th",
    "full_text_length": 84679,
    "chunk_length": 1346
  },
  {
    "chunk_id": 2548,
    "paper_filename": "nicoloas_2025_a_multitask_conversational_vision_language_model_for_radiology.pdf",
    "paper_title": "Nicoloas 2025 A Multitask Conversational Vision Language Model For Radiology",
    "chunk_index": 57,
    "total_chunks": 84,
    "text_content": "of RadVLM with other models, we hypothesize that the quality of the instruction dataset, coupled with the backbone architecture (as shown by Lauren\u00b8 con et al., 2024) are the most critical for the overall performance of such a visual foundation model. That said, for specific tasks like report generation, specialized solutions that incorporate additional patient details, prior and lateral images, or extended training, could offer greater gains. This likely explains why our model does not surpass ",
    "full_text_length": 84679,
    "chunk_length": 1330
  },
  {
    "chunk_id": 2549,
    "paper_filename": "nicoloas_2025_a_multitask_conversational_vision_language_model_for_radiology.pdf",
    "paper_title": "Nicoloas 2025 A Multitask Conversational Vision Language Model For Radiology",
    "chunk_index": 58,
    "total_chunks": 84,
    "text_content": "clinical practice. With RadVLM, we aim to contribute to this vibrant field by introducing a method that draws on prior work yet incorporates original ideas toward developing a multitask conversational vision-language model for radiology. 6 Acknowledgments This work was supported as part of the Swiss AI Initiative by a grant from the Swiss National Supercomputing Centre (CSCS) under project ID a02 on Alps, and by the LOOP Zurich as part of the application driver project supporting the LOOP Zurich",
    "full_text_length": 84679,
    "chunk_length": 1322
  },
  {
    "chunk_id": 2550,
    "paper_filename": "nicoloas_2025_a_multitask_conversational_vision_language_model_for_radiology.pdf",
    "paper_title": "Nicoloas 2025 A Multitask Conversational Vision Language Model For Radiology",
    "chunk_index": 59,
    "total_chunks": 84,
    "text_content": "by JSPS KAKENHI (Grant Number: 23KK0148). AR is supported by the StimuLoop grant #1-007811-002 and the Vontobel Foundation. MV and SL are supported by the Swiss State Secretariat for Education, Research, and Innovation (SERI) under contract number MB22.00047. MK is supported by the UZH Global Strategy and Partnerships Funding Scheme and a Research Partnership Grant with China, Japan, South Korea and the ASEAN region (RPG 072023 18). References Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkay",
    "full_text_length": 84679,
    "chunk_length": 1370
  },
  {
    "chunk_id": 2551,
    "paper_filename": "nicoloas_2025_a_multitask_conversational_vision_language_model_for_radiology.pdf",
    "paper_title": "Nicoloas 2025 A Multitask Conversational Vision Language Model For Radiology",
    "chunk_index": 60,
    "total_chunks": 84,
    "text_content": "generation of Claude. Anthropic News, https://www.anthropic.com/ news/claude-3-family . Accessed: 2024-11-26. Bannur, S., Bouzid, K., Castro, D. C., Schwaighofer, A., Thieme, A., Bond-Taylor, S., Ilse, M., P\u00b4 erez-Garc\u00b4 \u0131a, F., Salvatelli, V., Sharma, H., Meissen, F., Ranjit, M., Srivastav, S., Gong, J., Codella, N. C. F., Falck, F., Oktay, O., Lungren, M. P., Wetscherek, M. T., Alvarez-Valle, J., and Hyland, S. L. (2024). MAIRA-2: Grounded radiology report generation. arXiv [cs.CL] . Bluethgen,",
    "full_text_length": 84679,
    "chunk_length": 1357
  },
  {
    "chunk_id": 2552,
    "paper_filename": "nicoloas_2025_a_multitask_conversational_vision_language_model_for_radiology.pdf",
    "paper_title": "Nicoloas 2025 A Multitask Conversational Vision Language Model For Radiology",
    "chunk_index": 61,
    "total_chunks": 84,
    "text_content": "biomedical vision\u2013 language processing. In European conference on computer vision , pages 1\u201321. Springer. Brown, T. B. (2020). Language models are few-shot learners. arXiv preprint arXiv:2005.14165 . Bustos, A., Pertusa, A., Salinas, J.-M., and De La Iglesia-Vaya, M. (2020). Padchest: A large chest x-ray image dataset with multi-label annotated reports. Medical image analysis , 66:101797. Calamida, A., Nooralahzadeh, F., Rohanian, M., Fujimoto, K., Nishio, M., and Krauthammer, M. (2023). Radiolo",
    "full_text_length": 84679,
    "chunk_length": 1401
  },
  {
    "chunk_id": 2553,
    "paper_filename": "nicoloas_2025_a_multitask_conversational_vision_language_model_for_radiology.pdf",
    "paper_title": "Nicoloas 2025 A Multitask Conversational Vision Language Model For Radiology",
    "chunk_index": 62,
    "total_chunks": 84,
    "text_content": "anchez-Valverde, M. D., Jaques-P\u00b4 erez, L., P\u00b4 erez-Rodr\u00b4 \u0131guez, L., Takeda, K., et al. (2024). Padchest-gr: A bilingual chest x-ray dataset for grounded radiology report generation. arXiv preprint arXiv:2411.05085 . 15 Chai, L., Gharbi, M., Shechtman, E., Isola, P., and Zhang, R. (2022). Any-resolution training for high-resolution image synthesis. In European Conference on Computer Vision , pages 170\u2013188. Springer. Chambon, P., Delbrouck, J.-B., Sounack, T., Huang, S.-C., Chen, Z., Varma, M., T",
    "full_text_length": 84679,
    "chunk_length": 1286
  },
  {
    "chunk_id": 2554,
    "paper_filename": "nicoloas_2025_a_multitask_conversational_vision_language_model_for_radiology.pdf",
    "paper_title": "Nicoloas 2025 A Multitask Conversational Vision Language Model For Radiology",
    "chunk_index": 63,
    "total_chunks": 84,
    "text_content": "S., Langlotz, C. P., Wang, S., and Poon, H. (2024). Towards a clinically accessible radiology foundation model: open-access and lightweight, with automated evaluation. Chen, Z., Varma, M., Xu, J., Paschali, M., Veen, D. V., Johnston, A., Youssef, A., Blankemeier, L., Bluethgen, C., Altmayer, S., Valanarasu, J. M. J., Muneer, M. S. E., Reis, E. P., Cohen, J. P., Olsen, C., Abraham, T. M., Tsai, E. B., Beaulieu, C. F., Jitsev, J., Gatidis, S., Delbrouck, J.-B., Chaudhari, A. S., and Langlotz, C. P",
    "full_text_length": 84679,
    "chunk_length": 1312
  },
  {
    "chunk_id": 2555,
    "paper_filename": "nicoloas_2025_a_multitask_conversational_vision_language_model_for_radiology.pdf",
    "paper_title": "Nicoloas 2025 A Multitask Conversational Vision Language Model For Radiology",
    "chunk_index": 64,
    "total_chunks": 84,
    "text_content": "expert-annotated dataset for entity and relation extraction from radiology reports. In Findings of the Association for Computational Linguistics ACL 2024, pages 12902\u201312915. Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan, A., et al. (2024). The llama 3 herd of models. arXiv preprint arXiv:2407.21783 . Feng, J., Sun, Q., Xu, C., Zhao, P., Yang, Y., Tao, C., Zhao, D., and Lin, Q. (2022). Mmdialog: A large-scale multi-turn dialogue d",
    "full_text_length": 84679,
    "chunk_length": 1333
  },
  {
    "chunk_id": 2556,
    "paper_filename": "nicoloas_2025_a_multitask_conversational_vision_language_model_for_radiology.pdf",
    "paper_title": "Nicoloas 2025 A Multitask Conversational Vision Language Model For Radiology",
    "chunk_index": 65,
    "total_chunks": 84,
    "text_content": "via reinforcement learning. arXiv preprint arXiv:2501.12948 . He, K., Zhang, X., Ren, S., and Sun, J. (2016). Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition , pages 770\u2013778. Homayounieh, F., Digumarthy, S., Ebrahimian, S., Rueckel, J., Hoppe, B. F., Sabel, B. O., Conjeti, S., Ridder, K., Sistermanns, M., Wang, L., Preuhs, A., Ghesu, F., Mansoor, A., Moghbel, M., Botwin, A., Singh, R., Cartmell, S., Patti, J., Huemmer",
    "full_text_length": 84679,
    "chunk_length": 1336
  },
  {
    "chunk_id": 2557,
    "paper_filename": "nicoloas_2025_a_multitask_conversational_vision_language_model_for_radiology.pdf",
    "paper_title": "Nicoloas 2025 A Multitask Conversational Vision Language Model For Radiology",
    "chunk_index": 66,
    "total_chunks": 84,
    "text_content": ". Irvin, J., Rajpurkar, P., Ko, M., Yu, Y., Ciurea-Ilcus, S., Chute, C., Marklund, H., Haghgoo, B., Ball, R., Shpanskaya, K., et al. (2019). Chexpert: A large chest radiograph dataset with uncertainty labels and expert comparison. In Proceedings of the AAAI conference on artificial intelligence , volume 33, pages 590\u2013597. Jain, S., Agrawal, A., Saporta, A., Truong, S. Q., Duong, D. N., Bui, T., Chambon, P., Zhang, Y., Lungren, M. P., Ng, A. Y., et al. (2021). Radgraph: Extracting clinical entiti",
    "full_text_length": 84679,
    "chunk_length": 1287
  },
  {
    "chunk_id": 2558,
    "paper_filename": "nicoloas_2025_a_multitask_conversational_vision_language_model_for_radiology.pdf",
    "paper_title": "Nicoloas 2025 A Multitask Conversational Vision Language Model For Radiology",
    "chunk_index": 67,
    "total_chunks": 84,
    "text_content": "framework for cxr understanding. arXiv preprint arXiv:2403.15456 . 16 Kim, S., Nooralahzadeh, F., Rohanian, M., Fujimoto, K., Nishio, M., Sakamoto, R., Rinaldi, F., and Krautham- mer, M. (2023). Boosting radiology report generation by infusing comparison prior. In Demner-fushman, D., Ananiadou, S., and Cohen, K., editors, The 22nd Workshop on Biomedical Natural Language Processing and BioNLP Shared Tasks , pages 50\u201361, Toronto, Canada. Association for Computational Linguistics. Lauren\u00b8 con, H., ",
    "full_text_length": 84679,
    "chunk_length": 1378
  },
  {
    "chunk_id": 2559,
    "paper_filename": "nicoloas_2025_a_multitask_conversational_vision_language_model_for_radiology.pdf",
    "paper_title": "Nicoloas 2025 A Multitask Conversational Vision Language Model For Radiology",
    "chunk_index": 68,
    "total_chunks": 84,
    "text_content": "Yang, J., Naumann, T., Poon, H., and Gao, J. (2023a). Llava-med: Training a large language-and-vision assistant for biomedicine in one day. arXiv preprint arXiv:2306.00890 . Li, J., Li, D., Savarese, S., and Hoi, S. (2023b). Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In International conference on machine learning , pages 19730\u201319742. PMLR. Lin, C.-Y. (2004). Rouge: A package for automatic evaluation of summaries. In Text summarization",
    "full_text_length": 84679,
    "chunk_length": 1282
  },
  {
    "chunk_id": 2560,
    "paper_filename": "nicoloas_2025_a_multitask_conversational_vision_language_model_for_radiology.pdf",
    "paper_title": "Nicoloas 2025 A Multitask Conversational Vision Language Model For Radiology",
    "chunk_index": 69,
    "total_chunks": 84,
    "text_content": "J. (2023). Visual instruction tuning. arXiv preprint arXiv:2304.08485 . M\u00a8 uller, P., Kaissis, G., and Rueckert, D. (2024). Chex: Interactive localization and region description in chest x-rays. arXiv preprint arXiv:2404.15770 . Nguyen, H. Q., Lam, K., Le, L. T., Pham, H. H., Tran, D. Q., Nguyen, D. B., Le, D. D., Pham, C. M., Tong, H. T., Dinh, D. H., et al. (2022). Vindr-cxr: An open dataset of chest x-rays with radiologists\u2019 annotations. Scientific Data , 9(1):429. NIH (n.d.). Pubmed. https:/",
    "full_text_length": 84679,
    "chunk_length": 1442
  },
  {
    "chunk_id": 2561,
    "paper_filename": "nicoloas_2025_a_multitask_conversational_vision_language_model_for_radiology.pdf",
    "paper_title": "Nicoloas 2025 A Multitask Conversational Vision Language Model For Radiology",
    "chunk_index": 70,
    "total_chunks": 84,
    "text_content": "editors, Findings of the Association for Computational Linguistics: EMNLP 2021 , pages 2824\u20132832, Punta Cana, Dominican Republic. Association for Computational Linguistics. OpenAI (2024). ChatGPT can now See, Hear, and Speak. OpenAI Blog, https://openai.com/index/ chatgpt-can-now-see-hear-and-speak/ . Accessed: 2024-11-26. Ostmeier, S., Xu, J., Chen, Z., Varma, M., Blankemeier, L., Bluethgen, C., Michalson, A. E., Moseley, M., Langlotz, C., Chaudhari, A. S., et al. (2024). Green: Generative radi",
    "full_text_length": 84679,
    "chunk_length": 1431
  },
  {
    "chunk_id": 2562,
    "paper_filename": "nicoloas_2025_a_multitask_conversational_vision_language_model_for_radiology.pdf",
    "paper_title": "Nicoloas 2025 A Multitask Conversational Vision Language Model For Radiology",
    "chunk_index": 71,
    "total_chunks": 84,
    "text_content": "Navab, N., and Keicher, M. (2023). Radialog: A large vision-language model for radiology report generation and conversational assistance. arXiv preprint arXiv:2311.18681 . 17 Peng, B., Li, C., He, P., Galley, M., and Gao, J. (2023). Instruction tuning with gpt-4. arXiv preprint arXiv:2304.03277 . Peng, Y.-C., Lee, W.-J., Chang, Y.-C., Chan, W. P., and Chen, S.-J. (2022). Radiologist burnout: trends in medical imaging utilization under the national health insurance system with the universal code ",
    "full_text_length": 84679,
    "chunk_length": 1330
  },
  {
    "chunk_id": 2563,
    "paper_filename": "nicoloas_2025_a_multitask_conversational_vision_language_model_for_radiology.pdf",
    "paper_title": "Nicoloas 2025 A Multitask Conversational Vision Language Model For Radiology",
    "chunk_index": 72,
    "total_chunks": 84,
    "text_content": "object detection with region proposal networks. IEEE transactions on pattern analysis and machine intelligence , 39(6):1137\u20131149. R\u00a8 ontgen, W. C. (1895). Ueber eine neue Art von Strahlen . Phys.-med. Gesellschaft. Saab, K., Tu, T., Weng, W.-H., Tanno, R., Stutz, D., Wulczyn, E., Zhang, F., Strother, T., Park, C., Vedadi, E., et al. (2024). Capabilities of gemini models in medicine. arXiv preprint arXiv:2404.18416 . Schmidgall, S., Ziaei, R., Harris, C., Reis, E., Jopling, J., and Moor, M. (2024",
    "full_text_length": 84679,
    "chunk_length": 1340
  },
  {
    "chunk_id": 2564,
    "paper_filename": "nicoloas_2025_a_multitask_conversational_vision_language_model_for_radiology.pdf",
    "paper_title": "Nicoloas 2025 A Multitask Conversational Vision Language Model For Radiology",
    "chunk_index": 73,
    "total_chunks": 84,
    "text_content": "S., Ranjit, M. P., Falck, F., et al. (2024). Maira-seg: Enhancing radiology report generation with segmentation-aware multimodal large language models. arXiv preprint arXiv:2411.11362 . Singhal, K., Azizi, S., Tu, T., Mahdavi, S. S., Wei, J., Chung, H. W., Scales, N., Tanwani, A., Cole-Lewis, H., Pfohl, S., et al. (2023). Large language models encode clinical knowledge. Nature , 620(7972):172\u2013180. Smit, A., Jain, S., Rajpurkar, P., Pareek, A., Ng, A. Y., and Lungren, M. P. (2020). Chexbert: comb",
    "full_text_length": 84679,
    "chunk_length": 1376
  },
  {
    "chunk_id": 2565,
    "paper_filename": "nicoloas_2025_a_multitask_conversational_vision_language_model_for_radiology.pdf",
    "paper_title": "Nicoloas 2025 A Multitask Conversational Vision Language Model For Radiology",
    "chunk_index": 74,
    "total_chunks": 84,
    "text_content": "P., Kaissis, G., and Rueckert, D. (2023). Interactive and explainable region-guided radiology report generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 7433\u20137442. Team, G., Anil, R., Borgeaud, S., Alayrac, J.-B., Yu, J., Soricut, R., Schalkwyk, J., Dai, A. M., Hauth, A., Millican, K., et al. (2023). Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805 . Tu, T., Palepu, A., Schaekermann, M., Saab, K., Freyberg,",
    "full_text_length": 84679,
    "chunk_length": 1286
  },
  {
    "chunk_id": 2566,
    "paper_filename": "nicoloas_2025_a_multitask_conversational_vision_language_model_for_radiology.pdf",
    "paper_title": "Nicoloas 2025 A Multitask Conversational Vision Language Model For Radiology",
    "chunk_index": 75,
    "total_chunks": 84,
    "text_content": "Z., Bai, J., Chen, K., Liu, X., Wang, J., Ge, W., et al. (2024). Qwen2-vl: Enhancing vision-language model\u2019s perception of the world at any resolution. arXiv preprint arXiv:2409.12191 . Wang, Y., Kordi, Y., Mishra, S., Liu, A., Smith, N. A., Khashabi, D., and Hajishirzi, H. (2022). Self-instruct: Aligning language models with self-generated instructions. arXiv preprint arXiv:2212.10560 . 18 Wei, J., Bosma, M., Zhao, V. Y., Guu, K., Yu, A. W., Lester, B., Du, N., Dai, A. M., and Le, Q. V. (2021).",
    "full_text_length": 84679,
    "chunk_length": 1244
  },
  {
    "chunk_id": 2567,
    "paper_filename": "nicoloas_2025_a_multitask_conversational_vision_language_model_for_radiology.pdf",
    "paper_title": "Nicoloas 2025 A Multitask Conversational Vision Language Model For Radiology",
    "chunk_index": 76,
    "total_chunks": 84,
    "text_content": "et al. (2024a). Qwen2 technical report. arXiv preprint arXiv:2407.10671 . Yang, L., Xu, S., Sellergren, A., Kohlberger, T., Zhou, Y., Ktena, I., Kiraly, A., Ahmed, F., Hormozdiari, F., Jaroensri, T., et al. (2024b). Advancing multimodal medical capabilities of gemini. arXiv preprint arXiv:2405.03162 . Yang, S., Wu, X., Ge, S., Zheng, Z., Zhou, S. K., and Xiao, L. (2023). Radiology report generation with a learned knowledge base and multi-modal alignment. Medical Image Analysis , 86:102798. You, ",
    "full_text_length": 84679,
    "chunk_length": 1229
  },
  {
    "chunk_id": 2568,
    "paper_filename": "nicoloas_2025_a_multitask_conversational_vision_language_model_for_radiology.pdf",
    "paper_title": "Nicoloas 2025 A Multitask Conversational Vision Language Model For Radiology",
    "chunk_index": 77,
    "total_chunks": 84,
    "text_content": "4(9). Yu, F., Endo, M., Krishnan, R., Pan, I., Tsai, A., Reis, E. P., Kaiser Ururahy Nunes Fonseca, E., Lee, H., Shakeri, Z., Ng, A., Langlotz, C., Venugopal, V. K., and Rajpurkar, P. (2023b). Radiology report expert evaluation (ReXVal) dataset. Zhai, X., Mustafa, B., Kolesnikov, A., and Beyer, L. (2023). Sigmoid loss for language image pre-training. In Proceedings of the IEEE/CVF International Conference on Computer Vision , pages 11975\u201311986. Zhang, H., You, H., Dufter, P., Zhang, B., Chen, C.",
    "full_text_length": 84679,
    "chunk_length": 1318
  },
  {
    "chunk_id": 2569,
    "paper_filename": "nicoloas_2025_a_multitask_conversational_vision_language_model_for_radiology.pdf",
    "paper_title": "Nicoloas 2025 A Multitask Conversational Vision Language Model For Radiology",
    "chunk_index": 78,
    "total_chunks": 84,
    "text_content": "and Rajpurkar, P. (2024b). Rexrank: A public leaderboard for ai-powered radiology report generation. arXiv preprint arXiv:2411.15122 . Zheng, L., Chiang, W.-L., Sheng, Y., Li, T., Zhuang, S., Wu, Z., Zhuang, Y., Li, Z., Lin, Z., Xing, E., et al. (2023a). Lmsys-chat-1m: A large-scale real-world llm conversation dataset. arXiv preprint arXiv:2309.11998 . Zheng, L., Chiang, W.-L., Sheng, Y., Zhuang, S., Wu, Z., Zhuang, Y., Lin, Z., Li, Z., Li, D., Xing, E., et al. (2023b). Judging llm-as-a-judge wi",
    "full_text_length": 84679,
    "chunk_length": 1291
  },
  {
    "chunk_id": 2570,
    "paper_filename": "nicoloas_2025_a_multitask_conversational_vision_language_model_for_radiology.pdf",
    "paper_title": "Nicoloas 2025 A Multitask Conversational Vision Language Model For Radiology",
    "chunk_index": 79,
    "total_chunks": 84,
    "text_content": "visual details that can be reasonably inferred from the X-ray image alone. \u2022The user\u2019s questions should be diverse and related to the image (appearance, observations, possible abnormalities or bounding box coordinates). Do not hallucinate information outside what I provide you. \u2022If coordinates for any visual findings are given, ensure they follow the provided format [xmin, ymin, xmax, ymax]. It should invite the assistant to answer about it, like \u201cWhere is [observation] located on the image?\u201d \u2022T",
    "full_text_length": 84679,
    "chunk_length": 1325
  },
  {
    "chunk_id": 2571,
    "paper_filename": "nicoloas_2025_a_multitask_conversational_vision_language_model_for_radiology.pdf",
    "paper_title": "Nicoloas 2025 A Multitask Conversational Vision Language Model For Radiology",
    "chunk_index": 80,
    "total_chunks": 84,
    "text_content": "a right-sided PICC line. There are left-sided chest tubes. There is a small left apical pneumothorax. There is consolidation at the left lung base with prominence of the pulmonary vascular markings throughout the left lung. The right lung appears relatively clear. No pneumothorax on the right side is seen. There is aeration of the atelectasis at the right lung base. \u2022List of Abnormalities: Atelectasis, Consolidation, Pneumothorax, Support Devices \u2022View: AP \u2022Gender: F \u2022Selected observations with ",
    "full_text_length": 84679,
    "chunk_length": 1628
  },
  {
    "chunk_id": 2572,
    "paper_filename": "nicoloas_2025_a_multitask_conversational_vision_language_model_for_radiology.pdf",
    "paper_title": "Nicoloas 2025 A Multitask Conversational Vision Language Model For Radiology",
    "chunk_index": 81,
    "total_chunks": 84,
    "text_content": "round in the conver- sation, you have: \u2022User\u2019s question \u2022Expected answer (if the model had access to the data) \u2022Generated answer (from the model that does not have direct access to the data, but only to the image) Your task is to: \u2022Evaluate each generated answer individually, briefly comparing it to the expected answer in terms of correctness, completeness, and relevance. \u2022Assign a small \u201cper-question\u201d rating or a short note indicating how close the generated answer is to the expected answer. \u2022A",
    "full_text_length": 84679,
    "chunk_length": 1313
  },
  {
    "chunk_id": 2573,
    "paper_filename": "nicoloas_2025_a_multitask_conversational_vision_language_model_for_radiology.pdf",
    "paper_title": "Nicoloas 2025 A Multitask Conversational Vision Language Model For Radiology",
    "chunk_index": 82,
    "total_chunks": 84,
    "text_content": "the level of the right ventricular outflow tract. There is substantial distension of the stomach. What appears to be a ureteral stent is partially imaged. Bilateral pleural effusions, right more than left, are present. Vascular congestion is noted, borderline with mild interstitial pulmonary edema. \u2022View: AP \u2022Gender: F \u2022Selected observations with bounding boxes coordinates: [grounded-phrases] Conversation to Evaluate: \u2022User: What do you see in this X-ray image? Expected answer: I see an AP view ",
    "full_text_length": 84679,
    "chunk_length": 1042
  },
  {
    "chunk_id": 2574,
    "paper_filename": "nicoloas_2025_a_multitask_conversational_vision_language_model_for_radiology.pdf",
    "paper_title": "Nicoloas 2025 A Multitask Conversational Vision Language Model For Radiology",
    "chunk_index": 83,
    "total_chunks": 84,
    "text_content": "[score] Appendix-Figure 2: Prompting LLM to evaluate conversation inference of a VLM 21",
    "full_text_length": 84679,
    "chunk_length": 87
  },
  {
    "chunk_id": 2575,
    "paper_filename": "ning_2022_attention_based_deep_learning_for_breast_lesions_classification_contrast_enhanced_mammography.pdf",
    "paper_title": "Ning 2022 Attention Based Deep Learning For Breast Lesions Classification Contrast Enhanced Mammography",
    "chunk_index": 0,
    "total_chunks": 58,
    "text_content": "ARTICLE Clinical Studies Attention-based deep learning for breast lesions classi \ufb01cation on contrast enhanced spectral mammography: a multicentre study Ning Mao1,2,7, Haicheng Zhang2,7, Yi Dai3, Qin Li4, Fan Lin1, Jing Gao1, Tiantian Zheng1, Feng Zhao5, Haizhu Xie1, Cong Xu6\u2709and Heng Ma1\u2709 \u00a9 The Author(s), under exclusive licence to Springer Nature Limited 2022 BACKGROUND: This study aims to develop an attention-based deep learning model for distinguishing benign from malignant breast lesions on ",
    "full_text_length": 56769,
    "chunk_length": 1396
  },
  {
    "chunk_id": 2576,
    "paper_filename": "ning_2022_attention_based_deep_learning_for_breast_lesions_classification_contrast_enhanced_mammography.pdf",
    "paper_title": "Ning 2022 Attention Based Deep Learning For Breast Lesions Classification Contrast Enhanced Mammography",
    "chunk_index": 1,
    "total_chunks": 58,
    "text_content": "of the models was analysed in terms of the receiver operating characteristic (ROC) curve, accuracy, the positive predictive value (PPV), the negative predictive value (NPV), the F1 score, the precision recall curve (PRC), and heat maps. The \ufb01nal models were compared with the diagnostic performance of conventional CNNs, radiomics models, and two radiologists with specialised breast imaging experience.RESULTS: The best-performing deep learning model, that is, the CBAM-based Xception, achieved an a",
    "full_text_length": 56769,
    "chunk_length": 1397
  },
  {
    "chunk_id": 2577,
    "paper_filename": "ning_2022_attention_based_deep_learning_for_breast_lesions_classification_contrast_enhanced_mammography.pdf",
    "paper_title": "Ning 2022 Attention Based Deep Learning For Breast Lesions Classification Contrast Enhanced Mammography",
    "chunk_index": 2,
    "total_chunks": 58,
    "text_content": "distinguishing benign from malignant breast lesions, and the diagnostic performance of radiologists improved with deep learning assistance. British Journal of Cancer (2023) 128:793\u2013804; https://doi.org/10.1038/s41416-022-02092-y INTRODUCTION Breast cancer is one of the most common malignant tumours among women [ 1]. Early detection and classi \ufb01cation of breast lesions is closely related to improving the survival rate of women.According to the American College of Radiology, a short-term follow-up",
    "full_text_length": 56769,
    "chunk_length": 1445
  },
  {
    "chunk_id": 2578,
    "paper_filename": "ning_2022_attention_based_deep_learning_for_breast_lesions_classification_contrast_enhanced_mammography.pdf",
    "paper_title": "Ning 2022 Attention Based Deep Learning For Breast Lesions Classification Contrast Enhanced Mammography",
    "chunk_index": 3,
    "total_chunks": 58,
    "text_content": "patient anxiety, and radiation exposure [ 3]. Contrast enhanced spectral mammography (CESM) is a new breakthrough in mammography technology that can effectivelydetect breast lesions, but its speci \ufb01city in the diagnosis of breast cancer is limited [ 4]. Radiologists usually distinguish between benign and malignant breast lesions through morphological features, such as thediameter, volume, and edge of the lesions. However, these morphological features often overlap, preventing their accurate clas",
    "full_text_length": 56769,
    "chunk_length": 1530
  },
  {
    "chunk_id": 2579,
    "paper_filename": "ning_2022_attention_based_deep_learning_for_breast_lesions_classification_contrast_enhanced_mammography.pdf",
    "paper_title": "Ning 2022 Attention Based Deep Learning For Breast Lesions Classification Contrast Enhanced Mammography",
    "chunk_index": 4,
    "total_chunks": 58,
    "text_content": "University Shenzhen Hospital, 518000 Shenzhen, P. R. China. 4Department of Radiology, Fudan University Cancer Center, 200433 Shanghai, P. R. China.5School of Computer Science and Technology, Shandong Technology and Business University, 264005 Yantai, Shandong, P. R. China.6Physical Examination Center, Yantai Yuhuangding Hospital, Qingdao University, 264000 Yantai, Shandong, P. R. China.7These authors jointly supervised this work: Ning Mao, Haicheng Zhang.\u2709email: 616574369@qq.com; 827341627@qq.co",
    "full_text_length": 56769,
    "chunk_length": 1493
  },
  {
    "chunk_id": 2580,
    "paper_filename": "ning_2022_attention_based_deep_learning_for_breast_lesions_classification_contrast_enhanced_mammography.pdf",
    "paper_title": "Ning 2022 Attention Based Deep Learning For Breast Lesions Classification Contrast Enhanced Mammography",
    "chunk_index": 5,
    "total_chunks": 58,
    "text_content": "cancer diagnosis, preoperative prediction, and therapeutic effect prediction [ 12\u201314]. The exploration of deep learning on CESM is still at the initial phase, and few recentlypublished studies showed its extraordinary ability in this area[15,16]. However, the sample size of these studies was relatively small, and they were all single-centre studies. In addition, many works have demonstrated that the attention-based convolutionalneural networks (CNNs) could focus the attention of the networks on ",
    "full_text_length": 56769,
    "chunk_length": 1398
  },
  {
    "chunk_id": 2581,
    "paper_filename": "ning_2022_attention_based_deep_learning_for_breast_lesions_classification_contrast_enhanced_mammography.pdf",
    "paper_title": "Ning 2022 Attention Based Deep Learning For Breast Lesions Classification Contrast Enhanced Mammography",
    "chunk_index": 6,
    "total_chunks": 58,
    "text_content": "the backbone architectures and incorporated theconvolutional block attention module (CBAM) [ 19] into them for classi \ufb01cation. MATERIALS AND METHODS Patients and data sets This retrospective multicentre study was approved by the institutional review board of Yantai Yuhuangding Hospital, and the patient informedconsent was waived. For the primary cohort, we assessed YantaiYuhuangding Hospital \u2019s database of medical records from July 2017 to May 2020 to identify patients with histologically con \ufb01r",
    "full_text_length": 56769,
    "chunk_length": 1414
  },
  {
    "chunk_id": 2582,
    "paper_filename": "ning_2022_attention_based_deep_learning_for_breast_lesions_classification_contrast_enhanced_mammography.pdf",
    "paper_title": "Ning 2022 Attention Based Deep Learning For Breast Lesions Classification Contrast Enhanced Mammography",
    "chunk_index": 7,
    "total_chunks": 58,
    "text_content": "(2) biopsy or surgery was performed before CESM; (3) chemotherapy, radiotherapy, or hormone treatment were performed before CESM; and (4) non-mass lesions withoutdelineate boundaries. Additionally, the largest lesion was selected forevaluation for patients with more than one lesion [ 20]. Finally, 1093 patients were included in the training and validation sets, and 100 wereincluded in the internal test set according to the CESM examination time.From June 2018 to May 2019, an independent external",
    "full_text_length": 56769,
    "chunk_length": 1376
  },
  {
    "chunk_id": 2583,
    "paper_filename": "ning_2022_attention_based_deep_learning_for_breast_lesions_classification_contrast_enhanced_mammography.pdf",
    "paper_title": "Ning 2022 Attention Based Deep Learning For Breast Lesions Classification Contrast Enhanced Mammography",
    "chunk_index": 8,
    "total_chunks": 58,
    "text_content": "of experience in breast imaging diagnosis onlow-energy and recombined images of cranio-caudal (CC) view, respec-tively. Therefore, each lesion had two ROIs. Image segmentation wasperformed using ITK-SNAP (version 3.8.0). The dice similarity coef \ufb01cient (DSC) was utilised for evaluating the agreement of image segmentation. Two other radiologists (Segmenters 2 and 3), with 10 and 12 years ofexperience in breast imaging diagnosis, respectively, randomly segmented150 images to evaluate the agreement",
    "full_text_length": 56769,
    "chunk_length": 1453
  },
  {
    "chunk_id": 2584,
    "paper_filename": "ning_2022_attention_based_deep_learning_for_breast_lesions_classification_contrast_enhanced_mammography.pdf",
    "paper_title": "Ning 2022 Attention Based Deep Learning For Breast Lesions Classification Contrast Enhanced Mammography",
    "chunk_index": 9,
    "total_chunks": 58,
    "text_content": "Del) using the Keras ImageDataGenerator ( https://keras.io/ preprocessing/image/ ). Models building and testing The CBAM-based CNN architecture is presented in Fig. 1b. We selected and evaluated the models on the training and validation sets using\ufb01vefold cross-validation. Training in the training and validation sets resulted in \ufb01ve models; the averaged predictions from all \ufb01ve models were used for evaluation in the test sets. Given their advantages, three representative deep CNNs, namely, DenseN",
    "full_text_length": 56769,
    "chunk_length": 1352
  },
  {
    "chunk_id": 2585,
    "paper_filename": "ning_2022_attention_based_deep_learning_for_breast_lesions_classification_contrast_enhanced_mammography.pdf",
    "paper_title": "Ning 2022 Attention Based Deep Learning For Breast Lesions Classification Contrast Enhanced Mammography",
    "chunk_index": 10,
    "total_chunks": 58,
    "text_content": "Supplementary Materials 2.Then, we added a CBAM before the global average pooling layer on thebackbone architectures. CBAM, a lightweight and general attentionmodule, can be integrated into any feed-forward CNN to improve theperformance of classi \ufb01cation tasks. CBAM consists of two independent submodules, namely, the channel and spatial attention modules, which carry out channel and spatial attention tasks, respectively [ 19]. This module can ensure integration with any CNN architecture as a plu",
    "full_text_length": 56769,
    "chunk_length": 1308
  },
  {
    "chunk_id": 2586,
    "paper_filename": "ning_2022_attention_based_deep_learning_for_breast_lesions_classification_contrast_enhanced_mammography.pdf",
    "paper_title": "Ning 2022 Attention Based Deep Learning For Breast Lesions Classification Contrast Enhanced Mammography",
    "chunk_index": 11,
    "total_chunks": 58,
    "text_content": "rate was set to 0.0001 and decayed by a factor of 10 each timewhen the accuracy of the validation set showed no further improvement for 10 continuous epochs. Finally, the models with the lowest validation loss were selected. The loss function of the CNNs was cross-entropy. All theprograms were performed in Python version 3.6.6. Moreover, we also explored the predictive performance of the deep learning models with and without tumour segmentation, as detailed inSupplementary Materials 4. Radiologi",
    "full_text_length": 56769,
    "chunk_length": 1364
  },
  {
    "chunk_id": 2587,
    "paper_filename": "ning_2022_attention_based_deep_learning_for_breast_lesions_classification_contrast_enhanced_mammography.pdf",
    "paper_title": "Ning 2022 Attention Based Deep Learning For Breast Lesions Classification Contrast Enhanced Mammography",
    "chunk_index": 12,
    "total_chunks": 58,
    "text_content": "values were calculated between two radiologists. Radiomics model building and evaluation Radiomics features were extracted from the ROIs of each patient \u2019s CESM images (low-energy and recombined images) using the RadiomicsFea-tureExtractor toolbox in PyRadiomics, which is a \ufb02exible open-source platform implemented in Python. In the training and validation sets, radiomics features were selected from two ROIs separately using Spearmancorrelation analysis, analysis of variance, and least absolute s",
    "full_text_length": 56769,
    "chunk_length": 1497
  },
  {
    "chunk_id": 2588,
    "paper_filename": "ning_2022_attention_based_deep_learning_for_breast_lesions_classification_contrast_enhanced_mammography.pdf",
    "paper_title": "Ning 2022 Attention Based Deep Learning For Breast Lesions Classification Contrast Enhanced Mammography",
    "chunk_index": 13,
    "total_chunks": 58,
    "text_content": "repeated independently with access to the best-performingCNN model with a 2-month washout period, recording new predictions.The model provided radiologists with predicted probability of tumour malignancy [ 25], and the age, medical history, family history, and CESM images were also open to the radiologists. After taking consideration of theCBAM-based Xception model-predicted probability of malignancy, the tworadiologists evaluated the malignancy status of each patient onceagain.The deep learning",
    "full_text_length": 56769,
    "chunk_length": 1468
  },
  {
    "chunk_id": 2589,
    "paper_filename": "ning_2022_attention_based_deep_learning_for_breast_lesions_classification_contrast_enhanced_mammography.pdf",
    "paper_title": "Ning 2022 Attention Based Deep Learning For Breast Lesions Classification Contrast Enhanced Mammography",
    "chunk_index": 14,
    "total_chunks": 58,
    "text_content": "terms of the receiver operatingcharacteristic (ROC) curve, precision recall curve (PRC) and confusion matrix.Accuracy, sensitivity, speci \ufb01city, positive predictive value (PPV), negative predictive value (NPV), and F1 score were calculated from the ROC curve according to the cut off value that maximises the Youden index(sensitivity +speci \ufb01city\u22121), and their 95% con \ufb01dence intervals (CIs) were reported. In addition, a high sensitivity (95% sensitivity) and a high speci \ufb01city (95% speci \ufb01city) th",
    "full_text_length": 56769,
    "chunk_length": 1328
  },
  {
    "chunk_id": 2590,
    "paper_filename": "ning_2022_attention_based_deep_learning_for_breast_lesions_classification_contrast_enhanced_mammography.pdf",
    "paper_title": "Ning 2022 Attention Based Deep Learning For Breast Lesions Classification Contrast Enhanced Mammography",
    "chunk_index": 15,
    "total_chunks": 58,
    "text_content": "activation mapping (Grad-CAM) Patients underwent CESM in Yantai Yuhuangding Hospital from July 2017 to May 2020 ( n = 3297) Excluded due to: Lack of pathological results ( n = 1201); Biopsy or surgery before CESM ( n = 474); Chemotherapy, radiotherapy, or hormone treatment performed before CESM ( n = 221); Tumour diameter < 5 mm ( n = 52); Non-mass lesions without delineate boundaries ( n = 156).Excluded due to: Lack of pathological results ( n = 65); Biopsy or surgery before CESM ( n = 18); Che",
    "full_text_length": 56769,
    "chunk_length": 1195
  },
  {
    "chunk_id": 2591,
    "paper_filename": "ning_2022_attention_based_deep_learning_for_breast_lesions_classification_contrast_enhanced_mammography.pdf",
    "paper_title": "Ning 2022 Attention Based Deep Learning For Breast Lesions Classification Contrast Enhanced Mammography",
    "chunk_index": 16,
    "total_chunks": 58,
    "text_content": "attention moduleRefined featureElement-wise multiplicationInternal test set (n = 100) Conv CBAMGAP FCConv Conv ConvExternal test set (n = 46)Patients enrolled in this study ( n = 46)Patients underwent CESM in Fudan University cancer center from June 2018 to May 2019 ( n = 165) Fig. 1 The study \ufb02owchart. The patient inclusion work \ufb02ow (a), an illustration of the CBAM-based convolutional neural network architecture (b), and the architectures of CBAM ( c). GAP global average pooling layer, FC fully",
    "full_text_length": 56769,
    "chunk_length": 1175
  },
  {
    "chunk_id": 2592,
    "paper_filename": "ning_2022_attention_based_deep_learning_for_breast_lesions_classification_contrast_enhanced_mammography.pdf",
    "paper_title": "Ning 2022 Attention Based Deep Learning For Breast Lesions Classification Contrast Enhanced Mammography",
    "chunk_index": 17,
    "total_chunks": 58,
    "text_content": "<0.001* 42.82 \u00b1 11.96 54.74 \u00b1 9.01 <0.001* 46.54 \u00b1 6.15 53.30 \u00b1 8.27 0.005* Diameter, cm (%) 2.11 \u00b1 1.64 2.49 \u00b1 1.22 <0.001* 2.10 \u00b1 2.26 2.39 \u00b1 0.99 0.41 1.63 \u00b1 0.67 2.63 \u00b1 0.93 <0.001* \u22641 89 (30.9) 35 (4.3) 20 (40) 3 (6) 2 (15.4) 0 1\u20132 95 (33) 301 (37.4) 13 (26) 18 (36) 9 (69.2) 8 (24.2) >2 104 (36.1) 469 (58.3) 17 (34) 29 (58) 2 (15.4) 25 (75.8)BI-RADS (%) <0.001* 0.22 0.87 3 17 (5.9) 1 (0.1) 0 0 0 0 4A 105 (36.5) 21 (2.6) 24 (48) 0 6 (46.2) 2 (6.1)4B 108 (37.5) 122 (15.2) 13 (26) 6 (12) 6 (46",
    "full_text_length": 56769,
    "chunk_length": 884
  },
  {
    "chunk_id": 2593,
    "paper_filename": "ning_2022_attention_based_deep_learning_for_breast_lesions_classification_contrast_enhanced_mammography.pdf",
    "paper_title": "Ning 2022 Attention Based Deep Learning For Breast Lesions Classification Contrast Enhanced Mammography",
    "chunk_index": 18,
    "total_chunks": 58,
    "text_content": "6 (46.2) \u2013 Adenosis 68 (23.6) \u2013 12 (24) \u2013 7 (53.8) \u2013 Intraductal papilloma 42 (14.6) \u2013 4 (8) \u2013 0 \u2013 In\ufb02ammation 9 (3.1) \u2013 2 (4) \u2013 0 \u2013 Phyllodes tumour 9 (3.1) \u2013 2 (4) \u2013 0 \u2013 Tubular adenoma 0 \u2013 1 (2) \u2013 0 \u2013 Fibrocystic disease 14 (4.9) \u2013 1 (2) \u2013 0 \u2013 Unknown/other 22 (7.6) \u2013 5 (10) \u2013 0 \u2013 Malignant lesions IDC \u2013 689 (85.59) \u2013 47 (94) \u2013 33 (100) DCIS \u2013 30 (3.73) \u2013 1 (2) \u2013 0 ILC \u2013 17 (2.11) \u2013 2 (4) \u2013 0 Papillary carcinoma \u2013 17 (2.11) \u2013 0 \u2013 0 MAC \u2013 17 (2.11) \u2013 0 \u2013 0 Unknown/other \u2013 35 (4.35) \u2013 0 \u2013 0 DCI",
    "full_text_length": 56769,
    "chunk_length": 1009
  },
  {
    "chunk_id": 2594,
    "paper_filename": "ning_2022_attention_based_deep_learning_for_breast_lesions_classification_contrast_enhanced_mammography.pdf",
    "paper_title": "Ning 2022 Attention Based Deep Learning For Breast Lesions Classification Contrast Enhanced Mammography",
    "chunk_index": 19,
    "total_chunks": 58,
    "text_content": "128:793 \u2013 804 was used to produce heat maps to visualise the most indicative regions [ 26]. P< 0.05 was statistically signi \ufb01cant. Ethics statement All procedures performed in studies involving human participants were in accordance with the ethical standards of the institutional and/or nationalresearch committee and with the 1964 Helsinki Declaration and its lateramendments or comparable ethical standards. The study was approved bythe institutional review board of Yantai Yuhuangding Hospital and",
    "full_text_length": 56769,
    "chunk_length": 1285
  },
  {
    "chunk_id": 2595,
    "paper_filename": "ning_2022_attention_based_deep_learning_for_breast_lesions_classification_contrast_enhanced_mammography.pdf",
    "paper_title": "Ning 2022 Attention Based Deep Learning For Breast Lesions Classification Contrast Enhanced Mammography",
    "chunk_index": 20,
    "total_chunks": 58,
    "text_content": "(range, 17 \u201380 years) for internal test set, and 51.39 \u00b1 8.26 years (range, 36 \u201370 years) for external test set. The patient character- istics of the three sets are shown in Table 1. Segmentation similarity The DSC between Segmenters 1 and 2 was 0.87. The DSC between Segmenters 1 and 3 was 0.88. The DSC between Segmenters 2 and 3 was 0.89. The average DSC across all segmenters was 0.90 inbenign lesions and 0.87 in malignant lesions. Performance of deep learning models All CNN models exhibited go",
    "full_text_length": 56769,
    "chunk_length": 1214
  },
  {
    "chunk_id": 2596,
    "paper_filename": "ning_2022_attention_based_deep_learning_for_breast_lesions_classification_contrast_enhanced_mammography.pdf",
    "paper_title": "Ning 2022 Attention Based Deep Learning For Breast Lesions Classification Contrast Enhanced Mammography",
    "chunk_index": 21,
    "total_chunks": 58,
    "text_content": "model, 0.918 for the CBAM-based Dense- Net model, and 0.876 for the CBAM-based ResNet model onexternal test set (Fig. 2). The performance comparison of different CNN models is shown in Fig. 2and Table 2. The confusion matrices and the PRCs also indicated the favourable predictive perfor- mance of the CNN models (Fig. 3and S3). The best-performing deep learning model, that is, CBAM-based Xception, achieved an AUC of 0.912, an area under the PRC (AUPRC) of 0.923, a F1 score of 0.835, an accuracy o",
    "full_text_length": 56769,
    "chunk_length": 1196
  },
  {
    "chunk_id": 2597,
    "paper_filename": "ning_2022_attention_based_deep_learning_for_breast_lesions_classification_contrast_enhanced_mammography.pdf",
    "paper_title": "Ning 2022 Attention Based Deep Learning For Breast Lesions Classification Contrast Enhanced Mammography",
    "chunk_index": 22,
    "total_chunks": 58,
    "text_content": "high- speci \ufb01city threshold, and cutoff point maximising the Youden index were calculated as 0.60, 0.97, and 0.95, respectively. Theperformance of CBAM-based Xception model for high-sensitivityand high-speci \ufb01city thresholds is shown in Table 3. The high- sensitivity threshold of the model could prevent patients with benign breast lesions from undergoing unnecessary biopsy on thepremise of avoiding missed diagnosis, with speci \ufb01city of 0.620 and NPV of 0.939 in the internal test set, and speci \ufb01",
    "full_text_length": 56769,
    "chunk_length": 1258
  },
  {
    "chunk_id": 2598,
    "paper_filename": "ning_2022_attention_based_deep_learning_for_breast_lesions_classification_contrast_enhanced_mammography.pdf",
    "paper_title": "Ning 2022 Attention Based Deep Learning For Breast Lesions Classification Contrast Enhanced Mammography",
    "chunk_index": 23,
    "total_chunks": 58,
    "text_content": "obtained by the CBAM- based Xception model from the CESM images visualised the most indicative regions. The red and yellow regionshave higher predictive signi \ufb01cance than the green and blue regions, indicating that CNN focuses on the most predictive information for distinguishing benign from malignant breast lesions. Moreover, we analysed the failure cases misdiagnosed by the CBAM-based Xception model, and ideally in comparison with radiologists. In some cases, glandular structures overlapped on",
    "full_text_length": 56769,
    "chunk_length": 1312
  },
  {
    "chunk_id": 2599,
    "paper_filename": "ning_2022_attention_based_deep_learning_for_breast_lesions_classification_contrast_enhanced_mammography.pdf",
    "paper_title": "Ning 2022 Attention Based Deep Learning For Breast Lesions Classification Contrast Enhanced Mammography",
    "chunk_index": 24,
    "total_chunks": 58,
    "text_content": "of 0.793 and 0.748 in the internal and external test sets, respectively (Table 4). The ROC curve of the best-performing model, that is, CBAM-based Xception, is shown in Fig. 5. The speci \ufb01city and sensitivity points of the radiologists on the test sets were plotted in the ROC curve. The performance of the radiologists lies below that of our proposed model in the sameROC curve. On CESM images, the attention-based CNN performed best and can effectively predict benign from malignant breast lesions.",
    "full_text_length": 56769,
    "chunk_length": 1240
  },
  {
    "chunk_id": 2600,
    "paper_filename": "ning_2022_attention_based_deep_learning_for_breast_lesions_classification_contrast_enhanced_mammography.pdf",
    "paper_title": "Ning 2022 Attention Based Deep Learning For Breast Lesions Classification Contrast Enhanced Mammography",
    "chunk_index": 25,
    "total_chunks": 58,
    "text_content": "0.858 ResNel+CBAM AUC = 0.876 DenseNet+CBAM AUC = 0.918Xception+CBAM AUC = 0.970ResNet AUC = 0.816 DenseNet AUC = 0.884 Xception AUC = 0.850 ResNet+CBAM AUC = 0.872 DenseNet+CBAM AUC = 0.934 Xception+CBAM AUC = 0.912ResNet AUC = 0.813 DenseNet AUC = 0.852 Xception AUC = 0.876 ResNet+CBAM AUC = 0.852 DenseNet+CBAM AUC = 0.981 Xception+CBAM AUC = 0.938 1 \u2013 SpecificitySensitivity 0.80.8 1.0 0.0 0.2 0.4 0.6 1 \u2013 Specificity0.8 1.0 0.0 0.2 0.4 0.6 1 \u2013 Specificity0.8 1.01.0abc 0.00.20.40.6Sensitivity0.",
    "full_text_length": 56769,
    "chunk_length": 1343
  },
  {
    "chunk_id": 2601,
    "paper_filename": "ning_2022_attention_based_deep_learning_for_breast_lesions_classification_contrast_enhanced_mammography.pdf",
    "paper_title": "Ning 2022 Attention Based Deep Learning For Breast Lesions Classification Contrast Enhanced Mammography",
    "chunk_index": 26,
    "total_chunks": 58,
    "text_content": "ResNet +CBAM DenseNet +CBAM Xception +CBAM Training and validation sets AUC (95% CI) 0.711 (0.681 \u20130.743) 0.813 (0.784 \u20130.842) 0.852 (0.828 \u20130.877) 0.876 (0.852 \u20130.899) 0.852 (0.828 \u20130.877) 0.981 (0.974 \u20130.987) 0.938 (0.925 \u20130.952) ACC (95% CI) 0.689 (0.661 \u20130.716) 0.738 (0.711 \u20130.764) 0.762 (0.736 \u20130.787) 0.780 (0.754 \u20130.804) 0.762 (0.736 \u20130.787) 0.931 (0.914 \u20130.945) 0.871 (0.850 \u20130.890) SENS (95% CI) 0.718 (0.685 \u20130.749) 0.717 (0.684 \u20130.747) 0.743 (0.711 \u20130.772) 0.758 (0.726 \u20130.787) 0.743 (0.7",
    "full_text_length": 56769,
    "chunk_length": 1328
  },
  {
    "chunk_id": 2602,
    "paper_filename": "ning_2022_attention_based_deep_learning_for_breast_lesions_classification_contrast_enhanced_mammography.pdf",
    "paper_title": "Ning 2022 Attention Based Deep Learning For Breast Lesions Classification Contrast Enhanced Mammography",
    "chunk_index": 27,
    "total_chunks": 58,
    "text_content": "\u20130.858) 0.705 (0.654 \u20130.751) F1 score 0.773 0.801 0.821 0.835 0.821 0.951 0.908 Pvalue \u2013 <0.0001 <0.0001 <0.0001 <0.0001 <0.0001 <0.0001 Internal test set AUC (95% CI) 0.795 (0.719 \u20130.868) 0.816 (0.731 \u20130.901) 0.884 (0.815 \u20130.953) 0.850 (0.774 \u20130.926) 0.872 (0.805 \u20130.940) 0.934 (0.890 \u20130.979) 0.912 (0.857 \u20130.968) ACC (95% CI) 0.750 (0.653 \u20130.831) 0.770 (0.675 \u20130.848) 0.786 (0.775 \u20130.797) 0.800 (0.708 \u20130.873) 0.810 (0.719 \u20130.882) 0.860 (0.776 \u20130.921) 0.850 (0.765 \u20130.914) SENS (95% CI) 0.880 (0.75",
    "full_text_length": 56769,
    "chunk_length": 1306
  },
  {
    "chunk_id": 2603,
    "paper_filename": "ning_2022_attention_based_deep_learning_for_breast_lesions_classification_contrast_enhanced_mammography.pdf",
    "paper_title": "Ning 2022 Attention Based Deep Learning For Breast Lesions Classification Contrast Enhanced Mammography",
    "chunk_index": 28,
    "total_chunks": 58,
    "text_content": "0.708 (0.580 \u20130.811) 0.886 (0.659 \u20130.895) 0.800 (0.659 \u20130.895) 0.763 (0.631 \u20130.860) 0.821 (0.692 \u20130.907) 0.797 (0.668 \u20130.886) F1 score 0.779 0.729 0.849 0.800 0.791 0.851 0.835 Pvalue \u2013 0.746 0.115 0.365 0.188 0.006 0.029 External test set AUC (95% CI) 0.825 (0.708 \u20130.909) 0.834 (0.717 \u20130.952) 0.860 (0.754 \u20130.967) 0.858 (0.751 \u20130.965) 0.876 (0.764 \u20130.989) 0.918 (0.835 \u20131.000) 0.970 (0.929 \u20131.000) ACC (95% CI) 0.717 (0.565 \u20130.840) 0.696 (0.543 \u20130.823) 0.826 (0.686 \u20130.922) 0.804 (0.661 \u20130.906) 0.8",
    "full_text_length": 56769,
    "chunk_length": 1301
  },
  {
    "chunk_id": 2604,
    "paper_filename": "ning_2022_attention_based_deep_learning_for_breast_lesions_classification_contrast_enhanced_mammography.pdf",
    "paper_title": "Ning 2022 Attention Based Deep Learning For Breast Lesions Classification Contrast Enhanced Mammography",
    "chunk_index": 29,
    "total_chunks": 58,
    "text_content": "(0.758 \u20130.988) 0.901 (0.745 \u20130.976) 1.000 (0.850 \u20131.000) NPV (95% CI) 0.500 (0.304 \u20130.696) 0.481 (0.292 \u20130.676) 0.647 (0.386 \u20130.847) 0.591 (0.367 \u20130.785) 0.647 (0.386 \u20130.847) 0.769 (0.460 \u20130.938) 0.722 (0.464 \u20130.893) F1 score 0.755 0.731 0.871 0.842 0.871 0.909 0.918 Pvalue \u2013 0.913 0.668 0.690 0.540 0.211 0.027 CNN convolutional neural network, DTdecision tree, CBAM convolutional block attention module, CIcon\ufb01dence interval, AUC area under the curve, ACC accuracy, SENS sensitivity, SPEC speci \ufb01c",
    "full_text_length": 56769,
    "chunk_length": 1298
  },
  {
    "chunk_id": 2605,
    "paper_filename": "ning_2022_attention_based_deep_learning_for_breast_lesions_classification_contrast_enhanced_mammography.pdf",
    "paper_title": "Ning 2022 Attention Based Deep Learning For Breast Lesions Classification Contrast Enhanced Mammography",
    "chunk_index": 30,
    "total_chunks": 58,
    "text_content": "presented in Fig. 5and Table 5. For Radiologist 1, CBAM-based Xception model assistance led to increase in accuracy from 0.780 to 0.890 ( P=0.005) on the internal test set and from 0.804 to 0.891 ( P=0.134) on the external test set; and in speci \ufb01city from 0.680 to 0.920 ( P=0.005) and from 0.692 to 0.923 (P=0.083), respectively. For Radiologist 2, CBAM-based Xception model assistance led to increase in accuracy from 0.800 to 0.900(P=0.004) on the internal test set and from 0.847 to 0.891 (P=0.4",
    "full_text_length": 56769,
    "chunk_length": 1251
  },
  {
    "chunk_id": 2606,
    "paper_filename": "ning_2022_attention_based_deep_learning_for_breast_lesions_classification_contrast_enhanced_mammography.pdf",
    "paper_title": "Ning 2022 Attention Based Deep Learning For Breast Lesions Classification Contrast Enhanced Mammography",
    "chunk_index": 31,
    "total_chunks": 58,
    "text_content": "and external test sets,respectively (Table 5). The results showed that the CBAM-based Xception model could effectively reduce inter-rater variability. BI-RADS 4 subgroup analysis According to the American College of Radiology, BI-RADS 4 category is used for \ufb01ndings that have \u22652% but <95% chance of malignancy and for which biopsy is recommended [ 27]. Using CESM images, attention-based CNN model can effectively predictbenign from malignant breast lesions in the BI-RADS 4 subgroup. The CBAM-based ",
    "full_text_length": 56769,
    "chunk_length": 1336
  },
  {
    "chunk_id": 2607,
    "paper_filename": "ning_2022_attention_based_deep_learning_for_breast_lesions_classification_contrast_enhanced_mammography.pdf",
    "paper_title": "Ning 2022 Attention Based Deep Learning For Breast Lesions Classification Contrast Enhanced Mammography",
    "chunk_index": 32,
    "total_chunks": 58,
    "text_content": "\u20130.998) 0.769 (0.460 \u20130.938) 0.914 (0.758 \u20130.978) 0.909 (0.571 \u20130.995) 0.939 High speci \ufb01city Internal test set 0.912 (0.857 \u20130.968) 0.830 (0.742 \u20130.898) 0.700 (0.552 \u20130.817) 0.940 (0.825 \u20130.984) 0.923 (0.780 \u20130.980) 0.770 (0.642 \u20130.865) 0.795 External test set 0.970 (0.929 \u20131.000) 0.891 (0.764 \u20130.964) 0.848 (0.673 \u20130.943) 1.000 (0.717 \u20131.000) 1.000 (0.850 \u20131.000) 0.722 (0.464 \u20130.893) 0.918 AUC area under the curve, ACC accuracy, SENS sensitivity, SPEC speci \ufb01city, PPVpositive predictive value, ",
    "full_text_length": 56769,
    "chunk_length": 1225
  },
  {
    "chunk_id": 2608,
    "paper_filename": "ning_2022_attention_based_deep_learning_for_breast_lesions_classification_contrast_enhanced_mammography.pdf",
    "paper_title": "Ning 2022 Attention Based Deep Learning For Breast Lesions Classification Contrast Enhanced Mammography",
    "chunk_index": 33,
    "total_chunks": 58,
    "text_content": "100 700 600 500 400300 100200 600 500 400 300 1002001Ground truth0 1Ground truth0 1 Ground truth0 1Ground truth0 1Ground truth0 1 Ground truth0 1Ground truth0 1Ground truth0 101 Predict01 Predict01 Predict01Predict01 Predict01 Predict01Predict01 Predict01DenseNet+CBAM Xception+CBAM Fig. 3 The confusion matrices for CBAM-based CNN models across the three sets. CBAM convolutional block attention module, CNN convolutional neural network.N. Mao et al. 799 British Journal of Cancer (2023) 128:793 \u2013 8",
    "full_text_length": 56769,
    "chunk_length": 1339
  },
  {
    "chunk_id": 2609,
    "paper_filename": "ning_2022_attention_based_deep_learning_for_breast_lesions_classification_contrast_enhanced_mammography.pdf",
    "paper_title": "Ning 2022 Attention Based Deep Learning For Breast Lesions Classification Contrast Enhanced Mammography",
    "chunk_index": 34,
    "total_chunks": 58,
    "text_content": "AUCs of 0.927on the internal test set and 0.940 on the external test set. The performance is shown in Fig. 6, Fig. S5, and Table S3. DISCUSSION In this paper, we present an attention-based deep learning model for distinguishing benign from malignant breast lesions based onCESM with a satisfactory performance. The best-performing model, that is, the CBAM-based Xception model, yielded an AUC of 0.970 on the external test set. In addition, this model was testedon multicentre data, suggesting the st",
    "full_text_length": 56769,
    "chunk_length": 1377
  },
  {
    "chunk_id": 2610,
    "paper_filename": "ning_2022_attention_based_deep_learning_for_breast_lesions_classification_contrast_enhanced_mammography.pdf",
    "paper_title": "Ning 2022 Attention Based Deep Learning For Breast Lesions Classification Contrast Enhanced Mammography",
    "chunk_index": 35,
    "total_chunks": 58,
    "text_content": "study on CESM. In recent years, arti \ufb01cial intelligence has made substantial progress in medical decision support. Several studies developedsome radiomics models based on traditional machine learning, such as LR, KNN, and SVM, to preoperatively discriminate benign and malignant breast lesions [ 28\u201336]. However, these radiomic features were hand-craft and de \ufb01ned based on experience. In addition, traditional machine learning method may not improve the model performance further with the increase o",
    "full_text_length": 56769,
    "chunk_length": 1358
  },
  {
    "chunk_id": 2611,
    "paper_filename": "ning_2022_attention_based_deep_learning_for_breast_lesions_classification_contrast_enhanced_mammography.pdf",
    "paper_title": "Ning 2022 Attention Based Deep Learning For Breast Lesions Classification Contrast Enhanced Mammography",
    "chunk_index": 36,
    "total_chunks": 58,
    "text_content": "frommalignant breast lesions on multiparametric MRIs. Their resultsshow that the combination of deep learning and clinical information has a signi \ufb01cant contribution to the differentiation of benign from malignant breast lesions on breast MRIs with anAUC of 0.88. These studies are limited by the single centre and thesmall sample size. In the present study, our deep learning model abcd e Fig. 4 CESM images and heat maps of three breast lesions. The heat maps obtained by the deep learning from the",
    "full_text_length": 56769,
    "chunk_length": 1299
  },
  {
    "chunk_id": 2612,
    "paper_filename": "ning_2022_attention_based_deep_learning_for_breast_lesions_classification_contrast_enhanced_mammography.pdf",
    "paper_title": "Ning 2022 Attention Based Deep Learning For Breast Lesions Classification Contrast Enhanced Mammography",
    "chunk_index": 37,
    "total_chunks": 58,
    "text_content": "learning model and two radiologists correctly predicted the benign lesion. cImages in a 48-year-old woman with intraductal papillomas. CESM images shows a 1.2-cm mass, BI-RADS 4B.The deep learning model correctly predicted the benign lesion, whereas two radiologists made the wrong prediction. dImages in a 47-year-old woman with breast tubular adenoma. CESM images shows a 2.7-cm mass, BI-RADS 4B.The deep learning model erroneously predicted the malignancy lesion, whereas two radiologists made the",
    "full_text_length": 56769,
    "chunk_length": 1342
  },
  {
    "chunk_id": 2613,
    "paper_filename": "ning_2022_attention_based_deep_learning_for_breast_lesions_classification_contrast_enhanced_mammography.pdf",
    "paper_title": "Ning 2022 Attention Based Deep Learning For Breast Lesions Classification Contrast Enhanced Mammography",
    "chunk_index": 38,
    "total_chunks": 58,
    "text_content": "studies on the classi \ufb01cation of benign and malignant breast lesions in Table S4. Our study has several differences from previous studies. First, we selected CESM instead of MRI or ultrasound. Patel et al. [ 10] were the \ufb01rst to apply radiomics on CESM and develop a computer-aided diagnostic tool to test whether false positives inbreast cancer screening can be reduced. The SVM-based radiomicsmodel has higher speci \ufb01city and accuracy than the prediction performance of the radiologists, with an ov",
    "full_text_length": 56769,
    "chunk_length": 1320
  },
  {
    "chunk_id": 2614,
    "paper_filename": "ning_2022_attention_based_deep_learning_for_breast_lesions_classification_contrast_enhanced_mammography.pdf",
    "paper_title": "Ning 2022 Attention Based Deep Learning For Breast Lesions Classification Contrast Enhanced Mammography",
    "chunk_index": 39,
    "total_chunks": 58,
    "text_content": "published studies and tested the generalisation ability of our model on multicentre data. Second, we incorporated the attention module into different deep learning networks to increase the effective weight features,thereby reducing the invalid weight features and improving theperformance of the networks. Previous studies showed that squeeze-and-excitation networks (SENet) with a channel attention module can improve the predictive performance of breastimaging [ 43]. In the present study, the CBAM",
    "full_text_length": 56769,
    "chunk_length": 1387
  },
  {
    "chunk_id": 2615,
    "paper_filename": "ning_2022_attention_based_deep_learning_for_breast_lesions_classification_contrast_enhanced_mammography.pdf",
    "paper_title": "Ning 2022 Attention Based Deep Learning For Breast Lesions Classification Contrast Enhanced Mammography",
    "chunk_index": 40,
    "total_chunks": 58,
    "text_content": "with two radiologists. Performance metric Xception +CBAM R 1 Pvalue R 2 Pvalue Internal test set ACC (95% CI) 0.850 (0.765 \u20130.914) 0.780 (0.686 \u20130.857) 0.324 0.800 (0.708 \u20130.873) 0.499 SENS (95% CI) 0.760 (0.615 \u20130.865) 0.880 (0.760 \u20130.950) 0.157 0.880 (0.760 \u20130.950) 0.157 SPEC (95% CI) 0.940 (0.825 \u20130.984) 0.680 (0.532 \u20130.801) 0.003 0.720 (0.573 \u20130.833) 0.008 PPV (95% CI) 0.927 (0.790 \u20130.981) 0.733 (0.601 \u20130.835) 0.001 0.759 (0.625 \u20130.857) 0.015 NPV (95% CI) 0.797 (0.668 \u20130.886) 0.850 (0.695 \u20130",
    "full_text_length": 56769,
    "chunk_length": 1247
  },
  {
    "chunk_id": 2616,
    "paper_filename": "ning_2022_attention_based_deep_learning_for_breast_lesions_classification_contrast_enhanced_mammography.pdf",
    "paper_title": "Ning 2022 Attention Based Deep Learning For Breast Lesions Classification Contrast Enhanced Mammography",
    "chunk_index": 41,
    "total_chunks": 58,
    "text_content": "\u20130.959) 0.046 0.906 (0.738 \u20130.975) 0.083 NPV (95% CI) 0.722 (0.464 \u20130.893) 0.643 (0.356 \u20130.860) 0.589 0.714 (0.420 \u20130.904) 0.955 F1 score 0.918 0.861 \u2013 0.892 \u2013 Cohen \u2019s kappa 0.748 CNN convolutional neural network, CBAM convolutional block attention module, Rradiologist, AUC area under the curve, ACC accuracy, SENS sensitivity, SPEC speci \ufb01city, PPVpositive predictive value, NPV negative predictive value, CIcon\ufb01dence interval. 0.80.8 0.60.6 0.40.4 0.20.2 0.00.01.0ab 0.8 0.6 0.40.2 0.01.0 1.0 Sen",
    "full_text_length": 56769,
    "chunk_length": 1313
  },
  {
    "chunk_id": 2617,
    "paper_filename": "ning_2022_attention_based_deep_learning_for_breast_lesions_classification_contrast_enhanced_mammography.pdf",
    "paper_title": "Ning 2022 Attention Based Deep Learning For Breast Lesions Classification Contrast Enhanced Mammography",
    "chunk_index": 42,
    "total_chunks": 58,
    "text_content": "804 Table 5. Radiologists \u2019performance with deep learning assistance. Performance metric R 1 with DL Pvalue R 2 with DL Pvalue Internal test set ACC (95% CI) 0.890 (0.812 \u20130.944) 0.005 0.900 (0.824 \u20130.951) 0.004 SENS (95% CI) 0.860 (0.726 \u20130.937) 0.317 0.900 (0.774 \u20130.962) 0.317 SPEC (95% CI) 0.920 (0.799 \u20130.974) 0.0005 0.900 (0.774 \u20130.962) 0.003 PPV (95% CI) 0.915 (0.787 \u20130.972) 0.0006 0.900 (0.774 \u20130.962) 0.002 NPV (95% CI) 0.868 (0.740 \u20130.941) 0.449 0.900 (0.774 \u20130.962) 0.083 F1 score 0.887 \u2013",
    "full_text_length": 56769,
    "chunk_length": 1201
  },
  {
    "chunk_id": 2618,
    "paper_filename": "ning_2022_attention_based_deep_learning_for_breast_lesions_classification_contrast_enhanced_mammography.pdf",
    "paper_title": "Ning 2022 Attention Based Deep Learning For Breast Lesions Classification Contrast Enhanced Mammography",
    "chunk_index": 43,
    "total_chunks": 58,
    "text_content": "\u2013 0.921 \u2013 Cohen \u2019s kappa 0.808 RRadiologist, DLdeep learning, AUC areas under the curve, ACC accuracy, SENS sensitivity, SPEC speci \ufb01city, PPV positive predictive value, NPV negative predictive value, CIcon\ufb01dence interval. 0.8 0.6 0.4 0.2 0.0 1.0 1 \u2013 Specificity0.8 0.6 0.4 0.2 0.0 1.0 1 \u2013 Specificity 0.8 0.6 0.4 0.2 0.0 1.0 1 \u2013 Specificity0.8 0.6 0.4 0.2 0.0 1.0 1 \u2013 SpecificityBI-RADS 4 AUC = 0.906Lesion diameter <= 2 AUC = 0.887 Lesion diameter > 2 AUC = 0.927 Lesion diameter <= 2 AUC = 0.977 L",
    "full_text_length": 56769,
    "chunk_length": 1241
  },
  {
    "chunk_id": 2619,
    "paper_filename": "ning_2022_attention_based_deep_learning_for_breast_lesions_classification_contrast_enhanced_mammography.pdf",
    "paper_title": "Ning 2022 Attention Based Deep Learning For Breast Lesions Classification Contrast Enhanced Mammography",
    "chunk_index": 44,
    "total_chunks": 58,
    "text_content": "of CBAM-based Xception model in external test set. BI-RADS Breast Imaging Reporting and Data System, CBAM convolutional block attention module.N. Mao et al. 802 British Journal of Cancer (2023) 128:793 \u2013 804 radiomics model and radiologists and investigated the radiolo- gist \u2019s performance with deep learning assistance, which few previous studies have tried. The deep learning model performed better than the conventional radiomics model and the radiolo-gists, displaying clinical application prosp",
    "full_text_length": 56769,
    "chunk_length": 1404
  },
  {
    "chunk_id": 2620,
    "paper_filename": "ning_2022_attention_based_deep_learning_for_breast_lesions_classification_contrast_enhanced_mammography.pdf",
    "paper_title": "Ning 2022 Attention Based Deep Learning For Breast Lesions Classification Contrast Enhanced Mammography",
    "chunk_index": 45,
    "total_chunks": 58,
    "text_content": "method is without human intervention from imagine segmentation to prediction model construction, which can improve the repeatability of research andpracticability of the model. We did not \ufb01nd any literature reporting on the automatic segmentation of breast lesion on CESM. Third, this study only enrolled the largest lesion of patients with multiplemasses, and the model was not applied to breast lesions smallerthan 5 mm. Future studies will try to incorporate all lesions as possible. Lastly, only ",
    "full_text_length": 56769,
    "chunk_length": 1374
  },
  {
    "chunk_id": 2621,
    "paper_filename": "ning_2022_attention_based_deep_learning_for_breast_lesions_classification_contrast_enhanced_mammography.pdf",
    "paper_title": "Ning 2022 Attention Based Deep Learning For Breast Lesions Classification Contrast Enhanced Mammography",
    "chunk_index": 46,
    "total_chunks": 58,
    "text_content": "countries must beused and a fully end-to-end deep learning model should be performed to improve the performance and for high-level evidence of clinical application in future work. DATA AVAILABILITY Some or all data, models, or code generated or used during the study are available from the corresponding author by request. REFERENCES 1. DeSantis CE, Ma J, Gaudet MM, Newman LA, Miller KD, Goding Sauer A, et al. Breast cancer statistics, 2019. CA Cancer J Clin. 2019;69:438 \u201351. 2. Pace LE, Keating N",
    "full_text_length": 56769,
    "chunk_length": 1243
  },
  {
    "chunk_id": 2622,
    "paper_filename": "ning_2022_attention_based_deep_learning_for_breast_lesions_classification_contrast_enhanced_mammography.pdf",
    "paper_title": "Ning 2022 Attention Based Deep Learning For Breast Lesions Classification Contrast Enhanced Mammography",
    "chunk_index": 47,
    "total_chunks": 58,
    "text_content": "cancer: comparison of contrast-enhanced spectralmammography and breast MR imaging in the evaluation of extent of disease. Radiology. 2017;285:389 \u2013400. 5. Mao N, Yin P, Li Q, Wang Q, Liu M, Ma H, et al. Radiomics nomogram of contrast- enhanced spectral mammography for prediction of axillary lymph node metas-tasis in breast cancer: a multicenter study. Eur Radiol. 2020;30:6732 \u20139. 6. Mao N, Yin P, Wang Q, Liu M, Dong J, Zhang X, et al. Added value of radiomics on mammography for breast cancer dia",
    "full_text_length": 56769,
    "chunk_length": 1223
  },
  {
    "chunk_id": 2623,
    "paper_filename": "ning_2022_attention_based_deep_learning_for_breast_lesions_classification_contrast_enhanced_mammography.pdf",
    "paper_title": "Ning 2022 Attention Based Deep Learning For Breast Lesions Classification Contrast Enhanced Mammography",
    "chunk_index": 48,
    "total_chunks": 58,
    "text_content": "prediction of pathologic complete response to neoadjuvant che- motherapy in breast cancer: a multicenter study. Clin Cancer Res. 2019;25:3538 \u201347. 9. Li H, Zhu Y, Burnside ES, Drukker K, Hoadley KA, Fan C, et al. MR imaging radiomics signatures for predicting the risk of breast cancer recurrence as givenby research versions of MammaPrint, Oncotype DX, and PAM50 gene assays.Radiology. 2016;281:382 \u201391.10. Patel BK, Ranjbar S, Wu T, Pockaj BA, Li J, Zhang N, et al. Computer-aided diagnosis of cont",
    "full_text_length": 56769,
    "chunk_length": 1267
  },
  {
    "chunk_id": 2624,
    "paper_filename": "ning_2022_attention_based_deep_learning_for_breast_lesions_classification_contrast_enhanced_mammography.pdf",
    "paper_title": "Ning 2022 Attention Based Deep Learning For Breast Lesions Classification Contrast Enhanced Mammography",
    "chunk_index": 49,
    "total_chunks": 58,
    "text_content": "breast MRI. Radiology. 2019;290:290 \u20137. 13. Zheng X, Yao Z, Huang Y, Yu Y, Wang Y, Liu Y, et al. Deep learning radiomics can predict axillary lymph node status in early-stage breast cancer. Nat Commun. 2020;11:1236. 14. Jiang M, Li CL, Luo XM, Chuan ZR, Lv WZ, Li X, et al. Ultrasound-based deep learning radiomics in the assessment of pathological complete response toneoadjuvant chemotherapy in locally advanced breast cancer. Eur J Cancer.2021;147:95 \u2013105. 15. Perek S, Kiryati N, Zimmerman-Moreno",
    "full_text_length": 56769,
    "chunk_length": 1246
  },
  {
    "chunk_id": 2625,
    "paper_filename": "ning_2022_attention_based_deep_learning_for_breast_lesions_classification_contrast_enhanced_mammography.pdf",
    "paper_title": "Ning 2022 Attention Based Deep Learning For Breast Lesions Classification Contrast Enhanced Mammography",
    "chunk_index": 50,
    "total_chunks": 58,
    "text_content": "Y. Triple attention learning for clas- si\ufb01cation of 14 thoracic diseases using chest radiography. Med Image Anal. 2021;67:101846. 18. Zhang R, Duan H, Cheng J, Zheng Y. A study on tuberculosis classi \ufb01cation in chest X-ray using deep residual attention networks. Annu Int Conf IEEE Eng Med BiolSoc. 2020;2020:1552 \u20135. 19. Woo S, Park J, Lee J-Y, Kweon IS. CBAM: convolutional block attention module. arXiv:1807.06521v2 [Preprint]. 2018 [cited 2018 Jul 18]: [17 p]. Available from: https://arxiv.org/a",
    "full_text_length": 56769,
    "chunk_length": 1391
  },
  {
    "chunk_id": 2626,
    "paper_filename": "ning_2022_attention_based_deep_learning_for_breast_lesions_classification_contrast_enhanced_mammography.pdf",
    "paper_title": "Ning 2022 Attention Based Deep Learning For Breast Lesions Classification Contrast Enhanced Mammography",
    "chunk_index": 51,
    "total_chunks": 58,
    "text_content": "p]. Available from:https://arxiv.org/abs/1610.02357 23. He K, Zhang X, Ren S, Sun J. Deep residual learning for image recognition. arXiv:1512.03385v1 [Preprint]. 2015 [cited 2015 Dec 10]: [12 p]. Available from: https://arxiv.org/abs/1512.03385 24. Russakovsky O, Deng J, Su H, Krause J, Satheesh S, Ma S, et al. ImageNet large scale visual recognition challenge. Int J Comput Vis. 2015;115:211 \u201352. 25. Jiang Y, Zhang Z, Yuan Q, Wang W, Wang H, Li T, et al. Predicting peritoneal recurrence and dise",
    "full_text_length": 56769,
    "chunk_length": 1374
  },
  {
    "chunk_id": 2627,
    "paper_filename": "ning_2022_attention_based_deep_learning_for_breast_lesions_classification_contrast_enhanced_mammography.pdf",
    "paper_title": "Ning 2022 Attention Based Deep Learning For Breast Lesions Classification Contrast Enhanced Mammography",
    "chunk_index": 52,
    "total_chunks": 58,
    "text_content": "RADS/BIRADS_CEM_2022.pdf 28. Whitney HM, Taylor NS, Drukker K, Edwards AV, Papaioannou J, Schacht D, et al. Additive bene \ufb01t of radiomics over size alone in the distinction between benign lesions and luminal a cancers on a large clinical breast MRI dataset. Acad Radiol.2019;26:202 \u20139. 29. Parekh VS, Jacobs MA. Integrated radiomic framework for breast cancer and tumor biology using advanced machine learning and multiparametric MRI. NPJ Breast Cancer. 2017;3:43. 30. Bickelhaupt S, Jaeger PF, Laun ",
    "full_text_length": 56769,
    "chunk_length": 1272
  },
  {
    "chunk_id": 2628,
    "paper_filename": "ning_2022_attention_based_deep_learning_for_breast_lesions_classification_contrast_enhanced_mammography.pdf",
    "paper_title": "Ning 2022 Attention Based Deep Learning For Breast Lesions Classification Contrast Enhanced Mammography",
    "chunk_index": 53,
    "total_chunks": 58,
    "text_content": "D, et al. Breast DCE-MRI radiomics: a robust computer-aided system based on reproducible BI-RADS features acrossthe in \ufb02uence of datasets bias and segmentation methods. Int J Comput Assist Radiol Surg. 2020;15:921 \u201330. 33. Zhao S, Zhang X, Zhong H, Qin Y, Li Y, Song B, et al. Background parenchymal enhancement on contrast-enhanced spectral mammography: in \ufb02uence of age,N. Mao et al. 803 British Journal of Cancer (2023) 128:793 \u2013 804 breast density, menstruation status, and menstrual cycle timing",
    "full_text_length": 56769,
    "chunk_length": 1279
  },
  {
    "chunk_id": 2629,
    "paper_filename": "ning_2022_attention_based_deep_learning_for_breast_lesions_classification_contrast_enhanced_mammography.pdf",
    "paper_title": "Ning 2022 Attention Based Deep Learning For Breast Lesions Classification Contrast Enhanced Mammography",
    "chunk_index": 54,
    "total_chunks": 58,
    "text_content": "A, Parmar C, Quackenbush J, Schwartz LH, Aerts H. Arti \ufb01cial intelligence in radiology. Nat Rev Cancer. 2018;18:500 \u201310. 37. Becker AS, Marcon M, Ghafoor S, Wurnig MC, Frauenfelder T, Boss A. Deep learning in mammography: diagnostic accuracy of a multipurpose image analysissoftware in the detection of breast cancer. Invest Radiol. 2017;52:434 \u201340. 38. Yala A, Lehman C, Schuster T, Portnoi T, Barzilay R. A deep learning mammography-based model for improved breast cancer risk prediction. Radi-olog",
    "full_text_length": 56769,
    "chunk_length": 1275
  },
  {
    "chunk_id": 2630,
    "paper_filename": "ning_2022_attention_based_deep_learning_for_breast_lesions_classification_contrast_enhanced_mammography.pdf",
    "paper_title": "Ning 2022 Attention Based Deep Learning For Breast Lesions Classification Contrast Enhanced Mammography",
    "chunk_index": 55,
    "total_chunks": 58,
    "text_content": "SM, Sieniek M, Godbole V, Godwin J, Antropova N, Ashra \ufb01an H, et al. International evaluation of an AI system for breast cancer screening. Nature. 2020;577:89 \u201394. 42. Dalmis MU, Gubern-Merida A, Vreemann S, Bult P, Karssemeijer N, Mann R, et al. Arti\ufb01cial intelligence-based classi \ufb01cation of breast lesions imaged with a multi- parametric breast MRI protocol with Ultrafast DCE-MRI, T2, and DWI. Invest Radiol.2019;54:325 \u201332. 43. Qian X, Pei J, Zheng H, Xie X, Yan L, Zhang H, et al. Prospective a",
    "full_text_length": 56769,
    "chunk_length": 1319
  },
  {
    "chunk_id": 2631,
    "paper_filename": "ning_2022_attention_based_deep_learning_for_breast_lesions_classification_contrast_enhanced_mammography.pdf",
    "paper_title": "Ning 2022 Attention Based Deep Learning For Breast Lesions Classification Contrast Enhanced Mammography",
    "chunk_index": 56,
    "total_chunks": 58,
    "text_content": "and design: all authors. (3) Literature research: all authors. (4) Clinical studies: all authors.(5) Experimental studies/data analysis: all authors. (6) Statistical analysis: HZ. (7) Manuscript preparation: all authors. (8) Manuscript editing: all authors. FUNDING The study was supported by the National Natural Science Foundation of China(82001775 and 62176140), the Natural Science Foundation of Shandong Province ofChina (ZR2021MH120 and ZR2022MH274), the Special Fund for Breast DiseaseResearch",
    "full_text_length": 56769,
    "chunk_length": 1560
  },
  {
    "chunk_id": 2632,
    "paper_filename": "ning_2022_attention_based_deep_learning_for_breast_lesions_classification_contrast_enhanced_mammography.pdf",
    "paper_title": "Ning 2022 Attention Based Deep Learning For Breast Lesions Classification Contrast Enhanced Mammography",
    "chunk_index": 57,
    "total_chunks": 58,
    "text_content": "remains neutral with regard to jurisdictional claims in published maps and institutional af \ufb01liations. Springer Nature or its licensor (e.g. a society or other partner) holds exclusive rights to this article under a publishing agreement with the author(s) or other rightsholder(s); author self-archiving of the accepted manuscript version of this article is solely governed by the terms of such publishing agreement and applicable law.N. Mao et al. 804 British Journal of Cancer (2023) 128:793 \u2013 804",
    "full_text_length": 56769,
    "chunk_length": 499
  },
  {
    "chunk_id": 2633,
    "paper_filename": "nini_2024_breastcancer_dignosis_from_contrast_enhanced_mammography_using neural_networks.pdf",
    "paper_title": "Nini 2024 Breastcancer Dignosis From Contrast Enhanced Mammography Using Neural Networks",
    "chunk_index": 0,
    "total_chunks": 41,
    "text_content": "Vol.:(0123456789)1 3European Radiology (2024) 34:917\u2013927 https://doi.org/10.1007/s00330-023-10170-9 BREAST Breast cancer diagnosis from contrast\u2011enhanced mammography using multi\u2011feature fusion neural network Nini Qian1 \u00b7 Wei Jiang1,2 \u00b7 Yu Guo1 \u00b7 Jian Zhu3 \u00b7 Jianfeng Qiu4 \u00b7 Hui Yu1 \u00b7 Xian Huang1 Received: 23 December 2022 / Revised: 25 May 2023 / Accepted: 8 July 2023 / Published online: 23 August 2023 \u00a9 The Author(s), under exclusive licence to European Society of Radiology 2023 Abstract Objecti",
    "full_text_length": 40781,
    "chunk_length": 1385
  },
  {
    "chunk_id": 2634,
    "paper_filename": "nini_2024_breastcancer_dignosis_from_contrast_enhanced_mammography_using neural_networks.pdf",
    "paper_title": "Nini 2024 Breastcancer Dignosis From Contrast Enhanced Mammography Using Neural Networks",
    "chunk_index": 1,
    "total_chunks": 41,
    "text_content": "breast lesion classification. Its generalization performance was further evaluated on two external datasets. Results were reported using AUC, accuracy, sensitivity, and specificity. Results A total of 2496 patients (mean age, 53 years \u00b1 12 (standard deviation)) were included and divided into a training set (1718), a validation set (255), and a testing set (523). The proposed CEM-based multi-feature fusion network achieved the best diagnosis performance with an AUC of 0.96 (95% confidence interva",
    "full_text_length": 40781,
    "chunk_length": 1375
  },
  {
    "chunk_id": 2635,
    "paper_filename": "nini_2024_breastcancer_dignosis_from_contrast_enhanced_mammography_using neural_networks.pdf",
    "paper_title": "Nini 2024 Breastcancer Dignosis From Contrast Enhanced Mammography Using Neural Networks",
    "chunk_index": 2,
    "total_chunks": 41,
    "text_content": "facilitate CEM-based breast cancer diagnosis. Clinical relevance statement Compared with low-energy images, CEM images have greater sensitivity and similar specificity in malignant breast lesion detection. The multi-feature fusion neural network is a promising computer-aided diagnostic tool for the clinical diagnosis of breast cancer. Key Points \u2022 Deep convolutional neural networks have the potential to facilitate contrast-enhanced mammography-based breast cancer diagnosis. \u2022 The multi-feature f",
    "full_text_length": 40781,
    "chunk_length": 1517
  },
  {
    "chunk_id": 2636,
    "paper_filename": "nini_2024_breastcancer_dignosis_from_contrast_enhanced_mammography_using neural_networks.pdf",
    "paper_title": "Nini 2024 Breastcancer Dignosis From Contrast Enhanced Mammography Using Neural Networks",
    "chunk_index": 3,
    "total_chunks": 41,
    "text_content": "of Radiation Oncology Physics and Technology, Shandong Cancer Hospital, Jiyan Road, Jinan 250117, Shandong, China 4 Medical Engineering and Technology Research Center, School of Radiology, Shandong First Medical University & Shandong Academy of Medical Sciences, Taian 271000, Shandong, China 918 European Radiology (2024) 34:917\u2013927 1 3 Abbreviations CAM Class activation map CC Craniocaudal CEM Contrast-enhanced mammography CI Confidence interval CNN Convolutional neural network DES Dual-energy s",
    "full_text_length": 40781,
    "chunk_length": 1399
  },
  {
    "chunk_id": 2637,
    "paper_filename": "nini_2024_breastcancer_dignosis_from_contrast_enhanced_mammography_using neural_networks.pdf",
    "paper_title": "Nini 2024 Breastcancer Dignosis From Contrast Enhanced Mammography Using Neural Networks",
    "chunk_index": 4,
    "total_chunks": 41,
    "text_content": "mammography to show angiogenesis and vis- ualize abnormal breast lesions. Because of the improved abil- ity of CEM to detect tumors shielded by dense breast glands compared with mammography, CEM has great feasibility and potential in initial breast cancer screening [3 \u201310]. Recent studies have shown that CEM had a higher positive predictive value (PPV) and specificity compared with MRI for cancer detection and outperformed full-field digital mammography (FFDM) in terms of diagnostic accuracy [11",
    "full_text_length": 40781,
    "chunk_length": 1330
  },
  {
    "chunk_id": 2638,
    "paper_filename": "nini_2024_breastcancer_dignosis_from_contrast_enhanced_mammography_using neural_networks.pdf",
    "paper_title": "Nini 2024 Breastcancer Dignosis From Contrast Enhanced Mammography Using Neural Networks",
    "chunk_index": 5,
    "total_chunks": 41,
    "text_content": "cancer [18 , 19]. To assist radiologists in improving CEM-based breast can- cer diagnosis, many computer-aided methods have been pro- posed in the literature. Some researchers carried out radiomics analyses on lesion regions manually delineated in CEM images [20\u201322]. Gao et al [23] proposed a shallow-deep convolutional neural network for predicting whether breast lesions annotated by bounding boxes in CEM images were benign or malignant. However, methods in the literature required lesion delinea",
    "full_text_length": 40781,
    "chunk_length": 1432
  },
  {
    "chunk_id": 2639,
    "paper_filename": "nini_2024_breastcancer_dignosis_from_contrast_enhanced_mammography_using neural_networks.pdf",
    "paper_title": "Nini 2024 Breastcancer Dignosis From Contrast Enhanced Mammography Using Neural Networks",
    "chunk_index": 6,
    "total_chunks": 41,
    "text_content": "images, was proposed for the auto- matic diagnosis of breast cancer based on CEM images. Materials and methods Patient selection This retrospective study was approved by the Institutional Ethics Committee of the Yantai Yuhuangding Hospital (Approval Number: 2023-073), and the informed con- sent requirement was waived. All patients who underwent CEM examinations at the Yantai Yuhuangding Hospital from January 2019 to August 2021 were initially included. Indications for CEM were the assessment of ",
    "full_text_length": 40781,
    "chunk_length": 1357
  },
  {
    "chunk_id": 2640,
    "paper_filename": "nini_2024_breastcancer_dignosis_from_contrast_enhanced_mammography_using neural_networks.pdf",
    "paper_title": "Nini 2024 Breastcancer Dignosis From Contrast Enhanced Mammography Using Neural Networks",
    "chunk_index": 7,
    "total_chunks": 41,
    "text_content": "diagram of the study exclusion criteria 919 European Radiology (2024) 34:917\u2013927 1 3 agent injection (a dose of 1.5 mL/kg body weight, an injec - tion flow rate of 3.0 mL/s) before image acquisition. Two minutes later, the low-energy images of both breasts in craniocaudal (CC) and mediolateral oblique (MLO) views were obtained at a low-energy exposure of 26\u201332 kilovolt- age (kVp). Almost simultaneously, the high-energy images were acquired at a high-energy exposure of 45\u201349 kVp. Then, the dual-e",
    "full_text_length": 40781,
    "chunk_length": 1266
  },
  {
    "chunk_id": 2641,
    "paper_filename": "nini_2024_breastcancer_dignosis_from_contrast_enhanced_mammography_using neural_networks.pdf",
    "paper_title": "Nini 2024 Breastcancer Dignosis From Contrast Enhanced Mammography Using Neural Networks",
    "chunk_index": 8,
    "total_chunks": 41,
    "text_content": "CEM examination result. Suspicious findings were further proven by biopsy.Data division The CEM dataset was randomly divided into a training, valida- tion, and testing set with a ratio of 7:1:2. Neural network models were trained on the training set. When the prediction errors on the validation set began to increase, network training was stopped, and then, the fitted models were evaluated in the testing set. Data preprocessing and augmentation The pixel values of each image were adjusted and lim",
    "full_text_length": 40781,
    "chunk_length": 1271
  },
  {
    "chunk_id": 2642,
    "paper_filename": "nini_2024_breastcancer_dignosis_from_contrast_enhanced_mammography_using neural_networks.pdf",
    "paper_title": "Nini 2024 Breastcancer Dignosis From Contrast Enhanced Mammography Using Neural Networks",
    "chunk_index": 9,
    "total_chunks": 41,
    "text_content": "network. b Average fusion module for dual-view fusion. c Concatenate fusion module for dual-view fusion. LCC, dual-energy images of the left breast in craniocaudal (CC) view; LMLO, dual-energy images of the left breast in medi- olateral oblique (MLO) view; RCC, dual-energy images of the right breast in CC view; RMLO, dual-energy images of the right breast in MLO view; fLCC, left-right fusion features of the left breast in CC view; fLMLO, left-right fusion features of the left breast in MLO view;",
    "full_text_length": 40781,
    "chunk_length": 1215
  },
  {
    "chunk_id": 2643,
    "paper_filename": "nini_2024_breastcancer_dignosis_from_contrast_enhanced_mammography_using neural_networks.pdf",
    "paper_title": "Nini 2024 Breastcancer Dignosis From Contrast Enhanced Mammography Using Neural Networks",
    "chunk_index": 10,
    "total_chunks": 41,
    "text_content": "neu - ral network for breast cancer detection by combining information from CEM images of dual views of bilateral breasts. The general framework of the proposed network is shown in Fig. 2a. Each pair of LE and DES images was concatenated together and regarded as the two- channel input of a convolutional neural network (CNN), which extracted the fused features of the dual-energy images. Accordingly, a total of four CNNs, sharing the same weights, were employed to extract features from the images ",
    "full_text_length": 40781,
    "chunk_length": 1185
  },
  {
    "chunk_id": 2644,
    "paper_filename": "nini_2024_breastcancer_dignosis_from_contrast_enhanced_mammography_using neural_networks.pdf",
    "paper_title": "Nini 2024 Breastcancer Dignosis From Contrast Enhanced Mammography Using Neural Networks",
    "chunk_index": 11,
    "total_chunks": 41,
    "text_content": "were proposed for CC and MLO view fusion. One was a decision fusion approach, as shown in Fig. 2b. The features of each view were inputted in a fully connected layer with softmax output to predict the benign and malignant probabilities of a breast, and then, the prediction results based on different views were fused with an average operation to output the final benign and malig- nant probability of the breast. Eventually, the diagnosis output was the class with the largest predicted probability ",
    "full_text_length": 40781,
    "chunk_length": 1195
  },
  {
    "chunk_id": 2645,
    "paper_filename": "nini_2024_breastcancer_dignosis_from_contrast_enhanced_mammography_using neural_networks.pdf",
    "paper_title": "Nini 2024 Breastcancer Dignosis From Contrast Enhanced Mammography Using Neural Networks",
    "chunk_index": 12,
    "total_chunks": 41,
    "text_content": "layers (VGG16) [25] was used in this study. The detailed architecture of VGG16 is illustrated in Fig. 3. The CEM training set was represented as /braceleft.s1xn,yn,n=1, 2, 3, ..., N/braceright.s1,where N was the patient number in the training set, xn was the CEM images of the n th patient, and yn was the ground truth label of that patient, indicating whether each breast contained malignant tumors. Then, the cross-entropy loss function denoted as follows was employed to train the proposed network",
    "full_text_length": 40781,
    "chunk_length": 1228
  },
  {
    "chunk_id": 2646,
    "paper_filename": "nini_2024_breastcancer_dignosis_from_contrast_enhanced_mammography_using neural_networks.pdf",
    "paper_title": "Nini 2024 Breastcancer Dignosis From Contrast Enhanced Mammography Using Neural Networks",
    "chunk_index": 13,
    "total_chunks": 41,
    "text_content": "CNN model was fine-tuned in the final fusion framework. The SGD optimizer with a weight decay factor of 5 \u00d7 10\u22124 and momentum of 0.9 was used while training. All the experi- ments were implemented on an Intel(R) Core(TM) i7-9700 CPU @ 3.00 GHz and an NVIDIA GeForce RTX 3090 24 GB GPU. The software environments were Ubuntu 18.04 and Pytorch 3.9. The batch size was set to 8. The initial learn - ing rate was 0.001. Once the training epoch reached the cor - responding settings (10th, 30th, and 40th ",
    "full_text_length": 40781,
    "chunk_length": 1264
  },
  {
    "chunk_id": 2647,
    "paper_filename": "nini_2024_breastcancer_dignosis_from_contrast_enhanced_mammography_using neural_networks.pdf",
    "paper_title": "Nini 2024 Breastcancer Dignosis From Contrast Enhanced Mammography Using Neural Networks",
    "chunk_index": 14,
    "total_chunks": 41,
    "text_content": "size = height \u00d7 width \u00d7 channel. BN, batch normalization; RELU, rectified linear unit; FC, a fully con- nected layer 921 European Radiology (2024) 34:917\u2013927 1 3 multi-feature fusion model is publicly available (https:// github. com/ anne2 26/ MF_ MODEL. git). Statistical analysis To assess the classification performance of our models, several metrics, including accuracy, sensitivity, specificity, positive predictive value (PPV), negative predictive value (NPV), and the area under the receiver o",
    "full_text_length": 40781,
    "chunk_length": 1345
  },
  {
    "chunk_id": 2648,
    "paper_filename": "nini_2024_breastcancer_dignosis_from_contrast_enhanced_mammography_using neural_networks.pdf",
    "paper_title": "Nini 2024 Breastcancer Dignosis From Contrast Enhanced Mammography Using Neural Networks",
    "chunk_index": 15,
    "total_chunks": 41,
    "text_content": "A total of 2496 female patients (mean age, 53 years \u00b1 12 stand- ard deviation (SD); range, 17\u201385 years) were finally enrolled in this study. Detailed patient characteristics are shown in Table 1. These eligible patients were randomly allocated into a training set (1718 patients, mean age, 53 years \u00b1 12 (SD)), a validation set (255 patients, mean age, 53 years \u00b1 11 (SD)), and a testing set (523 patients, mean age, 52 years \u00b1 12 (SD)). More details about the dataset splitting are shown in Table 2.",
    "full_text_length": 40781,
    "chunk_length": 1251
  },
  {
    "chunk_id": 2649,
    "paper_filename": "nini_2024_breastcancer_dignosis_from_contrast_enhanced_mammography_using neural_networks.pdf",
    "paper_title": "Nini 2024 Breastcancer Dignosis From Contrast Enhanced Mammography Using Neural Networks",
    "chunk_index": 16,
    "total_chunks": 41,
    "text_content": "cases, 31 (31/59) were extremely dense breasts, and 9 breasts (9/51) were diagnosed as special types of breast cancer, such as mucinous carcinoma, malignant phyllodes tumor, and solid papillary carcinoma. The remain- ing malignant lesions that were incorrectly classified were either too small or not visibly enhanced in CEM images. As for different BI-RADS assessment categories, the multi-feature fusion network got an overall classification accuracy of 98%, a sensitivity of 96%, and a specificity",
    "full_text_length": 40781,
    "chunk_length": 1243
  },
  {
    "chunk_id": 2650,
    "paper_filename": "nini_2024_breastcancer_dignosis_from_contrast_enhanced_mammography_using neural_networks.pdf",
    "paper_title": "Nini 2024 Breastcancer Dignosis From Contrast Enhanced Mammography Using Neural Networks",
    "chunk_index": 17,
    "total_chunks": 41,
    "text_content": "models clas- sified input images based on the feature vector formed by the concatenation of the CNN features of a breast and its contralat- eral breast in the same view. CNN modules used to extract image features in all these models had the same architecture. Compared with the no-fusion model, the CEM-based left- right fusion model improved prediction AUC from 0.93 to 0.95. By combining images of not only bilateral breasts but also dual views, the proposed multi-feature fusion methods Table 1 Pa",
    "full_text_length": 40781,
    "chunk_length": 1201
  },
  {
    "chunk_id": 2651,
    "paper_filename": "nini_2024_breastcancer_dignosis_from_contrast_enhanced_mammography_using neural_networks.pdf",
    "paper_title": "Nini 2024 Breastcancer Dignosis From Contrast Enhanced Mammography Using Neural Networks",
    "chunk_index": 18,
    "total_chunks": 41,
    "text_content": "Percentage (n = 2496 \u00d7 2) 0 0.8 1 14.2 2 3 3 30.5 3\u20134A 2.3 4A 11.4 4B 8.4 4C 11.8 5 12 6 5.6 Total 100 Breast biopsy results (left/right breast) Percentage (n = 2496 \u00d7 2) Begin 64.5 Malignant 35.5 DCIS 1.9 IDC 18.5 DCIS + IDC 10.5 LCIS 0.1 ILC 0.8 LCIS + ILC 0.2 Papillary carcinoma 1.9 Mucinous carcinoma 1.1 Malignant phyllodes tumor 0.2 Paget\u2019s disease 0.2 Squamous cell carcinoma 0.1 Total 100 922 European Radiology (2024) 34:917\u2013927 1 3 furtherly improved the prediction performance. Besides, n",
    "full_text_length": 40781,
    "chunk_length": 1157
  },
  {
    "chunk_id": 2652,
    "paper_filename": "nini_2024_breastcancer_dignosis_from_contrast_enhanced_mammography_using neural_networks.pdf",
    "paper_title": "Nini 2024 Breastcancer Dignosis From Contrast Enhanced Mammography Using Neural Networks",
    "chunk_index": 19,
    "total_chunks": 41,
    "text_content": "and DES inputs. Delong\u2019s test was applied to evaluate the discrimination in different models. The analysis results (Table 5) showed that there was a significant difference between the no-fusion model and the left-right fusion model (p < 0.001) and between the models trained on CEM images (DES + LE) and only on LE images (p = 0.003 for multi-feature fusion by the average operation and p = 0.006 for multi-feature fusion by the concatenate operation). However, there was no significant difference be",
    "full_text_length": 40781,
    "chunk_length": 1248
  },
  {
    "chunk_id": 2653,
    "paper_filename": "nini_2024_breastcancer_dignosis_from_contrast_enhanced_mammography_using neural_networks.pdf",
    "paper_title": "Nini 2024 Breastcancer Dignosis From Contrast Enhanced Mammography Using Neural Networks",
    "chunk_index": 20,
    "total_chunks": 41,
    "text_content": "breast to visualize the regions that different networks were interested in. Figure 5a illustrates the CAMs of a breast with a malig- nant tumor generated by different models, including the no- fusion model, left-right fusion model, and multi-feature fusion Table 2 Standardized training/ validation/testing split in the internal contrast-enhanced mammography datasetBilateral benign Unilateral malignant Bilateral malignant Overall Training 542 (31.5%) 1149 (66.9%) 27 (1.6%) 1718 (100%) Validation 6",
    "full_text_length": 40781,
    "chunk_length": 1500
  },
  {
    "chunk_id": 2654,
    "paper_filename": "nini_2024_breastcancer_dignosis_from_contrast_enhanced_mammography_using neural_networks.pdf",
    "paper_title": "Nini 2024 Breastcancer Dignosis From Contrast Enhanced Mammography Using Neural Networks",
    "chunk_index": 21,
    "total_chunks": 41,
    "text_content": "(0.88, 0.91)0.85 (0.83, 0.87)0.74 (0.70, 0.77)0.91 (0.90, 0.93)0.82 (0.79, 0.85)0.86 (0.84, 0.88) LE (left-right fusion)0.93 (0.92, 0.94)0.86 (0.85, 0.88)0.75 (0.72, 0.78)0.92 (0.91, 0.94)0.84 (0.81, 0.87)0.87 (0.85, 0.89) LE (multi-feature fusion (average))0.94 (0.92, 0.95)0.86 (0.84, 0.88)0.74 (0.69, 0.79)0.93 (0.91, 0.95)0.86 (0.81, 0.89)0.87 (0.84, 0.89) LE (multi-feature fusion (concatenate))0.94 (0.93, 0.95)0.88 (0.86, 0.90)0.79 (0.74, 0.83)0.93 (0.91, 0.95)0.86 (0.82, 0.89)0.89 (0.86, 0.9",
    "full_text_length": 40781,
    "chunk_length": 1600
  },
  {
    "chunk_id": 2655,
    "paper_filename": "nini_2024_breastcancer_dignosis_from_contrast_enhanced_mammography_using neural_networks.pdf",
    "paper_title": "Nini 2024 Breastcancer Dignosis From Contrast Enhanced Mammography Using Neural Networks",
    "chunk_index": 22,
    "total_chunks": 41,
    "text_content": "(0.87, 0.91)0.85 (0.81, 0.88)0.92 (0.89, 0.94)0.85 (0.81, 0.88)0.92 (0.89, 0.94) CEM (multi-feature fusion (concatenate))0.96 (0.95, 0.97)0.90 (0.88, 0.92)0.84 (0.80, 0.88)0.93 (0.91, 0.95)0.87 (0.83, 0.90)0.91 (0.89, 0.93) 923 European Radiology (2024) 34:917\u2013927 1 3 model using the concatenate operation, which were all with CEM (DES + LE) image inputs. Compared with the no-fusion model and left-right fusion model, the multi-feature fusion model was able to concentrate more clearly on the lesio",
    "full_text_length": 40781,
    "chunk_length": 1284
  },
  {
    "chunk_id": 2656,
    "paper_filename": "nini_2024_breastcancer_dignosis_from_contrast_enhanced_mammography_using neural_networks.pdf",
    "paper_title": "Nini 2024 Breastcancer Dignosis From Contrast Enhanced Mammography Using Neural Networks",
    "chunk_index": 23,
    "total_chunks": 41,
    "text_content": "a public CEM database [29] were used. The INbreast dataset was acquired from the breast center in Centro Hospitalar de S\u00e3o Jo\u00e3o (CHSJ) at Porto, which contained 115 cases (410 images) in total. The masks of ROIs were annotated and validated by two specialists. In our study, only cases (a total of 86 cases) with bilateral FFDM images were used, which meant that each case had 4 FFDM images. As biopsy-proven annotations were not available, these cases were labeled as benign or malignant according t",
    "full_text_length": 40781,
    "chunk_length": 1246
  },
  {
    "chunk_id": 2657,
    "paper_filename": "nini_2024_breastcancer_dignosis_from_contrast_enhanced_mammography_using neural_networks.pdf",
    "paper_title": "Nini 2024 Breastcancer Dignosis From Contrast Enhanced Mammography Using Neural Networks",
    "chunk_index": 24,
    "total_chunks": 41,
    "text_content": "on the internal dataset using the concatenating operation with LE image inputs was used for the classification of FFDM images in INbreast. The same data preprocessing technique as for CEM images was utilized for FFDM image preprocessing. Finally, we got an Fig. 4 Receiver operating characteristic curve. a Models with low-energy image inputs. b Models with contrast-enhanced mammography image inputs Table 5 Significance levels of different models LE, low-energy; CEM, contrast-enhanced mammography.",
    "full_text_length": 40781,
    "chunk_length": 1398
  },
  {
    "chunk_id": 2658,
    "paper_filename": "nini_2024_breastcancer_dignosis_from_contrast_enhanced_mammography_using neural_networks.pdf",
    "paper_title": "Nini 2024 Breastcancer Dignosis From Contrast Enhanced Mammography Using Neural Networks",
    "chunk_index": 25,
    "total_chunks": 41,
    "text_content": "fusion) CEM (left-right fusion) < .001 LE (multi-feature fusion (concatenate)) CEM (multi-feature fusion (concatenate)) .006 924 European Radiology (2024) 34:917\u2013927 1 3 AUC of 0.90 (95% CI: 0.85, 0.94), an accuracy of 0.84 (95% CI: 0.78, 0.89), a sensitivity of 0.77 (95% CI: 0.61, 0.88), and a specificity of 0.87 (95% CI: 0.80, 0.92). As shown in Fig. 6a, the proposed multi-feature fusion network was able to precisely focus on the lesion areas in FFDM images.A CEM dataset gathered from the Radi",
    "full_text_length": 40781,
    "chunk_length": 1250
  },
  {
    "chunk_id": 2659,
    "paper_filename": "nini_2024_breastcancer_dignosis_from_contrast_enhanced_mammography_using neural_networks.pdf",
    "paper_title": "Nini 2024 Breastcancer Dignosis From Contrast Enhanced Mammography Using Neural Networks",
    "chunk_index": 26,
    "total_chunks": 41,
    "text_content": "mediolateral oblique view of a 57-year-old female biopsy- proven to be invasive ductal carcinoma. Left to right: low- energy images, dual-energy subtracted images, and CAMs based on the no-fusion model, the left-right fusion model, and the multi-feature fusion model by the concatenate operation. b Unilateral breast in cranio- caudal view and mediolateral oblique view of a 31-year-old female diagnosed with benign phyllodes tumors. Left to right: low-energy images, dual-energy subtracted images, a",
    "full_text_length": 40781,
    "chunk_length": 1341
  },
  {
    "chunk_id": 2660,
    "paper_filename": "nini_2024_breastcancer_dignosis_from_contrast_enhanced_mammography_using neural_networks.pdf",
    "paper_title": "Nini 2024 Breastcancer Dignosis From Contrast Enhanced Mammography Using Neural Networks",
    "chunk_index": 27,
    "total_chunks": 41,
    "text_content": "internal training set was used to classify CEM images in this external dataset. The classification results showed an AUC of 0.92 (95% CI: 0.89, 0.95), an accuracy of 0.85 (95% CI: 0.82, 0.89), a sensitivity of 0.86 (95% CI: 0.79, 0.91), and a specificity of 0.85 (95% CI: 0.80, 0.89). Figure 6b shows the final CAMs of one case, which indicate the location of a breast lesion. Discussion In this study, a convolutional neural network-based multi- feature fusion method was proposed for breast cancer ",
    "full_text_length": 40781,
    "chunk_length": 1237
  },
  {
    "chunk_id": 2661,
    "paper_filename": "nini_2024_breastcancer_dignosis_from_contrast_enhanced_mammography_using neural_networks.pdf",
    "paper_title": "Nini 2024 Breastcancer Dignosis From Contrast Enhanced Mammography Using Neural Networks",
    "chunk_index": 28,
    "total_chunks": 41,
    "text_content": "dual-energy subtracted images, and the CAMs of a patient from CDD-CEM dataset (left breast: malignant; right breast: normal). The lesions were highlighted with red points and circles. Top to bottom: craniocaudal view and medi- olateral oblique view of the left breast and of the right breast 926 European Radiology (2024) 34:917\u2013927 1 3 fusion model achieved better diagnosis performance with an AUC of 0.96 (95% confidence interval (CI): 0.95, 0.97) and an accuracy of 0.90 (95% CI: 0.88, 0.92). The",
    "full_text_length": 40781,
    "chunk_length": 1367
  },
  {
    "chunk_id": 2662,
    "paper_filename": "nini_2024_breastcancer_dignosis_from_contrast_enhanced_mammography_using neural_networks.pdf",
    "paper_title": "Nini 2024 Breastcancer Dignosis From Contrast Enhanced Mammography Using Neural Networks",
    "chunk_index": 29,
    "total_chunks": 41,
    "text_content": "models on breast cancer diagnosis, especially in sensitivity. Furthermore, the generalization per - formance of the developed multi-feature fusion model was evaluated on two external datasets: a public full-field digi- tal mammography (FFDM) dataset, INbreast, and a public CEM dataset. The test results demonstrated the good gen- eralization performance of our models in external datasets collected from different institutions and countries. Many methods have been proposed for breast lesion clas- s",
    "full_text_length": 40781,
    "chunk_length": 1314
  },
  {
    "chunk_id": 2663,
    "paper_filename": "nini_2024_breastcancer_dignosis_from_contrast_enhanced_mammography_using neural_networks.pdf",
    "paper_title": "Nini 2024 Breastcancer Dignosis From Contrast Enhanced Mammography Using Neural Networks",
    "chunk_index": 30,
    "total_chunks": 41,
    "text_content": "91.7%. Losurdo et al [21] developed a lesion classification model based on 55 breast lesions of 51 patients. Using significant CEM features extracted from lesion regions reached the best classifica- tion accuracy of 80%. Patel et al [22] studied 50 lesions in CEM images, from which textural and morphologic features were extracted. Using the support vector machine classi- fier, they finally achieved a 0.95 AUC, 90% accuracy, 88% sensitivity, and 92% specificity. Gao et al [23] developed a shallow",
    "full_text_length": 40781,
    "chunk_length": 1277
  },
  {
    "chunk_id": 2664,
    "paper_filename": "nini_2024_breastcancer_dignosis_from_contrast_enhanced_mammography_using neural_networks.pdf",
    "paper_title": "Nini 2024 Breastcancer Dignosis From Contrast Enhanced Mammography Using Neural Networks",
    "chunk_index": 31,
    "total_chunks": 41,
    "text_content": "the CEM images col - lected from a single institution. A highly standardized, diverse, large, and multicentric CEM database is required for further research. Besides, our model only got an accu- racy of 80% for Breast Imaging Reporting and Data Sys- tem (BI-RADS) category 4 lesions. In further work, focus- ing on the malignancy prediction of microcalcifications will enhance the diagnosis accuracy of models for these lesions. Furthermore, if pixel-level labels can also be pro- vided by specialist",
    "full_text_length": 40781,
    "chunk_length": 1331
  },
  {
    "chunk_id": 2665,
    "paper_filename": "nini_2024_breastcancer_dignosis_from_contrast_enhanced_mammography_using neural_networks.pdf",
    "paper_title": "Nini 2024 Breastcancer Dignosis From Contrast Enhanced Mammography Using Neural Networks",
    "chunk_index": 32,
    "total_chunks": 41,
    "text_content": "breast tumor diagnosis. Funding This study has received funding by Major Science and Tech- nology Projects in Tianjin (18ZXZNSY00240), Science and Technol- ogy Innovation Projects for Medical System Staff in Shandong Prov - ince (SDYWZGKCJH2022042). Declarations Guarantor The scientific guarantor of this publication is Yu GUO, Ph.D., Department of Biomedical Engineering, School of Precision Instrument and Opto-Electronics Engineering, Tianjin University, Tianjin, China. Conflict of interest The ",
    "full_text_length": 40781,
    "chunk_length": 1382
  },
  {
    "chunk_id": 2666,
    "paper_filename": "nini_2024_breastcancer_dignosis_from_contrast_enhanced_mammography_using neural_networks.pdf",
    "paper_title": "Nini 2024 Breastcancer Dignosis From Contrast Enhanced Mammography Using Neural Networks",
    "chunk_index": 33,
    "total_chunks": 41,
    "text_content": "have not been previously reported. Methodology \u2022 retrospective \u2022 diagnostic or prognostic study \u2022 performed at one institution References 1. Sung H, Ferlay J, Siegel RL et al (2021) Global cancer statis- tics 2020: GLOBOCAN estimates of incidence and mortality worldwide for 36 cancers in 185 countries. CA Cancer J Clin 71:209\u2013249. https:// doi. org/ 10. 3322/ caac. 21660 2. Cao W, Chen HD, Yu YW, Li N, Chen WQ (2021) Changing pro- files of cancer burden worldwide and in China: a secondary analy ",
    "full_text_length": 40781,
    "chunk_length": 1293
  },
  {
    "chunk_id": 2667,
    "paper_filename": "nini_2024_breastcancer_dignosis_from_contrast_enhanced_mammography_using neural_networks.pdf",
    "paper_title": "Nini 2024 Breastcancer Dignosis From Contrast Enhanced Mammography Using Neural Networks",
    "chunk_index": 34,
    "total_chunks": 41,
    "text_content": "Dual-energy contrast-enhanced digital subtraction mammography: feasibility. Radiology 229:261\u2013268. https:// doi. org/ 10. 1148/ radiol. 22910 21276 5. Fallenberg EM, Schmitzberger FF, Amer H et al (2017) Contrast- enhanced spectral mammography vs. mammography and MRI - clinical performance in a multi-reader evaluation. Eur Radiol 27:2752\u20132764. https:// doi. org/ 10. 1007/ s00330- 016- 4650-6 6. Cheung YC, Lin YC, Wan YL et al (2014) Diagnostic perfor - mance of dual-energy contrast-enhanced subt",
    "full_text_length": 40781,
    "chunk_length": 1378
  },
  {
    "chunk_id": 2668,
    "paper_filename": "nini_2024_breastcancer_dignosis_from_contrast_enhanced_mammography_using neural_networks.pdf",
    "paper_title": "Nini 2024 Breastcancer Dignosis From Contrast Enhanced Mammography Using Neural Networks",
    "chunk_index": 35,
    "total_chunks": 41,
    "text_content": "Womens Health (Lond) 10:289\u2013298. https:// doi. org/ 10. 2217/ whe. 14. 18 9. Lobbes MBI, Smidt ML, Houwers J, Tjan-Heijnen VC, Wild- berger JE (2013) Contrast enhanced mammography: techniques, current results, and potential indications. Clin Radiol 68:935\u2013944. https:// doi. org/ 10. 1016/j. crad. 2013. 04. 009 10. Lobbes MBI, Lalji U, Houwers J et al (2014) Contrast-enhanced spectral mammography in patients referred from the breast cancer screening programme. Eur Radiol 24:1668\u20131676. https:// do",
    "full_text_length": 40781,
    "chunk_length": 1401
  },
  {
    "chunk_id": 2669,
    "paper_filename": "nini_2024_breastcancer_dignosis_from_contrast_enhanced_mammography_using neural_networks.pdf",
    "paper_title": "Nini 2024 Breastcancer Dignosis From Contrast Enhanced Mammography Using Neural Networks",
    "chunk_index": 36,
    "total_chunks": 41,
    "text_content": "124:1006\u20131017. https:// doi. org/ 10. 1007/ s11547- 019- 01056-2 13. Jochelson MS, Dershaw DD, Sung JS et al (2013) Bilateral con- trast-enhanced dual-energy digital mammography: feasibility and comparison with conventional digital mammography and MR imaging in women with known breast carcinoma. Radiol- ogy 266:743\u2013751. https:// doi. org/ 10. 1148/ radiol. 12121 084 14. Sumkin JH, Berg WA, Carter GJ et al (2019) Diagnostic per - formance of MRI, molecular breast imaging, and contrast- enhanced m",
    "full_text_length": 40781,
    "chunk_length": 1328
  },
  {
    "chunk_id": 2670,
    "paper_filename": "nini_2024_breastcancer_dignosis_from_contrast_enhanced_mammography_using neural_networks.pdf",
    "paper_title": "Nini 2024 Breastcancer Dignosis From Contrast Enhanced Mammography Using Neural Networks",
    "chunk_index": 37,
    "total_chunks": 41,
    "text_content": "for accurate measurement of the size of breast cancer. Br J Radiol 92:20180929. https:// doi. org/ 10. 1259/ bjr. 20180 929 17. Hobbs MM, Taylor DB, Buzynski S, Peake RE (2015) Contrast- enhanced spectral mammography (CESM) and contrast enhanced MRI (CEMRI): patient preferences and tolerance. J Med Imag- ing Radiat Oncol 59:300\u2013305. https:// doi. org/ 10. 1111/ 1754- 9485. 12296 18. Jochelson MS, Lobbes MBI (2021) Contrast-enhanced mammog- raphy: state of the art. Radiology 299:36\u201348. https:// d",
    "full_text_length": 40781,
    "chunk_length": 1275
  },
  {
    "chunk_id": 2671,
    "paper_filename": "nini_2024_breastcancer_dignosis_from_contrast_enhanced_mammography_using neural_networks.pdf",
    "paper_title": "Nini 2024 Breastcancer Dignosis From Contrast Enhanced Mammography Using Neural Networks",
    "chunk_index": 38,
    "total_chunks": 41,
    "text_content": "org/ 10. 3390/ jcm80 60891 21. Losurdo L, Fanizzi A, Basile TMA et al (2019) Radiomics analy - sis on contrast-enhanced spectral mammography images for breast cancer diagnosis: a pilot study. Entropy 21:1110. https:// doi. org/ 10. 3390/ e2111 1110 22. Patel BK, Ranjbar S, Wu T et al (2018) Computer-aided diagnosis of contrast-enhanced spectral mammography: a feasibility study. Eur J Radiol 98:207\u2013213. https:// doi. org/ 10. 1016/j. ejrad. 2017. 11. 024 23. Gao F, Wu T, Li J et al (2018) SD-CNN:",
    "full_text_length": 40781,
    "chunk_length": 1326
  },
  {
    "chunk_id": 2672,
    "paper_filename": "nini_2024_breastcancer_dignosis_from_contrast_enhanced_mammography_using neural_networks.pdf",
    "paper_title": "Nini 2024 Breastcancer Dignosis From Contrast Enhanced Mammography Using Neural Networks",
    "chunk_index": 39,
    "total_chunks": 41,
    "text_content": "DeLong ER, DeLong DM, Clarke-Pearson DL (1988) Comparing the areas under two or more correlated receiver operating char - acteristic curves: a nonparametric approach. Biometrics 44:837\u2013 845. https:// doi. org/ 10. 2307/ 25315 95 27. Chattopadhay A, Sarkar A, Howlader P, Balasubramanian VN (2018) Grad-CAM++: generalized gradient-based visual expla - nations for deep convolutional networks, in: 2018 IEEE Winter Conference on Applications of Computer Vision (WACV). IEEE Lake Tahoe, NV. https:// doi",
    "full_text_length": 40781,
    "chunk_length": 1355
  },
  {
    "chunk_id": 2673,
    "paper_filename": "nini_2024_breastcancer_dignosis_from_contrast_enhanced_mammography_using neural_networks.pdf",
    "paper_title": "Nini 2024 Breastcancer Dignosis From Contrast Enhanced Mammography Using Neural Networks",
    "chunk_index": 40,
    "total_chunks": 41,
    "text_content": "Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations. Springer Nature or its licensor (e.g. a society or other partner) holds exclusive rights to this article under a publishing agreement with the author(s) or other rightsholder(s); author self-archiving of the accepted manuscript version of this article is solely governed by the terms of such publishing agreement and applicable law.",
    "full_text_length": 40781,
    "chunk_length": 442
  },
  {
    "chunk_id": 2674,
    "paper_filename": "olaide_2024_neuaral_netwrok_with_augmetnation_for_breast_cancer_detection_using_mammography.pdf",
    "paper_title": "Olaide 2024 Neuaral Netwrok With Augmetnation For Breast Cancer Detection Using Mammography",
    "chunk_index": 0,
    "total_chunks": 85,
    "text_content": "1 Vol.:(0123456789) Scientific Reports | (2022) 12:5913 | https://doi.org/10.1038/s41598-022-09905-3 www.nature.com/scientificreportsA novel wavelet decomposition and transformation convolutional neural network with data augmentation for breast cancer detection using digital mammogram Olaide N. Oyelade1,2 & Absalom E. Ezugwu1* Research in deep learning (DL) has continued to provide significant solutions to the challenges of detecting breast cancer in digital images. Image preprocessing methods a",
    "full_text_length": 87026,
    "chunk_length": 1536
  },
  {
    "chunk_id": 2675,
    "paper_filename": "olaide_2024_neuaral_netwrok_with_augmetnation_for_breast_cancer_detection_using_mammography.pdf",
    "paper_title": "Olaide 2024 Neuaral Netwrok With Augmetnation For Breast Cancer Detection Using Mammography",
    "chunk_index": 1,
    "total_chunks": 85,
    "text_content": "wavelet transform to restructure CNN architectures to improve the detection of discriminant features in digital mammography for increased classification accuracy. Therefore, this study addresses these problems through wavelet-CNN-wavelet architecture. The approach presented in this paper combines seam carving and wavelet decomposition algorithms for image preprocessing to find discriminative features. These features are passed as input to a CNN- wavelet structure that uses the new wavelet transf",
    "full_text_length": 87026,
    "chunk_length": 1488
  },
  {
    "chunk_id": 2676,
    "paper_filename": "olaide_2024_neuaral_netwrok_with_augmetnation_for_breast_cancer_detection_using_mammography.pdf",
    "paper_title": "Olaide 2024 Neuaral Netwrok With Augmetnation For Breast Cancer Detection Using Mammography",
    "chunk_index": 2,
    "total_chunks": 85,
    "text_content": "The study\u2019s findings demonstrate the usefulness of the wavelet transform function in restructuring CNN architectures for performance enhancement in detecting abnormalities leading to breast cancer in digital mammography. Abbreviations DL Deep learning CNN Convolutional neural networks DDSM + CBIS Digital database for screening mammography + curated breast imaging subset MNIST Modified National Institute of Standards and Technology ReLU Rectified linear unit CIFAR Canadian Institute for Advanced ",
    "full_text_length": 87026,
    "chunk_length": 1658
  },
  {
    "chunk_id": 2677,
    "paper_filename": "olaide_2024_neuaral_netwrok_with_augmetnation_for_breast_cancer_detection_using_mammography.pdf",
    "paper_title": "Olaide 2024 Neuaral Netwrok With Augmetnation For Breast Cancer Detection Using Mammography",
    "chunk_index": 3,
    "total_chunks": 85,
    "text_content": "AlexNet Ale network FFDM Full-field digital mammograms InceptionV3 Inception version 3 TPR True positive rate FPI Positives per image ImageNet Image network FCN Fully convolutional network CRF Conditional random fields DDSM-BCRP Digital Database for Screening Mammography -Breast Cancer Resistance Protein MLO Mediolateral-oblique VGGNet Visual geometry group network ResNet Residual network MIAS Mammographic Image Analysis Society R-CNN Region-based CNN MLP Multilayer perceptron GlimpseNet Glimpse",
    "full_text_length": 87026,
    "chunk_length": 1615
  },
  {
    "chunk_id": 2678,
    "paper_filename": "olaide_2024_neuaral_netwrok_with_augmetnation_for_breast_cancer_detection_using_mammography.pdf",
    "paper_title": "Olaide 2024 Neuaral Netwrok With Augmetnation For Breast Cancer Detection Using Mammography",
    "chunk_index": 4,
    "total_chunks": 85,
    "text_content": "CPU Central processing unit RAM Random access memory CC Craniocaudal BC Benign calcification BM Benign mass M Normal CALC Calcification M Mass ASYM Asymmetry LL Approximate coefficient LH Low pass horizontal (horizontal subbands) HL Horizontal low pass (vertical subbands) HH Horizontal high pass (diagnoal subbands) WPDP2 Wavelet packet decomposition 2 ROC Receiver operating characteristic MCC Matthew correlation coefficient TP True positive FP False positive TN True negative FN False negative Gl",
    "full_text_length": 87026,
    "chunk_length": 1425
  },
  {
    "chunk_id": 2679,
    "paper_filename": "olaide_2024_neuaral_netwrok_with_augmetnation_for_breast_cancer_detection_using_mammography.pdf",
    "paper_title": "Olaide 2024 Neuaral Netwrok With Augmetnation For Breast Cancer Detection Using Mammography",
    "chunk_index": 5,
    "total_chunks": 85,
    "text_content": "is being corroborated by the need for early detection, sometimes using mammography. Considering the limitation of human experts in detecting subtle features suggesting early stages of the disease in several cases, computer-aided detection (CAD) systems such as deep learning models have been proposed3. Several studies have demonstrated good performance in the use of deep learning to increase detection rates and lower false-positive rates4\u201310. To further advance the use of deep learning, performan",
    "full_text_length": 87026,
    "chunk_length": 1466
  },
  {
    "chunk_id": 2680,
    "paper_filename": "olaide_2024_neuaral_netwrok_with_augmetnation_for_breast_cancer_detection_using_mammography.pdf",
    "paper_title": "Olaide 2024 Neuaral Netwrok With Augmetnation For Breast Cancer Detection Using Mammography",
    "chunk_index": 6,
    "total_chunks": 85,
    "text_content": "to detect features from enhanced images often compliments nonlinear functions to support the exploitation of image samples13. A skillful combination of these feature enhancement and feature detection techniques supports classifying and detecting abnormalities in medical images. Mammography plays a pivotal role in screening and diagnosing breast cancer in the early stages. Digitized versions of mammography images have been widely used as samples in deep learning models for experimenta- tion. Howe",
    "full_text_length": 87026,
    "chunk_length": 1455
  },
  {
    "chunk_id": 2681,
    "paper_filename": "olaide_2024_neuaral_netwrok_with_augmetnation_for_breast_cancer_detection_using_mammography.pdf",
    "paper_title": "Olaide 2024 Neuaral Netwrok With Augmetnation For Breast Cancer Detection Using Mammography",
    "chunk_index": 7,
    "total_chunks": 85,
    "text_content": "wavelet decomposition method with a coefficient of 1. A multiresolution wavelet decomposition method was proposed in18 to extract spectral features in image samples. Meanwhile, improving the feature detection process through an architectural adjustment to a CNN has also been researched. Using histopathology image inputs in18, the CNN structure was improved using a wavelet function to detect the spectral features in the samples to achieve accurate classification. To monitor the large-scale fluoro",
    "full_text_length": 87026,
    "chunk_length": 1484
  },
  {
    "chunk_id": 2682,
    "paper_filename": "olaide_2024_neuaral_netwrok_with_augmetnation_for_breast_cancer_detection_using_mammography.pdf",
    "paper_title": "Olaide 2024 Neuaral Netwrok With Augmetnation For Breast Cancer Detection Using Mammography",
    "chunk_index": 8,
    "total_chunks": 85,
    "text_content": "COVID-19 were extracted from lung images using a proposed wavelet-CNN architecture23. Wavelet transforms also have been integrated into CNN architectures to improve the multiresolution analysis capability of hybrid structures24. Improving the classification accuracy of MNIST image samples has been proposed using a wavelet-convolution- wavelet-NN. The convolutional and fully connected layers are driven by a wavelet transform25. Wavelet transform has also been used in CNN to achieve spectral analy",
    "full_text_length": 87026,
    "chunk_length": 1427
  },
  {
    "chunk_id": 2683,
    "paper_filename": "olaide_2024_neuaral_netwrok_with_augmetnation_for_breast_cancer_detection_using_mammography.pdf",
    "paper_title": "Olaide 2024 Neuaral Netwrok With Augmetnation For Breast Cancer Detection Using Mammography",
    "chunk_index": 9,
    "total_chunks": 85,
    "text_content": "in real-life medical images may be substantially small, making it difficult for both human and vaguely implemented models to detect such anomalies. To address this gap, this study proposes a hybrid of seam carving and wavelet decomposition algorithms. The novel hybrid model is able to balance the optimization chal- lenge between feature enhancement and elimination so that suggestive features are enhanced while non-relevant features are eliminated. Secondly, we reinforce our method to address fur",
    "full_text_length": 87026,
    "chunk_length": 1365
  },
  {
    "chunk_id": 2684,
    "paper_filename": "olaide_2024_neuaral_netwrok_with_augmetnation_for_breast_cancer_detection_using_mammography.pdf",
    "paper_title": "Olaide 2024 Neuaral Netwrok With Augmetnation For Breast Cancer Detection Using Mammography",
    "chunk_index": 10,
    "total_chunks": 85,
    "text_content": "eliminate low contrast in the training datasets, which often impairs the performance of CNNs29. The technical contributions of this paper are highlighted below: i. Design of a new CNN structure that uses a novel wavelet transform function ii. Design of a hybrid algorithm of seam carving and wavelet decomposition to support feature enhancement in the image preprocessing phase. iii. Incorporation of a new GAN model for image synthesis and augmentation in the proposed CNN model. iv. Comparative ana",
    "full_text_length": 87026,
    "chunk_length": 1321
  },
  {
    "chunk_id": 2685,
    "paper_filename": "olaide_2024_neuaral_netwrok_with_augmetnation_for_breast_cancer_detection_using_mammography.pdf",
    "paper_title": "Olaide 2024 Neuaral Netwrok With Augmetnation For Breast Cancer Detection Using Mammography",
    "chunk_index": 11,
    "total_chunks": 85,
    "text_content": "of the proposed method are presented and discussed. Finally, in \u201c Conclusion \u201d, t h e conclusion of the paper is discussed. 4 Vol:.(1234567890) Scientific Reports | (2022) 12:5913 | https://doi.org/10.1038/s41598-022-09905-3 www.nature.com/scientificreports/Related works This section presents a review of some related works that used data augmented techniques for training deep learning models in detecting abnormalities from digital mammography and other related areas. Abnormalities in mammograms ",
    "full_text_length": 87026,
    "chunk_length": 1451
  },
  {
    "chunk_id": 2686,
    "paper_filename": "olaide_2024_neuaral_netwrok_with_augmetnation_for_breast_cancer_detection_using_mammography.pdf",
    "paper_title": "Olaide 2024 Neuaral Netwrok With Augmetnation For Breast Cancer Detection Using Mammography",
    "chunk_index": 12,
    "total_chunks": 85,
    "text_content": "at the last fully connected layer. The approach also used a segmentation technique, namely, threshold- and region-based, to automate the process of ROI extraction. The method for the classification was based on applying SVM on mammography images from the digital database for screening mammography (DDSM) and the Curated Breast Imaging Subset of DDSM (CBIS-DDSM). The research successfully classified benign and malignant mass tumors in breast mammography images by obtaining an accuracy of 87.2% wit",
    "full_text_length": 87026,
    "chunk_length": 1307
  },
  {
    "chunk_id": 2687,
    "paper_filename": "olaide_2024_neuaral_netwrok_with_augmetnation_for_breast_cancer_detection_using_mammography.pdf",
    "paper_title": "Olaide 2024 Neuaral Netwrok With Augmetnation For Breast Cancer Detection Using Mammography",
    "chunk_index": 13,
    "total_chunks": 85,
    "text_content": "original works but replaced the last fully connected (FC) layer to output classes. The shallow CNN proposed takes a 224 \u00d7 224 \u00d7 3 image as input, and it consists of 3 convolutional blocks composed of 3 \u00d7 3, 3 fully connected layers, and a soft- max layer. Furthermore, they employed ReLU activation functions, Xavier weight initialization, and the Adam update rule with a base learning rate of 10\u22123 and batched size 64. The best model presents a result of 0.934 for recall at 0.924 for precision. In ",
    "full_text_length": 87026,
    "chunk_length": 1235
  },
  {
    "chunk_id": 2688,
    "paper_filename": "olaide_2024_neuaral_netwrok_with_augmetnation_for_breast_cancer_detection_using_mammography.pdf",
    "paper_title": "Olaide 2024 Neuaral Netwrok With Augmetnation For Breast Cancer Detection Using Mammography",
    "chunk_index": 14,
    "total_chunks": 85,
    "text_content": "that their model obtained a good performance of an average number of false positives of 0.34 and 0.03 when the confidence score was 0.95 in INbreast and GURO, respectively. Similarly, Agarwal et al.33 employed transfer learning to propose a patch-based CNN method for automated mass detection in full-field digital mammograms (FFDM). In addi - tion, they investigated the performances of VGG16, ResNet50, and InceptionV3 architectures on the same dataset while applying the transfer learning techniqu",
    "full_text_length": 87026,
    "chunk_length": 1353
  },
  {
    "chunk_id": 2689,
    "paper_filename": "olaide_2024_neuaral_netwrok_with_augmetnation_for_breast_cancer_detection_using_mammography.pdf",
    "paper_title": "Olaide 2024 Neuaral Netwrok With Augmetnation For Breast Cancer Detection Using Mammography",
    "chunk_index": 15,
    "total_chunks": 85,
    "text_content": "0.91 at 2.1 FPI. In34, the authors demonstrated the existence of superiority when a deep learning-based classifier was used to distinguish malignant and benign breast masses without segmenting the lesions and extracting the predefined image features. In35, an adversarial deep structural network was adopted for use on mammographic images in detecting mass segmentation. The research employed a fully convolutional network (FCN) to model the potential function, followed by conditional random fields ",
    "full_text_length": 87026,
    "chunk_length": 1418
  },
  {
    "chunk_id": 2690,
    "paper_filename": "olaide_2024_neuaral_netwrok_with_augmetnation_for_breast_cancer_detection_using_mammography.pdf",
    "paper_title": "Olaide 2024 Neuaral Netwrok With Augmetnation For Breast Cancer Detection Using Mammography",
    "chunk_index": 16,
    "total_chunks": 85,
    "text_content": "in36 combined Craniocaudal (CC) and Mediolateral-oblique (MLO) mammography views to dif- ferentiate between malignant and benign tumors. They implemented a deep-learning classification method that is based on two view-level decisions, implemented by two neural networks, followed by a single-neuron layer that combines the view-level decisions into a global decision that mimics the biopsy results. The model exploited the detection of features of clustered breast microcalcifications to classify tum",
    "full_text_length": 87026,
    "chunk_length": 1411
  },
  {
    "chunk_id": 2691,
    "paper_filename": "olaide_2024_neuaral_netwrok_with_augmetnation_for_breast_cancer_detection_using_mammography.pdf",
    "paper_title": "Olaide 2024 Neuaral Netwrok With Augmetnation For Breast Cancer Detection Using Mammography",
    "chunk_index": 17,
    "total_chunks": 85,
    "text_content": "of 95.0% above human-level performance. Additionally, Xi et al.38 was able to use classifiers that are trained on labeled image patches and then adapted it to work on full mammogram images for localizing the abnormalities. The models investigated are VGGNet and ResNet, demonstrating the most appreciable accuracy at 92.53% in clas- sifications. Meanwhile, Murali and Dinesh39 employed a deep convolutional neural network (CNN) and random forest classifier to classify ROIs with malignant masses and ",
    "full_text_length": 87026,
    "chunk_length": 1396
  },
  {
    "chunk_id": 2692,
    "paper_filename": "olaide_2024_neuaral_netwrok_with_augmetnation_for_breast_cancer_detection_using_mammography.pdf",
    "paper_title": "Olaide 2024 Neuaral Netwrok With Augmetnation For Breast Cancer Detection Using Mammography",
    "chunk_index": 18,
    "total_chunks": 85,
    "text_content": "views for augmentation purposes. The resulting model was applied to ROIs from MIAS, whole images from INbreast, whole images from MIAS, and ROIs from DDSM + CBIS databases. Performance 5 Vol.:(0123456789) Scientific Reports | (2022) 12:5913 | https://doi.org/10.1038/s41598-022-09905-3 www.nature.com/scientificreports/evaluation of the proposed model showed that they achieved an accuracy of 93.75%. The use of Region-based (R-CNN) was introduced in42 to detect architectural distortion using a supe",
    "full_text_length": 87026,
    "chunk_length": 1407
  },
  {
    "chunk_id": 2693,
    "paper_filename": "olaide_2024_neuaral_netwrok_with_augmetnation_for_breast_cancer_detection_using_mammography.pdf",
    "paper_title": "Olaide 2024 Neuaral Netwrok With Augmetnation For Breast Cancer Detection Using Mammography",
    "chunk_index": 19,
    "total_chunks": 85,
    "text_content": "0.85 sensitivity. On the other hand35, GlimpseNet autonomously extracts multiple regions of interest, classifies them, and then pools them to obtain a diagnosis for the full image. They obtained the result that gained 4.1% improvement. Additionally, Qiu et al.44 proposed a framework using a deep convolutional neural network. The model is an 8-layer deep learning network that involves 3 pairs of convolution-max-pooling layers for automatic feature extraction and a multiple layer perceptron (MLP) ",
    "full_text_length": 87026,
    "chunk_length": 1292
  },
  {
    "chunk_id": 2694,
    "paper_filename": "olaide_2024_neuaral_netwrok_with_augmetnation_for_breast_cancer_detection_using_mammography.pdf",
    "paper_title": "Olaide 2024 Neuaral Netwrok With Augmetnation For Breast Cancer Detection Using Mammography",
    "chunk_index": 20,
    "total_chunks": 85,
    "text_content": "learning approach focused on classifying tumors in mammograms as malignant or benign, using the Softmax layer as a classifier. The proposed network was enhanced with a scaling process based on Gaussian pyramids to obtain normalized size regions of interest. The DDSM and BCDR datasets were used in addition to the data augmentation technique. The results of their experiments showed that they obtained an accuracy of 97.28%. In46, the authors presented a novel classification technique for a large da",
    "full_text_length": 87026,
    "chunk_length": 1369
  },
  {
    "chunk_id": 2695,
    "paper_filename": "olaide_2024_neuaral_netwrok_with_augmetnation_for_breast_cancer_detection_using_mammography.pdf",
    "paper_title": "Olaide 2024 Neuaral Netwrok With Augmetnation For Breast Cancer Detection Using Mammography",
    "chunk_index": 21,
    "total_chunks": 85,
    "text_content": "in47 used a wavelet convolution neural network to detect spiculated findings in low-contrast noisy mammograms, such as architectural distortions and spiculated masses. The dataset used for experimentation consisted of CBIS-DDSM, and it reached an accuracy of over 85% for architectural distortions and 88% for spiculated masses. The databases used are the IRMA version of the digital database for screening mammograms (DDSM) and the Mammographic Image Analysis Society (MIAS). The results pertain to ",
    "full_text_length": 87026,
    "chunk_length": 1365
  },
  {
    "chunk_id": 2696,
    "paper_filename": "olaide_2024_neuaral_netwrok_with_augmetnation_for_breast_cancer_detection_using_mammography.pdf",
    "paper_title": "Olaide 2024 Neuaral Netwrok With Augmetnation For Breast Cancer Detection Using Mammography",
    "chunk_index": 22,
    "total_chunks": 85,
    "text_content": "of lung cancer using computerized tomography (CT) scans12. In addition to using wavelet-based CNN in medical image classification, several domains have also received attention in applying the technique. For example, Peifeng et al.49 proposed integrating a wavelet function on time series data and into a backpropagation neural network (BPNN) and nonlinear autoregressive network with exog- enous inputs (NARX) to achieve WNN and WNARX hybrid models, which were applied as benchmark models. Experiment",
    "full_text_length": 87026,
    "chunk_length": 1379
  },
  {
    "chunk_id": 2697,
    "paper_filename": "olaide_2024_neuaral_netwrok_with_augmetnation_for_breast_cancer_detection_using_mammography.pdf",
    "paper_title": "Olaide 2024 Neuaral Netwrok With Augmetnation For Breast Cancer Detection Using Mammography",
    "chunk_index": 23,
    "total_chunks": 85,
    "text_content": "et al.51 attempted to predict the actual discharge using meteorological data based on a wavelet neural network method. Wang et al.52 analyzed, classified, and forecasted time series data for frequency-awareness using a multilevel Wavelet Decomposition Network (mWDN) supported by Residual Classification Flow (RCF) and multi-frequency Long Short-Term Memory (mLSTM) deep learning models. In a similar domain, Wuwei et al.53 investigated the use of both wavelet neural network and data fusion models. ",
    "full_text_length": 87026,
    "chunk_length": 1337
  },
  {
    "chunk_id": 2698,
    "paper_filename": "olaide_2024_neuaral_netwrok_with_augmetnation_for_breast_cancer_detection_using_mammography.pdf",
    "paper_title": "Olaide 2024 Neuaral Netwrok With Augmetnation For Breast Cancer Detection Using Mammography",
    "chunk_index": 24,
    "total_chunks": 85,
    "text_content": "As reported by30, the use of ROIs does not address the need for feature enhancement in the ROIs samples. Moreover, the ReLU activation function in31 still generalizes on a well-known activation function. Also, using two deep learning models in36 for feature detection is compu - tationally costly compared with the one-model feature-detection-enhancement inclusive mechanism proposed in the model presented in this paper. Similarly, the use of only mainstream preprocessing techniques has no guarante",
    "full_text_length": 87026,
    "chunk_length": 1392
  },
  {
    "chunk_id": 2699,
    "paper_filename": "olaide_2024_neuaral_netwrok_with_augmetnation_for_breast_cancer_detection_using_mammography.pdf",
    "paper_title": "Olaide 2024 Neuaral Netwrok With Augmetnation For Breast Cancer Detection Using Mammography",
    "chunk_index": 25,
    "total_chunks": 85,
    "text_content": "method competitive with what is reported in46,48 so that performance obtained based on the variation of both methods put this study ahead of46,48. 6 Vol:.(1234567890) Scientific Reports | (2022) 12:5913 | https://doi.org/10.1038/s41598-022-09905-3 www.nature.com/scientificreports/Methodology This section presents the proposed concept, which describes the application of seam carving and wavelet decom- position techniques to feature enhancement and extraction of CNN architectures. First, we discus",
    "full_text_length": 87026,
    "chunk_length": 1383
  },
  {
    "chunk_id": 2700,
    "paper_filename": "olaide_2024_neuaral_netwrok_with_augmetnation_for_breast_cancer_detection_using_mammography.pdf",
    "paper_title": "Olaide 2024 Neuaral Netwrok With Augmetnation For Breast Cancer Detection Using Mammography",
    "chunk_index": 26,
    "total_chunks": 85,
    "text_content": "representation for storage on the file system b. A GAN model trained in27 is applied for the image synthetization process to augment the class imbalance in the extracted image samples. The synthetization is necessitated by the need to allow the deep learning model to generalize well on all classes of image samples. c. A combination of the images drawn from the real and synthesized distributions are then applied to an image enhancement technique, namely, contrast-limited adaptive histogram equali",
    "full_text_length": 87026,
    "chunk_length": 1274
  },
  {
    "chunk_id": 2701,
    "paper_filename": "olaide_2024_neuaral_netwrok_with_augmetnation_for_breast_cancer_detection_using_mammography.pdf",
    "paper_title": "Olaide 2024 Neuaral Netwrok With Augmetnation For Breast Cancer Detection Using Mammography",
    "chunk_index": 27,
    "total_chunks": 85,
    "text_content": "performance of the traditional CNN and the proposed wCNN, we supplied the processed images to them for a complete training phase. g. The trained CNN and wCNN architectures are then tested on the test dataset for evaluation using selected metrics. h. The results are then compared for discussion on findings from the study. In Fig. 1, an illustration of the overview of the approach outlined above is presented. The block diagram highlights the flow of the methods applied to achieve the study\u2019s aim. ",
    "full_text_length": 87026,
    "chunk_length": 1281
  },
  {
    "chunk_id": 2702,
    "paper_filename": "olaide_2024_neuaral_netwrok_with_augmetnation_for_breast_cancer_detection_using_mammography.pdf",
    "paper_title": "Olaide 2024 Neuaral Netwrok With Augmetnation For Breast Cancer Detection Using Mammography",
    "chunk_index": 28,
    "total_chunks": 85,
    "text_content": "technique to strengthen further the performance of the deep learning model proposed in the study. We considered the high impact of image synthesis over image transformation, which are both types of data augmentation techniques, to enhance the performance and balance the class distribution of samples in our dataset. A detailed representation of the GAN model applied for the image synthesis task is described in Tables 1 and 2. A further illustration of the two architectures represented by the disc",
    "full_text_length": 87026,
    "chunk_length": 1404
  },
  {
    "chunk_id": 2703,
    "paper_filename": "olaide_2024_neuaral_netwrok_with_augmetnation_for_breast_cancer_detection_using_mammography.pdf",
    "paper_title": "Olaide 2024 Neuaral Netwrok With Augmetnation For Breast Cancer Detection Using Mammography",
    "chunk_index": 29,
    "total_chunks": 85,
    "text_content": "wavelet decomposition, and wavelet convolution (CNN and wCNN) architectures. 7 Vol.:(0123456789) Scientific Reports | (2022) 12:5913 | https://doi.org/10.1038/s41598-022-09905-3 www.nature.com/scientificreports/layer of dense/flattened and fully connected layers that uses a sigmoid activation function. To overcome the problem of poor parameter initialization, batch normalization is performed on each of the layers except for the last layer. Each layer uses a kernel size of 5 \u00d7 5 and filter sizes ",
    "full_text_length": 87026,
    "chunk_length": 1402
  },
  {
    "chunk_id": 2704,
    "paper_filename": "olaide_2024_neuaral_netwrok_with_augmetnation_for_breast_cancer_detection_using_mammography.pdf",
    "paper_title": "Olaide 2024 Neuaral Netwrok With Augmetnation For Breast Cancer Detection Using Mammography",
    "chunk_index": 30,
    "total_chunks": 85,
    "text_content": "different abnormalities associated with breast cancer in digital mammography. We then combined the synthesized images and the real samples for the image preprocessing method. Image preprocessing. Common image preprocessing involves color normalization, noise reduction, edge detection and histogram equalization. In this study, we prepared image samples for the CNN architecture by applying some image preprocessing techniques on samples for the purpose of histogram equalization and noise removal. T",
    "full_text_length": 87026,
    "chunk_length": 1450
  },
  {
    "chunk_id": 2705,
    "paper_filename": "olaide_2024_neuaral_netwrok_with_augmetnation_for_breast_cancer_detection_using_mammography.pdf",
    "paper_title": "Olaide 2024 Neuaral Netwrok With Augmetnation For Breast Cancer Detection Using Mammography",
    "chunk_index": 31,
    "total_chunks": 85,
    "text_content": "pixel(s) that have no significant information in the image and provide a multiresolution and high-resolution presentation of the image. This study combined two compression algorithms, namely, the seam carving and wavelet decomposition algorithms, to improve our image samples. Seam carving was applied to improve content awareness, thereby eliminating pixel(s) locations that, when removed, the image quality was not reduced, nor was the view distorted. The outcome of this is a resized image with no",
    "full_text_length": 87026,
    "chunk_length": 1664
  },
  {
    "chunk_id": 2706,
    "paper_filename": "olaide_2024_neuaral_netwrok_with_augmetnation_for_breast_cancer_detection_using_mammography.pdf",
    "paper_title": "Olaide 2024 Neuaral Netwrok With Augmetnation For Breast Cancer Detection Using Mammography",
    "chunk_index": 32,
    "total_chunks": 85,
    "text_content": "Gaussian distribution. Minibatch size: 32, optimizer: adaptive moment estimation (Adam) (\u03b7 = 0.00001, \u03b21 = 0.5, \u03b22 = 0.999). All weights were initialized using the normal distribution initializer.Input Projection Layer1 Layer2 Layer3 Layer4 Layer5 Layer6 Type Fully ConnectedFractionally Strided ConvolutionFractionally Strided ConvolutionFractionally Strided ConvolutionFractionally Strided ConvolutionFractionally Strided ConvolutionFractionally Strided Convolution Input [1 \u00d7 100] [4 \u00d7 4 \u00d7 1024] [",
    "full_text_length": 87026,
    "chunk_length": 1052
  },
  {
    "chunk_id": 2707,
    "paper_filename": "olaide_2024_neuaral_netwrok_with_augmetnation_for_breast_cancer_detection_using_mammography.pdf",
    "paper_title": "Olaide 2024 Neuaral Netwrok With Augmetnation For Breast Cancer Detection Using Mammography",
    "chunk_index": 33,
    "total_chunks": 85,
    "text_content": "1 - Padding \u2013 Same Same Same Same Same Same Kernel Size \u2013 5 5 5 5 5 5 Kernels \u2013 1024 512 256 128 64 32 Table 2. Discriminator architecture: minibatch size: 32; optimizer: Adam (\u03b7 = 0.0001, \u03b21 = 0.5, \u03b22 = 0.999).Layer1 Layer2 Layer3 Layer4 Layer5 Output Type Convolution Convolution Convolution Convolution Convolution Full Con Input [32 \u00d7 32 \u00d7 2] [64 \u00d7 64 \u00d7 64] [32 \u00d7 32 \u00d7 128] [16 \u00d7 16 \u00d7 256] [8 \u00d7 8 \u00d7 512] [4 \u00d7 4 \u00d7 1024] Output [64 \u00d7 64 \u00d7 64] [32 \u00d7 32 \u00d7 128] [16 \u00d7 16 \u00d7 256] [8 \u00d7 8 \u00d7 512] [4 \u00d7 4 \u00d7 ",
    "full_text_length": 87026,
    "chunk_length": 995
  },
  {
    "chunk_id": 2708,
    "paper_filename": "olaide_2024_neuaral_netwrok_with_augmetnation_for_breast_cancer_detection_using_mammography.pdf",
    "paper_title": "Olaide 2024 Neuaral Netwrok With Augmetnation For Breast Cancer Detection Using Mammography",
    "chunk_index": 34,
    "total_chunks": 85,
    "text_content": "size 5 5 5 5 5 \u2013 Kernels 64 128 256 512 1024 \u2013 8 Vol:.(1234567890) Scientific Reports | (2022) 12:5913 | https://doi.org/10.1038/s41598-022-09905-3 www.nature.com/scientificreports/In Eq. (2 ), we use the energy function to obtain the total gradient matrix for an image, say Img. This provides us with information on the pixels to be preserved in both the horizontal and vertical directions of the image while the remaining pixels are removed. Once the seams of the sample images have been carved out",
    "full_text_length": 87026,
    "chunk_length": 1235
  },
  {
    "chunk_id": 2709,
    "paper_filename": "olaide_2024_neuaral_netwrok_with_augmetnation_for_breast_cancer_detection_using_mammography.pdf",
    "paper_title": "Olaide 2024 Neuaral Netwrok With Augmetnation For Breast Cancer Detection Using Mammography",
    "chunk_index": 35,
    "total_chunks": 85,
    "text_content": "this study to obtain the best output. Generally, the wavelet generating function can be expressed as in Eq. ( 3): where a is the scaling factor and b represents the shift factor, so that a and b, which control the extension and translation operations, are defined as shown in Eq. (4 ). Additionally, Img(t) denotes the representation of our image, and \u03d5(t) represents the mother wavelet function, which is further described below. Now, given an image of size Img(N,M) , we show the 2D wavelet decompo",
    "full_text_length": 87026,
    "chunk_length": 1591
  },
  {
    "chunk_id": 2710,
    "paper_filename": "olaide_2024_neuaral_netwrok_with_augmetnation_for_breast_cancer_detection_using_mammography.pdf",
    "paper_title": "Olaide 2024 Neuaral Netwrok With Augmetnation For Breast Cancer Detection Using Mammography",
    "chunk_index": 36,
    "total_chunks": 85,
    "text_content": "Vol.:(0123456789) Scientific Reports | (2022) 12:5913 | https://doi.org/10.1038/s41598-022-09905-3 www.nature.com/scientificreports/The decomposition of our image Img will yield four (4) coefficients, namely, LL, LH, HL, and HH, which are further categorized into the approximate coefficient (LL), also known as low pass, and the detailed coefficients (LH, HL, and HH), also known as high pass. LH, HL, and HH represent the horizontal (H) view of the details of the image, vertical (V) view of the de",
    "full_text_length": 87026,
    "chunk_length": 1246
  },
  {
    "chunk_id": 2711,
    "paper_filename": "olaide_2024_neuaral_netwrok_with_augmetnation_for_breast_cancer_detection_using_mammography.pdf",
    "paper_title": "Olaide 2024 Neuaral Netwrok With Augmetnation For Breast Cancer Detection Using Mammography",
    "chunk_index": 37,
    "total_chunks": 85,
    "text_content": "V , and D wavelets, and the representation of the corresponding wavelet function is given in Eqs. ( 10), ( 12), and ( 14) as: Figure 3 illustrates using a hierarchical representation of how Img is decomposed using the wavelet functions described previously. To obtain a good resolution of images for our CNN architecture, we allowed the wavelet decomposition function to decompose the original image to the highest N level of n . The two-dimensional (2D) wavelet multilevel decomposition function was",
    "full_text_length": 87026,
    "chunk_length": 1689
  },
  {
    "chunk_id": 2712,
    "paper_filename": "olaide_2024_neuaral_netwrok_with_augmetnation_for_breast_cancer_detection_using_mammography.pdf",
    "paper_title": "Olaide 2024 Neuaral Netwrok With Augmetnation For Breast Cancer Detection Using Mammography",
    "chunk_index": 38,
    "total_chunks": 85,
    "text_content": "(7) LL=\u2205/parenleftbig x,y/parenrightbig =\u2205(x)\u2205(y) (8) W\u2205(s0,m,n)=1\u221amnm\u22121/summationdisplay x=0n\u22121/summationdisplay y=0Img/parenleftbig x,y/parenrightbig \u2205s0,m,n(x,y) (9) LH=\ufffdH/parenleftbig x,y/parenrightbig =\ufffd(x)\u2205(y) (10) WH \ufffd(s,m,n)=1\u221amnm\u22121/summationdisplay x=0n\u22121/summationdisplay y=0Img/parenleftbig x,y/parenrightbig \ufffdH s,m,n(x,y) (11) HL=\ufffdV/parenleftbig x,y/parenrightbig =\u2205(x)\ufffd(y ) (12) WV \ufffd(s,m,n)=1\u221amnm\u22121/summationdisplay x=0n\u22121/summationdisplay y=0Img/parenleftbig x,y/parenrightbig \ufffdV s,m,n(",
    "full_text_length": 87026,
    "chunk_length": 1707
  },
  {
    "chunk_id": 2713,
    "paper_filename": "olaide_2024_neuaral_netwrok_with_augmetnation_for_breast_cancer_detection_using_mammography.pdf",
    "paper_title": "Olaide 2024 Neuaral Netwrok With Augmetnation For Breast Cancer Detection Using Mammography",
    "chunk_index": 39,
    "total_chunks": 85,
    "text_content": "study is shown in Fig. 4. The input is sized in the dimension of 299 \u00d7 299 for the grey-style image. A zero-padding operation is first applied on the input before being passed into the CNN layers. There are six (6) blocks of convolutional operations, with each block comprising three layers of convolution operation followed by a pooling layer. In each convolutional layer, the L2 regularizer is applied with a factor of 0.0002. Additionally, we applied a 3 \u00d7 3 filter for each unit in the convolutio",
    "full_text_length": 87026,
    "chunk_length": 1185
  },
  {
    "chunk_id": 2714,
    "paper_filename": "olaide_2024_neuaral_netwrok_with_augmetnation_for_breast_cancer_detection_using_mammography.pdf",
    "paper_title": "Olaide 2024 Neuaral Netwrok With Augmetnation For Breast Cancer Detection Using Mammography",
    "chunk_index": 40,
    "total_chunks": 85,
    "text_content": "rate of 0.5 is applied after the flattened layer. On the other hand, we show the architecture of the wavelet CNN (wCNN) in Fig. 5 to describe its configura - tion. It assumes a similar architectural configuration compared with the vanilla CNN described earlier. However, the input supplied to it are features extracted from the Wavelet packet decomposition (WPD) function described in the image preprocessing section. The architecture also uses a wavelet function proposed to replace the RELU functio",
    "full_text_length": 87026,
    "chunk_length": 1310
  },
  {
    "chunk_id": 2715,
    "paper_filename": "olaide_2024_neuaral_netwrok_with_augmetnation_for_breast_cancer_detection_using_mammography.pdf",
    "paper_title": "Olaide 2024 Neuaral Netwrok With Augmetnation For Breast Cancer Detection Using Mammography",
    "chunk_index": 41,
    "total_chunks": 85,
    "text_content": "x=n/summationdisplay i=0m/summationdisplay j=0(zi,j.wi,j)+b Figure 4. The architecture of the proposed CNN model for characterization of abnormalities in breast images. Figure 5. The proposed wCNN model architecture for characterization abnormalities in breast images using features from the WPD operation and wavelet function in the convolutional layers. 11 Vol.:(0123456789) Scientific Reports | (2022) 12:5913 | https://doi.org/10.1038/s41598-022-09905-3 www.nature.com/scientificreports/This stud",
    "full_text_length": 87026,
    "chunk_length": 1320
  },
  {
    "chunk_id": 2716,
    "paper_filename": "olaide_2024_neuaral_netwrok_with_augmetnation_for_breast_cancer_detection_using_mammography.pdf",
    "paper_title": "Olaide 2024 Neuaral Netwrok With Augmetnation For Breast Cancer Detection Using Mammography",
    "chunk_index": 42,
    "total_chunks": 85,
    "text_content": "the input as shown in Eq. ( 19), assuming our input is of size m = 3, n = 3: After the zero-padding operation, the resulting input X is then passed into the units of the first convolutional layer so that the convolution operation is applied as described earlier. Then, the summation operation for that unit is computed considering the weights and bias values. The outcome of these operations is then passed into the proposed wavelet function to perform the activation operation. This is summarized in",
    "full_text_length": 87026,
    "chunk_length": 1181
  },
  {
    "chunk_id": 2717,
    "paper_filename": "olaide_2024_neuaral_netwrok_with_augmetnation_for_breast_cancer_detection_using_mammography.pdf",
    "paper_title": "Olaide 2024 Neuaral Netwrok With Augmetnation For Breast Cancer Detection Using Mammography",
    "chunk_index": 43,
    "total_chunks": 85,
    "text_content": "of the CNN architectures described in this section will be demonstrated in the following by applying the activation functions described in Eqs. (17 ) and ( 20) for the CNN and wCNN, respectively. Experimentation In this section, details on the image datasets used for the experimentation are given, and the performance of the image preprocessing operations are evaluated and discussed. Additionally, the parameters and hyperparam- eters used for training the CNN and wCNN architectures are presented.",
    "full_text_length": 87026,
    "chunk_length": 1356
  },
  {
    "chunk_id": 2718,
    "paper_filename": "olaide_2024_neuaral_netwrok_with_augmetnation_for_breast_cancer_detection_using_mammography.pdf",
    "paper_title": "Olaide 2024 Neuaral Netwrok With Augmetnation For Breast Cancer Detection Using Mammography",
    "chunk_index": 44,
    "total_chunks": 85,
    "text_content": "i5-7500 CPU 3.40 GHz, 3.41 GHz; RAM of 16 GB; 64-bit Windows 10 OS. Benchmark datasets for experimentation. The Mammographic Image Analysis Society (MIAS)55 and Curated Breast Imaging Subset (CBIS) of the Digital Database for Screening Mammography (DDSM + CBIS)38 datasets were used for experimental purposes in this study. The two datasets contain samples with normal and abnormal observations. For instance, abnormal samples were classified as either benign or malignant. Those with benign abnormal",
    "full_text_length": 87026,
    "chunk_length": 1298
  },
  {
    "chunk_id": 2719,
    "paper_filename": "olaide_2024_neuaral_netwrok_with_augmetnation_for_breast_cancer_detection_using_mammography.pdf",
    "paper_title": "Olaide 2024 Neuaral Netwrok With Augmetnation For Breast Cancer Detection Using Mammography",
    "chunk_index": 45,
    "total_chunks": 85,
    "text_content": "and DDSM + CBIS datasets, we plot the distribution of the samples across classes and further divide samples into malignant and nonmalignant. In Fig. 6, a comparison of the distribution of classes of samples into malignant and nonmalignant is shown. Additionally, separate graphs for the distribution of classes in the malignant and nonmalignant cases are displayed. Finally, we show a graph for the distribution of all classes. These graphs allow for understanding the spread of samples across the fi",
    "full_text_length": 87026,
    "chunk_length": 1451
  },
  {
    "chunk_id": 2720,
    "paper_filename": "olaide_2024_neuaral_netwrok_with_augmetnation_for_breast_cancer_detection_using_mammography.pdf",
    "paper_title": "Olaide 2024 Neuaral Netwrok With Augmetnation For Breast Cancer Detection Using Mammography",
    "chunk_index": 46,
    "total_chunks": 85,
    "text_content": "normal (N), (BC) benign calcification, benign mass (BM), (17) f(x)=relu (x)=max (0,x) (18) \ufffd(x)=cos(beta .x)e\u2212(X2 2) (19) X=x11x12x13 x21x22x23 x31x32x33Zeropad (X)=00 0x11 0x210 00 x12x130 x22x230 0x31x32x330 00000 (20) \ufffd(X)=cos(1.0.X )e\u2212(X2 2) (21)\u2202x \u2202f=\u2202(x.f) \u2202z1.\u2202(/summationtextn i=0/summationtextm j=0((x\u2217f)i,j.wi,j)+b) \u2202x.\u2202(cos(1.0.x )e\u2212(x2 2)) \u2202\ufffd 12 Vol:.(1234567890) Scientific Reports | (2022) 12:5913 | https://doi.org/10.1038/s41598-022-09905-3 www.nature.com/scientificreports/calcificat",
    "full_text_length": 87026,
    "chunk_length": 1511
  },
  {
    "chunk_id": 2721,
    "paper_filename": "olaide_2024_neuaral_netwrok_with_augmetnation_for_breast_cancer_detection_using_mammography.pdf",
    "paper_title": "Olaide 2024 Neuaral Netwrok With Augmetnation For Breast Cancer Detection Using Mammography",
    "chunk_index": 47,
    "total_chunks": 85,
    "text_content": "publicly available databases and are reported to be commonly diagnosed. We, however, note that Table 3. List of mammography databases applied for experimentation. MLO mediolateral oblique view, CC craniocaudal, N normal, BC benign calcification, BM benign mass, CALC calcification, and M- mass.Database image information Class Distribution Source description Dataset Image views No. samples N BC BM CALC M Description MIAS MLO 3075 2718 45 159 36 117From reduced 200-micron pixel of sizes 1024 \u00d7 1024",
    "full_text_length": 87026,
    "chunk_length": 1352
  },
  {
    "chunk_id": 2722,
    "paper_filename": "olaide_2024_neuaral_netwrok_with_augmetnation_for_breast_cancer_detection_using_mammography.pdf",
    "paper_title": "Olaide 2024 Neuaral Netwrok With Augmetnation For Breast Cancer Detection Using Mammography",
    "chunk_index": 48,
    "total_chunks": 85,
    "text_content": "and further grouped according to the labelling as follows: BC benign calcification, BM benign mass, CALC calcification, M mass. 13 Vol.:(0123456789) Scientific Reports | (2022) 12:5913 | https://doi.org/10.1038/s41598-022-09905-3 www.nature.com/scientificreports/other abnormalities, such as asymmetry and architectural distortion, have been shown to be delicate, subtle and fatal when overlooked3. As a result, an already trained GAN model was applied to generate image samples in these two categori",
    "full_text_length": 87026,
    "chunk_length": 1365
  },
  {
    "chunk_id": 2723,
    "paper_filename": "olaide_2024_neuaral_netwrok_with_augmetnation_for_breast_cancer_detection_using_mammography.pdf",
    "paper_title": "Olaide 2024 Neuaral Netwrok With Augmetnation For Breast Cancer Detection Using Mammography",
    "chunk_index": 49,
    "total_chunks": 85,
    "text_content": "samples were added to those used for training and evaluation while keeping the MIAS images for testing the fully trained CNN and wCNN models. Implementation of image preprocessing. Image preprocessing techniques were applied to all samples drawn from DDSM + CBIS, MIAS and those generated using the GAN model. First, to understand the need for improvement in the images, we plotted their corresponding histogram to investigate how pixel values are distributed. This understanding led to the use of im",
    "full_text_length": 87026,
    "chunk_length": 1248
  },
  {
    "chunk_id": 2724,
    "paper_filename": "olaide_2024_neuaral_netwrok_with_augmetnation_for_breast_cancer_detection_using_mammography.pdf",
    "paper_title": "Olaide 2024 Neuaral Netwrok With Augmetnation For Breast Cancer Detection Using Mammography",
    "chunk_index": 50,
    "total_chunks": 85,
    "text_content": "Methodology\u201d , the CLAHE technique was used for the improvement of the samples. The first row of Fig. 9 shows the raw image, which was not preprocessed, while the second row shows the corresponding image enhanced using the CLAHE method. Clearly, we see that the preprocessing method successfully improved the images with some blurriness eliminated, yielding quality images. The preprocessed image samples were further supplied as input to the seam carving technique to remove the least significant pi",
    "full_text_length": 87026,
    "chunk_length": 1340
  },
  {
    "chunk_id": 2725,
    "paper_filename": "olaide_2024_neuaral_netwrok_with_augmetnation_for_breast_cancer_detection_using_mammography.pdf",
    "paper_title": "Olaide 2024 Neuaral Netwrok With Augmetnation For Breast Cancer Detection Using Mammography",
    "chunk_index": 51,
    "total_chunks": 85,
    "text_content": "BM benign mass, CALC calcification, M mass. Figure 8. Samples of images generated using a GAN model for augmenting samples available from DDSM + CBIS and MIAS datasets. The samples generated images with architectural distortion (AD), asymmetry (ASYM) and calcification (CALC). 14 Vol:.(1234567890) Scientific Reports | (2022) 12:5913 | https://doi.org/10.1038/s41598-022-09905-3 www.nature.com/scientificreports/while the corresponding version applied to CLAHE is seen to be improved. Furthermore, th",
    "full_text_length": 87026,
    "chunk_length": 1412
  },
  {
    "chunk_id": 2726,
    "paper_filename": "olaide_2024_neuaral_netwrok_with_augmetnation_for_breast_cancer_detection_using_mammography.pdf",
    "paper_title": "Olaide 2024 Neuaral Netwrok With Augmetnation For Breast Cancer Detection Using Mammography",
    "chunk_index": 52,
    "total_chunks": 85,
    "text_content": "approximate coefficient (LL) image from the wavelet function is chosen for use here, as shown in Fig. 12. As mentioned in \u201c Methodology \u201d , the LL version of each image containing features supportive of the feature detection task was used for the CNN and wCNN models during the experimentation phase. CNN configuration and training parameters. The CNN and wCNN architectures were tested for twenty (20) epochs, and validation was also performed during the training. The Adam optimization algorithm wa",
    "full_text_length": 87026,
    "chunk_length": 1272
  },
  {
    "chunk_id": 2727,
    "paper_filename": "olaide_2024_neuaral_netwrok_with_augmetnation_for_breast_cancer_detection_using_mammography.pdf",
    "paper_title": "Olaide 2024 Neuaral Netwrok With Augmetnation For Breast Cancer Detection Using Mammography",
    "chunk_index": 53,
    "total_chunks": 85,
    "text_content": "improvement resulting from preprocessing all image inputs using the enhancement technique CLAHE. Figure 11. A progressive display of the preprocessing methods applied to a sample image from the data sources. The original image, its corresponding CLAHE-operated version, its corresponding seam-carving- operated version, and its wavelet-operated version. Figure 12. Comparison of the approximate coefficient (LL) and the detailed subbands (LH horizontal, HL vertical, HH diagonal), also known as high ",
    "full_text_length": 87026,
    "chunk_length": 1493
  },
  {
    "chunk_id": 2728,
    "paper_filename": "olaide_2024_neuaral_netwrok_with_augmetnation_for_breast_cancer_detection_using_mammography.pdf",
    "paper_title": "Olaide 2024 Neuaral Netwrok With Augmetnation For Breast Cancer Detection Using Mammography",
    "chunk_index": 54,
    "total_chunks": 85,
    "text_content": "false-positive rate (FPR), and recall. In this study, some of these metrics were selected to evaluate the performance of the two major deep learning architectures. The following are the metrics applied for result evaluation and the justification for their selection: Accuracy. Accuracy is a widely used metric in most classification and deep learning models. It allows us to evaluate whether we have trained our model well enough to generalize to new samples. This model evaluation using accuracy is ",
    "full_text_length": 87026,
    "chunk_length": 1263
  },
  {
    "chunk_id": 2729,
    "paper_filename": "olaide_2024_neuaral_netwrok_with_augmetnation_for_breast_cancer_detection_using_mammography.pdf",
    "paper_title": "Olaide 2024 Neuaral Netwrok With Augmetnation For Breast Cancer Detection Using Mammography",
    "chunk_index": 55,
    "total_chunks": 85,
    "text_content": "we show how to compute specificity. Sensitivity. This metric allows for computing the number of actual positive cases in our datasets that were pre- dicted as true positives. The equation for computing the sensitivity metric is given in Eq. (24) Precision. To eliminate the presence of false positives and ensure that our model correctly classifies negative cases as negative and positive cases as positive, we use the precision metric to evaluate our models. The precision metric supports the abilit",
    "full_text_length": 87026,
    "chunk_length": 1223
  },
  {
    "chunk_id": 2730,
    "paper_filename": "olaide_2024_neuaral_netwrok_with_augmetnation_for_breast_cancer_detection_using_mammography.pdf",
    "paper_title": "Olaide 2024 Neuaral Netwrok With Augmetnation For Breast Cancer Detection Using Mammography",
    "chunk_index": 56,
    "total_chunks": 85,
    "text_content": "to 0.0 demonstrates poor performance. The equation is given in (26). Recall. To measure the proposed models\u2019 ability to pick out positive samples from the data source used for the experiment, we evaluate them using recall metrics. A higher value obtained for recall implies how accurately our model can identify abnormalities in the datasets. The equation for computing the metric is given in (27). In the following section, the metrics discussed here are applied to all experiments carried out for a",
    "full_text_length": 87026,
    "chunk_length": 1351
  },
  {
    "chunk_id": 2731,
    "paper_filename": "olaide_2024_neuaral_netwrok_with_augmetnation_for_breast_cancer_detection_using_mammography.pdf",
    "paper_title": "Olaide 2024 Neuaral Netwrok With Augmetnation For Breast Cancer Detection Using Mammography",
    "chunk_index": 57,
    "total_chunks": 85,
    "text_content": "section then concludes by highlighting the relevance of the proposed approach in the models\u2019 applicability to breast cancer detection. Four (4) major experiments were conducted as follows: experimenting with the proposed CNN with normal samples, experimenting with the proposed CNN with CLAHE-WPD-operated samples, experimenting with the proposed wavelet-CNN with CLAHE-WPD-operated samples, and wavelet-CNN with CLAHE-WPD-GAN (22) Accuracy =TP+TN (TP+TN+FP+FN) (23) Speci\ufb01city =TN (TN+FP) (24) Speci",
    "full_text_length": 87026,
    "chunk_length": 1461
  },
  {
    "chunk_id": 2732,
    "paper_filename": "olaide_2024_neuaral_netwrok_with_augmetnation_for_breast_cancer_detection_using_mammography.pdf",
    "paper_title": "Olaide 2024 Neuaral Netwrok With Augmetnation For Breast Cancer Detection Using Mammography",
    "chunk_index": 58,
    "total_chunks": 85,
    "text_content": "to 1.77 at the tenth epoch. For the validation phase, we observed that the loss value dropped from 40.52 to 1.66. These patterns in the change of values for both accuracy and loss values demonstrate a good performance by the CNN architecture in detecting and classifying features. In Fig. 13, a graph illustrating the plot of the values obtained in both the training and validation phases is shown. Similarly, we trained the wavelet-CNN architecture under the same configuration with ten (10) trainin",
    "full_text_length": 87026,
    "chunk_length": 1279
  },
  {
    "chunk_id": 2733,
    "paper_filename": "olaide_2024_neuaral_netwrok_with_augmetnation_for_breast_cancer_detection_using_mammography.pdf",
    "paper_title": "Olaide 2024 Neuaral Netwrok With Augmetnation For Breast Cancer Detection Using Mammography",
    "chunk_index": 59,
    "total_chunks": 85,
    "text_content": "an improvement compared with the basic CNN structure. Relatively similar loss values are seen in basic CNN and that of wavelet-CNN. The implication is that the wavelet transformation function competes with those popularly used in the literature. The graphing of the values for both the accuracy and loss are shown in Fig. 14. Now, we investigate the performance combining the decomposing wavelet function and the wavelet trans - formation function to compare the output with that previously discussed",
    "full_text_length": 87026,
    "chunk_length": 1273
  },
  {
    "chunk_id": 2734,
    "paper_filename": "olaide_2024_neuaral_netwrok_with_augmetnation_for_breast_cancer_detection_using_mammography.pdf",
    "paper_title": "Olaide 2024 Neuaral Netwrok With Augmetnation For Breast Cancer Detection Using Mammography",
    "chunk_index": 60,
    "total_chunks": 85,
    "text_content": "for the first and last epoch are 60.85 and 4.15, respectively. This implies that the wavelet transform function sustains clas- sification accuracy, while the quality of image samples supplied as input contributes to the loss values obtained. In Fig. 15, the graph showing the plots for the accuracy and loss values obtained for ten (10) epochs are shown. Having confirmed that input samples determine the loss values during training and evaluation, we augment our datasets using samples synthesized u",
    "full_text_length": 87026,
    "chunk_length": 1353
  },
  {
    "chunk_id": 2735,
    "paper_filename": "olaide_2024_neuaral_netwrok_with_augmetnation_for_breast_cancer_detection_using_mammography.pdf",
    "paper_title": "Olaide 2024 Neuaral Netwrok With Augmetnation For Breast Cancer Detection Using Mammography",
    "chunk_index": 61,
    "total_chunks": 85,
    "text_content": "shown in Fig. 16. In this experiment, we investigate what performance improvement is obtainable when the synthesized samples are subjected to wavelet-CNN and when combined with samples derived from seam carving with wavelet decomposition. To demonstrate a comparison of the methods, we summarize the performance of the methods over the ten (10) epochs for all experiments performed and provide the outcome of the values obtained for both accuracy and loss. In Table 4, these values are listed and com",
    "full_text_length": 87026,
    "chunk_length": 1235
  },
  {
    "chunk_id": 2736,
    "paper_filename": "olaide_2024_neuaral_netwrok_with_augmetnation_for_breast_cancer_detection_using_mammography.pdf",
    "paper_title": "Olaide 2024 Neuaral Netwrok With Augmetnation For Breast Cancer Detection Using Mammography",
    "chunk_index": 62,
    "total_chunks": 85,
    "text_content": "to improve its learning curve, the classification accuracy improved to attain stability. The summary of performances of CNN, Wavelet-CNN + CLAHE, Wavelet-CNN + CLAHE + WPD, and Wave - let-CNN + CLAHE + WPD + GAN models as listed in the table reveals the marginal difference existing among them. This is particularly obvious in the classification accuracy for the four models experimented with. In the cases of wavelet-CNN + CLAHE and wavelet-CNN + CLAHE + WPD, there appeared to be a marginal differe",
    "full_text_length": 87026,
    "chunk_length": 1263
  },
  {
    "chunk_id": 2737,
    "paper_filename": "olaide_2024_neuaral_netwrok_with_augmetnation_for_breast_cancer_detection_using_mammography.pdf",
    "paper_title": "Olaide 2024 Neuaral Netwrok With Augmetnation For Breast Cancer Detection Using Mammography",
    "chunk_index": 63,
    "total_chunks": 85,
    "text_content": "+ CLAHE + WPD, use the proposed wavelet function, hence the competitive result obtained in both experiments. This confirmed that using the wavelet function improved the CNN model compared with what is obtained using the RELU activation function. After completely training the two models CNN and wavelet-CNN, we applied the MIAS dataset for the test- ing phase. This became necessary to demonstrate fairness for the testing procedure of the proposed models. We considered that since the MIAS samples a",
    "full_text_length": 87026,
    "chunk_length": 1243
  },
  {
    "chunk_id": 2738,
    "paper_filename": "olaide_2024_neuaral_netwrok_with_augmetnation_for_breast_cancer_detection_using_mammography.pdf",
    "paper_title": "Olaide 2024 Neuaral Netwrok With Augmetnation For Breast Cancer Detection Using Mammography",
    "chunk_index": 64,
    "total_chunks": 85,
    "text_content": "The trained models of wavelet-CNN + CLAHE and wavelet-CNN + CLAHE + WPD, Figure 15. A graphical illustration of accuracy and loss value distribution for ten (10) epochs on the wavelet- CNN model using CLAHE + WPD samples. Figure 16. A graphical illustration of accuracy and loss value distribution for ten (10) epochs on the wavelet- CNN + CLAHE + WPD model using real and GAN synthesized samples. 19 Vol.:(0123456789) Scientific Reports | (2022) 12:5913 | https://doi.org/10.1038/s41598-022-09905-3 ",
    "full_text_length": 87026,
    "chunk_length": 1332
  },
  {
    "chunk_id": 2739,
    "paper_filename": "olaide_2024_neuaral_netwrok_with_augmetnation_for_breast_cancer_detection_using_mammography.pdf",
    "paper_title": "Olaide 2024 Neuaral Netwrok With Augmetnation For Breast Cancer Detection Using Mammography",
    "chunk_index": 65,
    "total_chunks": 85,
    "text_content": "study. These prediction performances with the trained models demonstrate that using the wavelet transform function to extract features in the sample images in digital mammography is relevant. Meanwhile, we computed the training and prediction time for all four models experimented on in this study. In Fig. 17, the graphed results show that the training time for the basic CNN was lower than those of the proposed hybrid methods, so its prediction time was unattractively high. The other three models",
    "full_text_length": 87026,
    "chunk_length": 1301
  },
  {
    "chunk_id": 2740,
    "paper_filename": "olaide_2024_neuaral_netwrok_with_augmetnation_for_breast_cancer_detection_using_mammography.pdf",
    "paper_title": "Olaide 2024 Neuaral Netwrok With Augmetnation For Breast Cancer Detection Using Mammography",
    "chunk_index": 66,
    "total_chunks": 85,
    "text_content": "to be some closeness in their demand for computational power, we noticed that models with wavelet decomposition and seam carving algorithms consumed more time during the prediction phase. Further to comparative analysis of the performance of our models in the case of different experimentations carried out, we compared our proposed technique with those reported in the literature. This allows for justifying the relevance of the proposed approach compared with those that have shown state-of-the-art",
    "full_text_length": 87026,
    "chunk_length": 1333
  },
  {
    "chunk_id": 2741,
    "paper_filename": "olaide_2024_neuaral_netwrok_with_augmetnation_for_breast_cancer_detection_using_mammography.pdf",
    "paper_title": "Olaide 2024 Neuaral Netwrok With Augmetnation For Breast Cancer Detection Using Mammography",
    "chunk_index": 67,
    "total_chunks": 85,
    "text_content": "the outcome of this study gave 0.9990. Interestingly, we compared the performance of our wavelet-CNN with a similar wavelet- CNN in46, and the result showed that our approach outperformed their own with a 16.16% increase. This again confirms the viability and relevance of the proposed wavelet transform function and the hybrid of seam carving with wavelet decomposition algorithms in this study. We see that classification accuracy is greatly enhanced compared with state-of-the-art models that have",
    "full_text_length": 87026,
    "chunk_length": 1354
  },
  {
    "chunk_id": 2742,
    "paper_filename": "olaide_2024_neuaral_netwrok_with_augmetnation_for_breast_cancer_detection_using_mammography.pdf",
    "paper_title": "Olaide 2024 Neuaral Netwrok With Augmetnation For Breast Cancer Detection Using Mammography",
    "chunk_index": 68,
    "total_chunks": 85,
    "text_content": "+ CBIS benchmarked datasets used for training on last ten (10) epochs.EpochCNN Wavelet-CNN + CLAHEWavelet- CNN + CLAHE + WPDWavelet- CNN + CLAHE + WPD + GAN Accuracy Loss function Accuracy Loss function Accuracy Loss function Accuracy Loss function 1 0.8683 53.8682 0.8547 53.9556 0.8586 60.8583 0.8038 61.5979 2 0.8699 30.2180 0.8692 30.5985 0.8695 46.0458 0.8208 47.0644 3 0.8699 14.9185 0.8716 15.3767 0.8718 33.0377 0.8237 34.0883 4 0.8699 7.5588 0.8720 7.9842 0.8720 22.5305 0.8240 23.4251 5 0.8",
    "full_text_length": 87026,
    "chunk_length": 1304
  },
  {
    "chunk_id": 2743,
    "paper_filename": "olaide_2024_neuaral_netwrok_with_augmetnation_for_breast_cancer_detection_using_mammography.pdf",
    "paper_title": "Olaide 2024 Neuaral Netwrok With Augmetnation For Breast Cancer Detection Using Mammography",
    "chunk_index": 69,
    "total_chunks": 85,
    "text_content": "data augmentation techniques on the MIAS benchmarked datasets used for testing.Model Accuracy Specificity Precision F1-score Recall CNN 0.8751 1.0 0.8751 0.8751 0.8751 Wavelet-CNN + CLAHE 0.9990 1.0 0.9990 0.9990 0.9990 Wavelet-CNN + CLAHE + WPD 0.9990 1.0 0.9990 0.9990 0.9990 Wavelet-CNN + GAN + WPD 0.9990 1.0 0.9990 0.9990 0.9990 20 Vol:.(1234567890) Scientific Reports | (2022) 12:5913 | https://doi.org/10.1038/s41598-022-09905-3 www.nature.com/scientificreports/Conclusion In this paper, we pr",
    "full_text_length": 87026,
    "chunk_length": 1439
  },
  {
    "chunk_id": 2744,
    "paper_filename": "olaide_2024_neuaral_netwrok_with_augmetnation_for_breast_cancer_detection_using_mammography.pdf",
    "paper_title": "Olaide 2024 Neuaral Netwrok With Augmetnation For Breast Cancer Detection Using Mammography",
    "chunk_index": 70,
    "total_chunks": 85,
    "text_content": "category. The combined methods were applied to DDSM + CBIS and MIAS datasets for experimentation. The results and discussion of the findings showed that the proposed method in this study improved performance compared with the basic CNN structure. In the future, we propose investigating the performance increment that will result from applying the wavelet transform function in the fully connected layers of the CNN architecture. In addition, the proposed method demonstrates that it can enhance the ",
    "full_text_length": 87026,
    "chunk_length": 1406
  },
  {
    "chunk_id": 2745,
    "paper_filename": "olaide_2024_neuaral_netwrok_with_augmetnation_for_breast_cancer_detection_using_mammography.pdf",
    "paper_title": "Olaide 2024 Neuaral Netwrok With Augmetnation For Breast Cancer Detection Using Mammography",
    "chunk_index": 71,
    "total_chunks": 85,
    "text_content": ") CNNw avelet-CNN+CLAHE wavelet-CNN+CLAHE+WPD wavelet-CNN+CLAHE+WPD_GAN Figure 17. A comparison of computational time for (a ) training and (b ) prediction of basic CNN, wavelet- CNN on samples from CLAHE, WPD and GAN-based data augmentation techniques. Table 6. Comparison of the performance of the proposed CNN and wavelet-CNN methods with similar approaches and the same datasets.Author reference Method Performance 5CNN and data augmentation Accuracy: 0.90 56DeepCAD: multilayer deep-learning arc",
    "full_text_length": 87026,
    "chunk_length": 1495
  },
  {
    "chunk_id": 2746,
    "paper_filename": "olaide_2024_neuaral_netwrok_with_augmetnation_for_breast_cancer_detection_using_mammography.pdf",
    "paper_title": "Olaide 2024 Neuaral Netwrok With Augmetnation For Breast Cancer Detection Using Mammography",
    "chunk_index": 72,
    "total_chunks": 85,
    "text_content": "23 March 2022 References 1. Xu, S. et al. The global, regional, and national burden and trends of breast cancer from 1990 to 2019: Results From the global burden of disease study 2019. Front. Oncol. 20, 20 (2021). 2. Ferlay, J. et al. Estimates of worldwide burden of cancer in 2008. Int. J. Cancer 127(12), 893\u2013917 (2010). 3. Oyelade, O. & Ezugwu, A. A State-of-the-art survey on deep learning approaches in detection of architectural distortion from digital mammographic data. IEEE Access 8, 148644",
    "full_text_length": 87026,
    "chunk_length": 1293
  },
  {
    "chunk_id": 2747,
    "paper_filename": "olaide_2024_neuaral_netwrok_with_augmetnation_for_breast_cancer_detection_using_mammography.pdf",
    "paper_title": "Olaide 2024 Neuaral Netwrok With Augmetnation For Breast Cancer Detection Using Mammography",
    "chunk_index": 73,
    "total_chunks": 85,
    "text_content": "Process. Control 65, 20 (2021). 6. Oyelade, O. & Ezugwu, A. A bioinspired neural architecture search based convolutional neural network for breast cancer detection using histopathology images. Sci. Rep. 11(1), 1\u201328 (2021). 7. Oyelade, O. & Ezugwu, A. Characterization of abnormalities in breast cancer images using nature-inspired metaheuristic optimized convolutional neural networks model. Concurr. Comput. Pract. Exp. 20, 20 (2021). 8. Zeng, N. et al. A small-sized object detection oriented multi",
    "full_text_length": 87026,
    "chunk_length": 1318
  },
  {
    "chunk_id": 2748,
    "paper_filename": "olaide_2024_neuaral_netwrok_with_augmetnation_for_breast_cancer_detection_using_mammography.pdf",
    "paper_title": "Olaide 2024 Neuaral Netwrok With Augmetnation For Breast Cancer Detection Using Mammography",
    "chunk_index": 74,
    "total_chunks": 85,
    "text_content": "(2021). 11. Li, X. et al. A wavelet transform-assisted convolutional neural network multi-model framework for monitoring large-scale fluo - rochemical engineering processes. MDPI Processes 8, 20 (2020). 12. Sarhan, A. M. A novel lung cancer detection method using wavelet. J. Biomed. Sci. Eng. 20, 81\u201392 (2020). 13. Marsi, S., Bhattacharya, J., Molina, R. & Ramponi, G. A nonlinear convolution network for image processing. MDPI Electron. 10, 2 (2021). 14. Geras, K. J., Mann, R. M. & Moy, L. Artific",
    "full_text_length": 87026,
    "chunk_length": 1285
  },
  {
    "chunk_id": 2749,
    "paper_filename": "olaide_2024_neuaral_netwrok_with_augmetnation_for_breast_cancer_detection_using_mammography.pdf",
    "paper_title": "Olaide 2024 Neuaral Netwrok With Augmetnation For Breast Cancer Detection Using Mammography",
    "chunk_index": 75,
    "total_chunks": 85,
    "text_content": "(2021). 17. Matsuyama, E., Takehara, M. & Tsai, D.-Y . Using a wavelet-based and fine-tuned convolutional neural network for classification of breast density in mammographic images. Open J. Med. Imaging 20, 17\u201329 (2020). 18. Mewada, H. K., Patel, A. V ., Hassaballah, M., Alkinani, M. H. & Mahant, K. Spectral\u2013spatial features integrated convolution neural network for breast cancer classification. MDPI Sens. 20(17), 4747 (2020). 19. Li, X. et al. A wavelet transform-assisted convolutional neural n",
    "full_text_length": 87026,
    "chunk_length": 1341
  },
  {
    "chunk_id": 2750,
    "paper_filename": "olaide_2024_neuaral_netwrok_with_augmetnation_for_breast_cancer_detection_using_mammography.pdf",
    "paper_title": "Olaide 2024 Neuaral Netwrok With Augmetnation For Breast Cancer Detection Using Mammography",
    "chunk_index": 76,
    "total_chunks": 85,
    "text_content": "Sun, Z. & Tan, T. Wavelet-SRNet: A wavelet-based CNN for multi-scale face super resolution. In: Proceedings of the IEEE International Conference on Computer Vision, 1689\u20131697. 23. Gunasekaran, S., Rajan, S., Moses, L., Vikram, S., Subalakshmi, M. & Shudhersini, B. Wavelet based CNN for diagnosis of COVID 19 using chest X ray. IOP Conf. Series: Materials Science and Engineering (2021). 24. Fujieda, S., Takayama, K. & Hachisuka, T. Wavelet convolutional neural networks. arXiv: 1805. 08620 (2018). ",
    "full_text_length": 87026,
    "chunk_length": 1330
  },
  {
    "chunk_id": 2751,
    "paper_filename": "olaide_2024_neuaral_netwrok_with_augmetnation_for_breast_cancer_detection_using_mammography.pdf",
    "paper_title": "Olaide 2024 Neuaral Netwrok With Augmetnation For Breast Cancer Detection Using Mammography",
    "chunk_index": 77,
    "total_chunks": 85,
    "text_content": "Computer and Energy Technologies (ICECET) (2021). 28. Zheng, Q., Y ang, M., Y ang, J., Zhang, Q. & Zhang, X. Improvement of generalization ability of deep CNN via implicit regularization in two-stage training process. IEEE Access 6, 15844\u201315869 (2018). 29. Feng, J. et al. Breast mass detection in digital mammogram based on gestalt psychology. J. Healthc. Eng. 20, 1\u201313 (2018). 30. Ragab, D. A., Sharkas, M., Marshall, S. & Ren, J. Breast cancer detection using deep convolutional neural networks an",
    "full_text_length": 87026,
    "chunk_length": 1271
  },
  {
    "chunk_id": 2752,
    "paper_filename": "olaide_2024_neuaral_netwrok_with_augmetnation_for_breast_cancer_detection_using_mammography.pdf",
    "paper_title": "Olaide 2024 Neuaral Netwrok With Augmetnation For Breast Cancer Detection Using Mammography",
    "chunk_index": 78,
    "total_chunks": 85,
    "text_content": "& Y ap, M. H. Automatic mass detection in mammograms using deep convolutional neural networks. J. Med. Imaging 6, 3 (2019). 34. Arevalo, J., Gonz\u00e1lez, F., Ramos-Poll\u00e1n, R., Oliveira, J. & Lopez, M. Convolutional neural networks for mammography mass lesion classification. In: Engineering in Medicine and Biology Society (EMBC) 2015 37th Annual International Conference of the IEEE (2015). 35. Hang, W ., Liu, Z., & Hannun, A. GlimpseNet: Attentional methods for full-image mammogram diagnosis. Stanfo",
    "full_text_length": 87026,
    "chunk_length": 1353
  },
  {
    "chunk_id": 2753,
    "paper_filename": "olaide_2024_neuaral_netwrok_with_augmetnation_for_breast_cancer_detection_using_mammography.pdf",
    "paper_title": "Olaide 2024 Neuaral Netwrok With Augmetnation For Breast Cancer Detection Using Mammography",
    "chunk_index": 79,
    "total_chunks": 85,
    "text_content": "the IEEE. IEEE: 201 (2017). 38. Xi, P ., Shu, C. & Goubran, R. Abnormality detection in mammography using deep convolutional neural networks. arXiv: 1803. 01906 v1 [cs.CV] (2018). 39. Murali, S. M. & Dinesh, M. S. Model based approach for detection of architectural distortions and spiculated masses in mam- mograms. Int. J. Comput. Sci. Eng. 3(11), 3534\u20133546 (2011). 22 Vol:.(1234567890) Scientific Reports | (2022) 12:5913 | https://doi.org/10.1038/s41598-022-09905-3 www.nature.com/scientificrepor",
    "full_text_length": 87026,
    "chunk_length": 1366
  },
  {
    "chunk_id": 2754,
    "paper_filename": "olaide_2024_neuaral_netwrok_with_augmetnation_for_breast_cancer_detection_using_mammography.pdf",
    "paper_title": "Olaide 2024 Neuaral Netwrok With Augmetnation For Breast Cancer Detection Using Mammography",
    "chunk_index": 80,
    "total_chunks": 85,
    "text_content": "mammograms. In Biomedical Imaging (ISBI 2017) 2017 IEEE 14th International Symposium on IEEE, pp. 552\u2013556 (2017). 43. Bakalo, R., Goldberger, J. & Ben-Ari, R. A dual branch deep neural network for classification and detection in mammograms. arXiv: 1904. 12589 (2019). 44. Qiu, Y . et al. A new approach to develop computer-aided diagnosis scheme of breast mass classification using deep learning tech- nology. J X-Ray Sci Technol 20, 1\u201313 (2017). 45. Bakkour, I. I. & Afdel, K. Breast tumor classific",
    "full_text_length": 87026,
    "chunk_length": 1285
  },
  {
    "chunk_id": 2755,
    "paper_filename": "olaide_2024_neuaral_netwrok_with_augmetnation_for_breast_cancer_detection_using_mammography.pdf",
    "paper_title": "Olaide 2024 Neuaral Netwrok With Augmetnation For Breast Cancer Detection Using Mammography",
    "chunk_index": 81,
    "total_chunks": 85,
    "text_content": "networks and genetically discovered false color input enhancement. J. Digit. Imaging 4(30), 499\u2013505 (2017). 48. Liu, J.-W ., Zuo, F.-L., Guo, Y .-X., Li, T.-Y . & Chen, J.-M. Research on improved wavelet convolutional wavelet. Appl. Intell. 20, 4106\u20134126 (2021). 49. Li, P . et al. A comparative analysis of artificial neural networks and wavelet hybrid approaches to long-term toxic heavy metal prediction. Sci. Rep. 10, 20 (2020). 50. Nourani, V . & Andalib, G. Wavelet based artificial intelligenc",
    "full_text_length": 87026,
    "chunk_length": 1275
  },
  {
    "chunk_id": 2756,
    "paper_filename": "olaide_2024_neuaral_netwrok_with_augmetnation_for_breast_cancer_detection_using_mammography.pdf",
    "paper_title": "Olaide 2024 Neuaral Netwrok With Augmetnation For Breast Cancer Detection Using Mammography",
    "chunk_index": 82,
    "total_chunks": 85,
    "text_content": "Conference on Knowledge Discovery & Data Mining, pp. 2437\u20132446 (2018). 53. Liu, W . & Y an, J. Financial time series image algorithm based on wavelet analysis and data fusion. J. Sens. 20, 21 (2021). 54. Shah, F. A. & Debnath, L. Wavelet neural network model for yield spread forecasting. MDPI Math. 5(4), 72 (2017). 55. Marches, M. Megapixel size image creation using generative adversarial networks. ArXiv (2017). 56. Abbas, Q. DeepCAD: A computer-aided diagnosis system for mammographic masses usi",
    "full_text_length": 87026,
    "chunk_length": 1397
  },
  {
    "chunk_id": 2757,
    "paper_filename": "olaide_2024_neuaral_netwrok_with_augmetnation_for_breast_cancer_detection_using_mammography.pdf",
    "paper_title": "Olaide 2024 Neuaral Netwrok With Augmetnation For Breast Cancer Detection Using Mammography",
    "chunk_index": 83,
    "total_chunks": 85,
    "text_content": "A.E.E. Reprints and permissions information is available at www.nature.com/reprints. Publisher\u2019s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations. Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, pr",
    "full_text_length": 87026,
    "chunk_length": 1100
  },
  {
    "chunk_id": 2758,
    "paper_filename": "olaide_2024_neuaral_netwrok_with_augmetnation_for_breast_cancer_detection_using_mammography.pdf",
    "paper_title": "Olaide 2024 Neuaral Netwrok With Augmetnation For Breast Cancer Detection Using Mammography",
    "chunk_index": 84,
    "total_chunks": 85,
    "text_content": "of this licence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/. \u00a9 The Author(s) 2022",
    "full_text_length": 87026,
    "chunk_length": 96
  },
  {
    "chunk_id": 2759,
    "paper_filename": "parita_2024_new_mammography_dataset_for-breast_cancer.pdf",
    "paper_title": "Parita 2024 New Mammography Dataset For-Breast Cancer",
    "chunk_index": 0,
    "total_chunks": 49,
    "text_content": "Vol.:(0123456789)1 3Biomedical Engineering Letters (2024) 14:317\u2013330 https://doi.org/10.1007/s13534-023-00339-y ORIGINAL ARTICLE Digital mammography dataset for breast cancer diagnosis research (DMID) with breast mass segmentation analysis Parita Oza1 \u00b7 Urvi Oza2 \u00b7 Rajiv Oza3 \u00b7 Paawan Sharma4 \u00b7 Samir Patel4 \u00b7 Pankaj Kumar1 \u00b7 Bakul Gohel2 Received: 24 February 2023 / Revised: 8 November 2023 / Accepted: 28 November 2023 / Published online: 21 December 2023 \u00a9 Korean Society of Medical and Biologic",
    "full_text_length": 47726,
    "chunk_length": 1434
  },
  {
    "chunk_id": 2760,
    "paper_filename": "parita_2024_new_mammography_dataset_for-breast_cancer.pdf",
    "paper_title": "Parita 2024 New Mammography Dataset For-Breast Cancer",
    "chunk_index": 1,
    "total_chunks": 49,
    "text_content": "etc. This paper aims to introduce and describe a new mammographic database. Methods: The proposed dataset comprises mammograms with several lesions, such as masses, calcifications, architectural distortions, and asymmetries. In addition, a radiologist report is provided, describing the details of the breast, such as breast density, description of abnormality present, condition of the skin, nipple and pectoral muscles, etc., for each mammogram. Results:We present results of commonly used segmenta",
    "full_text_length": 47726,
    "chunk_length": 1387
  },
  {
    "chunk_id": 2761,
    "paper_filename": "parita_2024_new_mammography_dataset_for-breast_cancer.pdf",
    "paper_title": "Parita 2024 New Mammography Dataset For-Breast Cancer",
    "chunk_index": 2,
    "total_chunks": 49,
    "text_content": "start of clinical signs are the most common ways to identify this disease [1 ]. It is the most common disease, affecting 2.1 million people yearly and accounting for most cancer fatali- ties. About 500,000 individuals each year die from breast cancer, which accounts for 15% of cancer mortality [2 ]. Early identification and diagnosis of breast cancer are criti- cal for lowering cancer-related death rates. As a result, the medical community advises regular screening for breast tumours. Digital ma",
    "full_text_length": 47726,
    "chunk_length": 1362
  },
  {
    "chunk_id": 2762,
    "paper_filename": "parita_2024_new_mammography_dataset_for-breast_cancer.pdf",
    "paper_title": "Parita 2024 New Mammography Dataset For-Breast Cancer",
    "chunk_index": 3,
    "total_chunks": 49,
    "text_content": "undiscovered lesions [4 ]. Writing medical reports with descriptive information about symptoms and interpreta- tion of findings is a critical task radiologists perform. Deep learning (DL) based computer-aided detection and diagnosis (CAD) technologies have been developed in the last two decades to assist radiologists in interpreting mammogram images and help reduce this rate [4 ]. DL is an improvement of artificial neural networks, consisting of more layers that permit higher levels of abstracti",
    "full_text_length": 47726,
    "chunk_length": 1447
  },
  {
    "chunk_id": 2763,
    "paper_filename": "parita_2024_new_mammography_dataset_for-breast_cancer.pdf",
    "paper_title": "Parita 2024 New Mammography Dataset For-Breast Cancer",
    "chunk_index": 4,
    "total_chunks": 49,
    "text_content": "1 3 shows that the training set\u2019s size significantly impacts the effectiveness of supervised DL algorithms [5\u20137]. Moreover, these data must be accurately annotated by specialized radi- ologists. The most prevalent issue in medical imaging is the need for large, balanced, and well-annotated datasets since manual annotation of medical images is time-consuming and tedious [8 , 9]. Hence, the contribution of a new dataset is always essential and valuable for the research commu - nity of this domain.",
    "full_text_length": 47726,
    "chunk_length": 1328
  },
  {
    "chunk_id": 2764,
    "paper_filename": "parita_2024_new_mammography_dataset_for-breast_cancer.pdf",
    "paper_title": "Parita 2024 New Mammography Dataset For-Breast Cancer",
    "chunk_index": 5,
    "total_chunks": 49,
    "text_content": "in the corresponding sections of the paper. 1.1 Motivation and research contribution Researchers need datasets to develop, test, and evaluate mammogram-based CAD systems for breast cancer diag- nosis. Most mammography datasets are private to the organi- zation, and few are publicly available. These mammogram datasets are also crucial for comparing the outcomes of dif- ferent investigations. Additionally, these datasets may also be used to teach and train students in the field of medicine [4]. Th",
    "full_text_length": 47726,
    "chunk_length": 1302
  },
  {
    "chunk_id": 2765,
    "paper_filename": "parita_2024_new_mammography_dataset_for-breast_cancer.pdf",
    "paper_title": "Parita 2024 New Mammography Dataset For-Breast Cancer",
    "chunk_index": 6,
    "total_chunks": 49,
    "text_content": "our knowledge, this would be the first dataset in India supported by a radiology report and other essential details needed to train and test the DL models for breast cancer diagnosis. We also analyze the breast mass segmentation model on our proposed model. The goal of designing an AI-based breast cancer diagnosis model is to detect and seg- ment the mass/abnormality boundaries and classify them as benign or malignant in an end-to-end framework. As breast mass is one of the most distinctive symp",
    "full_text_length": 47726,
    "chunk_length": 1277
  },
  {
    "chunk_id": 2766,
    "paper_filename": "parita_2024_new_mammography_dataset_for-breast_cancer.pdf",
    "paper_title": "Parita 2024 New Mammography Dataset For-Breast Cancer",
    "chunk_index": 7,
    "total_chunks": 49,
    "text_content": "density and mass classes. 1.2 Paper organization The rest of the paper is organized as follows: Sect. 2 dis- cusses some existing mammogram repositories. We give detailed descriptions of our proposed dataset in Sect. 3. We present some experiments and result-analysis of the segmentation model on a proposed dataset in Sect. 4. We finally end with the discussion and conclusion in Sects. 5 and 6, respectively. 2 Some existing mammogram repositories Researchers in the field of breast cancer utilize ",
    "full_text_length": 47726,
    "chunk_length": 1259
  },
  {
    "chunk_id": 2767,
    "paper_filename": "parita_2024_new_mammography_dataset_for-breast_cancer.pdf",
    "paper_title": "Parita 2024 New Mammography Dataset For-Breast Cancer",
    "chunk_index": 8,
    "total_chunks": 49,
    "text_content": "1024). The dataset comprises all possible findings, such as malignant and benign abnormalities, along with normal mam- mograms. The dataset has less number of instances of benign and malignant findings as compared to normal. \u2022 INbreast: INbreast [4 ] is a mammography collection that includes both screening and diagnostic mam- mograms. Images from both MLO and CC views are included in this collection. There are 410 images in the collection, which were created from 115 patient cases. \u2022 The Digital",
    "full_text_length": 47726,
    "chunk_length": 1285
  },
  {
    "chunk_id": 2768,
    "paper_filename": "parita_2024_new_mammography_dataset_for-breast_cancer.pdf",
    "paper_title": "Parita 2024 New Mammography Dataset For-Breast Cancer",
    "chunk_index": 9,
    "total_chunks": 49,
    "text_content": "with ground truth and other information. The images in the datasets have different resolutions. \u2022 King Abdulaziz University Breast Cancer Mammogram Dataset (KAU-BCMD): Another recently published dataset is KAU-BCMD [ 20], the first in Saudi Ara- bia to include a substantial number of mammography 319 Biomedical Engineering Letters (2024) 14:317\u2013330 1 3 images. There are 1416 instances in all. The dataset also contains 205 ultrasound cases that match a portion of the mammography cases. \u2022 Other: IR",
    "full_text_length": 47726,
    "chunk_length": 1310
  },
  {
    "chunk_id": 2769,
    "paper_filename": "parita_2024_new_mammography_dataset_for-breast_cancer.pdf",
    "paper_title": "Parita 2024 New Mammography Dataset For-Breast Cancer",
    "chunk_index": 10,
    "total_chunks": 49,
    "text_content": "[ 24\u201326]: The MIAS dataset is small and unbalanced; there are fewer positive (abnormal) samples than normal ones. This data- set comprises an aberrant mammogram area\u2019s centre point and radius, which need further processing for ground truth. Mammograms from DDSM are compressed using the nonstandard JPEG format. Moreover, some images need to be adequately annotated, necessitating the interven- tion of a skilled radiologist to provide the proper ground truth. Similar to MIAS, the SureMaPP dataset a",
    "full_text_length": 47726,
    "chunk_length": 1332
  },
  {
    "chunk_id": 2770,
    "paper_filename": "parita_2024_new_mammography_dataset_for-breast_cancer.pdf",
    "paper_title": "Parita 2024 New Mammography Dataset For-Breast Cancer",
    "chunk_index": 11,
    "total_chunks": 49,
    "text_content": "report. Even though Inbreast offers medical reports, they are in Portuguese, so addi- tional language conversion is required. Inbreast dataset offers segmented masks, but they are in XML format, so additional processing is required to utilize them. For ease of implementation, we provide all images (mammograms and related masks) in our dataset in a simple image for - mat. Additionally, compared to the INBreast dataset, our dataset has more images in total (510) and more abnormal images (300 plus)",
    "full_text_length": 47726,
    "chunk_length": 1329
  },
  {
    "chunk_id": 2771,
    "paper_filename": "parita_2024_new_mammography_dataset_for-breast_cancer.pdf",
    "paper_title": "Parita 2024 New Mammography Dataset For-Breast Cancer",
    "chunk_index": 12,
    "total_chunks": 49,
    "text_content": "and will contribute to the ongoing developments in DL-based mammography systems in the future. The ground-truth annotation using pixel level boundary and corresponding segmented mask Table 1 Summary of Mammogram Datasets Dataset MIAS DDSM INbreast SureMaPP KAU-BCMD BCDR BancoWeb LAPIMOIRMA Trueta DMID Origin UK USA Portugal UK Saudi Arabia Portugal Brazil Germany Spain India Year 1994 1999 2011 2020 2021 2012 2010 2008 2008 2023 Total images 322 10,480 410 343 5662 7315 1473 10,509 320 510 Image",
    "full_text_length": 47726,
    "chunk_length": 1119
  },
  {
    "chunk_id": 2772,
    "paper_filename": "parita_2024_new_mammography_dataset_for-breast_cancer.pdf",
    "paper_title": "Parita 2024 New Mammography Dataset For-Breast Cancer",
    "chunk_index": 13,
    "total_chunks": 49,
    "text_content": "NO YES YES NO NO YES NO NO NO YES Supporting Modality NO NO NO NO Yes (US) NO NO NO NO NO Public YES YES YES YES YES YES YES NO NO YES 320 Biomedical Engineering Letters (2024) 14:317\u2013330 1 3 and radiology reports are the most distinguishing features of this dataset. We summarize the main features of our proposed dataset as follows: \u2022 Radiologist report with various pathology \u2022 ACR breast density categories \u2022 Pixel-level annotation and corresponding ROI masks \u2022 Breast Imaging Reporting and Data ",
    "full_text_length": 47726,
    "chunk_length": 1293
  },
  {
    "chunk_id": 2773,
    "paper_filename": "parita_2024_new_mammography_dataset_for-breast_cancer.pdf",
    "paper_title": "Parita 2024 New Mammography Dataset For-Breast Cancer",
    "chunk_index": 14,
    "total_chunks": 49,
    "text_content": "4743\u00d76000 and 3520\u00d74784 . Fur - ther processing is performed on the dataset to eliminate floating background artefacts. Images of the dataset contain findings such as masses, calcification, architectural distor - tion, asymmetries, and images with multiple findings. Fig- ure 1 shows mammograms from our collection that include these abnormalities. We also show some examples of normal mammograms and annotated mammograms with multiple findings in Fig. 3 (first row). The dataset includes normal, ben",
    "full_text_length": 47726,
    "chunk_length": 1396
  },
  {
    "chunk_id": 2774,
    "paper_filename": "parita_2024_new_mammography_dataset_for-breast_cancer.pdf",
    "paper_title": "Parita 2024 New Mammography Dataset For-Breast Cancer",
    "chunk_index": 15,
    "total_chunks": 49,
    "text_content": "Fig. 1 Breast abnormalities. A Mass-Benign, B Mass-Malignant, C Calcifications, D Architectural distortion, E Asymmetry Table 2 BI-RADS Assessment Categories [27, 28] Category Assessment Discriptors Follow-up 0 Incomplete assessment, Need additional imaging evaluation\u2013 Need further assistance 1 Normal \u2013 Suggested annual screening mammography (if age >40) 2 Benign finding (Non-cancerous lesion) - Suggested annual screening mammography (if age >40) 3 Probably benign Circumscribed mass/Obscured mas",
    "full_text_length": 47726,
    "chunk_length": 1470
  },
  {
    "chunk_id": 2775,
    "paper_filename": "parita_2024_new_mammography_dataset_for-breast_cancer.pdf",
    "paper_title": "Parita 2024 New Mammography Dataset For-Breast Cancer",
    "chunk_index": 16,
    "total_chunks": 49,
    "text_content": "separate columns, such as reference number, Laterality (CCRT, CCLT, MLORT, MLOLT), Character of background tissue (F: Fatty, G: Fatty-glandular, D: Dense-glandular), Types of abnormality (Calcification, Well-defined / circumscribed masses, Spiculated masses, Ill-defined masses, architectural distortion, a and Normal), Class of abnormality (benign, malignant, no Defined), (x,y) image-coordinates of the cen- tre of abnormality and approximate radius (in pixels) of a circle enclosing the abnormalit",
    "full_text_length": 47726,
    "chunk_length": 1345
  },
  {
    "chunk_id": 2776,
    "paper_filename": "parita_2024_new_mammography_dataset_for-breast_cancer.pdf",
    "paper_title": "Parita 2024 New Mammography Dataset For-Breast Cancer",
    "chunk_index": 17,
    "total_chunks": 49,
    "text_content": "abnormalities, while red colour is used to show malignant abnormalities. These pixel-level annota- tions are then used in Python script to generate segmented masks. For each abnormal mammogram, a corresponding ROI mask(s) is/are provided (see Fig. 3 third row). With the feature of the app \"Efilm,\" (x y) the centre of abnormal- ity and approximate radius in the pixel are exported into a CSV file. 3.2 Radiology report Each mammography image in the dataset is provided with a medical report. The rep",
    "full_text_length": 47726,
    "chunk_length": 1246
  },
  {
    "chunk_id": 2777,
    "paper_filename": "parita_2024_new_mammography_dataset_for-breast_cancer.pdf",
    "paper_title": "Parita 2024 New Mammography Dataset For-Breast Cancer",
    "chunk_index": 18,
    "total_chunks": 49,
    "text_content": "dense in the extreme (ACR-D). Figure 3 (second row) presents the images pertaining to each of these cases from the dataset. In addition, the report provides a BIRADS score for each mammogram image, as shown in Table 2. Based on the imaging test, each number corresponds to a classification Fig. 2 A sample of the metadata, stored in CSV file Table 3 Description of dataset Imaging modality Digital mammography Image type DICOM / TIFF Data acquisition device MAMMOMAT 300 Nova, Siemens Image resolutio",
    "full_text_length": 47726,
    "chunk_length": 1326
  },
  {
    "chunk_id": 2778,
    "paper_filename": "parita_2024_new_mammography_dataset_for-breast_cancer.pdf",
    "paper_title": "Parita 2024 New Mammography Dataset For-Breast Cancer",
    "chunk_index": 19,
    "total_chunks": 49,
    "text_content": "detection and classifications More specific subject matter Breast cancer diagnosis using BI-RADS assessment, Automatic radiology report generation, Breast Composition assessment 322 Biomedical Engineering Letters (2024) 14:317\u2013330 1 3 that evaluates your breast cancer risk. For example, in our dataset, BIRADS-4 (defined as a suspicious lesion) is fur - ther classified into BIRADS 4a (low level of suspicion for malignancy), BIRADS 4b (moderate level of suspicion for malignancy), and BIRADS 4c (hi",
    "full_text_length": 47726,
    "chunk_length": 1367
  },
  {
    "chunk_id": 2779,
    "paper_filename": "parita_2024_new_mammography_dataset_for-breast_cancer.pdf",
    "paper_title": "Parita 2024 New Mammography Dataset For-Breast Cancer",
    "chunk_index": 20,
    "total_chunks": 49,
    "text_content": "Imaging and Report- ing Data Systems) by the experts. Figure 5A shows breast Fig. 3 First row: Examples of mammograms with various findings. Second row: Examples of mammograms with ACR categories. Third row: Abnormal mammograms and corresponding mask 323 Biomedical Engineering Letters (2024) 14:317\u2013330 1 3 composition categorization of all the dataset images. The graph shows that ACR-B is assigned to the highest num- ber of images (around 40%), followed by ACR-C (around 36%). On the other hand, ",
    "full_text_length": 47726,
    "chunk_length": 1255
  },
  {
    "chunk_id": 2780,
    "paper_filename": "parita_2024_new_mammography_dataset_for-breast_cancer.pdf",
    "paper_title": "Parita 2024 New Mammography Dataset For-Breast Cancer",
    "chunk_index": 21,
    "total_chunks": 49,
    "text_content": "Such images, according to doctors, should be classified as cancerous only. Around 36 abnormal images are tagged as \"not defined\" out of 310 abnormal images. \"Not defined\" are those mammo- gram images that need further examination to confirm the diagnosis. So from the remaining 274 images, finally, we have around 47% malignant class images and 53% benign class images in our data collection, which is a nearly equal class distribution (see Fig. 5C). The dataset collection includes images with vario",
    "full_text_length": 47726,
    "chunk_length": 1289
  },
  {
    "chunk_id": 2781,
    "paper_filename": "parita_2024_new_mammography_dataset_for-breast_cancer.pdf",
    "paper_title": "Parita 2024 New Mammography Dataset For-Breast Cancer",
    "chunk_index": 22,
    "total_chunks": 49,
    "text_content": "5% of the total number of images. Only one image in the dataset is classified as BIRADS 0, indicating that the evaluation is incomplete; hence, further assistance is Fig. 4 A mammogram with corresponding radiologist report Fig. 5 A Breast composition as per ACR categories. B Normal/Abnormal class distribution. C Benign/Malignant class distribution 324 Biomedical Engineering Letters (2024) 14:317\u2013330 1 3 required. Around 23% of scans have a BIRADS 4 rating, indicating a possible abnormality. Figu",
    "full_text_length": 47726,
    "chunk_length": 1239
  },
  {
    "chunk_id": 2782,
    "paper_filename": "parita_2024_new_mammography_dataset_for-breast_cancer.pdf",
    "paper_title": "Parita 2024 New Mammography Dataset For-Breast Cancer",
    "chunk_index": 23,
    "total_chunks": 49,
    "text_content": "com/ autho rs/ Parita_ Oza/ 17353 984. The data is organized into five folders. All DCM files can be found in the \"DICOM Images\" folder while corre- sponding tiff files can be found in the \"TIFF Images\" folder. The folder \"Reports\" contains the radiologist\u2019s reports. We provide pixel-level annotation for each anomalous image; these images can be accessed in the folder \"Pixel-level anno- tation.\" We also produced segmented masks for each anoma- lous image, which are found in the folder \"ROI Masks",
    "full_text_length": 47726,
    "chunk_length": 1276
  },
  {
    "chunk_id": 2783,
    "paper_filename": "parita_2024_new_mammography_dataset_for-breast_cancer.pdf",
    "paper_title": "Parita 2024 New Mammography Dataset For-Breast Cancer",
    "chunk_index": 24,
    "total_chunks": 49,
    "text_content": "bright white borders/corners by cropping out boundary pixels. \u2022 Removing background artifacts - Detecting the largest contour in the mammogram (i.e., breast region) and crop- ping the rectangle box enclosing it to remove extra back - ground artifacts and pixels. \u2022 Flipping operation - As the dataset has left and right breast mammograms having different breast directions, We apply the flipping operation to orient all the mam- mograms in the left order. \u2022 Padding - Once all the mammograms have the",
    "full_text_length": 47726,
    "chunk_length": 1190
  },
  {
    "chunk_id": 2784,
    "paper_filename": "parita_2024_new_mammography_dataset_for-breast_cancer.pdf",
    "paper_title": "Parita 2024 New Mammography Dataset For-Breast Cancer",
    "chunk_index": 25,
    "total_chunks": 49,
    "text_content": "add pixels on the left side of the breast to have the breast region in the center so that any geometric transformation doesn\u2019t cut the breast region. \u2022 Normalization - We applied min-max normalization on mammograms to have all the pixel values range between 0\u20131. \u2022 Augmentation - We augmented the image with rotation, translation, zooming, and shearing operations. All the preprocessed images and masks are resized to 512*512 to train the model. Fig. 6 Left: Findings in the dataset. Right: BI-RADS c",
    "full_text_length": 47726,
    "chunk_length": 1235
  },
  {
    "chunk_id": 2785,
    "paper_filename": "parita_2024_new_mammography_dataset_for-breast_cancer.pdf",
    "paper_title": "Parita 2024 New Mammography Dataset For-Breast Cancer",
    "chunk_index": 26,
    "total_chunks": 49,
    "text_content": "all types of tissue density, i.e., Fatty (F), Glandular fatty (G), and Dense (D). While training, four parts of the dataset were used as training, and the remaining one part was used as a test dataset. Thus, we trained five segmentation models with different train and test datasets. From the training dataset, 30% of mammograms are chosen randomly as validation dataset. Table 4 shows the number of mammograms belong- ing to each class of mammogram and each type of tissue density in the training an",
    "full_text_length": 47726,
    "chunk_length": 1272
  },
  {
    "chunk_id": 2786,
    "paper_filename": "parita_2024_new_mammography_dataset_for-breast_cancer.pdf",
    "paper_title": "Parita 2024 New Mammography Dataset For-Breast Cancer",
    "chunk_index": 27,
    "total_chunks": 49,
    "text_content": "frameworks on our proposed dataset to analyze their performance on our proposed dataset. There are two types of commonly used segmentation frameworks available in the literature\u2014(1) Whole mammogram segmentation models [15, 16, 29] and (2) Integrated mass detection and mass patch segmentation frameworks [30\u2013 32]. Whole mammogram segmentation models are DL algorithms designed to perform segmentation on a whole mammogram image, with a primary focus on identify - ing breast masses or abnormalities. ",
    "full_text_length": 47726,
    "chunk_length": 1333
  },
  {
    "chunk_id": 2787,
    "paper_filename": "parita_2024_new_mammography_dataset_for-breast_cancer.pdf",
    "paper_title": "Parita 2024 New Mammography Dataset For-Breast Cancer",
    "chunk_index": 28,
    "total_chunks": 49,
    "text_content": "target structures of varying shapes and sizes for medical image segmentation tasks. The architecture of the Attention-Unet model is presented in Fig. 7. It can be seen from the figure that it is a modified version of the traditional Unet model. The input image is progressively filtered and downsampled by a factor of 2 at each scale in the encoding part of the network. Attention gates (AGs) filter the features propagated through the skip connections. These features are then upsampled in the decod",
    "full_text_length": 47726,
    "chunk_length": 1215
  },
  {
    "chunk_id": 2788,
    "paper_filename": "parita_2024_new_mammography_dataset_for-breast_cancer.pdf",
    "paper_title": "Parita 2024 New Mammography Dataset For-Breast Cancer",
    "chunk_index": 29,
    "total_chunks": 49,
    "text_content": "combining two essential tasks: identifying potential mass regions using detection mod- els and precisely outlining their boundaries using patch segmentation models. We trained the YOLOv8 detection model to detect the breast mass. Based on the location of the detected mass, a mammogram patch of size 1024*1024 is extracted, centering the detected mass. We trained a simple Unet segmentation model to segment the detected mammogram mass patches. We evaluate the performance of these methods on our pro",
    "full_text_length": 47726,
    "chunk_length": 1359
  },
  {
    "chunk_id": 2789,
    "paper_filename": "parita_2024_new_mammography_dataset_for-breast_cancer.pdf",
    "paper_title": "Parita 2024 New Mammography Dataset For-Breast Cancer",
    "chunk_index": 30,
    "total_chunks": 49,
    "text_content": "dicted segmentation output. |GT| and |PR| is the number of pixels in the ground truth and predicted output, respec- tively, and /uni007C.varGT\u2229PR/uni007C.var is the number of positive pixels predicted correctly.DSC=2\u2217/uni007C.varGT\u2229PR/uni007C.var /uni007C.varGT /uni007C.var+/uni007C.varPR /uni007C.varTable 4 Train-test split of images in each fold of segmentation model training Training Testing Benign 114/115 28/29 Malignant 84 21 Benign and Malignant 16 4 Normal 160 40 F - Fatty 63/64 12/13 G -",
    "full_text_length": 47726,
    "chunk_length": 1282
  },
  {
    "chunk_id": 2790,
    "paper_filename": "parita_2024_new_mammography_dataset_for-breast_cancer.pdf",
    "paper_title": "Parita 2024 New Mammography Dataset For-Breast Cancer",
    "chunk_index": 31,
    "total_chunks": 49,
    "text_content": "ratio of the sum of correctly predicted mass pixels to the total mass pixels. It is a measure of the quality of the predictions/segmentation by the model. Recall =TP TP+FN Precision =TP TP+FPThe performance of different segmentation frameworks on our proposed dataset is shown in Table 5. For our dataset, Attention Unet seems to have comparatively better results than the other two models. Thus, we used the Attention Unet model to perform further analysis of segmentation Fig. 7 DL model architectu",
    "full_text_length": 47726,
    "chunk_length": 1259
  },
  {
    "chunk_id": 2791,
    "paper_filename": "parita_2024_new_mammography_dataset_for-breast_cancer.pdf",
    "paper_title": "Parita 2024 New Mammography Dataset For-Breast Cancer",
    "chunk_index": 32,
    "total_chunks": 49,
    "text_content": "A and model B) on the test dataset are shown in Table 6. It can be seen that the model trained with only abnormal mammograms achieved better DSC, recall, and precision scores than the model trained with both normal and abnormal mammograms. However, the recall score of model B is only slightly better than the model A. To get a detailed insight into how accurately both models segment breast masses, we calculated and compared mean DSC, pre- cision, and recall scores for abnormal mammograms only. Se",
    "full_text_length": 47726,
    "chunk_length": 1185
  },
  {
    "chunk_id": 2792,
    "paper_filename": "parita_2024_new_mammography_dataset_for-breast_cancer.pdf",
    "paper_title": "Parita 2024 New Mammography Dataset For-Breast Cancer",
    "chunk_index": 33,
    "total_chunks": 49,
    "text_content": "into one of the three categories of breast tissue density - \u2019F\u2019, \u2019G\u2019, and \u2019D\u2019, and calculated mean DSC, precision, and recall scores for each category. It can be seen from the table that mammogram masses with dense breast tissue are more brutal to segment for both models. Both models have the highest segmentation results on mammograms with fatty tis - sue density. Model A shows similar performance segmenting masses in breast tissue density of types G and D. On the contrary, model B had low perfo",
    "full_text_length": 47726,
    "chunk_length": 1284
  },
  {
    "chunk_id": 2793,
    "paper_filename": "parita_2024_new_mammography_dataset_for-breast_cancer.pdf",
    "paper_title": "Parita 2024 New Mammography Dataset For-Breast Cancer",
    "chunk_index": 34,
    "total_chunks": 49,
    "text_content": "patch detection and segmentation framework on our pro- posed dataset. Attention mechanisms in the Unet model help the model selectively focus on the regions of interest (mass regions), allowing the network to highlight mass boundaries and characteristics, leading to more accurate segmentation. Utilizing normal and mass mammograms provided by our dataset, we analyze Attention Unet\u2019s performance further. Our results suggest that training segmentation models with only abnormal mammograms performed ",
    "full_text_length": 47726,
    "chunk_length": 1329
  },
  {
    "chunk_id": 2794,
    "paper_filename": "parita_2024_new_mammography_dataset_for-breast_cancer.pdf",
    "paper_title": "Parita 2024 New Mammography Dataset For-Breast Cancer",
    "chunk_index": 35,
    "total_chunks": 49,
    "text_content": "normal breast tissue, which could be one of the reasons for lower dice scores than malignant Table 6 Results analysis of segmentation model over proposed data- set Model Mean DSC Mean recall Mean precision Model trained on Normal and abnormal cases0.5580 0.5269 0.6635 Model trained on Only abnormal cases0.6400 0.5533 0.7858 Table 7 Results analysis of proposed end-to-end model segmentation model over mammograms with different breast tissue characteristicsBackground tissue Model trained on Normal",
    "full_text_length": 47726,
    "chunk_length": 1376
  },
  {
    "chunk_id": 2795,
    "paper_filename": "parita_2024_new_mammography_dataset_for-breast_cancer.pdf",
    "paper_title": "Parita 2024 New Mammography Dataset For-Breast Cancer",
    "chunk_index": 36,
    "total_chunks": 49,
    "text_content": "DSC Mean recall Mean precision Benign 0.5148 0.4413 0.6347 0.5259 0.4835 0.6170 Malignant 0.6620 0.5593 0.8959 0.6851 0.5677 0.8940 Both 0.6247 0.5173 0.8380 0.6644 0.5548 0.8515 328 Biomedical Engineering Letters (2024) 14:317\u2013330 1 3 masses. Our further analysis includes examining the effect of breast density over breast mass segmentation. Dense breasts have more fibroglandular breast tissues reflecting as bright areas in mammograms resembling a mass. Thus, breasts with dense tissues have a lo",
    "full_text_length": 47726,
    "chunk_length": 1314
  },
  {
    "chunk_id": 2796,
    "paper_filename": "parita_2024_new_mammography_dataset_for-breast_cancer.pdf",
    "paper_title": "Parita 2024 New Mammography Dataset For-Breast Cancer",
    "chunk_index": 37,
    "total_chunks": 49,
    "text_content": "on our dataset are comparable to the INbreast dataset when tested on the same ground. Fig. 8 Model predictions Table 9 Comparision with other benchmark dataset Method Dataset Dice scores Unet [7] INbreast 0.62 Fusion Net [8] INbreast 0.62 FCDenseNet103[9] INbreast 0.42 AUnet [10] INbreast 0.64 Unet++Xception [11] BCDR 0.58 U-Xception [11] BCDR 0.58 U-ResNet50 [11] BCDR 0.48 Attention Unet [12] DMID (proposed dataset) 0.64 Unet [7] DMID (proposed dataset) 0.60 329 Biomedical Engineering Letters (",
    "full_text_length": 47726,
    "chunk_length": 1319
  },
  {
    "chunk_id": 2797,
    "paper_filename": "parita_2024_new_mammography_dataset_for-breast_cancer.pdf",
    "paper_title": "Parita 2024 New Mammography Dataset For-Breast Cancer",
    "chunk_index": 38,
    "total_chunks": 49,
    "text_content": "mammography scans with ground truth provided by sub - ject-matter experts. This dataset can be utilized for various tasks in the breast cancer research domain. The ground-truth annotation and radiology reports are the most distinguishing features of this work. Additionally, we have nearly equal numbers of benign and malignant classes, resulting in bal- anced datasets. We trained and analyzed the segmentation model using breast mass masks provided for each mammo- gram. Our motive was not just to ",
    "full_text_length": 47726,
    "chunk_length": 1296
  },
  {
    "chunk_id": 2798,
    "paper_filename": "parita_2024_new_mammography_dataset_for-breast_cancer.pdf",
    "paper_title": "Parita 2024 New Mammography Dataset For-Breast Cancer",
    "chunk_index": 39,
    "total_chunks": 49,
    "text_content": "Dinesh Patel and Dr. Trupti Patel for their support. Funding Funding. Data availability The dataset can be used to train ML or DL models for mammogram classification, BI-RADS classification, as well as breast composition classification. Moreover, it can be used to train segmen- tation models to segment breast lesions. Additionally, the radiology reports can be utilized to train report generation models. This dataset will be available for research purposes only on the link provided in the article",
    "full_text_length": 47726,
    "chunk_length": 1294
  },
  {
    "chunk_id": 2799,
    "paper_filename": "parita_2024_new_mammography_dataset_for-breast_cancer.pdf",
    "paper_title": "Parita 2024 New Mammography Dataset For-Breast Cancer",
    "chunk_index": 40,
    "total_chunks": 49,
    "text_content": "https:// novot ech- cro. com/ sites/ defau lt/ files/ 2021- 02/ Breas t20Ca ncer2 0Land scape 20in/ 20Asia- Pacifi c_ 2021. pdf. Accessed 2022-03-10 (2021). 3. Fenton JJ, Zhu W, Balch S, Smith-Bindman R, Fishman P, Hub- bard RA. Distinguishing screening from diagnostic mammograms using Medicare claims data. Med Care. 2014;52(7):244. https:// doi. org/ 10. 1097/ MLR. 0b013 e3182 69e0f5. 4. Moreira IC, Amaral I, Domingues I, Cardoso A, Cardoso MJ, Cardoso JS. Inbreast: toward a full-field digital ",
    "full_text_length": 47726,
    "chunk_length": 1351
  },
  {
    "chunk_id": 2800,
    "paper_filename": "parita_2024_new_mammography_dataset_for-breast_cancer.pdf",
    "paper_title": "Parita 2024 New Mammography Dataset For-Breast Cancer",
    "chunk_index": 41,
    "total_chunks": 49,
    "text_content": "2022;23(3):115\u201327. https:// doi. org/ 10. 12694/ scpe. v23i2. 1975. 7. Oza P, Sharma P, Patel S. A drive through computer-aided diagnosis of breast cancer: a comprehensive study of clini- cal and technical aspects. In Recent innovations in comput- ing: proceedings of ICRIC 2021, Vol 1, pp 233\u2013249 (2022c). 10.1007/978-981-16-8248-3_19 8. Oza P, Sharma P, Patel S. Breast lesion classification from mammograms using deep neural network and test-time aug- mentation. Neural Comput Appl. 2023. https://",
    "full_text_length": 47726,
    "chunk_length": 1325
  },
  {
    "chunk_id": 2801,
    "paper_filename": "parita_2024_new_mammography_dataset_for-breast_cancer.pdf",
    "paper_title": "Parita 2024 New Mammography Dataset For-Breast Cancer",
    "chunk_index": 42,
    "total_chunks": 49,
    "text_content": "A. Image aug - mentation techniques for mammogram analysis. J Imaging. 2022;8(5):141. https:// doi. org/ 10. 3390/ jimag ing80 50141. 12. Oza P, Sharma P, Patel S. Deep ensemble transfer learning-based framework for mammographic image classification. J Supercom- put. 2022. https:// doi. org/ 10. 1007/ s11227- 022- 04992-5. 13. Li H, Chen D, Nailon WH, Davies ME, Laurenson DI. Dual con- volutional neural networks for breast mass segmentation and diag- nosis in mammography. IEEE Trans Med Imaging.",
    "full_text_length": 47726,
    "chunk_length": 1296
  },
  {
    "chunk_id": 2802,
    "paper_filename": "parita_2024_new_mammography_dataset_for-breast_cancer.pdf",
    "paper_title": "Parita 2024 New Mammography Dataset For-Breast Cancer",
    "chunk_index": 43,
    "total_chunks": 49,
    "text_content": ". 16. Sun H, Li C, Liu B, Liu Z, Wang M, Zheng H, Feng DD, Wang S. Aunet: attention-guided dense-upsampling networks for breast mass segmentation in whole mammograms. Phys Med Biol. 2020;65(5):055005. https:// doi. org/ 10. 1088/ 1361- 6560/ ab5745. 17. Suckling J, Parker J, Dance D, Astley S, Hutt I, Boggis C, Ricketts I, Stamatakis E, Cerneaz N, Kok S, et al. Mammographic image analysis society (mias) database v1. 21. (2015) 330 Biomedical Engineering Letters (2024) 14:317\u2013330 1 3 18. Michael ",
    "full_text_length": 47726,
    "chunk_length": 1245
  },
  {
    "chunk_id": 2803,
    "paper_filename": "parita_2024_new_mammography_dataset_for-breast_cancer.pdf",
    "paper_title": "Parita 2024 New Mammography Dataset For-Breast Cancer",
    "chunk_index": 44,
    "total_chunks": 49,
    "text_content": "regions in mammogram images. J Med Signals Sens. 2020;10(3):158. https:// doi. org/ 10. 4103/ jmss. JMSS_ 31_ 19. 20. Alsolami AS, Shalash W, Alsaggaf W, Ashoor S, Refaat H, Elm- ogy M. King abdulaziz university breast cancer mammogram dataset (kau-bcmd). Data. 2021;6(11):111. https:// doi. org/ 10. 3390/ data6 110111. 21. Oliveira JEE et al. Toward a standard reference database for com- puter-aided mammography. In: Medical imaging 2008: computer- aided diagnosis, vol 6915, pp 606\u2013614. SPIE (200",
    "full_text_length": 47726,
    "chunk_length": 1307
  },
  {
    "chunk_id": 2804,
    "paper_filename": "parita_2024_new_mammography_dataset_for-breast_cancer.pdf",
    "paper_title": "Parita 2024 New Mammography Dataset For-Breast Cancer",
    "chunk_index": 45,
    "total_chunks": 49,
    "text_content": "24. Oza P, Sharma P, Patel S, Kumar P. Computer-aided breast cancer diagnosis: comparative analysis of breast imaging modalities and mammogram repositories. Current Med Imaging. 2022;18:1\u201313. https:// doi. org/ 10. 2174/ 15734 05618 66622 06211 23156. 25. Tariq M, Iqbal S, Ayesha H, Abbas I, Ahmad KT, Niazi MFK. Medical image based breast cancer diagnosis: state of the art and future directions. Expert Syst Appl. 2021;167:114095. https:// doi. org/ 10. 1016/j. eswa. 2020. 114095. 26. Lee RS, Gim",
    "full_text_length": 47726,
    "chunk_length": 1314
  },
  {
    "chunk_id": 2805,
    "paper_filename": "parita_2024_new_mammography_dataset_for-breast_cancer.pdf",
    "paper_title": "Parita 2024 New Mammography Dataset For-Breast Cancer",
    "chunk_index": 46,
    "total_chunks": 49,
    "text_content": "system (bi-rads). https:// radio paedia. org/ artic les/ 10003(2022). Accessed: 10 May 2022 29. Li S, Dong M, Guangming D, Xiaomin M. Attention dense-u- net for automatic breast mass segmentation in digital mammo- gram. IEEE Access. 2019;7:59037\u201347. https:// doi. org/ 10. 1109/ ACCESS. 2019. 29148 73. 30. Al-Antari MA, Al-Masni MA, Choi M-T, Han S-M, Kim T-S. A fully integrated computer-aided diagnosis system for digital x-ray mammograms via deep learning detection, segmentation, and classificat",
    "full_text_length": 47726,
    "chunk_length": 1375
  },
  {
    "chunk_id": 2806,
    "paper_filename": "parita_2024_new_mammography_dataset_for-breast_cancer.pdf",
    "paper_title": "Parita 2024 New Mammography Dataset For-Breast Cancer",
    "chunk_index": 47,
    "total_chunks": 49,
    "text_content": "1016/j. media. 2017. 01. 009. 33. Ronneberger O, Fischer P, Brox T. U-net: convolutional networks for biomedical image segmentation. In Medical image computing and computer-assisted intervention\u2013MICCAI 2015: 18th interna- tional conference, Munich, Germany, October 5-9, 2015, Proceed- ings, Part III 18, pp 234\u2013241. Springer (2015) . https:// doi. org/ 10. 48550/ arXiv. 1505. 04597. 34. Oktay O, Schlemper J, Le Folgoc L, Lee M, Heinrich M, Misawa K, Mori K, McDonagh S, Hammerla NY, Kainz B, et al",
    "full_text_length": 47726,
    "chunk_length": 1104
  },
  {
    "chunk_id": 2807,
    "paper_filename": "parita_2024_new_mammography_dataset_for-breast_cancer.pdf",
    "paper_title": "Parita 2024 New Mammography Dataset For-Breast Cancer",
    "chunk_index": 48,
    "total_chunks": 49,
    "text_content": "of this article is solely governed by the terms of such publishing agreement and applicable law.",
    "full_text_length": 47726,
    "chunk_length": 96
  },
  {
    "chunk_id": 2808,
    "paper_filename": "popic_2025_how_should_ai_be_used_in_breast_screening.pdf",
    "paper_title": "Popic 2025 How Should Ai Be Used In Breast Screening",
    "chunk_index": 0,
    "total_chunks": 42,
    "text_content": "PLOS One | https://doi.org/10.1371/journal.pone.0323528 May 30, 2025 1 / 13 OPEN ACCESS Citation: Popic D, Marinovich ML, Houssami N, Hall J, Carter S (2025) How should artificial intelligence be used in breast screening? Women\u2019s reasoning about workflow options. PLoS One 20(5): e0323528. https://doi. org/10.1371/journal.pone.0323528 Editor: Wenjie Shi, Universit\u00e4tsklinikum Magdeburg: Universitatsklinikum Magdeburg, GERMANY Received: October 11, 2024 Accepted: April 10, 2025 Published: May 30, 2",
    "full_text_length": 43195,
    "chunk_length": 1435
  },
  {
    "chunk_id": 2809,
    "paper_filename": "popic_2025_how_should_ai_be_used_in_breast_screening.pdf",
    "paper_title": "Popic 2025 How Should Ai Be Used In Breast Screening",
    "chunk_index": 1,
    "total_chunks": 42,
    "text_content": "artificial intelligence be used in breast screening? Women\u2019s reasoning about workflow options Diana Popic1, M. Luke Marinovich2, Nehmat Houssami2,3, Julie Hall1, Stacy M. Carter1* 1 Australian Centre for Health Engagement Evidence and Values, School of Social Sciences, Faculty of Arts, Social Sciences and Humanities, University of Wollongong, Wollongong, New South Wales, Australia, 2 The Daffodil Centre, The University of Sydney, Sydney, New South Wales, Australia, a Joint Venture with Cancer Co",
    "full_text_length": 43195,
    "chunk_length": 1337
  },
  {
    "chunk_id": 2810,
    "paper_filename": "popic_2025_how_should_ai_be_used_in_breast_screening.pdf",
    "paper_title": "Popic 2025 How Should Ai Be Used In Breast Screening",
    "chunk_index": 2,
    "total_chunks": 42,
    "text_content": "matters most to women regarding AI use in the workflow of publicly funded breast screening programs, and how women choose between workflow options. We recruited forty women of screening age to learn about AI, the Australian breast screening program, and four possible workflows that include AI \u2013 one where AI works alone, and three different combinations of humans and AI. Participants then joined one of eight 90-minute dialogue groups to discuss their normative judgements on workflow options. Wome",
    "full_text_length": 43195,
    "chunk_length": 1310
  },
  {
    "chunk_id": 2811,
    "paper_filename": "popic_2025_how_should_ai_be_used_in_breast_screening.pdf",
    "paper_title": "Popic 2025 How Should Ai Be Used In Breast Screening",
    "chunk_index": 3,
    "total_chunks": 42,
    "text_content": "workflows should continue to be designed around them. Public breast screening services are trusted and valued by women, so significant changes require careful attention to outcomes relevant to women. Our results \u2013 women\u2019s detailed judgements on workflow design options \u2013 are new to the research literature. We conclude that women expect that AI only be deployed to do tasks it can do well, only where necessary, and only to fill gaps that radiologists cannot meet. Advancements in AI accuracy alone a",
    "full_text_length": 43195,
    "chunk_length": 1282
  },
  {
    "chunk_id": 2812,
    "paper_filename": "popic_2025_how_should_ai_be_used_in_breast_screening.pdf",
    "paper_title": "Popic 2025 How Should Ai Be Used In Breast Screening",
    "chunk_index": 4,
    "total_chunks": 42,
    "text_content": "suggest a need for systematic engagement with the patients and publics who depend on health services. Recent systematic and scoping reviews of the rela - tively new literature on patient and public views [ 2\u20136] provide some insights. Knowl - edge of AI is variable [ 2,4\u20136]. Knowledge about healthcare AI may be less than in other contexts [ 2,4]. While consumers are mostly open to healthcare AI and its poten - tial benefits [ 2\u20136], this is contingent on AI performance [ 2,6], use in low-risk sett",
    "full_text_length": 43195,
    "chunk_length": 1268
  },
  {
    "chunk_id": 2813,
    "paper_filename": "popic_2025_how_should_ai_be_used_in_breast_screening.pdf",
    "paper_title": "Popic 2025 How Should Ai Be Used In Breast Screening",
    "chunk_index": 5,
    "total_chunks": 42,
    "text_content": "Patients and publics call for education and consultation [ 2,4], remaining patient-centred [ 5,6], and addressing performance, governance and ethics issues before and during implementation [ 3\u20136]. One well-advanced use-case for AI in healthcare is mammographic screening for breast cancer [ 7]. Screen-reading AI is deployed in some breast screening contexts, including some private practices, but was not used in Australian public breast screen - ing at the time of this study. All Australian women ",
    "full_text_length": 43195,
    "chunk_length": 1326
  },
  {
    "chunk_id": 2814,
    "paper_filename": "popic_2025_how_should_ai_be_used_in_breast_screening.pdf",
    "paper_title": "Popic 2025 How Should Ai Be Used In Breast Screening",
    "chunk_index": 6,
    "total_chunks": 42,
    "text_content": "in breast screening are similar to those expressed about AI in healthcare generally. This includes evidence that first-time screeners may hold stronger positive and negative views of AI [ 11], and that women are concerned about dependence on AI [ 8]. Women appear to have stronger expectations regarding responsibility, accountability and governance in breast screening AI than in AI in general [ 8,10\u201312]; equitable access is also important to women [ 8]. We have shown women are especially concerne",
    "full_text_length": 43195,
    "chunk_length": 1246
  },
  {
    "chunk_id": 2815,
    "paper_filename": "popic_2025_how_should_ai_be_used_in_breast_screening.pdf",
    "paper_title": "Popic 2025 How Should Ai Be Used In Breast Screening",
    "chunk_index": 7,
    "total_chunks": 42,
    "text_content": "control, evidence to assure performance, time to become familiar with the technology, and clearly justifying the need for implementation, summarised in Table 1 [13]. In this paper, we build on this earlier work to examine a key issue for those con - sidering the implementation of AI in breast screening programs: how AI should be included into the screening workflow. We aimed to answer the questions:data sharing. Our processed data has been de-identified and is shared in our manuscript \u2013 we have ",
    "full_text_length": 43195,
    "chunk_length": 1295
  },
  {
    "chunk_id": 2816,
    "paper_filename": "popic_2025_how_should_ai_be_used_in_breast_screening.pdf",
    "paper_title": "Popic 2025 How Should Ai Be Used In Breast Screening",
    "chunk_index": 8,
    "total_chunks": 42,
    "text_content": "data as this is restricted under the terms of our Human Research Ethics Approval. Funding: The authors disclose support for the research of this work from the National Breast Cancer Foundation, IIRS-20-011. SMC declares research funding from the Australian National Health and Medical Research Council (1181960), National Breast Cancer Foundation (IIRS-20-011) and Australian Commission on Safety and Quality in Health Care for work on AI in healthcare, membership of the NSW Health AI Taskforce, and",
    "full_text_length": 43195,
    "chunk_length": 1351
  },
  {
    "chunk_id": 2817,
    "paper_filename": "popic_2025_how_should_ai_be_used_in_breast_screening.pdf",
    "paper_title": "Popic 2025 How Should Ai Be Used In Breast Screening",
    "chunk_index": 9,
    "total_chunks": 42,
    "text_content": "study design, data collection and anal - ysis, decision to publish, or preparation of the manuscript. National Breast Cancer Foundation can be found at www.nbcf.org.au . Australian National Health and Medical Research Council can be found at www.nhmrc.gov.au . Competing interests: SMC, DP, MLM and JH declare no competing interests. NH declares membership of the TGA (Therapeutic Goods Administration) Advisory Committee on Medical Devices. PLOS One | https://doi.org/10.1371/journal.pone.0323528 Ma",
    "full_text_length": 43195,
    "chunk_length": 1417
  },
  {
    "chunk_id": 2818,
    "paper_filename": "popic_2025_how_should_ai_be_used_in_breast_screening.pdf",
    "paper_title": "Popic 2025 How Should Ai Be Used In Breast Screening",
    "chunk_index": 10,
    "total_chunks": 42,
    "text_content": "scenarios, asking them to make normative judgements about options, consider the potential to change their views, and provide reasons [ 13]. Participant recruitment and selection Consistent with qualitative methodologies, we aimed to recruit diverse participants whose experience and perspectives were relevant to our research questions: in this case, those eligible to be invited to public breast cancer screening. Recruitment and selection was undertaken by Taverner Research [ 13], via social media",
    "full_text_length": 43195,
    "chunk_length": 1360
  },
  {
    "chunk_id": 2819,
    "paper_filename": "popic_2025_how_should_ai_be_used_in_breast_screening.pdf",
    "paper_title": "Popic 2025 How Should Ai Be Used In Breast Screening",
    "chunk_index": 11,
    "total_chunks": 42,
    "text_content": "the experience of breast cancer changes women\u2019s experience and perception of screening Table 1 . Four conditions women imposed on AI implementation in breast cancer screening \u2013 a summary [ 13]. Human control and decision making is preservedWomen valued human attributes, which meant the current workflow in many programs of 2\u20133 radiol - ogists screen-reading independently was highly valued as an assurance of system performance. Women wanted radiologists to continue reading mammograms, retaining ov",
    "full_text_length": 43195,
    "chunk_length": 1371
  },
  {
    "chunk_id": 2820,
    "paper_filename": "popic_2025_how_should_ai_be_used_in_breast_screening.pdf",
    "paper_title": "Popic 2025 How Should Ai Be Used In Breast Screening",
    "chunk_index": 12,
    "total_chunks": 42,
    "text_content": "and tested; AI was not. Evidence should be relevant to practice, including trials and global case studies; some women emphasised they did not want to be the trial. Women have time to get informed and familiar with AIAI was generally seen as unfamiliar, and sometimes futuristic, or even unrealistic. Women sometimes ascribed this to their age or generation, and sometimes worried it might be a barrier to participation. To address unfamiliarity, women suggested transparency about AI use, slow, stage",
    "full_text_length": 43195,
    "chunk_length": 1279
  },
  {
    "chunk_id": 2821,
    "paper_filename": "popic_2025_how_should_ai_be_used_in_breast_screening.pdf",
    "paper_title": "Popic 2025 How Should Ai Be Used In Breast Screening",
    "chunk_index": 13,
    "total_chunks": 42,
    "text_content": "adequately exploring other solutions, e.g., workforce development. Some women accepted AI as a viable solution to a radiologist shortage; even they stressed that policy environments should not over- emphasise AI or grow dependent on it, and should not permit automation or radiologist job loss simply because they could. AI should not be used just because it was available: it should be used only when there were adequate reasons to support service change, and for some, only if it was necessary to k",
    "full_text_length": 43195,
    "chunk_length": 1378
  },
  {
    "chunk_id": 2822,
    "paper_filename": "popic_2025_how_should_ai_be_used_in_breast_screening.pdf",
    "paper_title": "Popic 2025 How Should Ai Be Used In Breast Screening",
    "chunk_index": 14,
    "total_chunks": 42,
    "text_content": "skills and access to a computer/tablet and secure internet connection to participate. Informed consent was recorded orally to reduce burden on participants to print, sign and scan their written consent. After participants received the Participant Information Statement, their oral consent was recorded using a digital recorder and stored on secure University of Wollongong data systems. Oral consent was approved by the Human Research Ethics Committee at the University of Wollongong. The committee d",
    "full_text_length": 43195,
    "chunk_length": 1338
  },
  {
    "chunk_id": 2823,
    "paper_filename": "popic_2025_how_should_ai_be_used_in_breast_screening.pdf",
    "paper_title": "Popic 2025 How Should Ai Be Used In Breast Screening",
    "chunk_index": 15,
    "total_chunks": 42,
    "text_content": "days. The videos explained: AI, including its uses; screening and breast screening, including current and potential workflows; and evidence on AI performance (as of 2022) in each workflow [ 19,20]. Workflows and evidence on AI performance in each is presented in Fig 1 . We encouraged participant engagement and interaction. Questions women posted on the bulletin board were answered by investigators. Women were required to view all videos and post com - ments on at least two before participating i",
    "full_text_length": 43195,
    "chunk_length": 1364
  },
  {
    "chunk_id": 2824,
    "paper_filename": "popic_2025_how_should_ai_be_used_in_breast_screening.pdf",
    "paper_title": "Popic 2025 How Should Ai Be Used In Breast Screening",
    "chunk_index": 16,
    "total_chunks": 42,
    "text_content": "of Australian women aged 50\u201374 using the Australian Bureau of Statistics\u2019 Tablebuilder [ 21]. Dialogue group analysis focused on women\u2019s judgements about the four workflows. We used reflexive thematic analysis (RTA) [ 22\u201326], a proven approach in qualitative health policy research offering both robustness and flexibility. DP developed codes inductively from the data, largely coded semantically and summarised patterns and insights for each group. Records of the analysis process include a spreadsh",
    "full_text_length": 43195,
    "chunk_length": 1344
  },
  {
    "chunk_id": 2825,
    "paper_filename": "popic_2025_how_should_ai_be_used_in_breast_screening.pdf",
    "paper_title": "Popic 2025 How Should Ai Be Used In Breast Screening",
    "chunk_index": 17,
    "total_chunks": 42,
    "text_content": "24]. Results Participant characteristics We recruited eight groups of 4\u20136 women, totalling 40 participants. We achieved diversity across age groups, residential location, levels of education (from early school leaver to postgraduate), most states and territories, and birthplace in or outside of Australia. Groups contained proportionally more younger and university-educated women, and more women born in Australia, compared to the Australian population. More demographic details are available in ou",
    "full_text_length": 43195,
    "chunk_length": 1408
  },
  {
    "chunk_id": 2826,
    "paper_filename": "popic_2025_how_should_ai_be_used_in_breast_screening.pdf",
    "paper_title": "Popic 2025 How Should Ai Be Used In Breast Screening",
    "chunk_index": 18,
    "total_chunks": 42,
    "text_content": "2025 6 / 13introduction [ 13]. Women were open to potential benefits, but also recognised that contemporary life featured extensive, disappointing automation. Women not screened in the last four years appeared overall to be more cautious about AI than women who had screened. However, their substantive concerns were similar, so they are not reported separately in our analysis. Women\u2019s judgements about four potential workflows for AI in breast screening The four conditions in Table 1 informed wome",
    "full_text_length": 43195,
    "chunk_length": 1384
  },
  {
    "chunk_id": 2827,
    "paper_filename": "popic_2025_how_should_ai_be_used_in_breast_screening.pdf",
    "paper_title": "Popic 2025 How Should Ai Be Used In Breast Screening",
    "chunk_index": 19,
    "total_chunks": 42,
    "text_content": "responses in relation to their four conditions. Computerised decision support (CDS) Women who preferred CDS emphasised the importance of human control and familiarity. They supported CDS because two radiologists continued to read every mammogram, with AI relegated to being a complementary tool used by humans. Familiarity\u2014resemblance to the current workflow\u2014was also reassuring. These women were mostly unconvinced by the evidence presented. When pushed on what they thought about the potential for ",
    "full_text_length": 43195,
    "chunk_length": 1376
  },
  {
    "chunk_id": 2828,
    "paper_filename": "popic_2025_how_should_ai_be_used_in_breast_screening.pdf",
    "paper_title": "Popic 2025 How Should Ai Be Used In Breast Screening",
    "chunk_index": 20,
    "total_chunks": 42,
    "text_content": "to the current one\u2014and human control, because every mammogram was seen by at least one human. Women saw this workflow as a reasonable trade-off: every mammogram was seen by a radiologist, but with fewer radiologists than the current workflow, thus addressing workforce shortages. With respect to evidence, some of these women were unmoved, others were reas - sured by evidence that arbitration could reduce false positives. This workflow was seen to combine the best attributes of AI and humans. Some",
    "full_text_length": 43195,
    "chunk_length": 1274
  },
  {
    "chunk_id": 2829,
    "paper_filename": "popic_2025_how_should_ai_be_used_in_breast_screening.pdf",
    "paper_title": "Popic 2025 How Should Ai Be Used In Breast Screening",
    "chunk_index": 21,
    "total_chunks": 42,
    "text_content": "it could instead entail directing human expertise and resources to complex, high-risk mammograms. These women valued the capacity for programs to set a flexible, performance-based threshold, ensuring only very low-risk mammograms would be read by AI alone, while human expertise could be applied to higher risk mammograms, where it was most needed. Women who preferred triage also appeared more likely to reason from the evidence, and while they accepted and gave weight to evidence on all four workf",
    "full_text_length": 43195,
    "chunk_length": 1424
  },
  {
    "chunk_id": 2830,
    "paper_filename": "popic_2025_how_should_ai_be_used_in_breast_screening.pdf",
    "paper_title": "Popic 2025 How Should Ai Be Used In Breast Screening",
    "chunk_index": 22,
    "total_chunks": 42,
    "text_content": "2025 9 / 13In contrast, some women disliked or strongly opposed an AI system triaging mammograms out of radiology review. To these women, maintaining human control meant preserving human decision making over every mammogram. They saw the evidence as insufficient to warrant AI making any final decisions and tended to reject or question the current evidence. For quality assurance or to improve acceptability, some women suggested human review of a subset of AI-read low-risk mammograms. Triage was a",
    "full_text_length": 43195,
    "chunk_length": 1270
  },
  {
    "chunk_id": 2831,
    "paper_filename": "popic_2025_how_should_ai_be_used_in_breast_screening.pdf",
    "paper_title": "Popic 2025 How Should Ai Be Used In Breast Screening",
    "chunk_index": 23,
    "total_chunks": 42,
    "text_content": "loss of radiology skills and knowledge, and loss of human accountability. Patterns of judgments across the workflows Across the four workflows, patterns of intuition and judgement help explain women\u2019s choices. Women trade workflow attributes against each other Women chose workflows by weighing up how well each met their four conditions. Each woman weighted each condition differently. When multiple conditions were equally important, choosing a preferred workflow became more difficult. Some women ",
    "full_text_length": 43195,
    "chunk_length": 1321
  },
  {
    "chunk_id": 2832,
    "paper_filename": "popic_2025_how_should_ai_be_used_in_breast_screening.pdf",
    "paper_title": "Popic 2025 How Should Ai Be Used In Breast Screening",
    "chunk_index": 24,
    "total_chunks": 42,
    "text_content": "with the shortage of medical staff, qualified medical people. I guess inevitably AI is going to come in whether we like it or not, but to what degree. (DG1) Most women accept a workforce shortage as reason to introduce AI Many women took seriously the shortage of radiologists; those more open to this premise typically preferred either replac - ing one radiologist or triage. Conversely, women preferring CDS tended not to discuss workforce shortages, so may have seen this as insufficient justifica",
    "full_text_length": 43195,
    "chunk_length": 1329
  },
  {
    "chunk_id": 2833,
    "paper_filename": "popic_2025_how_should_ai_be_used_in_breast_screening.pdf",
    "paper_title": "Popic 2025 How Should Ai Be Used In Breast Screening",
    "chunk_index": 25,
    "total_chunks": 42,
    "text_content": "in who interpreted their mammograms. Current evidence influences some women\u2019s preferences, but not others Women were invited to consider whether their preferences had changed based on the evidence on AI\u2019s performance. Some women were unmoved, staying with their preferred workflows, typically CDS or replacing one radiologist. Here, human control and familiarity were often more important than AI performance, diminishing the impact of evidence. PLOS One | https://doi.org/10.1371/journal.pone.032352",
    "full_text_length": 43195,
    "chunk_length": 1306
  },
  {
    "chunk_id": 2834,
    "paper_filename": "popic_2025_how_should_ai_be_used_in_breast_screening.pdf",
    "paper_title": "Popic 2025 How Should Ai Be Used In Breast Screening",
    "chunk_index": 26,
    "total_chunks": 42,
    "text_content": "to avoid false positives, the more likely they were to prefer triage or replacing one specialist, based on the evidence provided. Women were asked how accurate AI needed to be for women to have faith in it: some accepted AI performing as well as radiologists, others said it needed to perform better; some nominated 95\u201399% accuracy, others 100%. Women responded with the same range of views, whichever workflow they considered. Humans mitigate risk the best, so workflows should be designed around th",
    "full_text_length": 43195,
    "chunk_length": 1245
  },
  {
    "chunk_id": 2835,
    "paper_filename": "popic_2025_how_should_ai_be_used_in_breast_screening.pdf",
    "paper_title": "Popic 2025 How Should Ai Be Used In Breast Screening",
    "chunk_index": 27,
    "total_chunks": 42,
    "text_content": "between risk and human mitigation differently. For some, every mammogram was at equally high risk of an inaccurate result: thus, humans should read every mammogram to mitigate this risk, and CDS or replacing one radiologist were preferred. Others saw only a subset of complex mammograms as high risk: thus, AI as triage was preferred to focus radiologists\u2019 attention on these high-risk mammograms. Discussion There is a small and growing literature on women\u2019s views on the use of AI in breast screeni",
    "full_text_length": 43195,
    "chunk_length": 1249
  },
  {
    "chunk_id": 2836,
    "paper_filename": "popic_2025_how_should_ai_be_used_in_breast_screening.pdf",
    "paper_title": "Popic 2025 How Should Ai Be Used In Breast Screening",
    "chunk_index": 28,
    "total_chunks": 42,
    "text_content": "consumer views of AI in healthcare [ 2\u20136]. The second two conditions are less prevalent in systematic reviews, so may be particularly relevant to breast screening. They may be explained in part by the cohort (older women are reportedly sometimes more apprehensive about healthcare AI [ 28]) and our method - ology (we began from a premise for introducing AI in screen-reading). Women rejected use of AI alone in mammography screen-reading and made different judgements about three other workflows (CD",
    "full_text_length": 43195,
    "chunk_length": 1289
  },
  {
    "chunk_id": 2837,
    "paper_filename": "popic_2025_how_should_ai_be_used_in_breast_screening.pdf",
    "paper_title": "Popic 2025 How Should Ai Be Used In Breast Screening",
    "chunk_index": 29,
    "total_chunks": 42,
    "text_content": "unconditionally. For women who value radiologists, accountability, screen - ing services contributing to employment and the advancement of human skills and knowledge, it will be harder to accept a larger role for AI. Different women prioritise different conditions, and so make different judgements. However, it seems likely that improved accuracy will not lead all women to accept AI making final decisions, if clinicians are available to per - form the same task. Strengths and limitations Limitati",
    "full_text_length": 43195,
    "chunk_length": 1378
  },
  {
    "chunk_id": 2838,
    "paper_filename": "popic_2025_how_should_ai_be_used_in_breast_screening.pdf",
    "paper_title": "Popic 2025 How Should Ai Be Used In Breast Screening",
    "chunk_index": 30,
    "total_chunks": 42,
    "text_content": "bility, speed of results), such that the conversations were necessarily somewhat speculative. As AI technology is evolving quickly, recent evidence about AI performance [ 7] is not captured in the information presented to participants. The fact that women had an opportunity to learn about breast screening and AI before the discussion groups is a strength: we note that they contain the kind of information that might be required to support informed consent to participate in breast screening using ",
    "full_text_length": 43195,
    "chunk_length": 1202
  },
  {
    "chunk_id": 2839,
    "paper_filename": "popic_2025_how_should_ai_be_used_in_breast_screening.pdf",
    "paper_title": "Popic 2025 How Should Ai Be Used In Breast Screening",
    "chunk_index": 31,
    "total_chunks": 42,
    "text_content": "social media, and the online format), we note that in Austra - lia more than 90% of the population use social media and internet respectively [ 29]. As in all group-based qualitative research, group dynamics may have had some impact on the findings, although we note that the facilitator is an expe - rienced qualitative researcher who aimed to mitigate such impacts as much as possible (for example, ensuring that all participants were able to speak). There is some evidence that when patients consi",
    "full_text_length": 43195,
    "chunk_length": 1270
  },
  {
    "chunk_id": 2840,
    "paper_filename": "popic_2025_how_should_ai_be_used_in_breast_screening.pdf",
    "paper_title": "Popic 2025 How Should Ai Be Used In Breast Screening",
    "chunk_index": 32,
    "total_chunks": 42,
    "text_content": "decision making about service objectives and design. The balance of outcomes (lives lengthened by early interven - tion, false negatives and positives, cancer overdiagnosis) is a matter of ongoing debate.[ 30] Future screening modalities are also in flux, including possible future introduction of polygenic risk scoring [ 31]. The potential introduction of AI is one more element in this complex policy landscape [ 32]. However, for the women who participate, breast screening is both a trusted and ",
    "full_text_length": 43195,
    "chunk_length": 1302
  },
  {
    "chunk_id": 2841,
    "paper_filename": "popic_2025_how_should_ai_be_used_in_breast_screening.pdf",
    "paper_title": "Popic 2025 How Should Ai Be Used In Breast Screening",
    "chunk_index": 33,
    "total_chunks": 42,
    "text_content": "required both for public engagement and individual patient consent in the use of AI. Conclusion We have shown that women\u2019s judgements about workflows for AI in breast screening rely on different interpretations and combinations of four conditions: preserving human control and decision making; availability of relevant, high-quality evidence of performance; familiarity; and being convinced that the reasons for change outweigh the potential harms, risks, or costs. Although different women preferred",
    "full_text_length": 43195,
    "chunk_length": 1425
  },
  {
    "chunk_id": 2842,
    "paper_filename": "popic_2025_how_should_ai_be_used_in_breast_screening.pdf",
    "paper_title": "Popic 2025 How Should Ai Be Used In Breast Screening",
    "chunk_index": 34,
    "total_chunks": 42,
    "text_content": "checklist. (DOCX) Acknowledgments The authors would like to acknowledge Lucy Carolan who supported participant engagement in dialogue groups and Emma Frost who designed the videos and power point slides, and advised us on our final figures. Author contributions Conceptualization: M. Luke Marinovich, Nehmat Houssami, Stacy Carter. Data curation: Diana Popic, Julie Hall, Stacy Carter. Formal analysis: Diana Popic, Julie Hall. Funding acquisition: M. Luke Marinovich, Nehmat Houssami. Investigation:",
    "full_text_length": 43195,
    "chunk_length": 1515
  },
  {
    "chunk_id": 2843,
    "paper_filename": "popic_2025_how_should_ai_be_used_in_breast_screening.pdf",
    "paper_title": "Popic 2025 How Should Ai Be Used In Breast Screening",
    "chunk_index": 35,
    "total_chunks": 42,
    "text_content": "systems in breast cancer care. Breast. 2020;49:25\u201332. https://doi.org/10.1016/j.breast.2019.10.001 PMID: 31677530 2. Young AT, Amara D, Bhattacharya A, Wei ML. Patient and general public attitudes towards clinical artificial intelligence: a mixed methods systematic review. Lancet Digit Health. 2021;3(9):e599\u2013611. https://doi.org/10.1016/S2589-7500(21)00132-1 PMID: 34446266 3. Tang L, Li J, Fantus S. Medical artificial intelligence ethics: a systematic review of empirical studies. Digit Health. 2",
    "full_text_length": 43195,
    "chunk_length": 1650
  },
  {
    "chunk_id": 2844,
    "paper_filename": "popic_2025_how_should_ai_be_used_in_breast_screening.pdf",
    "paper_title": "Popic 2025 How Should Ai Be Used In Breast Screening",
    "chunk_index": 36,
    "total_chunks": 42,
    "text_content": "use of artificial intelligence in healthcare: a system - atic review and thematic analysis. Soc Sci Med. 2023;338:116357. https://doi.org/10.1016/j.socscimed.2023.116357 PMID: 37949020 7. Houssami N, Marinovich ML. AI for mammography screening: enter evidence from prospective trials. Lancet Digital Health. 2023;5(10):e641\u20132. https://doi.org/10.1016/s2589-7500(23)00176-0 8. Lennox-Chhugani N, Chen Y, Pearson V, Trzcinski B, James J. Women\u2019s attitudes to the use of AI image readers: a case study f",
    "full_text_length": 43195,
    "chunk_length": 1566
  },
  {
    "chunk_id": 2845,
    "paper_filename": "popic_2025_how_should_ai_be_used_in_breast_screening.pdf",
    "paper_title": "Popic 2025 How Should Ai Be Used In Breast Screening",
    "chunk_index": 37,
    "total_chunks": 42,
    "text_content": "A):79\u201386. https://doi.org/10.1016/j.jacr.2020.09.042 PMID: 33058789 11. Pesapane F, Rotili A, Valconi E, Agazzi GM, Montesano M, Penco S, et al. Women\u2019s perceptions and attitudes to the use of AI in breast cancer screening: a survey in a cancer referral centre. Br J Radiol. 2023;96(1141):20220569. https://doi.org/10.1259/bjr.20220569 PMID: 36314388 12. Carter SM, Carolan L, Saint James Aquino Y, Frazer H, Rogers WA, Hall J, et al. Australian women\u2019s judgements about using artificial intelligence",
    "full_text_length": 43195,
    "chunk_length": 1580
  },
  {
    "chunk_id": 2846,
    "paper_filename": "popic_2025_how_should_ai_be_used_in_breast_screening.pdf",
    "paper_title": "Popic 2025 How Should Ai Be Used In Breast Screening",
    "chunk_index": 38,
    "total_chunks": 42,
    "text_content": "15. Absetz P, Aro AR, Sutton SR. Experience with breast cancer, pre-screening perceived susceptibility and the psychological impact of screening. Psychooncology. 2003;12(4):305\u201318. https://doi.org/10.1002/pon.644 PMID: 12748969 16. University of Wollongong. What is AI?: YouTube; 2023 [updated 23 November 2023; cited 2023 December 5]. Available from: https://www.youtube. com/watch?v=sr-sQgVnLD0 17. University of Wollongong. The use of AI in breast screening. 2023 [Accessed 2023 December 5]. Avail",
    "full_text_length": 43195,
    "chunk_length": 1647
  },
  {
    "chunk_id": 2847,
    "paper_filename": "popic_2025_how_should_ai_be_used_in_breast_screening.pdf",
    "paper_title": "Popic 2025 How Should Ai Be Used In Breast Screening",
    "chunk_index": 39,
    "total_chunks": 42,
    "text_content": "of cancer detection. EBioMedicine. 2023;90:104498. https://doi.org/10.1016/j.ebiom.2023.104498 PMID: 36863255 21. Australian Bureau of Statistics. Tablebuilder Canberra: ABS. 2021. Available from: https://www.abs.gov.au/statistics/microdata-tablebuilder/ tablebuilder 22. Byrne D. A worked example of Braun and Clarke\u2019s approach to reflexive thematic analysis. Qual Quant. 2021;56(3):1391\u2013412. https://doi. org/10.1007/s11135-021-01182-y 23. Braun V, Clarke V. Thematic analysis: a practical guide. L",
    "full_text_length": 43195,
    "chunk_length": 1760
  },
  {
    "chunk_id": 2848,
    "paper_filename": "popic_2025_how_should_ai_be_used_in_breast_screening.pdf",
    "paper_title": "Popic 2025 How Should Ai Be Used In Breast Screening",
    "chunk_index": 40,
    "total_chunks": 42,
    "text_content": "28. Fritsch SJ, Blankenheim A, Wahl A, Hetfeld P, Maassen O, Deffge S, et al. Attitudes and perception of artificial intelligence in healthcare: a cross-sectional survey among patients. Digit Health. 2022;8:20552076221116772. https://doi.org/10.1177/20552076221116772 PMID: 35983102 29. Statista. Internet in Australia - statistics & facts. 2025 [Accessed 2025 February 15]. Available from: https://www.statista.com/topics/5261/ internet-usage-in-australia/#editorsPicks 30. Ding L, Poelhekken K, Gre",
    "full_text_length": 43195,
    "chunk_length": 1557
  },
  {
    "chunk_id": 2849,
    "paper_filename": "popic_2025_how_should_ai_be_used_in_breast_screening.pdf",
    "paper_title": "Popic 2025 How Should Ai Be Used In Breast Screening",
    "chunk_index": 41,
    "total_chunks": 42,
    "text_content": "women aged 70-74 be invited to participate in screening mammogra - phy? A report on two Australian community juries. BMJ Open. 2018;8(6):e021174. https://doi.org/10.1136/bmjopen-2017-021174 PMID: 29903796 34. Taylor-Phillips S, Seedat F, Kijauskaite G, Marshall J, Halligan S, Hyde C, et al. UK National Screening Committee\u2019s approach to reviewing evi - dence on artificial intelligence in breast cancer screening. Lancet Digit Health. 2022;4(7):e558\u201365. https://doi.org/10.1016/S2589-7500(22)00088-7",
    "full_text_length": 43195,
    "chunk_length": 515
  },
  {
    "chunk_id": 2850,
    "paper_filename": "rana_2022_categorized_contrast_enhanced_mammographydataset_cdd_cesm.pdf",
    "paper_title": "Rana 2022 Categorized Contrast Enhanced Mammographydataset Cdd Cesm",
    "chunk_index": 0,
    "total_chunks": 28,
    "text_content": "1 Scientific Data | (2022) 9:122 | https://doi.org/10.1038/s41597-022-01238-0 www.nature.com/scientificdataCategorized contrast enhanced mammography dataset for diagnostic and artificial intelligence research Rana Khaled 1 \u2709, Maha Helal1, Omar Alfarghaly 2 \u2709, Omnia Mokhtar1, Abeer Elkorany2 \u2709, Hebatalla El Kassas1 & Aly Fahmy2 \u2709 Contrast-enhanced spectral mammography (CESM) is a relatively recent imaging modality with increased diagnostic accuracy compared to digital mammography (DM). New deep l",
    "full_text_length": 29070,
    "chunk_length": 1446
  },
  {
    "chunk_id": 2851,
    "paper_filename": "rana_2022_categorized_contrast_enhanced_mammographydataset_cdd_cesm.pdf",
    "paper_title": "Rana 2022 Categorized Contrast Enhanced Mammographydataset Cdd Cesm",
    "chunk_index": 1,
    "total_chunks": 28,
    "text_content": "334 mass enhancement images, 184 non-mass enhancement images, 159 postoperative images, 8 post neoadjuvant chemotherapy images, and 751 normal images, with 248 images having more than one finding. This is the first dataset to incorporate data selection, segmentation annotation, medical reports, and pathological diagnosis for all cases. Moreover, we propose and evaluate a DL-based technique to automatically segment abnormal findings in images. Background & Summary Digital mammography (DM) is the ",
    "full_text_length": 29070,
    "chunk_length": 1410
  },
  {
    "chunk_id": 2852,
    "paper_filename": "rana_2022_categorized_contrast_enhanced_mammographydataset_cdd_cesm.pdf",
    "paper_title": "Rana 2022 Categorized Contrast Enhanced Mammographydataset Cdd Cesm",
    "chunk_index": 2,
    "total_chunks": 28,
    "text_content": "that low-energy images obtained appear like the standard DM images and are non-inferior to them 2. High-energy images are non-interpretable; to over - come this, low and high-energy images are recombined and subtracted through appropriate image processing to suppress the background breast parenchyma after the acquisition. Figure 1 shows the resulting subtracted images obtained for interpretation, revealing contrast enhancement areas in a suppressed breast tissue back - ground. Findings could be ",
    "full_text_length": 29070,
    "chunk_length": 1391
  },
  {
    "chunk_id": 2853,
    "paper_filename": "rana_2022_categorized_contrast_enhanced_mammographydataset_cdd_cesm.pdf",
    "paper_title": "Rana 2022 Categorized Contrast Enhanced Mammographydataset Cdd Cesm",
    "chunk_index": 3,
    "total_chunks": 28,
    "text_content": "radiologists 5. Currently, the use of artificial intelligence (AI) in radiology is still in its early stages. Nonetheless, algorithms that analyze pixel data distinguish patterns from images that might not have been previously identified even by expert radiologists 6. Deep learning (DL) has a promising potential in performing many tasks such as automatically detecting lesions and helping radiologists provide a more accurate diagnosis. Moreover, new multimodal DL models like the perceiver 7 make ",
    "full_text_length": 29070,
    "chunk_length": 1510
  },
  {
    "chunk_id": 2854,
    "paper_filename": "rana_2022_categorized_contrast_enhanced_mammographydataset_cdd_cesm.pdf",
    "paper_title": "Rana 2022 Categorized Contrast Enhanced Mammographydataset Cdd Cesm",
    "chunk_index": 4,
    "total_chunks": 28,
    "text_content": "for training new DL networks or fine-tuning existing pre-trained DL networks and evaluating them. This is why it is important for radiologists to understand the impact of these machine-learning (ML) based analytical tools and recognize how they might influence and change the radiological practice soon 8. In the past couple of years, a small number of public mammography datasets were released, including the Digital Database for Screening Mammography (DDSM)9, the Image Retrieval in Medical Applica",
    "full_text_length": 29070,
    "chunk_length": 1401
  },
  {
    "chunk_id": 2855,
    "paper_filename": "rana_2022_categorized_contrast_enhanced_mammographydataset_cdd_cesm.pdf",
    "paper_title": "Rana 2022 Categorized Contrast Enhanced Mammographydataset Cdd Cesm",
    "chunk_index": 5,
    "total_chunks": 28,
    "text_content": "propose a new DL-based technique to automatically segment the abnormal findings in the images without intervention from radiologists, as segmentation annotation is a time-consuming task. Methods We collected and reformatted the data into an easily-accessible format. Figure 2 displays the flow diagram of the process to prepare our dataset: image preprocessing, manual annotations, and the automatic segmentation. Technique of contrast enhanced mammography examination. CESM is done using the standar",
    "full_text_length": 29070,
    "chunk_length": 1379
  },
  {
    "chunk_id": 2856,
    "paper_filename": "rana_2022_categorized_contrast_enhanced_mammographydataset_cdd_cesm.pdf",
    "paper_title": "Rana 2022 Categorized Contrast Enhanced Mammographydataset Cdd Cesm",
    "chunk_index": 6,
    "total_chunks": 28,
    "text_content": "low-energy images with their corresponding sub- tracted CESM images gathered from the Radiology Department of the National Cancer Institute, Cairo University, Egypt over the period from January 2019 to February 2021. The images are all high resolution with an average of 2355 \u00d7 1315 pixels. Institutional review board approval and patient informed consent to carry out and publish data were obtained from 326 female patients aged from 18 to 90 years. The dataset contains 2006 images with CC and MLO ",
    "full_text_length": 29070,
    "chunk_length": 1220
  },
  {
    "chunk_id": 2857,
    "paper_filename": "rana_2022_categorized_contrast_enhanced_mammographydataset_cdd_cesm.pdf",
    "paper_title": "Rana 2022 Categorized Contrast Enhanced Mammographydataset Cdd Cesm",
    "chunk_index": 7,
    "total_chunks": 28,
    "text_content": "87 patients with missing images as some were not available or removed due to quality concerns. Two different machines were used for image acquisition; GE Healthcare Senographe DS and Hologic Selenia Dimensions Mammography Systems. The two machines provide similar quality, and all other steps in the data acquisition and post-processing phases were kept the same. The images are manually-annotated by expert radiologists according to the American College of Radiology Breast Imaging Reporting and Dat",
    "full_text_length": 29070,
    "chunk_length": 1532
  },
  {
    "chunk_id": 2858,
    "paper_filename": "rana_2022_categorized_contrast_enhanced_mammographydataset_cdd_cesm.pdf",
    "paper_title": "Rana 2022 Categorized Contrast Enhanced Mammographydataset Cdd Cesm",
    "chunk_index": 8,
    "total_chunks": 28,
    "text_content": "the annotations, as pathological results are the gold-standard reference for radiologically-suspicious or malignant-looking lesions, and follow-up is the gold standard for benign-looking lesions. Moreover, full medical reports, written by an ensemble of radiologists, are provided for each case along with manual segmentation annotation for the abnormal findings in each image. Annotations. Data are gathered and stored in a DICOM format. Some irrelevant annotations that are not used for lesion iden",
    "full_text_length": 29070,
    "chunk_length": 1399
  },
  {
    "chunk_id": 2859,
    "paper_filename": "rana_2022_categorized_contrast_enhanced_mammographydataset_cdd_cesm.pdf",
    "paper_title": "Rana 2022 Categorized Contrast Enhanced Mammographydataset Cdd Cesm",
    "chunk_index": 9,
    "total_chunks": 28,
    "text_content": "the case. All patients\u2019 identification data were removed. We believe that releasing the full-text med-ical reports is important, as research studies concerned with radiology report-writing often struggle with the lack of full reports not being present in large datasets 14. image processing. DICOM images were exported losslessly to a joint photographic experts group (JPEG) format using RadiAnt DICOM viewer application( https://www.radiantviewer.com/ ). After automatically remov - ing all irreleva",
    "full_text_length": 29070,
    "chunk_length": 1357
  },
  {
    "chunk_id": 2860,
    "paper_filename": "rana_2022_categorized_contrast_enhanced_mammographydataset_cdd_cesm.pdf",
    "paper_title": "Rana 2022 Categorized Contrast Enhanced Mammographydataset Cdd Cesm",
    "chunk_index": 10,
    "total_chunks": 28,
    "text_content": "the image that contrib- uted to the model\u2019s prediction. A threshold of the top 25% GradCam intensities is then used on the highlights Fig. 2 Flow diagram of the preparation of (CDD-CESM) and the deep learning method to automatically generate the segmentation annotation. 4 Scientific Data | (2022) 9:122 | https://doi.org/10.1038/s41597-022-01238-0 www.nature.com/scientificdata www.nature.com/scientificdata/to generate the segments. Furthermore, a threshold of the top 15% white pixels is used to f",
    "full_text_length": 29070,
    "chunk_length": 1385
  },
  {
    "chunk_id": 2861,
    "paper_filename": "rana_2022_categorized_contrast_enhanced_mammographydataset_cdd_cesm.pdf",
    "paper_title": "Rana 2022 Categorized Contrast Enhanced Mammographydataset Cdd Cesm",
    "chunk_index": 11,
    "total_chunks": 28,
    "text_content": "- tations were used. Model & training. An EfficientNetB017, pre-trained on ImageNet18, was used as the starting model in our experiments. We finetuned the model by removing the final layer and adding a layer with three output classes (Normal, Benign, Malignant). All the weights are left to be fine-tuned during the training. Categorical cross-entropy was used as the loss function with Adam optimizer 19 as shown in Eq. 1, where CE( b) is the cross entropy loss for batch b, C the number of classes,",
    "full_text_length": 29070,
    "chunk_length": 1220
  },
  {
    "chunk_id": 2862,
    "paper_filename": "rana_2022_categorized_contrast_enhanced_mammographydataset_cdd_cesm.pdf",
    "paper_title": "Rana 2022 Categorized Contrast Enhanced Mammographydataset Cdd Cesm",
    "chunk_index": 12,
    "total_chunks": 28,
    "text_content": "Data | (2022) 9:122 | https://doi.org/10.1038/s41597-022-01238-0 www.nature.com/scientificdata www.nature.com/scientificdata//circumflexnosp CEby logy () (1) cC iN ii 11cc \u2211\u2211 =\u2212 . == Highlights. After the model achieved a good accuracy on all the images, we used GradCam15 to get heatmaps representing the parts of the image that had the highest impact on the model\u2019s decision. The heatmaps are traced back from the ground-truth class and not the predicted class. Moreover, we removed any highlights ",
    "full_text_length": 29070,
    "chunk_length": 1276
  },
  {
    "chunk_id": 2863,
    "paper_filename": "rana_2022_categorized_contrast_enhanced_mammographydataset_cdd_cesm.pdf",
    "paper_title": "Rana 2022 Categorized Contrast Enhanced Mammographydataset Cdd Cesm",
    "chunk_index": 13,
    "total_chunks": 28,
    "text_content": "CESM images are distributed as JPEG files. They include both MLO and CC views of the mammograms. Metadata for each image is incorporated as an associated CSV file consisting of:Annotation Description Method Format Patient\u2019s ageAge of the patient at time of examination.Calculated from the date of birth.Numbers Side of breast Right or left breast Manually annotated. Categorical Breast Composition ACR categoryBreast density describes the amount of fibroglandular tissue present in a breast relative ",
    "full_text_length": 29070,
    "chunk_length": 1549
  },
  {
    "chunk_id": 2864,
    "paper_filename": "rana_2022_categorized_contrast_enhanced_mammographydataset_cdd_cesm.pdf",
    "paper_title": "Rana 2022 Categorized Contrast Enhanced Mammographydataset Cdd Cesm",
    "chunk_index": 14,
    "total_chunks": 28,
    "text_content": "1: Normal examination BIRADS 2: Benign findings BIRADS 3: Probably benign findings <2% malignancy BIRADS 4: Suspicious >2 but <95% malignancy BIRADS 5: Highly suspicious of malignancy >95% BIRADS 6: Known biopsy-proven malignancy Type of image viewUsually two standard views are acquired for each breast: Manually annotated.Categorical: \u2022 MLO: most important because it allows depiction of most of the breast\u2019s tissues\u2022 MLO \u2022 CC: reveals medial part and external lateral portion of the breast\u2022 CC Tag",
    "full_text_length": 29070,
    "chunk_length": 1407
  },
  {
    "chunk_id": 2865,
    "paper_filename": "rana_2022_categorized_contrast_enhanced_mammographydataset_cdd_cesm.pdf",
    "paper_title": "Rana 2022 Categorized Contrast Enhanced Mammographydataset Cdd Cesm",
    "chunk_index": 15,
    "total_chunks": 28,
    "text_content": "www.nature.com/scientificdata www.nature.com/scientificdata/\u2022 Path to image files \u2022 Patient number \u2022 Breast side: Left or Right \u2022 Type of Examination: DM (low energy image) or CESM (subtracted image) \u2022 View: CC or MLO \u2022 Density category (if low energy image) \u2022 Number of findings (if multiple) \u2022 Mass shape, density, and margin (if present) \u2022 Mass enhancement pattern (if present) \u2022 Architectural distortion (if present) \u2022 Asymmetry (if present) \u2022 Calcification type and distribution (if present) \u2022 N",
    "full_text_length": 29070,
    "chunk_length": 1343
  },
  {
    "chunk_id": 2866,
    "paper_filename": "rana_2022_categorized_contrast_enhanced_mammographydataset_cdd_cesm.pdf",
    "paper_title": "Rana 2022 Categorized Contrast Enhanced Mammographydataset Cdd Cesm",
    "chunk_index": 16,
    "total_chunks": 28,
    "text_content": "per patient)<40 58 (17.8%) 40\u201349 100 (30.7%) 50\u201359 95 (29.1%) 60\u201369 59 (18.1%) \u226570 14 (4.3%) Cancer TypeInvasive ductal carcinoma 445 (67.5%) Invasive lobular carcinoma 42 (6.3%) Mixed invasive ductal carcinoma and invasive lobular carcinoma28 (4.2%) Ductal carcinoma insitu purely 17 (2.5%) Inflammatory breast cancer 40 (6%) Other 90 (13.5%) Table 2 . Characteristics of the CDD-CESM dataset. The 757 normal images consist of 751 normal images and 6 post-neoadjuvant images considered normal (no re",
    "full_text_length": 29070,
    "chunk_length": 1478
  },
  {
    "chunk_id": 2867,
    "paper_filename": "rana_2022_categorized_contrast_enhanced_mammographydataset_cdd_cesm.pdf",
    "paper_title": "Rana 2022 Categorized Contrast Enhanced Mammographydataset Cdd Cesm",
    "chunk_index": 17,
    "total_chunks": 28,
    "text_content": "Technical Validation For the segmentation evaluation of our DL model, experienced radiologist provided hand-drawn segmenta - tions for each abnormal finding in the CDD-CESM dataset. We calculated the intersection over union (IOU) and the dice coefficients (F1) between the computed and hand-drawn segmentations, after applying the same white-intensity threshold on the hand-drawn segmentations. Furthermore, we added another metric which we called overlap50, which is the percentage of images where t",
    "full_text_length": 29070,
    "chunk_length": 1364
  },
  {
    "chunk_id": 2868,
    "paper_filename": "rana_2022_categorized_contrast_enhanced_mammographydataset_cdd_cesm.pdf",
    "paper_title": "Rana 2022 Categorized Contrast Enhanced Mammographydataset Cdd Cesm",
    "chunk_index": 18,
    "total_chunks": 28,
    "text_content": "accurately or completely observed by our DL model. Fig. 5 Histograms for the CDD-CESM dataset showing distribution of (a) BIRADS category for each abnormality, ( b) Benign and malignant lesions. Images Overlap50 IOU F1 FindingsMass 310 0.85 0.65 0.72 Distortion 48 0.87 0.70 0.79 Asymmetry 222 0.87 0.70 0.78 Calcifications 238 0.81 0.62 0.70 Postoperative 159 0.77 0.61 0.68 Mass enhancement 334 0.91 0.66 0.73 Non mass enhancement 184 0.89 0.72 0.79 Image TypeDM 665 0.81 0.64 0.71 CM 590 0.86 0.65",
    "full_text_length": 29070,
    "chunk_length": 1279
  },
  {
    "chunk_id": 2869,
    "paper_filename": "rana_2022_categorized_contrast_enhanced_mammographydataset_cdd_cesm.pdf",
    "paper_title": "Rana 2022 Categorized Contrast Enhanced Mammographydataset Cdd Cesm",
    "chunk_index": 19,
    "total_chunks": 28,
    "text_content": "patient. Patients aged seventy years and higher had the highest overlap50 = 94%. Forty years and lower had the lowest overlap50 = 78%. As expected, the accuracy of visualization decreases as the breast density increases. Fig. 6 Examples of different cases and their corresponding automatic segmentations. 9 Scientific Data | (2022) 9:122 | https://doi.org/10.1038/s41597-022-01238-0 www.nature.com/scientificdata www.nature.com/scientificdata/Low energy or subtracted image. Low energy image overlap5",
    "full_text_length": 29070,
    "chunk_length": 1402
  },
  {
    "chunk_id": 2870,
    "paper_filename": "rana_2022_categorized_contrast_enhanced_mammographydataset_cdd_cesm.pdf",
    "paper_title": "Rana 2022 Categorized Contrast Enhanced Mammographydataset Cdd Cesm",
    "chunk_index": 20,
    "total_chunks": 28,
    "text_content": "Benign findings had the lower overlap50 = 75% compared to 90% for malig- nant findings. Most of the benign lesions were non-enhancing in subtracted images. Furthermore, in low-energy images, benign lesions were either hidden behind the dense breast tissues, had equal density or parallel orienta-tion to the surrounding breast parenchyma. However, highly cellular benign findings were accurately depicted by our DL model. Decreased accuracy was found with multiplicity and retroareolar locations. Gen",
    "full_text_length": 29070,
    "chunk_length": 1367
  },
  {
    "chunk_id": 2871,
    "paper_filename": "rana_2022_categorized_contrast_enhanced_mammographydataset_cdd_cesm.pdf",
    "paper_title": "Rana 2022 Categorized Contrast Enhanced Mammographydataset Cdd Cesm",
    "chunk_index": 21,
    "total_chunks": 28,
    "text_content": "to segment the lesions. Furthermore, the full-text medical reports can be used to train report generation models. Code availability A Github repository is publicly available (https://github.com/omar-mohamed/CDD-CESM-Dataset) which contains helper scripts to make training a DL model on the dataset easier like reading the annotations, pre-processing the images by resizing and normalizing, training different existing models, augmenting the images while training, and evaluating the different models ",
    "full_text_length": 29070,
    "chunk_length": 1428
  },
  {
    "chunk_id": 2872,
    "paper_filename": "rana_2022_categorized_contrast_enhanced_mammographydataset_cdd_cesm.pdf",
    "paper_title": "Rana 2022 Categorized Contrast Enhanced Mammographydataset Cdd Cesm",
    "chunk_index": 22,
    "total_chunks": 28,
    "text_content": "euref image quality criteria. European radiology. 25, 2813\u20132820 (2015). 3. Bhimani, C. et al . Contrast-enhanced spectral mammography: technique, indications, and clinical applications. Academic radiology. 24, 84\u201388 (2017). 4. Lewin, J. M., Isaacs, P . K., Vance, V . & Larke, F. J. Dual-energy contrast-enhanced digital subtraction mammography: feasibility. Radiology. 229, 261\u2013268 (2003). 5. Hupse, R. et al . Computer-aided detection of masses at mammography: interactive decision support versus p",
    "full_text_length": 29070,
    "chunk_length": 1385
  },
  {
    "chunk_id": 2873,
    "paper_filename": "rana_2022_categorized_contrast_enhanced_mammographydataset_cdd_cesm.pdf",
    "paper_title": "Rana 2022 Categorized Contrast Enhanced Mammographydataset Cdd Cesm",
    "chunk_index": 23,
    "total_chunks": 28,
    "text_content": "Fifth International Workshop on Digital Mammography , 212\u2013218, http://www.eng.usf.edu/cvprg/Mammography/Database.html (2001). 10. Lehmann, T. M. et al . Content-based image retrieval in medical applications. Methods of information in medicine 43, 354\u2013361 (2004). 11. Suckling J, P . The mammographic image analysis society digital mammogram database. Digital Mammo 375\u2013386 (1994). 12. Lee, R. S. et al . A curated mammography data set for use in computer-aided detection and diagnosis research. Scien",
    "full_text_length": 29070,
    "chunk_length": 1389
  },
  {
    "chunk_id": 2874,
    "paper_filename": "rana_2022_categorized_contrast_enhanced_mammographydataset_cdd_cesm.pdf",
    "paper_title": "Rana 2022 Categorized Contrast Enhanced Mammographydataset Cdd Cesm",
    "chunk_index": 24,
    "total_chunks": 28,
    "text_content": "& Fahmy, A. Deep learning approaches for data augmentation and classification of breast masses using ultrasound images. International Journal of Advanced Computer Science and Applications 10 (2019). 17. Tan, M. & Le, Q. V . Efficientnet: Rethinking model scaling for convolutional neural networks. Proceedings of the 36th International Conference on Machine Learning 97, 6105\u20136114 (2019). 18. Krizhevsky, A., Sutskever, I. & Hinton, G. E. Imagenet classification with deep convolutional neural networ",
    "full_text_length": 29070,
    "chunk_length": 1354
  },
  {
    "chunk_id": 2875,
    "paper_filename": "rana_2022_categorized_contrast_enhanced_mammographydataset_cdd_cesm.pdf",
    "paper_title": "Rana 2022 Categorized Contrast Enhanced Mammographydataset Cdd Cesm",
    "chunk_index": 25,
    "total_chunks": 28,
    "text_content": "15, 1929\u20131958 (2014). 21. Khaled, R. et al. Categorized digital database for low energy and subtracted contrast enhanced spectral mammography images. The Cancer Imaging Archive https://doi.org/10.7937/29kw-ae92 (2021). 22. Clark, K. W . et al . The cancer imaging archive (TCIA): maintaining and operating a public information repository. J. Digit. Imaging 26, 1045\u20131057, https://doi.org/10.1007/s10278-013-9622-7 (2013). 10 Scientific Data | (2022) 9:122 | https://doi.org/10.1038/s41597-022-01238-0",
    "full_text_length": 29070,
    "chunk_length": 1519
  },
  {
    "chunk_id": 2876,
    "paper_filename": "rana_2022_categorized_contrast_enhanced_mammographydataset_cdd_cesm.pdf",
    "paper_title": "Rana 2022 Categorized Contrast Enhanced Mammographydataset Cdd Cesm",
    "chunk_index": 26,
    "total_chunks": 28,
    "text_content": "the paper. A.E. and A.F. supervised the training of the segmentation model, and all the technical details. Funding Open access funding provided by The Science, Technology & Innovation Funding Authority (STDF) in cooperation with The Egyptian Knowledge Bank (EKB). Competing interests The authors declare no competing interests. Additional information Correspondence and requests for materials should be addressed to R.K., O.A., A.E. or A.F. Reprints and permissions information is available at www.na",
    "full_text_length": 29070,
    "chunk_length": 1367
  },
  {
    "chunk_id": 2877,
    "paper_filename": "rana_2022_categorized_contrast_enhanced_mammographydataset_cdd_cesm.pdf",
    "paper_title": "Rana 2022 Categorized Contrast Enhanced Mammographydataset Cdd Cesm",
    "chunk_index": 27,
    "total_chunks": 28,
    "text_content": "this article are included in the article\u2019s Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the article\u2019s Creative Commons license and your intended use is not per- mitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this license, visit http://creativecommons.org/licenses/by/4.0/. \u00a9 The Author(s) 2022",
    "full_text_length": 29070,
    "chunk_length": 469
  },
  {
    "chunk_id": 2878,
    "paper_filename": "riyadh_2020_early_breast_cancer_detection_via_optimized_cnn.pdf",
    "paper_title": "Riyadh 2020 Early Breast Cancer Detection Via Optimized Cnn",
    "chunk_index": 0,
    "total_chunks": 101,
    "text_content": "Early breast cancer detection via infrared thermography using a CNN enhanced with particle swarm optimization Riyadh M. Alzahrani1, Mohamed Yacin Sikkandar1\uf02a, S. Sabarunisha Begum2, Ahmed Farag Salem Babetat3, Maryam Alhashim4, Abdulrahman Alduraywish1, N. B. Prakash5 & Eddie Y. K. Ng6 Breast cancer remains the most prevalent cause of cancer-related mortality among women worldwide, with an estimated incidence exceeding 500,000 new cases annually. Timely diagnosis is vital for enhancing therapeut",
    "full_text_length": 101814,
    "chunk_length": 1549
  },
  {
    "chunk_id": 2879,
    "paper_filename": "riyadh_2020_early_breast_cancer_detection_via_optimized_cnn.pdf",
    "paper_title": "Riyadh 2020 Early Breast Cancer Detection Via Optimized Cnn",
    "chunk_index": 1,
    "total_chunks": 101,
    "text_content": "convolutional neural networks ( CNNs) for distinguishing between malignant and benign thermographic breast images. An Enhanced Particle Swarm Optimization (EPSO) algorithm is integrated to automatically fine-tune CNN hyperparameters, thereby minimizing manual effort and enhancing computational efficiency . The methodology also incorporates advanced image preprocessing techniques\u2014including Mamdani fuzzy logic-based edge detection, Contrast-Limited Adaptive Histogram Equalization (CLAHE) for contr",
    "full_text_length": 101814,
    "chunk_length": 1644
  },
  {
    "chunk_id": 2880,
    "paper_filename": "riyadh_2020_early_breast_cancer_detection_via_optimized_cnn.pdf",
    "paper_title": "Riyadh 2020 Early Breast Cancer Detection Via Optimized Cnn",
    "chunk_index": 2,
    "total_chunks": 101,
    "text_content": "breast cancer-related deaths. Advances in current technologies and data mining methodologies have significantly enhanced the capabilities of medical systems in analyzing, predicting, and accurately classifying cancer cases2. Breast cancer originates from the abnormal proliferation of cells within breast tissue, leading to the formation of various types of lesions. These lesions are typically characterized by asymmetries between the left and right breasts, disruption of normal tissue architecture",
    "full_text_length": 101814,
    "chunk_length": 1521
  },
  {
    "chunk_id": 2881,
    "paper_filename": "riyadh_2020_early_breast_cancer_detection_via_optimized_cnn.pdf",
    "paper_title": "Riyadh 2020 Early Breast Cancer Detection Via Optimized Cnn",
    "chunk_index": 3,
    "total_chunks": 101,
    "text_content": ".S.R. Engineering College, Sivakasi 626140, India. 3Dr. Mohammad Alfagih Hospital, Riyadh 13223, Saudi Arabia. 4Department of Radiology, College of Medicine, Imam Abdulrahman Bin Faisal University, Dammam 34212, Saudi Arabia. 5Department of Department of Electrical and Electronics Engineering, National Engineering College, Kovilpatti 628503, Tamilnadu, India. 6School of Mechanical & Aerospace Engineering, College of Engineering, Nanyang Technological University, Singapore 639798, Singapore. \uf02aema",
    "full_text_length": 101814,
    "chunk_length": 1669
  },
  {
    "chunk_id": 2882,
    "paper_filename": "riyadh_2020_early_breast_cancer_detection_via_optimized_cnn.pdf",
    "paper_title": "Riyadh 2020 Early Breast Cancer Detection Via Optimized Cnn",
    "chunk_index": 4,
    "total_chunks": 101,
    "text_content": "cancer into triple-negative and non\u2013triple-negative subtypes, predicting metastatic progression, and identifying early recurrence of the disease9. Moreover, advanced ensemble learning algorithms such as Gradient Boosted Trees and eXtreme Gradient Boosting (XGBoost) have been effectively employed to predict metastasis in breast cancer and survival outcomes in epithelial ovarian cancer, respectively10,11. Recent developments in machine learning have incorporated deep learning and radiomics to clas",
    "full_text_length": 101814,
    "chunk_length": 1675
  },
  {
    "chunk_id": 2883,
    "paper_filename": "riyadh_2020_early_breast_cancer_detection_via_optimized_cnn.pdf",
    "paper_title": "Riyadh 2020 Early Breast Cancer Detection Via Optimized Cnn",
    "chunk_index": 5,
    "total_chunks": 101,
    "text_content": "Forests, and Random Forests\u2014have been performed for breast cancer detection and diagnosis15. These studies, particularly those employing the Wisconsin Breast Cancer dataset with 32 distinct features, revealed that Logistic Regression yielded the highest classification accuracy among all tested models. This was primarily due to its computational efficiency and ease of training. Additionally, Wakili et al. (2022) proposed a homogeneous ensemble model leveraging a Multilayer Perceptron (MLP) archit",
    "full_text_length": 101814,
    "chunk_length": 1487
  },
  {
    "chunk_id": 2884,
    "paper_filename": "riyadh_2020_early_breast_cancer_detection_via_optimized_cnn.pdf",
    "paper_title": "Riyadh 2020 Early Breast Cancer Detection Via Optimized Cnn",
    "chunk_index": 6,
    "total_chunks": 101,
    "text_content": "Artificial Bee Colony algorithm17. This hybrid model employed Momentum-based Gradient Descent Backpropagation and utilized Simulated Annealing to enhance local search capabilities. It demonstrated strong classification capabilities, achieving accuracy exceeding 99%. In a big data context, Sakri et al. (2018) developed a breast cancer detection model using an optimized ANN framework18. The model incorporated a modified Dragonfly Algorithm for feature selection and utilized the Grey Wolf Optimizat",
    "full_text_length": 101814,
    "chunk_length": 1549
  },
  {
    "chunk_id": 2885,
    "paper_filename": "riyadh_2020_early_breast_cancer_detection_via_optimized_cnn.pdf",
    "paper_title": "Riyadh 2020 Early Breast Cancer Detection Via Optimized Cnn",
    "chunk_index": 7,
    "total_chunks": 101,
    "text_content": "On the image enhancement front, histogram-based enhancement techniques have been widely explored. Ibrahim et al. (2023) presented a PSO-based method for tuning the enhancement parameters of Contrast Limited Adaptive Histogram Equalization (CLAHE), which incorporated Local Contrast Modification (LCM). Their study concluded that the PSO-optimized LCM-CLAHE method significantly outperformed existing techniques, producing superior-quality mammographic images20. CNNs have demonstrated exceptional cap",
    "full_text_length": 101814,
    "chunk_length": 1604
  },
  {
    "chunk_id": 2886,
    "paper_filename": "riyadh_2020_early_breast_cancer_detection_via_optimized_cnn.pdf",
    "paper_title": "Riyadh 2020 Early Breast Cancer Detection Via Optimized Cnn",
    "chunk_index": 8,
    "total_chunks": 101,
    "text_content": "necessarily guarantee convergence to a global optimum. Therefore, the process of hyperparameter tuning often involves extensive trial-and-error procedures and reliance on expert domain knowledge. This limitation significantly hinders the effective deployment of CNN-based solutions in real-world medical applications, particularly in the analysis of MRI images6. The remainder of this article is structured as follows: Section II provides a comprehensive review of the literature, highlights the limi",
    "full_text_length": 101814,
    "chunk_length": 1452
  },
  {
    "chunk_id": 2887,
    "paper_filename": "riyadh_2020_early_breast_cancer_detection_via_optimized_cnn.pdf",
    "paper_title": "Riyadh 2020 Early Breast Cancer Detection Via Optimized Cnn",
    "chunk_index": 9,
    "total_chunks": 101,
    "text_content": "biopsy, have been the standard for years. While these methods are effective, they often involve invasive procedures (e.g., biopsy) or require physical contact (e.g., mammography), which can cause discomfort to patients. Moreover, these methods are not always accessible due to high costs, and mammography, in particular, is less effective in women with dense breast tissue. As a result, there is a growing interest in alternative non-invasive methods, such as thermographic imaging, which offers the ",
    "full_text_length": 101814,
    "chunk_length": 1510
  },
  {
    "chunk_id": 2888,
    "paper_filename": "riyadh_2020_early_breast_cancer_detection_via_optimized_cnn.pdf",
    "paper_title": "Riyadh 2020 Early Breast Cancer Detection Via Optimized Cnn",
    "chunk_index": 10,
    "total_chunks": 101,
    "text_content": "to handle the complexity of thermal patterns in breast tissue, resulting in poor accuracy and high false-positive rates. Additionally, many existing systems rely on basic machine learning techniques or standard convolutional neural networks (CNNs), which, while powerful, require manual tuning of hyperparameters\u2014a process that is both time-consuming and prone to human error. Traditional optimization methods, such as grid search or random search, are often computationally expensive and do not guar",
    "full_text_length": 101814,
    "chunk_length": 1457
  },
  {
    "chunk_id": 2889,
    "paper_filename": "riyadh_2020_early_breast_cancer_detection_via_optimized_cnn.pdf",
    "paper_title": "Riyadh 2020 Early Breast Cancer Detection Via Optimized Cnn",
    "chunk_index": 11,
    "total_chunks": 101,
    "text_content": "current systems and leads to suboptimal performance. In6,11,21, thermography patterns were fed as input to the training algorithm in medical image applications. The major drawback of the above-mentioned approaches was the pre-processing step of data before feature extraction. They commonly used filters, gray scale image conversion and cropping of patterns were proposed. These techniques failed to extract high level features from the pattern, which may cause a reduction in efficiency of the recog",
    "full_text_length": 101814,
    "chunk_length": 1318
  },
  {
    "chunk_id": 2890,
    "paper_filename": "riyadh_2020_early_breast_cancer_detection_via_optimized_cnn.pdf",
    "paper_title": "Riyadh 2020 Early Breast Cancer Detection Via Optimized Cnn",
    "chunk_index": 12,
    "total_chunks": 101,
    "text_content": "and gray-wolf optimization was used to select features22. The major drawback behind the algorithm is that the use of MCSVM causes high computation time and algorithm suffers from overfitting problems. Three of the eight classifiers used for ensemble classification using a voting mechanism were SVM learning with stochastics gradient de-scent optimizations, MLP , and common logistic regression learning. The classifiers\u2019 performances were assessed using both hard and soft voting procedures. A machi",
    "full_text_length": 101814,
    "chunk_length": 1357
  },
  {
    "chunk_id": 2891,
    "paper_filename": "riyadh_2020_early_breast_cancer_detection_via_optimized_cnn.pdf",
    "paper_title": "Riyadh 2020 Early Breast Cancer Detection Via Optimized Cnn",
    "chunk_index": 13,
    "total_chunks": 101,
    "text_content": "technique was applied24. The basic learners were defined as Random Forest, Extra Tree, Adaboost, KNN, and Gradient Boosting classifiers. The last estimator was the logistic regression model. They failed to hybrid the appropriate ML and DL approach for recognizing the type of breast cancer. Techniques based on machine learning (ML) have demonstrated excellent performance for various image recognition tasks25. Because of its excellent discriminating ability, SVM is frequently used to solve classif",
    "full_text_length": 101814,
    "chunk_length": 1393
  },
  {
    "chunk_id": 2892,
    "paper_filename": "riyadh_2020_early_breast_cancer_detection_via_optimized_cnn.pdf",
    "paper_title": "Riyadh 2020 Early Breast Cancer Detection Via Optimized Cnn",
    "chunk_index": 14,
    "total_chunks": 101,
    "text_content": "et al., (2013) reported an accuracy of 94% using a local seed growing technique combined with spherical wavelet transformation and SVM classifier for mass/non-mass classification27. Iqbal et al., (2022) used Gabor filters with different scales and directions, and the result was 0.98 area under ROC28. Berbar et al., (2012) reported accuracy rates of 98.63% and 97.25% with SVM and k-NN, respectively, by utilizing hybrid features based on statistical measures and local binary patterns29. Mukhmetov ",
    "full_text_length": 101814,
    "chunk_length": 1380
  },
  {
    "chunk_id": 2893,
    "paper_filename": "riyadh_2020_early_breast_cancer_detection_via_optimized_cnn.pdf",
    "paper_title": "Riyadh 2020 Early Breast Cancer Detection Via Optimized Cnn",
    "chunk_index": 15,
    "total_chunks": 101,
    "text_content": "that can be hybrid with some more architecture and make the discriminator fool for collecting artificial databases. Computation time can be minimized by using various batch normalization algorithms. Edge detection is crucial in breast cancer tumor identification because it highlights the boundaries of abnormal growth, allowing for precise localization and measurement of tumors. This helps in distinguishing malignant masses from surrounding tissues in medical images like mammograms or MRIs, enhan",
    "full_text_length": 101814,
    "chunk_length": 1526
  },
  {
    "chunk_id": 2894,
    "paper_filename": "riyadh_2020_early_breast_cancer_detection_via_optimized_cnn.pdf",
    "paper_title": "Riyadh 2020 Early Breast Cancer Detection Via Optimized Cnn",
    "chunk_index": 16,
    "total_chunks": 101,
    "text_content": "to longer processing times that are not suitable for real-time clinical diagnostics. As a result, these methods are insufficient in providing the speed, accuracy, and reliability needed for widespread use in clinical practice. Scientific Reports | (2025) 15:25290 3 | https://doi.org/10.1038/s41598-025-11218-0www.nature.com/scientificreports/ This research work introduces a novel solution to the above challenges by utilizing an Enhanced Particle Swarm Optimization (EPSO) algorithm to automaticall",
    "full_text_length": 101814,
    "chunk_length": 1522
  },
  {
    "chunk_id": 2895,
    "paper_filename": "riyadh_2020_early_breast_cancer_detection_via_optimized_cnn.pdf",
    "paper_title": "Riyadh 2020 Early Breast Cancer Detection Via Optimized Cnn",
    "chunk_index": 17,
    "total_chunks": 101,
    "text_content": "faster, and clinically viable solution for breast cancer detection using infrared thermography. Hyperparameter optimization and deep learning in Cancer research The optimization of hyperparameters is a critical component of training robust and efficient deep learning models. In cancer detection tasks, particularly in the classification of breast cancer, the role of hyperparameter tuning becomes even more significant as it directly impacts the model\u2019s ability to generalize and accurately classify",
    "full_text_length": 101814,
    "chunk_length": 1492
  },
  {
    "chunk_id": 2896,
    "paper_filename": "riyadh_2020_early_breast_cancer_detection_via_optimized_cnn.pdf",
    "paper_title": "Riyadh 2020 Early Breast Cancer Detection Via Optimized Cnn",
    "chunk_index": 18,
    "total_chunks": 101,
    "text_content": "rates, batch sizes, and network architectures for accurate cancer detection (Wang et al., 2022 ). Similarly, Choi et al. (2023) presented a comparison of several optimization algorithms, including PSO, for hyperparameter tuning in cancer classification tasks. The study underscored how PSO, when applied to convolutional neural networks (CNNs), significantly improved classification accuracy in identifying malignant tumor cells in medical images (Choi et al., 2023 ). Furthermore, Patel et al. (2024",
    "full_text_length": 101814,
    "chunk_length": 1436
  },
  {
    "chunk_id": 2897,
    "paper_filename": "riyadh_2020_early_breast_cancer_detection_via_optimized_cnn.pdf",
    "paper_title": "Riyadh 2020 Early Breast Cancer Detection Via Optimized Cnn",
    "chunk_index": 19,
    "total_chunks": 101,
    "text_content": "and genetic algorithms, alongside PSO, to fine-tune deep learning models. Their findings indicate that these optimization techniques improve model robustness, which is crucial when developing systems for medical decision support (Liu et al., 2023). Additionally, Shuwei et al. (2024) discussed how integrating PSO with advanced CNN architectures can substantially enhance the performance of automated breast cancer detection systems. Their results demonstrated an increase in diagnostic accuracy, emp",
    "full_text_length": 101814,
    "chunk_length": 1465
  },
  {
    "chunk_id": 2898,
    "paper_filename": "riyadh_2020_early_breast_cancer_detection_via_optimized_cnn.pdf",
    "paper_title": "Riyadh 2020 Early Breast Cancer Detection Via Optimized Cnn",
    "chunk_index": 20,
    "total_chunks": 101,
    "text_content": "may not be accessible in all regions due to cost and infrastructure limitations. Thermographic imaging, while promising, has not been fully utilized in automated systems for breast cancer classification due to challenges in image processing, accuracy, and the complexity of optimizing machine learning models. Furthermore, existing deep learning models for thermographic image classification often rely on traditional optimization techniques that require extensive manual tuning of hyperparameters, w",
    "full_text_length": 101814,
    "chunk_length": 1380
  },
  {
    "chunk_id": 2899,
    "paper_filename": "riyadh_2020_early_breast_cancer_detection_via_optimized_cnn.pdf",
    "paper_title": "Riyadh 2020 Early Breast Cancer Detection Via Optimized Cnn",
    "chunk_index": 21,
    "total_chunks": 101,
    "text_content": "45 \u03bcm. The dataset consists of 82 patients and 7909 thermography images of BC. Images of both benign and malignant cases are included in this dataset. Patient demographics The dataset consists of breast thermal images from both healthy individuals and patients with diagnosed breast cancer. The dataset includes a variety of patient profiles to ensure diversity and robustness for research purposes. The patient demographic information includes: \u2022 Age Range: The dataset includes patients aged betwee",
    "full_text_length": 101814,
    "chunk_length": 1362
  },
  {
    "chunk_id": 2900,
    "paper_filename": "riyadh_2020_early_breast_cancer_detection_via_optimized_cnn.pdf",
    "paper_title": "Riyadh 2020 Early Breast Cancer Detection Via Optimized Cnn",
    "chunk_index": 22,
    "total_chunks": 101,
    "text_content": "with breast cancer, as well as healthy controls. All images were collected from patients who had undergone thermographic scans as part of routine clinical assessments at HUAP . \u2022 Exclusion Criteria : Images from patients with incomplete clinical records, those who underwent non-stand - ard imaging procedures, or those with other unrelated health conditions (e.g., active infections or inflamma - tory diseases affecting the breast area) were excluded from the dataset. The dataset can be accessed a",
    "full_text_length": 101814,
    "chunk_length": 1460
  },
  {
    "chunk_id": 2901,
    "paper_filename": "riyadh_2020_early_breast_cancer_detection_via_optimized_cnn.pdf",
    "paper_title": "Riyadh 2020 Early Breast Cancer Detection Via Optimized Cnn",
    "chunk_index": 23,
    "total_chunks": 101,
    "text_content": "as Mamdani fuzzy logic for edge detection, CLAHE for contrast enhancement, and a median filter for noise reduction\u2014enhances diagnostic accuracy. By combining these novel approaches, our method not only improves the classification accuracy but also accelerates the detection process, offering a viable solution to the limitations of current thermographic detection systems. Generally, enhanced particle swarm optimization (EPSO) can be clustered into three groups namely, PSO parameters, structure of ",
    "full_text_length": 101814,
    "chunk_length": 1408
  },
  {
    "chunk_id": 2902,
    "paper_filename": "riyadh_2020_early_breast_cancer_detection_via_optimized_cnn.pdf",
    "paper_title": "Riyadh 2020 Early Breast Cancer Detection Via Optimized Cnn",
    "chunk_index": 24,
    "total_chunks": 101,
    "text_content": "global optimum with reduced computational time and lower population diversity. In the second group, the CNN architecture determines the range and values of five different parameters: Quantity of layers, Quantity of Neurons in FC layer, Size of Convolution Kernel, Pooling Kernel Size and Stride Size. Initial and foremost step in the projected work is to generate the preliminary swarm for designing the structure of CNN. This preliminary swarm will cover entities with structures selected at chance ",
    "full_text_length": 101814,
    "chunk_length": 1328
  },
  {
    "chunk_id": 2903,
    "paper_filename": "riyadh_2020_early_breast_cancer_detection_via_optimized_cnn.pdf",
    "paper_title": "Riyadh 2020 Early Breast Cancer Detection Via Optimized Cnn",
    "chunk_index": 25,
    "total_chunks": 101,
    "text_content": "represents Fig. 1 . Proposed EPSO with EGAN Algorithm for Breast cancer recognition. Scientific Reports | (2025) 15:25290 5 | https://doi.org/10.1038/s41598-025-11218-0www.nature.com/scientificreports/ the proposed work, initially pre-processing plays a major role in database management station. Next to that, GAN is used to generate artificial images with low penalty gradient rate. And at last optimized CNN architecture is used to recognition the breast cancer. In the third group, the training o",
    "full_text_length": 101814,
    "chunk_length": 1345
  },
  {
    "chunk_id": 2904,
    "paper_filename": "riyadh_2020_early_breast_cancer_detection_via_optimized_cnn.pdf",
    "paper_title": "Riyadh 2020 Early Breast Cancer Detection Via Optimized Cnn",
    "chunk_index": 26,
    "total_chunks": 101,
    "text_content": "CNN parameters is computed in the testing dataset. During training process to avoid the overfitting problem dropout parameter. These dropping proportion estimates the connection amid the neurons in the FC layer. Summary of the CNN Architecture : Layer Type Output Size Activation Function Input Layer 224 \u00d7 224 \u00d7 3 - Convolutional Layer 1 224 \u00d7 224 \u00d7 32 ReLU Convolutional Layer 2 224 \u00d7 224 \u00d7 64 ReLU Convolutional Layer 3 224 \u00d7 224 \u00d7 128 ReLU Max Pooling Layer 1 112 \u00d7 112 \u00d7 128 - Max Pooling Layer ",
    "full_text_length": 101814,
    "chunk_length": 1156
  },
  {
    "chunk_id": 2905,
    "paper_filename": "riyadh_2020_early_breast_cancer_detection_via_optimized_cnn.pdf",
    "paper_title": "Riyadh 2020 Early Breast Cancer Detection Via Optimized Cnn",
    "chunk_index": 27,
    "total_chunks": 101,
    "text_content": "spatial dimensions and computational cost while maintaining essential information. \u2022 Fully connected layers combine the features extracted by the convolutional layers and make the final pre - dictions. \u2022 The dropout layers help regularize the model and prevent overfitting by randomly setting some of the weights to zero during training. \u2022 Softmax activation in the output layer ensures that the model outputs a probability distribution over the 10 possible classes (abnormal conditions). Pre-process",
    "full_text_length": 101814,
    "chunk_length": 1335
  },
  {
    "chunk_id": 2906,
    "paper_filename": "riyadh_2020_early_breast_cancer_detection_via_optimized_cnn.pdf",
    "paper_title": "Riyadh 2020 Early Breast Cancer Detection Via Optimized Cnn",
    "chunk_index": 28,
    "total_chunks": 101,
    "text_content": "for edge detection that has gained the most traction is the canny detector. The current differentiation-based edge detection algorithms mainly use first- or second-order derivatives of an image\u2019s light intensity with one or more scales to identify edge contours in images. Since Russo33 introduced fuzzy inference as a useful method for extracting edge features with strong noise robustness, the application of fuzzy theory in edge detection has gained more and more attention. The module of Type 2 f",
    "full_text_length": 101814,
    "chunk_length": 1402
  },
  {
    "chunk_id": 2907,
    "paper_filename": "riyadh_2020_early_breast_cancer_detection_via_optimized_cnn.pdf",
    "paper_title": "Riyadh 2020 Early Breast Cancer Detection Via Optimized Cnn",
    "chunk_index": 29,
    "total_chunks": 101,
    "text_content": "forms the foundation of this edge detector. The fuzzy-based approach performs smoother, more effectively in noisy environments, and requires less computation power simultaneously. Melin et al.35 proposed an edge detector based on the morphological gradient technique in addition to generalizing type-2 fuzzy logic. They created generalized type-2 fuzzy logic for edge detection using the theory of alpha planes. Fuzzy logic system has the capability to compute inexact info problems which exist in re",
    "full_text_length": 101814,
    "chunk_length": 1226
  },
  {
    "chunk_id": 2908,
    "paper_filename": "riyadh_2020_early_breast_cancer_detection_via_optimized_cnn.pdf",
    "paper_title": "Riyadh 2020 Early Breast Cancer Detection Via Optimized Cnn",
    "chunk_index": 30,
    "total_chunks": 101,
    "text_content": "Y in a range of [0, 1] by B(y) is a MF. Fuzzy set B is represented by Eq. ( 1). B={{(y, B (y))|y\u2208Y}} (1) . In FLS MF plays a major role in the structure of a fuzzy set. Frequently used MFs are sigmoidal, triangular, Gaussian. Fig. demonstrates the T1-FLS Gaussian MF. Type reducer in IT2-FLS plays an important role in execution of uncertainty problems. Initially, Fuzzifier converts the crisp input into the fuzzy set. Next to that, rules can be determined in the form of linguistic variables by exp",
    "full_text_length": 101814,
    "chunk_length": 1165
  },
  {
    "chunk_id": 2909,
    "paper_filename": "riyadh_2020_early_breast_cancer_detection_via_optimized_cnn.pdf",
    "paper_title": "Riyadh 2020 Early Breast Cancer Detection Via Optimized Cnn",
    "chunk_index": 31,
    "total_chunks": 101,
    "text_content": "random noise R by following Gaussian distribution P_g. The generator module of GAN is shown in Fig. 5. The Dr is used to detect whether the PRPD sample is original or artificial generator using Gr. During the process when Dr fails to predict the dataset then they both are lively balance26. At that instant, Gr can generate more likely patterns like input PRPD sample. The loss function of the two categories is defined in the following equations: Fig. 4 . Edge Detected breast image with cancer of d",
    "full_text_length": 101814,
    "chunk_length": 1288
  },
  {
    "chunk_id": 2910,
    "paper_filename": "riyadh_2020_early_breast_cancer_detection_via_optimized_cnn.pdf",
    "paper_title": "Riyadh 2020 Early Breast Cancer Detection Via Optimized Cnn",
    "chunk_index": 32,
    "total_chunks": 101,
    "text_content": "anticipation function. Pr&Pg are real and generated data distribution in GAN27. The main drawback of traditional GAN is the instability of network during training because of Gr and Dr. During initial condition of training, Dr can predict the false dataset easily because Gr fails to replicate the same features as the input dataset. Thus, LF is conveyed as Eq. ( 4). LFGr=2Jen(Pr||Pg)\u22122log2 (4) . Where 2Jen (Pr||Pg) is a degree of the resemblance between Pr&Pg and it is determined by: 2Jen (Pr||Pg)",
    "full_text_length": 101814,
    "chunk_length": 1303
  },
  {
    "chunk_id": 2911,
    "paper_filename": "riyadh_2020_early_breast_cancer_detection_via_optimized_cnn.pdf",
    "paper_title": "Riyadh 2020 Early Breast Cancer Detection Via Optimized Cnn",
    "chunk_index": 33,
    "total_chunks": 101,
    "text_content": "which causes very large computation time. Discriminator Module of GAN is shown in Fig. 6. Enhanced generative adversarial network To address the issues, improvements can be made through the Enhanced Generative Adversarial Network (E-GAN). Wasserstein Generative Adversarial Network (WGAN)-GP28 has the advantage of stabilized training and in29 conditional sample generation (CGAN). The network training stability is attained by Wasserstein distance which negligible the interaction during training. A",
    "full_text_length": 101814,
    "chunk_length": 1599
  },
  {
    "chunk_id": 2912,
    "paper_filename": "riyadh_2020_early_breast_cancer_detection_via_optimized_cnn.pdf",
    "paper_title": "Riyadh 2020 Early Breast Cancer Detection Via Optimized Cnn",
    "chunk_index": 34,
    "total_chunks": 101,
    "text_content": "The range of \u03b2 exists between 0 and 1. /hatwidey determines the random interruption between the actual and generated dataset. Figures 6 and 7 shows the enhanced WGAN module of generator and discriminator. Thus, in these proposed advantages of WGAN-GP , CGW AN and CNN put together to form a network for data augmentation. The architecture of Gr and Dr is shown in figure. Thus, in this proposed work the updated LFGr&LFDr are characterized as Eqs. ( 8) and ( 9) (Fig. 8). LFGr=EF\u223cy\u223cPg[ fgp(\u223c y|x)] (8",
    "full_text_length": 101814,
    "chunk_length": 1421
  },
  {
    "chunk_id": 2913,
    "paper_filename": "riyadh_2020_early_breast_cancer_detection_via_optimized_cnn.pdf",
    "paper_title": "Riyadh 2020 Early Breast Cancer Detection Via Optimized Cnn",
    "chunk_index": 35,
    "total_chunks": 101,
    "text_content": "square propagation (RMSP) AdaGrad is improved by the adaptive technique known as root mean square prop, or RMSprop. In place of AdaGrad\u2019s \u201ccumulative sum of squared gradients, \u201d it uses the \u201cexponential moving average\u201d . ATO builds on the benefits or positive traits of the previous two techniques to deliver the most significant optimization gradient descent. Even with great precautions taken to prevent potential local minimal difficulties along the route, attempting to combine the advantages of ",
    "full_text_length": 101814,
    "chunk_length": 1262
  },
  {
    "chunk_id": 2914,
    "paper_filename": "riyadh_2020_early_breast_cancer_detection_via_optimized_cnn.pdf",
    "paper_title": "Riyadh 2020 Early Breast Cancer Detection Via Optimized Cnn",
    "chunk_index": 36,
    "total_chunks": 101,
    "text_content": "gets closer to the global minimum. The algorithm used in gradient descent is Adam with the bias-corrected weight parameters ( /hatwidefi) and ( /hatwide\u03b4i), shown in Eqs. ( 11) and ( 12). /hatwidefi=fi (1\u2212\u03b1i1) (11) Fig. 8 . Generator Module of Enhanced WGAN. Fig. 7 . Discriminator Module of Enhanced WGAN. Scientific Reports | (2025) 15:25290 9 | https://doi.org/10.1038/s41598-025-11218-0www.nature.com/scientificreports/ /hatwide\u03b4i=\u03b4i (1\u2212\u03b1i2) (12) wi+1=wi\u2212/hatwidefi/parenleftBigg \u03c3/radicalbig /ha",
    "full_text_length": 101814,
    "chunk_length": 1428
  },
  {
    "chunk_id": 2915,
    "paper_filename": "riyadh_2020_early_breast_cancer_detection_via_optimized_cnn.pdf",
    "paper_title": "Riyadh 2020 Early Breast Cancer Detection Via Optimized Cnn",
    "chunk_index": 37,
    "total_chunks": 101,
    "text_content": "Optimization (EPSO) algorithm is introduced in this research to optimize the hyperparameters of the Convolutional Neural Network (CNN) used for breast cancer detection from thermographic images. EPSO is a modification of the traditional Particle Swarm Optimization (PSO) algorithm, designed to improve convergence speed and avoid local optima, which are common challenges in training deep learning models. Overview of the EPSO Algorithm : EPSO operates by simulating a swarm of particles that \u201cfly\u201d t",
    "full_text_length": 101814,
    "chunk_length": 1389
  },
  {
    "chunk_id": 2916,
    "paper_filename": "riyadh_2020_early_breast_cancer_detection_via_optimized_cnn.pdf",
    "paper_title": "Riyadh 2020 Early Breast Cancer Detection Via Optimized Cnn",
    "chunk_index": 38,
    "total_chunks": 101,
    "text_content": "to the number of particles in the swarm, which determines the diversity of potential solutions explored. In our implementation, a swarm size of 30 particles was chosen. This value was selected after testing several sizes, balancing computational efficiency and exploration of the hyperparameter space. A smaller swarm size would limit the search capacity, while a larger swarm size could lead to excessive computational demands without significant performance gains. \u2022 Inertia Weight : The inertia we",
    "full_text_length": 101814,
    "chunk_length": 1296
  },
  {
    "chunk_id": 2917,
    "paper_filename": "riyadh_2020_early_breast_cancer_detection_via_optimized_cnn.pdf",
    "paper_title": "Riyadh 2020 Early Breast Cancer Detection Via Optimized Cnn",
    "chunk_index": 39,
    "total_chunks": 101,
    "text_content": "its own best-known position, while the social component (c2) determines the influence of the best-known position in the swarm. In our approach, c1 = 1.5 and c2 = 1.5 were chosen, which have been empirically found to strike a balance between individual exploration and collective learning. Higher values of c1 and c2 tend to encourage faster convergence but may risk overshooting optimal solutions. \u2022 Velocity Clamping : To prevent the particles from moving too quickly and missing optimal regions in ",
    "full_text_length": 101814,
    "chunk_length": 1292
  },
  {
    "chunk_id": 2918,
    "paper_filename": "riyadh_2020_early_breast_cancer_detection_via_optimized_cnn.pdf",
    "paper_title": "Riyadh 2020 Early Breast Cancer Detection Via Optimized Cnn",
    "chunk_index": 40,
    "total_chunks": 101,
    "text_content": "between 0.0001 and 0.1, number of layers between 3 and 10). These boundaries were determined based on prior research and experimentation, ensuring that the search space is both mean - ingful and computationally feasible. The parameters chosen for the EPSO algorithm were based on both theoretical considerations and empirical tuning. The swarm size of 30 is a typical value that balances the ability to explore the hyperparameter space with computational efficiency, as larger swarms significantly in",
    "full_text_length": 101814,
    "chunk_length": 1432
  },
  {
    "chunk_id": 2919,
    "paper_filename": "riyadh_2020_early_breast_cancer_detection_via_optimized_cnn.pdf",
    "paper_title": "Riyadh 2020 Early Breast Cancer Detection Via Optimized Cnn",
    "chunk_index": 41,
    "total_chunks": 101,
    "text_content": "widely used for hyperparameter optimization in deep learning, its traditional approach is often prone to premature convergence, especially in high-dimensional spaces like CNN hyperparameter tuning. EPSO addresses this issue by incorporating adaptive velocity adjustments and enhanced inertia weight strategies , which promote better exploration of the search space and avoid getting stuck in local minimum. This results in more robust and accurate hyperparameter optimization. Eberhart and Kennedy [1",
    "full_text_length": 101814,
    "chunk_length": 1489
  },
  {
    "chunk_id": 2920,
    "paper_filename": "riyadh_2020_early_breast_cancer_detection_via_optimized_cnn.pdf",
    "paper_title": "Riyadh 2020 Early Breast Cancer Detection Via Optimized Cnn",
    "chunk_index": 42,
    "total_chunks": 101,
    "text_content": ". ,P Bestaxn ). Meanwhile, the best position attained by the swarm is determined by GBestax =(GBestax 1,GBestax2 ,... . ,G Bestaxn ). During the process, computation of velocity in every particle is pretentious by three various factors in upcoming iteration: weight w\u00d7vxy(i), perceptive component m1\u00d7n1\u00d7(PBestxy\u2212Paxy(i)) and community component m2\u00d7n2\u00d7(GBestxy\u2212Paxy(i)). vxy(i+ 1) = w\u00d7vxy(i)+ m1\u00d7n1\u00d7(PBestxy\u2212Paxy(i)) + m2\u00d7n2\u00d7(GBestxy\u2212Paxy(i)) (14) vxy(i+ 1) = vxy(i)+ vxy(i+ 1) (15) . where Pxy determ",
    "full_text_length": 101814,
    "chunk_length": 1347
  },
  {
    "chunk_id": 2921,
    "paper_filename": "riyadh_2020_early_breast_cancer_detection_via_optimized_cnn.pdf",
    "paper_title": "Riyadh 2020 Early Breast Cancer Detection Via Optimized Cnn",
    "chunk_index": 43,
    "total_chunks": 101,
    "text_content": "9 demonstrates the flow chart of EPSO. Algorithm of the proposed enhanced particle swarm optimization method \u2022 Input: Initialize the Input parameters Popnsze, niter, T rain epo and T rain Dwith the help of Table 1 swarm is initialized based on random combination of hyperparameters to be tuned. \u2022 Compute f(x) for every Pax based on the CNN model training. Make ready GBest=Pa1 using GBest(i+ 1) = PBestamaxiarg(f(PBesta (i+ 1) )) where a limits to the number of particles in the pro - posed algorith",
    "full_text_length": 101814,
    "chunk_length": 1227
  },
  {
    "chunk_id": 2922,
    "paper_filename": "riyadh_2020_early_breast_cancer_detection_via_optimized_cnn.pdf",
    "paper_title": "Riyadh 2020 Early Breast Cancer Detection Via Optimized Cnn",
    "chunk_index": 44,
    "total_chunks": 101,
    "text_content": "x agent and y dimension, m1&m 2 independent random numbers and n1&n2 are constants on perception and social hastening. Fig. 9 . Flowchart of Enhanced PSO Algorithm. Scientific Reports | (2025) 15:25290 11 | https://doi.org/10.1038/s41598-025-11218-0www.nature.com/scientificreports/ \u2022 Compute f(x) for every Pax \u2022 If f(Pa x.PBest )\u2264f(Pa x)then Pax.PBest=Pax \u2022 Iff(GBest )\u2264f(Pa x.PBest )then Pax.PBest=GBest \u2022 Finalize GBest and accordingly modify the CNN architecture as optimized hyperparameter. Tra",
    "full_text_length": 101814,
    "chunk_length": 1393
  },
  {
    "chunk_id": 2923,
    "paper_filename": "riyadh_2020_early_breast_cancer_detection_via_optimized_cnn.pdf",
    "paper_title": "Riyadh 2020 Early Breast Cancer Detection Via Optimized Cnn",
    "chunk_index": 45,
    "total_chunks": 101,
    "text_content": "is shown in equation below in Eqs. ( 16), Paxy(i+ 1) = m1\u00d7n1\u00d7PBestxy (i)+m2\u00d7n2\u00d7GBestxy (i) (16) . During the search process the value of perception and social hastening adjust stability among the stages of modification and strengthening. According to the proposed algorithm, every particle in each iteration movies between the PBestax and GBestax values. Thus, a new mechanism is introduced to compute the following position of particles which aims to acquire a best solution with a smaller number of",
    "full_text_length": 101814,
    "chunk_length": 1048
  },
  {
    "chunk_id": 2924,
    "paper_filename": "riyadh_2020_early_breast_cancer_detection_via_optimized_cnn.pdf",
    "paper_title": "Riyadh 2020 Early Breast Cancer Detection Via Optimized Cnn",
    "chunk_index": 46,
    "total_chunks": 101,
    "text_content": "90 90 89 88 81 86 88 89 90 89 90 88.2 MBO-CNN 87 89 88 88 89 88 87 88 89 89 88 88.2 BO-CNN 85 84 85 88 87 88 86 87 88 85 85 86.2 GA-CNN 80 87 81 88 78 80 82 84 85 85 80 82.7 SVM-Poly 78 79 82 81 83 80 83 81 83 81 83 81.3 SVM-RBF 77 78 75 77 78 78 72 72 80 75 73 75.9 Auxiliary Generative Adversarial Network Proposed Work 96 93 93 96 96 92 95 96 95 93 95 94.5 PSO-CNN 93 92 93 92 92 91 91 92 94 95 95 92.7 IntelliSwAS-CNN 90 92 91 90 91 92 92 90 96 95 92 91.9 MBO-CNN 90 88 87 87 86 85 88 87 90 92 89",
    "full_text_length": 101814,
    "chunk_length": 770
  },
  {
    "chunk_id": 2925,
    "paper_filename": "riyadh_2020_early_breast_cancer_detection_via_optimized_cnn.pdf",
    "paper_title": "Riyadh 2020 Early Breast Cancer Detection Via Optimized Cnn",
    "chunk_index": 47,
    "total_chunks": 101,
    "text_content": "85 85 86 82 81 83 82 82 81 82.8 SVM-Poly 87 82 81 80 80 80 82 82 80 82 81 81.5 SVM-RBF 78 77 78 79 80 82 81 82 81 82 81 80.1 Conditional Generative Adversarial Network Proposed Work 97 98 97 96 95 100 96 98 97 96 99 97.2 PSO-CNN 95 96 97 94 94 93 95 93 93 96 97 94.8 IntelliSwAS-CNN 92 92 92 97 90 94 93 93 93 92 92 92.7 MBO-CNN 91 89 89 88 87 89 90 90 93 90 91 89.7 BO-CNN 92 92 87 88 85 85 84 83 87 88 89 87.3 GA-CNN 85 87 89 84 81 83 85 87 89 82 84 85.1 SVM-Poly 86 86 88 81 87 87 86 86 84 76 83 8",
    "full_text_length": 101814,
    "chunk_length": 791
  },
  {
    "chunk_id": 2926,
    "paper_filename": "riyadh_2020_early_breast_cancer_detection_via_optimized_cnn.pdf",
    "paper_title": "Riyadh 2020 Early Breast Cancer Detection Via Optimized Cnn",
    "chunk_index": 48,
    "total_chunks": 101,
    "text_content": "with Gradient Penalty (WGAN-GP) Proposed Work 99 98 99 100 100 99 98 98 98 99 99 98.8 PSO-CNN 98 95 95 96 98 95 100 94 98 94 99 96.5 IntelliSwAS-CNN 93 93 91 92 91 93 94 95 95 96 96 93.5 MBO-CNN 93 92 98 98 91 89 92 92 93 90 91 92.6 BO-CNN 90 89 88 91 89 87 88 89 87 88 89 88.6 GA-CNN 89 88 87 88 89 90 87 88 87 88 87 88.0 SVM-Poly 85 88 89 88 81 82 84 83 88 87 86 85.5 SVM-RBF 90 91 85 88 89 88 89 91 90 85 82 88.0 Table 1 . Recognition rate for type 2 fuzzy edge detected images. Scientific Reports",
    "full_text_length": 101814,
    "chunk_length": 1078
  },
  {
    "chunk_id": 2927,
    "paper_filename": "riyadh_2020_early_breast_cancer_detection_via_optimized_cnn.pdf",
    "paper_title": "Riyadh 2020 Early Breast Cancer Detection Via Optimized Cnn",
    "chunk_index": 49,
    "total_chunks": 101,
    "text_content": "Figure. The flowchart of epsoCNN consists of three categories. In the first category, CNN structure, dataset both training and testing and algorithm parameters are formed. In second category, initialization of population size of the particles and updating particle position both PBestax and GBestax respectively. Until the maximum number of iterations is completed, the above process is repeated for better training of CNN architecture with respect to the input dataset. Finally, once the number of i",
    "full_text_length": 101814,
    "chunk_length": 1377
  },
  {
    "chunk_id": 2928,
    "paper_filename": "riyadh_2020_early_breast_cancer_detection_via_optimized_cnn.pdf",
    "paper_title": "Riyadh 2020 Early Breast Cancer Detection Via Optimized Cnn",
    "chunk_index": 50,
    "total_chunks": 101,
    "text_content": "challenging. To assess the effectiveness of the proposed method, the quality of the generated dataset is compared with various generative models. The average Structural Resemblance Index Degree (SRID) between actual and generated patterns is calculated and shown in Tables 1 and 2. During the evaluation, the number of iterations is kept constant across all methods. From the results in Table 1, it is evident that the SRID of the proposed algorithm is comparatively higher than other GAN models. Gen",
    "full_text_length": 101814,
    "chunk_length": 982
  },
  {
    "chunk_id": 2929,
    "paper_filename": "riyadh_2020_early_breast_cancer_detection_via_optimized_cnn.pdf",
    "paper_title": "Riyadh 2020 Early Breast Cancer Detection Via Optimized Cnn",
    "chunk_index": 51,
    "total_chunks": 101,
    "text_content": "86 89 81 84.5 BO-CNN 87 82 81 88 86 85 84 83 82 81 80 83.5 GA-CNN 87 86 85 84 81 78 79 76 74 82 75 80.6 SVM-Poly 75 74 73 72 76 80 81 76 75 74 80 76.0 SVM-RBF 75 74 73 72 78 74 75 73 71 70 72 73.4 Auxiliary Generative Adversarial Network Proposed Work 92 93 94 92 96 91 90 96 93 94 92 93.0 PSO-CNN 91 93 95 92 91 90 90 93 95 92 93 92.3 IntelliSwAS-CNN 89 90 92 91 92 93 90 89 88 85 85 89.5 MBO-CNN 89 88 82 81 84 83 85 81 82 84 83 83.8 BO-CNN 89 90 87 86 85 87 86 82 85 86 81 85.8 GA-CNN 81 82 87 86 ",
    "full_text_length": 101814,
    "chunk_length": 784
  },
  {
    "chunk_id": 2930,
    "paper_filename": "riyadh_2020_early_breast_cancer_detection_via_optimized_cnn.pdf",
    "paper_title": "Riyadh 2020 Early Breast Cancer Detection Via Optimized Cnn",
    "chunk_index": 52,
    "total_chunks": 101,
    "text_content": "78 81.2 SVM-RBF 77 78 79 80 81 82 83 78 77 78 77 79.1 Conditional Generative Adversarial Network Proposed Work 95 93 96 95 94 93 98 97 96 95 97 95.4 PSO-CNN 94 93 92 95 93 96 94 94 96 95 93 94.1 IntelliSwAS-CNN 90 89 93 96 91 93 95 92 90 93 93 92.3 MBO-CNN 91 89 89 88 87 89 90 90 93 90 91 89.7 BO-CNN 89 90 92 88 87 86 85 84 88 83 82 86.7 GA-CNN 88 81 85 86 81 84 82 82 81 81 84 83.2 SVM-Poly 85 84 87 86 85 82 81 83 81 82 78 83.1 SVM-RBF 84 83 85 82 81 80 81 78 79 80 81 81.3 Proposed Work - Wasser",
    "full_text_length": 101814,
    "chunk_length": 791
  },
  {
    "chunk_id": 2931,
    "paper_filename": "riyadh_2020_early_breast_cancer_detection_via_optimized_cnn.pdf",
    "paper_title": "Riyadh 2020 Early Breast Cancer Detection Via Optimized Cnn",
    "chunk_index": 53,
    "total_chunks": 101,
    "text_content": "94 94 93 96 99 98 97 96 96 95.7 IntelliSwAS-CNN 93 92 91 90 90 92 89 93 93 92 91 91.5 MBO-CNN 89 88 87 86 88 89 90 91 92 90 92 89.3 BO-CNN 89 88 87 85 85 83 88 87 86 85 84 86.1 GA-CNN 85 84 83 82 81 88 85 84 83 88 85 84.4 SVM-Poly 82 81 80 85 80 83 85 82 87 86 85 83.3 SVM-RBF 89 85 84 83 82 80 81 85 92 86 84 84.6 Table 2 . Recognition rate for canny edge detected images. Scientific Reports | (2025) 15:25290 13 | https://doi.org/10.1038/s41598-025-11218-0www.nature.com/scientificreports/ In the p",
    "full_text_length": 101814,
    "chunk_length": 1110
  },
  {
    "chunk_id": 2932,
    "paper_filename": "riyadh_2020_early_breast_cancer_detection_via_optimized_cnn.pdf",
    "paper_title": "Riyadh 2020 Early Breast Cancer Detection Via Optimized Cnn",
    "chunk_index": 54,
    "total_chunks": 101,
    "text_content": "condition. To measure the performance of the proposed algorithm, K-fold cross-validation is employed. This method, due to its low variance, outperforms other validation techniques in terms of recognition rate. The dataset is divided into k groups, and training is repeated k times, with one group serving as test data and the remaining groups as training data in each iteration. The average recognition rate across all k trials is then calculated. In this study, 15-fold cross-validation was used. Al",
    "full_text_length": 101814,
    "chunk_length": 1299
  },
  {
    "chunk_id": 2933,
    "paper_filename": "riyadh_2020_early_breast_cancer_detection_via_optimized_cnn.pdf",
    "paper_title": "Riyadh 2020 Early Breast Cancer Detection Via Optimized Cnn",
    "chunk_index": 55,
    "total_chunks": 101,
    "text_content": "learning techniques with Type 2 fuzzy edge-detected images. The PSO-CNN approach, after edge detection, demonstrated a 96.5% recognition rate, while MBO-CNN yielded 93.5% accuracy when applied to edge-detected PRPD patterns. A plot of PSO iteration and Gbest training accuracy for different runs is presented in Fig. 10. It is also observed that large datasets are not well-suited for the SVM algorithm. SVM tends to perform poorly when there is significant noise in the dataset, particularly when ta",
    "full_text_length": 101814,
    "chunk_length": 1316
  },
  {
    "chunk_id": 2934,
    "paper_filename": "riyadh_2020_early_breast_cancer_detection_via_optimized_cnn.pdf",
    "paper_title": "Riyadh 2020 Early Breast Cancer Detection Via Optimized Cnn",
    "chunk_index": 56,
    "total_chunks": 101,
    "text_content": "model can be created utilizing edge detected patterns. On the other hand, the recognition rate of all algorithms decreases when the suggested method handles tainted patterns. Significance of hyperparameter optimization In the proposed study, we examine the effectiveness of Particle Swarm Optimization (PSO) for hyperparameter tuning in the detection and classification of breast cancer, specifically focusing on the recognition of 10 different abnormal conditions. To assess the significance of opti",
    "full_text_length": 101814,
    "chunk_length": 1421
  },
  {
    "chunk_id": 2935,
    "paper_filename": "riyadh_2020_early_breast_cancer_detection_via_optimized_cnn.pdf",
    "paper_title": "Riyadh 2020 Early Breast Cancer Detection Via Optimized Cnn",
    "chunk_index": 57,
    "total_chunks": 101,
    "text_content": "iteration for different run vs. Training Accuracy. Scientific Reports | (2025) 15:25290 14 | https://doi.org/10.1038/s41598-025-11218-0www.nature.com/scientificreports/ In the research work, the default values of these hyperparameters (before optimization) and the optimized values obtained by PSO after running the algorithm are given below. The default values for the hyperparameters before optimization (i.e., the starting point for the PSO). \u2022 Default learning rate: 0.01. \u2022 Default momentum: 0.9",
    "full_text_length": 101814,
    "chunk_length": 1417
  },
  {
    "chunk_id": 2936,
    "paper_filename": "riyadh_2020_early_breast_cancer_detection_via_optimized_cnn.pdf",
    "paper_title": "Riyadh 2020 Early Breast Cancer Detection Via Optimized Cnn",
    "chunk_index": 58,
    "total_chunks": 101,
    "text_content": "learning model is trained using standard default settings. The hyperparameters (such as learning rate, batch size, and number of layers) are set based on commonly used values in similar studies. The performance metrics for this baseline model were as follows: \u2022 Recognition Rate (RR): 89.5%. \u2022 Accuracy: 87.2%. \u2022 Precision: 85.1%. \u2022 Recall: 88.3%. \u2022 F1-Score: 86.7%. These results were achieved using a standard 15-fold cross-validation approach, where the dataset was divided into 15 subsets, and ea",
    "full_text_length": 101814,
    "chunk_length": 1355
  },
  {
    "chunk_id": 2937,
    "paper_filename": "riyadh_2020_early_breast_cancer_detection_via_optimized_cnn.pdf",
    "paper_title": "Riyadh 2020 Early Breast Cancer Detection Via Optimized Cnn",
    "chunk_index": 59,
    "total_chunks": 101,
    "text_content": "based on the training data. The results after optimization were as follows: \u2022 Recognition Rate (RR): 98.8%. \u2022 Accuracy: 97.5%. \u2022 Precision: 96.8%. \u2022 Recall: 98.4%. \u2022 F1-Score: 97.6%. This represents a substantial improvement in model performance compared to the baseline, particularly in terms of recognition rate (a 9.3% increase) and F1-Score (a 10.9% increase). The optimized model consistently outperformed the baseline model, with higher accuracy and reduced false positives/negatives. Statistic",
    "full_text_length": 101814,
    "chunk_length": 1511
  },
  {
    "chunk_id": 2938,
    "paper_filename": "riyadh_2020_early_breast_cancer_detection_via_optimized_cnn.pdf",
    "paper_title": "Riyadh 2020 Early Breast Cancer Detection Via Optimized Cnn",
    "chunk_index": 60,
    "total_chunks": 101,
    "text_content": "that the optimization process significantly enhanced the model\u2019s ability to correctly classify breast cancer cases, reinforcing the importance of hyperparameter tuning in achieving high-performance classification. Scientific Reports | (2025) 15:25290 15 | https://doi.org/10.1038/s41598-025-11218-0www.nature.com/scientificreports/ Comparison of proposed model performance with other models This paper introduces a novel method for optimizing the hyperparameters of CNNs used for breast cancer detect",
    "full_text_length": 101814,
    "chunk_length": 1484
  },
  {
    "chunk_id": 2939,
    "paper_filename": "riyadh_2020_early_breast_cancer_detection_via_optimized_cnn.pdf",
    "paper_title": "Riyadh 2020 Early Breast Cancer Detection Via Optimized Cnn",
    "chunk_index": 61,
    "total_chunks": 101,
    "text_content": "proposed algorithm is 11% higher than the best-performing GAN model, illustrating the superior quality of the synthetic data generated by our method. To measure classification performance, we employed 15-fold cross-validation, a method known for minimizing variance and providing reliable results. The recognition rate (RR) achieved by the proposed algorithm was 98.8%, outperforming all baseline models. The results in Table 5 show that the proposed method outperforms standard CNNs and other advanc",
    "full_text_length": 101814,
    "chunk_length": 1418
  },
  {
    "chunk_id": 2940,
    "paper_filename": "riyadh_2020_early_breast_cancer_detection_via_optimized_cnn.pdf",
    "paper_title": "Riyadh 2020 Early Breast Cancer Detection Via Optimized Cnn",
    "chunk_index": 62,
    "total_chunks": 101,
    "text_content": "classification accuracy for most breast cancer types, with minimal misclassifications. Notably, classes such as F4 and F5 achieved perfect recognition (100% accuracy), while others like F2, F7, F8, and F9 showed minor confusion with neighboring classes, typically misclassifying 1\u20132 samples. The overall high accuracy across all classes reflects the robustness of the feature extraction and classification framework. The normalized row and column summaries further confirm the model\u2019s strong generali",
    "full_text_length": 101814,
    "chunk_length": 1459
  },
  {
    "chunk_id": 2941,
    "paper_filename": "riyadh_2020_early_breast_cancer_detection_via_optimized_cnn.pdf",
    "paper_title": "Riyadh 2020 Early Breast Cancer Detection Via Optimized Cnn",
    "chunk_index": 63,
    "total_chunks": 101,
    "text_content": "of the Convolutional Neural Network (CNN) model with Particle Swarm Optimization (PSO) for the early diagnosis of breast cancer using infrared thermography. The results indicate near-perfect classification performance, with the Area Under the Curve (AUC) reaching a value of 1.00 in all categories. This suggests that the model exhibits outstanding levels of sensitivity and specificity, accurately distinguishing among different states of breast tissue with minimal instances of false positives. The",
    "full_text_length": 101814,
    "chunk_length": 1388
  },
  {
    "chunk_id": 2942,
    "paper_filename": "riyadh_2020_early_breast_cancer_detection_via_optimized_cnn.pdf",
    "paper_title": "Riyadh 2020 Early Breast Cancer Detection Via Optimized Cnn",
    "chunk_index": 64,
    "total_chunks": 101,
    "text_content": "the ROC analysis supports the effectiveness of the CNN-PSO model in the early diagnosis of breast cancer, demonstrating its compelling implications for clinical use in thermographic image analysis. Dataset augmentation The dataset, initially containing 50 samples per condition, was augmented to 500 samples per condition using the Wasserstein GAN (WGAN) module. The comparison between the original and augmented datasets further Model Recognition Rate (RR) Proposed Method (EPSO-CNN) 98.8% Standard ",
    "full_text_length": 101814,
    "chunk_length": 1474
  },
  {
    "chunk_id": 2943,
    "paper_filename": "riyadh_2020_early_breast_cancer_detection_via_optimized_cnn.pdf",
    "paper_title": "Riyadh 2020 Early Breast Cancer Detection Via Optimized Cnn",
    "chunk_index": 65,
    "total_chunks": 101,
    "text_content": "both SRID and recognition rate. Computational considerations While increasing the number of k-fold rounds (i.e., expanding cross-validation) can reduce fluctuation in results, it also increases computational time. The 15-fold cross-validation used in this study strikes a balance between accuracy and computational efficiency, ensuring that our results are both reliable and practical for real-world applications. Discussion The primary goal of the suggested approach is to select the ideal hyperpara",
    "full_text_length": 101814,
    "chunk_length": 1435
  },
  {
    "chunk_id": 2944,
    "paper_filename": "riyadh_2020_early_breast_cancer_detection_via_optimized_cnn.pdf",
    "paper_title": "Riyadh 2020 Early Breast Cancer Detection Via Optimized Cnn",
    "chunk_index": 66,
    "total_chunks": 101,
    "text_content": "The users specify the parameters to be tuned within minimum and maximum range. The search space dimension is the primary determinant of the issues\u2019 performance, computation time, and complexity. A clever method that is frequently applied to a variety of optimization issues is BO. Nevertheless, BO has several shortcomings, such as a set step length, a poor convergence pace, and an inability to leap out of local optima. From the above results, we can clearly understand that the primary advantage o",
    "full_text_length": 101814,
    "chunk_length": 1356
  },
  {
    "chunk_id": 2945,
    "paper_filename": "riyadh_2020_early_breast_cancer_detection_via_optimized_cnn.pdf",
    "paper_title": "Riyadh 2020 Early Breast Cancer Detection Via Optimized Cnn",
    "chunk_index": 67,
    "total_chunks": 101,
    "text_content": "process, while the EPSO CNN optimizes the network\u2019s parameters effectively, ensuring that the model achieves superior recognition performance. To further highlight the advanced nature of the proposed methodology, we have conducted a detailed comparison with other state-of-the-art algorithms suggested by various researchers in the existing literature. This comparison is crucial in demonstrating the competitive edge of our approach in terms of accuracy and efficiency. The corresponding accuracy ra",
    "full_text_length": 101814,
    "chunk_length": 1389
  },
  {
    "chunk_id": 2946,
    "paper_filename": "riyadh_2020_early_breast_cancer_detection_via_optimized_cnn.pdf",
    "paper_title": "Riyadh 2020 Early Breast Cancer Detection Via Optimized Cnn",
    "chunk_index": 68,
    "total_chunks": 101,
    "text_content": "noise, the fuzzy edge detection technique may identify the pattern\u2019s weak edges. Features from the edge-detected input patterns are retrieved using a transfer learning algorithm. Enhanced PSO is used to fine-tune the hyperparameters of the optimized classifier, replacing the physical technique. In [4 & 13] even though they are using CNN for feature extraction and a separate algorithm for classifier they failed to pre-process the image. Since pre-processing of images with appropriate techniques p",
    "full_text_length": 101814,
    "chunk_length": 1471
  },
  {
    "chunk_id": 2947,
    "paper_filename": "riyadh_2020_early_breast_cancer_detection_via_optimized_cnn.pdf",
    "paper_title": "Riyadh 2020 Early Breast Cancer Detection Via Optimized Cnn",
    "chunk_index": 69,
    "total_chunks": 101,
    "text_content": "Optimizing deep learning architecture increases training time and demands significant memory storage. The primary future work involves extending the proposed algorithm to support both semi-supervised and unsupervised learning methods. This would allow the model to handle labeled and unlabeled data more effectively, increasing its versatility. By incorporating semi-supervised learning, the model can leverage a small amount of labeled data alongside a larger pool of unlabeled data for training. Un",
    "full_text_length": 101814,
    "chunk_length": 1425
  },
  {
    "chunk_id": 2948,
    "paper_filename": "riyadh_2020_early_breast_cancer_detection_via_optimized_cnn.pdf",
    "paper_title": "Riyadh 2020 Early Breast Cancer Detection Via Optimized Cnn",
    "chunk_index": 70,
    "total_chunks": 101,
    "text_content": "breast cancer detection using thermography and deep learning have made significant contributions, with a variety of approaches and techniques employed. To contextualize our findings, we compare our results with several recent studies that utilized convolutional neural networks (CNNs) and other machine learning techniques for breast cancer detection in Table 8. The following are key studies relevant to our work:Model Recognition Rate (RR) Precision Recall F1-Score Computation Time Proposed CNN + ",
    "full_text_length": 101814,
    "chunk_length": 1469
  },
  {
    "chunk_id": 2949,
    "paper_filename": "riyadh_2020_early_breast_cancer_detection_via_optimized_cnn.pdf",
    "paper_title": "Riyadh 2020 Early Breast Cancer Detection Via Optimized Cnn",
    "chunk_index": 71,
    "total_chunks": 101,
    "text_content": "Imbalance 256*256 CNN with EPSO 98.8% 214.514 Table 6 . Comparison of proposed model performance with existing models. Scientific Reports | (2025) 15:25290 18 | https://doi.org/10.1038/s41598-025-11218-0www.nature.com/scientificreports/ Zhang et al. (2022) proposed a CNN-based model for breast cancer classification using thermographic images44. They reported an accuracy of 95.6% , which is lower than the 98.8% recognition rate achieved by the proposed method. Their model, however, used a smaller",
    "full_text_length": 101814,
    "chunk_length": 1435
  },
  {
    "chunk_id": 2950,
    "paper_filename": "riyadh_2020_early_breast_cancer_detection_via_optimized_cnn.pdf",
    "paper_title": "Riyadh 2020 Early Breast Cancer Detection Via Optimized Cnn",
    "chunk_index": 72,
    "total_chunks": 101,
    "text_content": "the model\u2019s performance compared to the 10-fold cross-validation used in their study. Rojas et al. (2021) applied a deep CNN model on thermographic breast images for classification, achieving an accuracy of 97.3%45. However, their study did not explore any data augmentation techniques or advanced hyperparameter tuning. Furthermore, they did not use edge detection or advanced image processing methods like Mamdani fuzzy logic or CLAHE for enhancing image quality, which are key elements in the prop",
    "full_text_length": 101814,
    "chunk_length": 1380
  },
  {
    "chunk_id": 2951,
    "paper_filename": "riyadh_2020_early_breast_cancer_detection_via_optimized_cnn.pdf",
    "paper_title": "Riyadh 2020 Early Breast Cancer Detection Via Optimized Cnn",
    "chunk_index": 73,
    "total_chunks": 101,
    "text_content": "images46. Their GAN model was able to generate synthetic breast cancer images to balance the dataset, but they reported an accuracy of 90%. However, their approach did not include optimization of the CNN model\u2019s architecture or hyperparameters, which could have led to a lower performance. In contrast to Bohlouli et al., the proposed method integrates GAN-based data augmentation along with the powerful EPSO algorithm for hyperparameter optimization. This combination resulted in a substantial incr",
    "full_text_length": 101814,
    "chunk_length": 1382
  },
  {
    "chunk_id": 2952,
    "paper_filename": "riyadh_2020_early_breast_cancer_detection_via_optimized_cnn.pdf",
    "paper_title": "Riyadh 2020 Early Breast Cancer Detection Via Optimized Cnn",
    "chunk_index": 74,
    "total_chunks": 101,
    "text_content": "approach leverages a fully automated deep learning model (CNN) with end-to-end learning. The major difference between Kumar et al., work and the proposed is the choice of classifier. While SVMs are effective, they do not perform as well as CNNs for complex image classification tasks47. The proposed CNN-based model outperforms SVM in terms of accuracy, and by incorporating EPSO and GAN-based data augmentation, which reduces the need for manual feature engineering and improve model performance. De",
    "full_text_length": 101814,
    "chunk_length": 1439
  },
  {
    "chunk_id": 2953,
    "paper_filename": "riyadh_2020_early_breast_cancer_detection_via_optimized_cnn.pdf",
    "paper_title": "Riyadh 2020 Early Breast Cancer Detection Via Optimized Cnn",
    "chunk_index": 75,
    "total_chunks": 101,
    "text_content": "in this research. Hyperparameter optimization is needed to enhance the performance of deep learning models. Karaman et al. (2022) proved that artificial bee colony (ABC) optimization improves colorectal cancer (CRC) polyp detection significantly, enhancing accuracy and real-time performance48. Ahmet et al. (2024) also improved upon hyperparameter tuning by incorporating ABC into YOLO-based models to provide enhanced robustness in clinical environments49. Likewise, EPSO tunes CNN hyperparameters ",
    "full_text_length": 101814,
    "chunk_length": 1515
  },
  {
    "chunk_id": 2954,
    "paper_filename": "riyadh_2020_early_breast_cancer_detection_via_optimized_cnn.pdf",
    "paper_title": "Riyadh 2020 Early Breast Cancer Detection Via Optimized Cnn",
    "chunk_index": 76,
    "total_chunks": 101,
    "text_content": "median filtering for noise reduction enhance the quality of the image and feature extraction in the field of thermographic imaging, resulting in better classification accuracy. Technological advancements in skin cancer detection have utilized new architectures. Burhanettin and Ishak (2025) proposed a ConvNeXtV2 model with focal self-attention mechanisms, which provided better classification Study Methodology Accuracy (%) Key Differences Zhang et al. (2022) CNN with thermographic images 95.6 No h",
    "full_text_length": 101814,
    "chunk_length": 1590
  },
  {
    "chunk_id": 2955,
    "paper_filename": "riyadh_2020_early_breast_cancer_detection_via_optimized_cnn.pdf",
    "paper_title": "Riyadh 2020 Early Breast Cancer Detection Via Optimized Cnn",
    "chunk_index": 77,
    "total_chunks": 101,
    "text_content": "of proposed model performance with other models. Scientific Reports | (2025) 15:25290 19 | https://doi.org/10.1038/s41598-025-11218-0www.nature.com/scientificreports/ accuracy51. Their research highlights the role of attention mechanisms in enhancing feature representation and interpretability. Attention-based methods improve feature learning, whereas EPSO-driven CNN optimization optimizes the training process, leading to faster convergence and greater efficiency in thermographic breast cancer c",
    "full_text_length": 101814,
    "chunk_length": 1594
  },
  {
    "chunk_id": 2956,
    "paper_filename": "riyadh_2020_early_breast_cancer_detection_via_optimized_cnn.pdf",
    "paper_title": "Riyadh 2020 Early Breast Cancer Detection Via Optimized Cnn",
    "chunk_index": 78,
    "total_chunks": 101,
    "text_content": "a combination of hyperparameter optimization, sophisticated image processing methods, and optimized CNN-based classification, the proposed method opens the scale of AI-enabled diagnostics, corroborating the applicability of deep learning in furthering breast cancer detection using thermographic imaging. While the results of this study are encouraging, several directions for future research could further enhance the proposed methodology. One important area is the exploration of multimodal approac",
    "full_text_length": 101814,
    "chunk_length": 1533
  },
  {
    "chunk_id": 2957,
    "paper_filename": "riyadh_2020_early_breast_cancer_detection_via_optimized_cnn.pdf",
    "paper_title": "Riyadh 2020 Early Breast Cancer Detection Via Optimized Cnn",
    "chunk_index": 79,
    "total_chunks": 101,
    "text_content": "SHAP values, could provide clinicians with insights into the model\u2019s decision-making process, fostering greater trust and clinical acceptance. Future work should also focus on optimizing the model for real-time performance in clinical environments, potentially through model compression or deployment via edge computing, to enable faster inference without sacrificing accuracy. Lastly, longitudinal studies that monitor the system\u2019s performance over time in real-world settings would offer valuable e",
    "full_text_length": 101814,
    "chunk_length": 1514
  },
  {
    "chunk_id": 2958,
    "paper_filename": "riyadh_2020_early_breast_cancer_detection_via_optimized_cnn.pdf",
    "paper_title": "Riyadh 2020 Early Breast Cancer Detection Via Optimized Cnn",
    "chunk_index": 80,
    "total_chunks": 101,
    "text_content": "and efficiency of cancer detection systems. The proposed method shows great promise for improving the accuracy and speed of BC detection in clinical settings. The ability to automatically classify thermographic images with such high accuracy can aid radiologists and healthcare professionals in making faster, more reliable diagnoses. Given that the system is non-invasive, cost-effective, and easy to deploy, it could serve as an important tool for early detection, particularly in resource- limited",
    "full_text_length": 101814,
    "chunk_length": 1415
  },
  {
    "chunk_id": 2959,
    "paper_filename": "riyadh_2020_early_breast_cancer_detection_via_optimized_cnn.pdf",
    "paper_title": "Riyadh 2020 Early Breast Cancer Detection Via Optimized Cnn",
    "chunk_index": 81,
    "total_chunks": 101,
    "text_content": "integrated into clinical workflows to assist radiologists in diagnosing breast cancer using thermographic images. This integration could serve as an alternative to conventional screening techniques, particularly in regions with limited access to mammography or ultrasound facilities. Additionally, due to its high classification accuracy, the system could be a valuable asset in early detection programs, helping to identify breast cancer at its initial stages and thereby enhancing treatment outcome",
    "full_text_length": 101814,
    "chunk_length": 1236
  },
  {
    "chunk_id": 2960,
    "paper_filename": "riyadh_2020_early_breast_cancer_detection_via_optimized_cnn.pdf",
    "paper_title": "Riyadh 2020 Early Breast Cancer Detection Via Optimized Cnn",
    "chunk_index": 82,
    "total_chunks": 101,
    "text_content": "\u2013 15 August 2024). The data is publicly available at h t t p s : / / v i s u a l . i c . u ff . b r / d m i / p r o n t u a r i o / h o m e . p h p . Dataset is published in Silva, L. F.; Saade, D. C. M.; Sequeiros, G. O.; Silva, A. C.; Paiva, A. C.; Bravo, R. S.; Conci, A., A New Database for Breast Research with Infrared Image, Journal of Medical Imaging and Health Informatics, Volume 4, Number 1, March 2014, pp. 92-100(9). Received: 29 October 2024; Accepted: 8 July 2025 References 1. Centers",
    "full_text_length": 101814,
    "chunk_length": 1115
  },
  {
    "chunk_id": 2961,
    "paper_filename": "riyadh_2020_early_breast_cancer_detection_via_optimized_cnn.pdf",
    "paper_title": "Riyadh 2020 Early Breast Cancer Detection Via Optimized Cnn",
    "chunk_index": 83,
    "total_chunks": 101,
    "text_content": "& Ergenc Vertical Pattern Mining Algorithm for Multiple Support Thresholds, Procedia Computer Science, Volume 112, Pages 417\u2013426, ISSN 1877 \u2013 0509, (2017). https://doi.org/10 .1016/j.procs.2017.0 8.051 3. Spak, D. A., Plaxco, J. S., Santiago, L., Dryden, M. J. & Dogan, B. E. BI-RADS fifth edition: A summary of changes. DiagnInterv Imaging . 98 (3), 179\u2013190. https://doi. org/10.1016/j.diii.2 017.01.001 (2017). Epub 2017 Jan 25. PMID: 28131457. 4. Hiba Asri, H., Mousannif, H. A., Moatassime, T. & ",
    "full_text_length": 101814,
    "chunk_length": 1171
  },
  {
    "chunk_id": 2962,
    "paper_filename": "riyadh_2020_early_breast_cancer_detection_via_optimized_cnn.pdf",
    "paper_title": "Riyadh 2020 Early Breast Cancer Detection Via Optimized Cnn",
    "chunk_index": 84,
    "total_chunks": 101,
    "text_content": "W . L. Artificial intelligence for clinical oncology. Cancer Cell. 39 (7), 916\u2013927. h t t p s : / / d o i . o r g / 1 0 . 1 0 1 6 / j . c c e l l . 2 0 2 1 . 0 4 . 0 0 2 (2021). Epub 2021 Apr 29. PMID: 33930310; PMCID: PMC8282694. 7. Smith, J. et al. Advances in multimodal data integration for breast Cancer diagnosis. Nat. Cancer Reviews . 30 (1), 1\u201315. h t t p s : / / d o i . o r g / 1 0 . 1 0 3 8 / s 4 1 5 6 8 - 0 2 3 - 0 0 6 2 3 - 9 (2024). 8. Iqbal, M. S., Ahmad, W ., Alizadehsani, R., Hussa",
    "full_text_length": 101814,
    "chunk_length": 870
  },
  {
    "chunk_id": 2963,
    "paper_filename": "riyadh_2020_early_breast_cancer_detection_via_optimized_cnn.pdf",
    "paper_title": "Riyadh 2020 Early Breast Cancer Detection Via Optimized Cnn",
    "chunk_index": 85,
    "total_chunks": 101,
    "text_content": "deep learning. Healthc. (Basel) . 10 (12), 2395. https://do i.org/10.3390/health care10122395 (2022). PMID: 36553919; PMCID: PMC9778593. 9. Sarker, I. H. Machine learning: algorithms, Real-World applications and research directions. SN Comput. Sci. 2 (3), 160. h t t p s : / / d o i . o r g / 1 0 . 1 0 0 7 / s 4 2 9 7 9 - 0 2 1 - 0 0 5 9 2 - x (2021). Epub 2021 Mar 22. PMID: 33778771; PMCID: PMC7983091. 10. Kabiraj, S. et al. Breast Cancer Risk Prediction using XGBoost and Random Forest Algorithm",
    "full_text_length": 101814,
    "chunk_length": 994
  },
  {
    "chunk_id": 2964,
    "paper_filename": "riyadh_2020_early_breast_cancer_detection_via_optimized_cnn.pdf",
    "paper_title": "Riyadh 2020 Early Breast Cancer Detection Via Optimized Cnn",
    "chunk_index": 86,
    "total_chunks": 101,
    "text_content": "9 . 2 0 2 0 . 9 2 2 5 4 5 1 11. Li, Q. et al. XGBoost-based and tumor-immune characterized gene signature for the prediction of metastatic status in breast cancer. J. Transl Med. 20 (1), 177. https://doi.org/ 10.1186/s12967-022-0 3369-9 (2022). PMID: 35436939; PMCID: PMC9014628. 12. Tseng, Y . J. et al. Predicting breast cancer metastasis by using serum biomarkers and clinicopathological data with machine learning technologies. Int. J. Med. Inf. 128, 79\u201386 (2019). Epub 2019 May 7. PMID: 31103449",
    "full_text_length": 101814,
    "chunk_length": 1090
  },
  {
    "chunk_id": 2965,
    "paper_filename": "riyadh_2020_early_breast_cancer_detection_via_optimized_cnn.pdf",
    "paper_title": "Riyadh 2020 Early Breast Cancer Detection Via Optimized Cnn",
    "chunk_index": 87,
    "total_chunks": 101,
    "text_content": "modeling techniques. Diagnostics 12, 1812. h t t p s : / / d o i . o r g / 1 0 . 3 3 9 0 / d i a g n o s t i c s 1 2 0 8 1 8 1 2 (2022). 15. Debendra Muduli, R., Dash, B., Majhi & Part, B. Automated diagnosis of breast cancer using multi-modal datasets: A deep convolution neural network based approach, Biomedical Signal Processing and Control, 71, 1746\u20138094, h t t p s : / / d o i . o r g / 1 0 . 1 0 1 6 / j . b s p c . 2 0 2 1 . 1 0 2 8 2 5 . (2022). 16. Wakili, M. A. et al. Classification of br",
    "full_text_length": 101814,
    "chunk_length": 1025
  },
  {
    "chunk_id": 2966,
    "paper_filename": "riyadh_2020_early_breast_cancer_detection_via_optimized_cnn.pdf",
    "paper_title": "Riyadh 2020 Early Breast Cancer Detection Via Optimized Cnn",
    "chunk_index": 88,
    "total_chunks": 101,
    "text_content": "36262621; PMCID: PMC9576400. 17. Heenaye-Mamode Khan, M. et al. Multi- class classification of breast cancer abnormalities using deep convolutional neural network (CNN). PLoS One . 16 (8), e0256500. https: //doi.org/10.1371/jo urnal.pone.0256500 (2021). PMID: 34437623; PMCID: PMC8389446. 18. Sakri, S. B., Abdul Rashid, N. B. & Muhammad Zain, Z. Particle Swarm Optimization Feature Selection for Breast Cancer Recurrence Prediction, in IEEE Access, vol. 6, pp. 29637\u201329647, (2018). https://doi.org/1",
    "full_text_length": 101814,
    "chunk_length": 1263
  },
  {
    "chunk_id": 2967,
    "paper_filename": "riyadh_2020_early_breast_cancer_detection_via_optimized_cnn.pdf",
    "paper_title": "Riyadh 2020 Early Breast Cancer Detection Via Optimized Cnn",
    "chunk_index": 89,
    "total_chunks": 101,
    "text_content": "Improving alzheimer\u2019s disease and brain tumor detection using deep learning with particle swarm optimization. AI 4, 551\u2013573. https://doi.org/ 10.3390/ai4030030 (2023). 21. Karimi Jafarbigloo, S. & Danyali, H. Nuclear atypia grading in breast cancer histopathological images based on CNN feature extraction and LSTM classification. CAAI Trans. Intell. Technol. 6 (4), 426\u2013439. https://do i.org/10.1049/cit2.12061 (2021). 22. Kamel, S. R., Y aghoubZadeh, R. & &Kheirabadi, M. Improving the performance ",
    "full_text_length": 101814,
    "chunk_length": 1213
  },
  {
    "chunk_id": 2968,
    "paper_filename": "riyadh_2020_early_breast_cancer_detection_via_optimized_cnn.pdf",
    "paper_title": "Riyadh 2020 Early Breast Cancer Detection Via Optimized Cnn",
    "chunk_index": 90,
    "total_chunks": 101,
    "text_content": "learning machine. Diagnostics 11, 241. https://doi.org/10 .3390/diagnostics110 20241 (2021). 24. Jakhar, A. K., Gupta, A. & Singh, M. SELF: a stacked-based ensemble learning framework for breast cancer classification. Evol. Intel . 17, 1341\u20131356. https://doi. org/10.1007/s12065-0 23-00824-4 (2024). 25. Nawaz, M., Sewissy, A. A. & Soliman, T. H. A. Multi-class breast cancer classification using deep learning convolutional neural network. Int. J. Adv. Comput. Sci. Appl. (IJACSA) . 9 (6), 316\u2013322 (",
    "full_text_length": 101814,
    "chunk_length": 1385
  },
  {
    "chunk_id": 2969,
    "paper_filename": "riyadh_2020_early_breast_cancer_detection_via_optimized_cnn.pdf",
    "paper_title": "Riyadh 2020 Early Breast Cancer Detection Via Optimized Cnn",
    "chunk_index": 91,
    "total_chunks": 101,
    "text_content": "& Rehman, R. Breast Cancer Dataset, Classification and Detection Using Deep Learning 102395 (Healthcare, 2022). 29. Berbar, M. A., Reyad, Y . A. & Hussain, M. Breast Mass Classification using Statistical and Local Binary Pattern Features, 16th International Conference on Information Visualisation, Montpellier, France, 2012, pp. 486\u2013490. https://doi.org/ 10.1109/IV .2012.84 30. Zhang, S. et al. Potential rapid intraoperative cancer diagnosis using dynamic full-field optical coherence tomography a",
    "full_text_length": 101814,
    "chunk_length": 1218
  },
  {
    "chunk_id": 2970,
    "paper_filename": "riyadh_2020_early_breast_cancer_detection_via_optimized_cnn.pdf",
    "paper_title": "Riyadh 2020 Early Breast Cancer Detection Via Optimized Cnn",
    "chunk_index": 92,
    "total_chunks": 101,
    "text_content": "Number 1, March pp. 92\u2013100(9). (2014). 32. Silva, L. F. et al. Hybrid analysis for indicating patients with breast cancer using temperature time series. Comput. Methods Programs Biomed. 130, 142\u2013153. https://doi.org/10.1016/ j.scib.2024.03.061 (2016). 33. Russo, F. Edge detection in noisy images using fuzzy reasoning, IMTC/98 Conference Proceedings. IEEE Instrumentation and Measurement Technology Conference. Where Instrumentation is Going (Cat. No.98CH36222), St. Paul, MN, USA, pp. 369\u2013372 vol.1",
    "full_text_length": 101814,
    "chunk_length": 1476
  },
  {
    "chunk_id": 2971,
    "paper_filename": "riyadh_2020_early_breast_cancer_detection_via_optimized_cnn.pdf",
    "paper_title": "Riyadh 2020 Early Breast Cancer Detection Via Optimized Cnn",
    "chunk_index": 93,
    "total_chunks": 101,
    "text_content": "detection in color images using bacterial foraging algorithm. IEEE Trans. Fuzzy Syst. 25 (1), 114\u2013127. https: //doi.org/10.1109/TF UZZ.2016.2551289 (Feb. 2017). Scientific Reports | (2025) 15:25290 21 | https://doi.org/10.1038/s41598-025-11218-0www.nature.com/scientificreports/ 37. Eberhart, R. C. & Kennedy, J. A new optimizer using particle swarm theory. Proceedings of the Sixth International Symposium on Micromachine and Human Science, Nagoya, Japan. pp. 39\u201343, (1995). 38. Kennedy, J. & Eberha",
    "full_text_length": 101814,
    "chunk_length": 1449
  },
  {
    "chunk_id": 2972,
    "paper_filename": "riyadh_2020_early_breast_cancer_detection_via_optimized_cnn.pdf",
    "paper_title": "Riyadh 2020 Early Breast Cancer Detection Via Optimized Cnn",
    "chunk_index": 94,
    "total_chunks": 101,
    "text_content": "and low-quality found data, arXiv:1803.00860. 41. Eirikur Agustsson, M., Tschannen, F., Mentzer, R., Timofte, L. & Van Gool Generative Adversarial Networks for Extreme Learned Image Compression, arXiv:180402958 42. Arjovsky, M., Chintala, S. & Bottou, L. and Wasserstein Generative Adversarial Networks. In Proceedings of the 34th International Conference on Machine Learning (ICML)2017, pp. 214\u2013223. 43. Furdui, A., Zhang, T., Worring, M., Cesar, P . & Abdallah El, A. AC-WGAN-GP: Augmenting ECG and",
    "full_text_length": 101814,
    "chunk_length": 1223
  },
  {
    "chunk_id": 2973,
    "paper_filename": "riyadh_2020_early_breast_cancer_detection_via_optimized_cnn.pdf",
    "paper_title": "Riyadh 2020 Early Breast Cancer Detection Via Optimized Cnn",
    "chunk_index": 95,
    "total_chunks": 101,
    "text_content": "1 8 . 3 4 7 9 3 0 1 44. Zhang, Y ., Liu, H., Wang, X. & Chen, J. Deep convolutional neural network for breast Cancer classification using infrared thermal images. IEEE Access. 10, 32645\u201332656. https://doi. org/10.1109/ACCESS.2 022.3157432 (2022). 45. Rojas, M., Naranjo-Torres, J. & Meneses, J. Breast cancer detection using thermal images and deep learning. In H. Florez & S. Misra (Eds.), Applied Informatics: Fourth International Conference, ICAI 2021 (pp. 428\u2013440). Springer, (2021). h t t p s : ",
    "full_text_length": 101814,
    "chunk_length": 1171
  },
  {
    "chunk_id": 2974,
    "paper_filename": "riyadh_2020_early_breast_cancer_detection_via_optimized_cnn.pdf",
    "paper_title": "Riyadh 2020 Early Breast Cancer Detection Via Optimized Cnn",
    "chunk_index": 96,
    "total_chunks": 101,
    "text_content": "Conference on Web Research (ICWR), Tehran, Iran, Islamic Republic of, pp. 27\u201331, (2024). https:// doi.org/10.1109/ICWR 61162.2024.10533359 47. Shobhana Periyasamy, A., Prakasarao, M., Menaka, B., Venkatraman, M. & Jayashree Support vector machine based methodology for classification of thermal images pertaining to breast cancer. J. Therm. Biology Volume . 110, 103337. h t t p s : / / d o i . o r g / 1 0 . 1 0 1 6 / j . j t h e r b i o . 2 0 2 2 . 1 0 3 3 3 7 (2022). 48. Karaman, A. et al. Hyper-",
    "full_text_length": 101814,
    "chunk_length": 1274
  },
  {
    "chunk_id": 2975,
    "paper_filename": "riyadh_2020_early_breast_cancer_detection_via_optimized_cnn.pdf",
    "paper_title": "Riyadh 2020 Early Breast Cancer Detection Via Optimized Cnn",
    "chunk_index": 97,
    "total_chunks": 101,
    "text_content": "with artificial bee colony (ABC), expert systems with applications, 221, 2023, 119741, ISSN 0957\u2013 4174, https://doi.org/ 10.1016/j.eswa.2023. 119741 50. Bayram, B., Kunduracioglu, I., Ince, S. & Pacal, I. A systematic review of deep learning in MRI-based cerebral vascular occlusion- based brain diseases. Neuroscience. ;568:76\u201394. (2025). https://doi.or g/10.1016/j.neurosci ence.2025.01.020 . Epub 2025 Jan 11. PMID: 39805420. 51. Burhanettin Ozdemir, I. & Pacal An innovative deep learning framewo",
    "full_text_length": 101814,
    "chunk_length": 1200
  },
  {
    "chunk_id": 2976,
    "paper_filename": "riyadh_2020_early_breast_cancer_detection_via_optimized_cnn.pdf",
    "paper_title": "Riyadh 2020 Early Breast Cancer Detection Via Optimized Cnn",
    "chunk_index": 98,
    "total_chunks": 101,
    "text_content": "36, 12047\u201312059. https: //doi.org/10.1007/s0 0521-024-09767-y (2024). Author contributions R.M.A., and M.Y .S proposed the main idea. S.B.S, M.A, A.F.S, A.A and S.S.B. checked and discussed the results and the whole manuscript. P .N.B and E.Y .K.N contributed to the discussion of this study. All authors have ap - proved the final version of this manuscript. Funding This research was funded by the Deanship of Postgraduate Studies and Scientific Research at Majmaah Univer - sity through the projec",
    "full_text_length": 101814,
    "chunk_length": 1409
  },
  {
    "chunk_id": 2977,
    "paper_filename": "riyadh_2020_early_breast_cancer_detection_via_optimized_cnn.pdf",
    "paper_title": "Riyadh 2020 Early Breast Cancer Detection Via Optimized Cnn",
    "chunk_index": 99,
    "total_chunks": 101,
    "text_content": "in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if you modified the licensed material. Y ou do not have permission under this licence to share adapted material derived from this article or parts of it. The images or other third party material in this article are included in the article\u2019s Creative Commons licence, unless indicated otherwise in a credit line to the material. If ma",
    "full_text_length": 101814,
    "chunk_length": 1033
  },
  {
    "chunk_id": 2978,
    "paper_filename": "riyadh_2020_early_breast_cancer_detection_via_optimized_cnn.pdf",
    "paper_title": "Riyadh 2020 Early Breast Cancer Detection Via Optimized Cnn",
    "chunk_index": 100,
    "total_chunks": 101,
    "text_content": "s . o r g / l i c e n s e s / b y - n c - n d / 4 . 0 / . \u00a9 The Author(s) 2025, corrected publication 2025 Scientific Reports | (2025) 15:25290 22 | https://doi.org/10.1038/s41598-025-11218-0www.nature.com/scientificreports/",
    "full_text_length": 101814,
    "chunk_length": 224
  },
  {
    "chunk_id": 2979,
    "paper_filename": "robert_2024_the_virtual_reference_radiologist.pdf",
    "paper_title": "Robert 2024 The Virtual Reference Radiologist",
    "chunk_index": 0,
    "total_chunks": 55,
    "text_content": "Siepmann et al .European Radiology (2024) 34:6652 \u20136666 https://doi.org/10.1007/s00330-024-10727-2 IMAGING INFORMATICS AND ARTIFICIAL INTELLIGENCE Open Access The virtual reference radiologist: comprehensive AI assistance for clinicalimage reading and interpretation Robert Siepmann1, Marc Huppertz1, Annika Rastkhiz1, Matthias Reen1, Eric Corban1, Christian Schmidt1, Stephan Wilke1, Philipp Schad1, Can Y\u00fcksel1, Christiane Kuhl1, Daniel Truhn1and Sven Nebelung1* Abstract Objectives Large language ",
    "full_text_length": 57535,
    "chunk_length": 1518
  },
  {
    "chunk_id": 2980,
    "paper_filename": "robert_2024_the_virtual_reference_radiologist.pdf",
    "paper_title": "Robert 2024 The Virtual Reference Radiologist",
    "chunk_index": 1,
    "total_chunks": 55,
    "text_content": "tool. The impact of Arti \ufb01cial Intelligence (AI) on diagnostic accuracy, con \ufb01dence, user experience, input prompts, and generated responses was assessed. False information was registered. Linear mixed-effect models were used to quantify the factors ( \ufb01xed: experience, modality, AI assistance; random: radiologist) in \ufb02uencing diagnostic accuracy and con \ufb01dence. Results When assessing if the correct diagnosis was among the top-3 differential diagnoses, diagnostic accuracy improved slightly from 1",
    "full_text_length": 57535,
    "chunk_length": 1504
  },
  {
    "chunk_id": 2981,
    "paper_filename": "robert_2024_the_virtual_reference_radiologist.pdf",
    "paper_title": "Robert 2024 The Virtual Reference Radiologist",
    "chunk_index": 2,
    "total_chunks": 55,
    "text_content": "and diagnostic con\ufb01dence signi \ufb01cantly. Potentially harmful hallucinations and misinterpretations call for caution and highlight the need for further safeguarding measures. Clinical relevance statement Using GPT-4 as a virtual assistant when reading images made six radiologists of different experience levels feel more con \ufb01dent and provide more accurate diagnoses; yet, GPT-4 gave factually incorrect and potentially harmful information in 7.4% of its responses. \u00a9 The Author(s) 2024, corrected pub",
    "full_text_length": 57535,
    "chunk_length": 1365
  },
  {
    "chunk_id": 2982,
    "paper_filename": "robert_2024_the_virtual_reference_radiologist.pdf",
    "paper_title": "Robert 2024 The Virtual Reference Radiologist",
    "chunk_index": 3,
    "total_chunks": 55,
    "text_content": "line to the material. If material is not included in the article \u2019s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permissiondirectly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/ .Daniel Truhn and Sven Nebelung contributed equally to this publication. *Correspondence: Sven Nebelung snebelung@ukaachen.de 1Department of Diagnostic and Interv",
    "full_text_length": 57535,
    "chunk_length": 1538
  },
  {
    "chunk_id": 2983,
    "paper_filename": "robert_2024_the_virtual_reference_radiologist.pdf",
    "paper_title": "Robert 2024 The Virtual Reference Radiologist",
    "chunk_index": 4,
    "total_chunks": 55,
    "text_content": "\ufb01cial intelligence (AI) model, chatGPT has gained immense popularity [ 1]. Even though prior LLMs, such as BERT (Bidirectional EncoderRepresentations from Transformers), gained popularity inthe past [ 2], attention transformer-based LLMs, such as chatGPT, have largely replaced them. Potential use cases involve radiologic reporting [ 3,4] and guidance on utilizing imaging services [ 5]. ChatGPT passed the United States Medical Licensing Exam [ 6] and nearly passed a radiology board-style examinat",
    "full_text_length": 57535,
    "chunk_length": 1366
  },
  {
    "chunk_id": 2984,
    "paper_filename": "robert_2024_the_virtual_reference_radiologist.pdf",
    "paper_title": "Robert 2024 The Virtual Reference Radiologist",
    "chunk_index": 5,
    "total_chunks": 55,
    "text_content": "applications in radiology and the -by and large- \u201cimpressive performance \u201d[10]. Speci \ufb01cally, ChatGPT \u2019s capability to evaluate patient studies and provide radiologic diagnoses has been studied in well- presented literature case series such as the American Journal of Neuroradiology \u2019s\u201cCase of the Month \u201d[11] and Radiology \u2019s\u201cDiagnosis Please \u201dseries [ 12]. ChatGPT was also fed appropriateness criteria (by the AmericanCollege of Radiology) to cr eate a context-aware chatbot for improved decision-",
    "full_text_length": 57535,
    "chunk_length": 1272
  },
  {
    "chunk_id": 2985,
    "paper_filename": "robert_2024_the_virtual_reference_radiologist.pdf",
    "paper_title": "Robert 2024 The Virtual Reference Radiologist",
    "chunk_index": 6,
    "total_chunks": 55,
    "text_content": "was that radi-ologists would bene \ufb01tf r o mc h a t G P T \u2019s assistance, par- ticularly when inexperienced. Here, we implicitly operated under the null hypothesis (H 0) of no difference in diagnostic accuracy and con \ufb01dence with and withoutAI assistance. Conversely, the alternative hypothesis (H1) assumed that AI assistance provides a measurable difference. Materials & methods Study design and dataset characteristics Approval was granted by the local ethical committee (reference number 028/19), a",
    "full_text_length": 57535,
    "chunk_length": 1371
  },
  {
    "chunk_id": 2986,
    "paper_filename": "robert_2024_the_virtual_reference_radiologist.pdf",
    "paper_title": "Robert 2024 The Virtual Reference Radiologist",
    "chunk_index": 7,
    "total_chunks": 55,
    "text_content": "lands) of our tertiary academic medical center (UniversityHospital Aachen, Aachen, Germany), two resident radi-ologists with three years of experience (R.S. and M.H.) andtwo board-certi \ufb01ed clinical radiologists (D.T. and S.N. [with 10 and 8 years of experience]) selected ten radiographic, CT,MRI, and angiographic studies each (Table 1). The imaging studies re \ufb02ected various demographic characteristics, i.e., patient age and sex, and conditions of variable severity and complexity. Only studies w",
    "full_text_length": 57535,
    "chunk_length": 1456
  },
  {
    "chunk_id": 2987,
    "paper_filename": "robert_2024_the_virtual_reference_radiologist.pdf",
    "paper_title": "Robert 2024 The Virtual Reference Radiologist",
    "chunk_index": 8,
    "total_chunks": 55,
    "text_content": "removed. The studieswere fed back to the research section of the PACS indivi-dually, where they could be accessed on clinical work-stations. Standardized case descriptions were framed foreach study, indicating the relevant clinical context and allowing radiologists to put their \ufb01ndings into the appro- priate clinical perspective.Siepmann et al .European Radiology (2024) 34:6652 \u20136666 6653 Experimental setup and data collection Six clinical resident radiologists with varying experience levels wer",
    "full_text_length": 57535,
    "chunk_length": 1425
  },
  {
    "chunk_id": 2988,
    "paper_filename": "robert_2024_the_virtual_reference_radiologist.pdf",
    "paper_title": "Robert 2024 The Virtual Reference Radiologist",
    "chunk_index": 9,
    "total_chunks": 55,
    "text_content": "external references, e.g., online searches or textbooks, were prohibited. \u201cAI-assisted \u201dmeant that GPT- 4 could be prompted without restrictions. Additionalexternal references were similarly prohibited from singlingout the effect of GPT-4. We did not use any additionalGPT-4 add-ons, meaning GPT-4 had to resort to itsinternal knowledge. The radiologists were introduced tothe setup and adequately trained to interact with GPT-4.They were also instructed that prompts could query any aspect of diagno",
    "full_text_length": 57535,
    "chunk_length": 1434
  },
  {
    "chunk_id": 2989,
    "paper_filename": "robert_2024_the_virtual_reference_radiologist.pdf",
    "paper_title": "Robert 2024 The Virtual Reference Radiologist",
    "chunk_index": 10,
    "total_chunks": 55,
    "text_content": "After completing the questionnaires unassisted (session one),the radiologists re-read the same studies using AI assis-tance (session two). Time restrictions or a minimumwashout period were not instituted, and the radiologistscould re-read the imaging studies at their chosen time. However, they were instructed not to collect additional information or seek assistance on the patients, studies, ordifferential diagnoses between the readings. Furtherdetails on the experimental setup are provided inthe",
    "full_text_length": 57535,
    "chunk_length": 1510
  },
  {
    "chunk_id": 2990,
    "paper_filename": "robert_2024_the_virtual_reference_radiologist.pdf",
    "paper_title": "Robert 2024 The Virtual Reference Radiologist",
    "chunk_index": 11,
    "total_chunks": 55,
    "text_content": "studies Modality Diagnosis (reference) Case description Radiography Osgood-Schlatter Disease (Apophysitis of Tibial Tubercle)13 yo male with pain and swelling over the tibial tuberosity exacerbated with exercise. Osteopetrosis 49 yo male with recurrent bone fractures and infections. Rickets 9 yo female with recurrent bone fractures.Duodenal Atresia 1-day-old male with nonbilious vomiting.Kienb\u00f6ck \u2019s Disease (Lunate Malacia) 31 yo male patient with wrist pain. Cystic Fibrosis 17 yo male with recu",
    "full_text_length": 57535,
    "chunk_length": 1413
  },
  {
    "chunk_id": 2991,
    "paper_filename": "robert_2024_the_virtual_reference_radiologist.pdf",
    "paper_title": "Robert 2024 The Virtual Reference Radiologist",
    "chunk_index": 12,
    "total_chunks": 55,
    "text_content": "Interstitial Pancreatitis (Uncomplicated) 81 yo female with upper abdominal pain radiating to the back.Endocrine Orbitopathy 51 yo male with eye pain. His family says his eyes look \u2018weird \u2019. Intralobar Pulmonary Sequestration 32 yo female with recurrent pulmonary infections since childhood. Gall Stone Ileus 66 yo male with nausea, vomiting, abdominal distention, and cramping abdominal pain. Sigmoid Volvulus 18 yo male with abdominal pain, abdominal distension, vomiting, and bloody stools.Portal ",
    "full_text_length": 57535,
    "chunk_length": 1366
  },
  {
    "chunk_id": 2992,
    "paper_filename": "robert_2024_the_virtual_reference_radiologist.pdf",
    "paper_title": "Robert 2024 The Virtual Reference Radiologist",
    "chunk_index": 13,
    "total_chunks": 55,
    "text_content": "female with recurrent pelvic pain after prolonged sitting and sexual intercourse. Iron Overload (Sickle Cell Anemia) 18 yo male with anemia and recurrent acute bone pain. Hoffa Pad Impingement Syndrome 24 yo female with unspeci \ufb01c knee pain. Focal Nodular Hyperplasia (Liver) 38 yo female with an incidental liver mass on ultrasound.Primary Sclerosing Cholangitis (Liver) 65 yo male with recurrent bloody diarrhea and abdominal pain.Hemangioma (Spleen) 66 yo patient with unclear spleen \ufb01nding in ult",
    "full_text_length": 57535,
    "chunk_length": 1381
  },
  {
    "chunk_id": 2993,
    "paper_filename": "robert_2024_the_virtual_reference_radiologist.pdf",
    "paper_title": "Robert 2024 The Virtual Reference Radiologist",
    "chunk_index": 14,
    "total_chunks": 55,
    "text_content": "(Accessory Renal Artery) 72 yo male with hypotension, tachycardia, paleness, and right \ufb02ank pain. Bile Leakage (Insuf \ufb01ciency of the Percutaneous Transhepatic Biliary Drainage)69 yo male after hepatobiliary surgery and percutaneous transhepatic biliary drainage. Increasing markers of in \ufb02ammation and fever. Peripheral Artery Disease (Proximal Stenosis of Super \ufb01cial Femoral Artery)54 yo male with leg pain during physical activity that resolves with rest. Thrombosis (Port Catheter Tip) 75 yo fema",
    "full_text_length": 57535,
    "chunk_length": 1405
  },
  {
    "chunk_id": 2994,
    "paper_filename": "robert_2024_the_virtual_reference_radiologist.pdf",
    "paper_title": "Robert 2024 The Virtual Reference Radiologist",
    "chunk_index": 15,
    "total_chunks": 55,
    "text_content": "ential diagnosis ( \u2018top-1 performance \u2019). Superordinate diagnoses (e.g., \u2018peripheral artery disease \u2019) instead of the more speci \ufb01c diagnosis (e.g., \u2018super \ufb01cial femoral artery stenosis \u2019) were considered correct if the clinical pre- sentation and imaging \ufb01ndings did not differ considerably. Overly vague diagnoses (like \u2018vasculopathy \u2019) or con- siderably different clinical presentations and imaging\ufb01ndings (like \u2018tuberculosis \u2019instead of \u2018miliary tubercu- losis \u2019) were rejected. Ambiguous diagnos",
    "full_text_length": 57535,
    "chunk_length": 1498
  },
  {
    "chunk_id": 2995,
    "paper_filename": "robert_2024_the_virtual_reference_radiologist.pdf",
    "paper_title": "Robert 2024 The Virtual Reference Radiologist",
    "chunk_index": 16,
    "total_chunks": 55,
    "text_content": "study. Prompts could beassigned to different prompt types. Response quality wasassessed qualitatively, and four radiologists (M.H., R.S., D.T., S.N.) evaluated whether the information provided by GPT-4 was veri \ufb01ed, up-to-date, and reliable. They ana- lyzed all prompts and responses independently and notedpossibly incorrect or inconsistent responses that weresubsequently discussed until a consensus was reached.\u2018hallucinations \u2019were de \ufb01ned as seemingly correct responses that (i) were nonsensical",
    "full_text_length": 57535,
    "chunk_length": 1487
  },
  {
    "chunk_id": 2996,
    "paper_filename": "robert_2024_the_virtual_reference_radiologist.pdf",
    "paper_title": "Robert 2024 The Virtual Reference Radiologist",
    "chunk_index": 17,
    "total_chunks": 55,
    "text_content": "to provide general feedback on satisfaction,utility, ease of use, and trust on \ufb01ve-point Likert scales, ranging from \u2018very poor \u2019(score 1) to \u2018very good \u2019(score 5). Statistical analysis and power analysis Statistical analyses were performed using GraphPad Prism software (v9.5, San Diego, CA, USA) and Python (v3.11)and its library statsmodel by R.S., M.H., D.T., and S.N. Diagnostic accuracy was calculated as the number of cor-rect diagnoses divided by the number of correct and incorrect diagnoses",
    "full_text_length": 57535,
    "chunk_length": 1357
  },
  {
    "chunk_id": 2997,
    "paper_filename": "robert_2024_the_virtual_reference_radiologist.pdf",
    "paper_title": "Robert 2024 The Virtual Reference Radiologist",
    "chunk_index": 18,
    "total_chunks": 55,
    "text_content": "was used to study the predictors \u2019 impact on diagnostic con \ufb01dence. For the top-3 and the top- 1 performance approaches, a two-proportion z-test wasused to determine whether differences in diagnostic accu-racy were signi \ufb01cant between AI-assisted and unassisted radiologists. Post hoc, the effect size was quanti \ufb01ed using Cohen \u2019shas a measure of the difference between two proportions. Means and 95% con \ufb01dence intervals are given, and the signi \ufb01cance level was set at \u03b1\u22640.05.Table 1 continued Mod",
    "full_text_length": 57535,
    "chunk_length": 1432
  },
  {
    "chunk_id": 2998,
    "paper_filename": "robert_2024_the_virtual_reference_radiologist.pdf",
    "paper_title": "Robert 2024 The Virtual Reference Radiologist",
    "chunk_index": 19,
    "total_chunks": 55,
    "text_content": "for establishing an unequi vocal diagnosis. yoyear(s)-oldSiepmann et al .European Radiology (2024) 34:6652 \u20136666 6656 Given the scarce availability of literature evidence on diagnostic accuracy as a function of AI assistance, arudimentary sample size estimation was conducted.Informed by related literature evidence [ 14], we assumed a small effect size of 0.2. Consequently, the minimum sample size was determined a priori as 208 using the power of 0.8, the probability of an \u03b1error of 0.05, a t-tes",
    "full_text_length": 57535,
    "chunk_length": 1439
  },
  {
    "chunk_id": 2999,
    "paper_filename": "robert_2024_the_virtual_reference_radiologist.pdf",
    "paper_title": "Robert 2024 The Virtual Reference Radiologist",
    "chunk_index": 20,
    "total_chunks": 55,
    "text_content": "time delay between the readingsessions was 6.3 \u00b1 6.3 days (range, 0 \u201318 days). \u2018Top-3 performance \u2019: we found moderately improved diagnostic accuracy when considering the threeradiologist-provided diagnoses. Speci \ufb01cally, accuracy improved from 181/240 (75.4%, unassisted) to 188/240 studies (78.3%, AI-assisted), which aligns with theexpected effect direction and magnitude. Yet, thehypothesis that radiologists \u2019diagnostic accuracy would bene\ufb01t from using AI assistance cannot be accepted based on ",
    "full_text_length": 57535,
    "chunk_length": 1483
  },
  {
    "chunk_id": 3000,
    "paper_filename": "robert_2024_the_virtual_reference_radiologist.pdf",
    "paper_title": "Robert 2024 The Virtual Reference Radiologist",
    "chunk_index": 21,
    "total_chunks": 55,
    "text_content": "incorrect differential diagnoses were revised afterinteraction with GPT-4 and rendered correct (Fig. 3). Conversely, in 4/240 re-read studies (two radiographic,one MRI, and one angiographic study), initially correct differential diagnoses (unassisted) were rendered incor- rect (AI-assisted) (Fig. 4). Fig. 2 Diagnostic accuracy as a function of modality, experience level, and AI assistance. Detailed breakdown of the correct and incorrect readings per imaging study when considering the correct dia",
    "full_text_length": 57535,
    "chunk_length": 1498
  },
  {
    "chunk_id": 3001,
    "paper_filename": "robert_2024_the_virtual_reference_radiologist.pdf",
    "paper_title": "Robert 2024 The Virtual Reference Radiologist",
    "chunk_index": 22,
    "total_chunks": 55,
    "text_content": "coef \ufb01cient was -0.18 ( p=0.428), indicating a slightly yet non- signi\ufb01cantly decreased likelihood of a correct diagnosis without AI assistance. Supplementary Table 2 details the coef\ufb01cients and pvalues of the principal predictors that in\ufb02uenced diagnostic accuracy. \u2018Top-1 performance \u2019: when considering only the \ufb01rst radiologist-provided diagnoses, diagnostic accuracyimproved slightly from 154/240 (64.2%, unassisted) to163/240 studies (67.9%, AI-assisted) read correctly. Onceagain, the most pro",
    "full_text_length": 57535,
    "chunk_length": 1455
  },
  {
    "chunk_id": 3002,
    "paper_filename": "robert_2024_the_virtual_reference_radiologist.pdf",
    "paper_title": "Robert 2024 The Virtual Reference Radiologist",
    "chunk_index": 23,
    "total_chunks": 55,
    "text_content": "n=10]). Four prompts (1.3%) were related to gradings and classi \ufb01ca- tions, and one prompt (0.3%) demanded general guidance(Fig. 5a). Prompts were evenly distributed among the modalities (Fig. 5b). Mainly, one prompt was used per patient (42.5% of prompts), while two (22.5%) or threeprompts (10.0%) were used frequently, too (Fig. 5c). Fourty-two percentage of the prompts were provided by radiologists with limited experience, 34.3% by radiologists with moderate experience, and 23.6% by radiologis",
    "full_text_length": 57535,
    "chunk_length": 1453
  },
  {
    "chunk_id": 3003,
    "paper_filename": "robert_2024_the_virtual_reference_radiologist.pdf",
    "paper_title": "Robert 2024 The Virtual Reference Radiologist",
    "chunk_index": 24,
    "total_chunks": 55,
    "text_content": "of GPT-4 were classi \ufb01ed as \u2018hallucinations \u2019(7.4%), and two as \u2018misinterpretations \u2019 (0.6%), while no \u2018clari\ufb01cations \u2019were necessary. Halluci- nations involved all modalities and various aspects of Fig. 3 Positive effects of AI consultation \u2014example case. In this patient with intralobar pulmonary sequestration (CT, sagittal reconstruction, lung window), the consultation of GPT-4 changed the initially incorrect differential diagnosis to the correct differential diagnosisSiepmann et al .European ",
    "full_text_length": 57535,
    "chunk_length": 1475
  },
  {
    "chunk_id": 3004,
    "paper_filename": "robert_2024_the_virtual_reference_radiologist.pdf",
    "paper_title": "Robert 2024 The Virtual Reference Radiologist",
    "chunk_index": 25,
    "total_chunks": 55,
    "text_content": "\u2019. Even though the responses were largely consistent with current medical knowledge, the noted instances ofhallucinations partially refute hypothesis (iii). User experience GPT-4 \u2019s utility was rated as fair (2.8 \u00b1 0.4), satisfaction and trust as good (4.0 \u00b1 0.6, 3.6 \u00b1 0.8), and ease of use asexcellent (4.5 \u00b1 0.5). Discussion Earlier studies investigating the potential value of GPT-4in the clinic demonstrated excellent performance across various tasks and disciplines, particularly standardized m",
    "full_text_length": 57535,
    "chunk_length": 1360
  },
  {
    "chunk_id": 3005,
    "paper_filename": "robert_2024_the_virtual_reference_radiologist.pdf",
    "paper_title": "Robert 2024 The Virtual Reference Radiologist",
    "chunk_index": 26,
    "total_chunks": 55,
    "text_content": "the preeminent factor determining diag-nostic accuracy, while the modality and AI assistance hada more nuanced in \ufb02uence. AI assistance was bene \ufb01cial by trend, yet its in \ufb02uence on diagnostic accuracy was sta- tistically nonsigni \ufb01cant. Our primary \ufb01nding of improved diagnostic accuracy was valid for the \u2018top-3 performance \u2019. This \ufb01nding is plausible given GPT-4 \u2019s broad, detailed knowledge of radiology. Notably, for radiography, we found a decline indiagnostic accuracy when radiographs were re",
    "full_text_length": 57535,
    "chunk_length": 1380
  },
  {
    "chunk_id": 3006,
    "paper_filename": "robert_2024_the_virtual_reference_radiologist.pdf",
    "paper_title": "Robert 2024 The Virtual Reference Radiologist",
    "chunk_index": 27,
    "total_chunks": 55,
    "text_content": "[Dixon]), axial image, 20 min after injection of gadoxetic acid), the consultation of GPT-4 changed the list and order of differential diagn oses. While focal nodular hyperplasia was the \ufb01rst differential diagnosis without AI assistance, it was only the second diagnosis with AI assistance, most likely because of adherence to the responseSiepmann et al .European Radiology (2024) 34:6652 \u20136666 6659 Sporadically, our radiologists cast wider nets of broader differentials under AI assistance instead ",
    "full_text_length": 57535,
    "chunk_length": 1395
  },
  {
    "chunk_id": 3007,
    "paper_filename": "robert_2024_the_virtual_reference_radiologist.pdf",
    "paper_title": "Robert 2024 The Virtual Reference Radiologist",
    "chunk_index": 28,
    "total_chunks": 55,
    "text_content": "to GPT-4 \u2019s response and involved pri- marily rare diagnoses with speci \ufb01c imaging features, such as pulmonary sequestration. For the latter, radiologistssporadically followed GPT-4 \u2019s guidance and agreed to the suggested and frequently generic differentials. This \ufb01nd- ing may be a potential sign of overreliance and Fig. 5 Prompt quantities and characteristics. During the AI-assisted readings of the imaging studies, the six radiologists provided n=309 prompts altogether. Detailed are the prompt ",
    "full_text_length": 57535,
    "chunk_length": 1360
  },
  {
    "chunk_id": 3008,
    "paper_filename": "robert_2024_the_virtual_reference_radiologist.pdf",
    "paper_title": "Robert 2024 The Virtual Reference Radiologist",
    "chunk_index": 29,
    "total_chunks": 55,
    "text_content": "\ufb01cantly more likely to follow the suggestions, even when blatantly false [ 18]. Undoubtedly, knowledgeable, skilled, and con \ufb01dent radi- ologists are key to mitigating these issues. AI may coverthe width of potential diagnoses excellently, but it (still)requires trained radiologists to check consistency andreasoning. Diagnostic accuracy dropped when de \ufb01ned more strictly as the \u2018top-1 performance \u2019. While the \ufb01rst diag- nosis remained unchanged in most patients after inter-action with GPT-4, we ",
    "full_text_length": 57535,
    "chunk_length": 1439
  },
  {
    "chunk_id": 3009,
    "paper_filename": "robert_2024_the_virtual_reference_radiologist.pdf",
    "paper_title": "Robert 2024 The Virtual Reference Radiologist",
    "chunk_index": 30,
    "total_chunks": 55,
    "text_content": "focused onimaging features and information on pathology andanatomy. However, interactions with GPT-4 could not compensate for overlooked, ill-evaluated, and ill-described \ufb01ndings and provided valuable assistanceonly when speci \ufb01cally prompted. The complexity of describing speci \ufb01c\ufb01ndings can hardly be overcome for radiologists unfamiliar with a particular modality. Diagnostic con \ufb01dence was greater with AI assistance, yet this \ufb01nding is only partially re \ufb02ected by user experi- ence ratings. Whil",
    "full_text_length": 57535,
    "chunk_length": 1448
  },
  {
    "chunk_id": 3010,
    "paper_filename": "robert_2024_the_virtual_reference_radiologist.pdf",
    "paper_title": "Robert 2024 The Virtual Reference Radiologist",
    "chunk_index": 31,
    "total_chunks": 55,
    "text_content": "of LLMs must be accompanied by appropriate safeguarding measures toensure their accuracy and reliability and to preventpatient harm. Increasing user awareness, instituting quality checks by medical professionals, enhancing the LLM \u2019s robustness against hallucinations, e.g., by process supervision instead of outcome supervision [ 20], and auditing adherence to regulatory standards may be partsof a strategy to counteract hallucinations effectively. Our study has limitations. First, ten imaging stu",
    "full_text_length": 57535,
    "chunk_length": 1490
  },
  {
    "chunk_id": 3011,
    "paper_filename": "robert_2024_the_virtual_reference_radiologist.pdf",
    "paper_title": "Robert 2024 The Virtual Reference Radiologist",
    "chunk_index": 32,
    "total_chunks": 55,
    "text_content": "value in thereading room. Third, we did not institute a washoutperiod between the reading sessions, which resulted insome studies being read on the same day and others morethan 2 weeks apart. While performance metrics quanti-fying the impact of additional AI assistance on thebenchmarked (unassisted) performance could be affected, we consider this approach acceptable nonetheless because (i) interpreting the studies unaided and then, ifnecessary, accessing assistance re \ufb02ects the clinical prac- ti",
    "full_text_length": 57535,
    "chunk_length": 1287
  },
  {
    "chunk_id": 3012,
    "paper_filename": "robert_2024_the_virtual_reference_radiologist.pdf",
    "paper_title": "Robert 2024 The Virtual Reference Radiologist",
    "chunk_index": 33,
    "total_chunks": 55,
    "text_content": "1.07 3.82 \u00b1 1.10 MRI 2.33 \u00b1 1.10 2.93 \u00b1 1.12 Angiography 3.25 \u00b1 1.08 3.77 \u00b1 0.95 Low experience Radiography 3.40 \u00b1 1.19 4.00 \u00b1 1.03 CT 2.95 \u00b1 1.15 3.70 \u00b1 1.38 MRI 2.35 \u00b1 1.27 3.20 \u00b1 1.28 Angiography 3.45 \u00b1 1.28 4.30 \u00b1 0.86 Moderate experience Radiography 3.50 \u00b1 1.05 4.25 \u00b1 0.72 CT 4.30 \u00b1 0.73 4.55 \u00b1 0.60 MRI 2.00 \u00b1 0.97 3.10 \u00b1 1.25 Angiography 3.25 \u00b1 0.85 4.05 \u00b1 0.69 Advanced experience Radiography 2.60 \u00b1 0.75 2.95 \u00b1 0.60 CT 3.35 \u00b1 0.81 3.20 \u00b1 0.70 MRI 2.65 \u00b1 0.99 2.50 \u00b1 0.61 Angiography 3.05 \u00b1 ",
    "full_text_length": 57535,
    "chunk_length": 1165
  },
  {
    "chunk_id": 3013,
    "paper_filename": "robert_2024_the_virtual_reference_radiologist.pdf",
    "paper_title": "Robert 2024 The Virtual Reference Radiologist",
    "chunk_index": 34,
    "total_chunks": 55,
    "text_content": "using a linear mixed-effect model. Its results are detailed in Supplementary Table 4Siepmann et al .European Radiology (2024) 34:6652 \u20136666 6661 Table 3 \u2018Hallucinations \u2019by GPT-4 Radiologist \u2019s prompt GPT -4 \u2019s hallucinatory response Analysis of hallucination [Radiography] 25-year-old male patient with weight loss, fever, weakness, dyspnea, and hypercalcemia. Finely speckled densities are observed bilaterally inthe lungs. What are the most likely diagnoses?[\u2026] Tuberculosis: The classic symptoms ",
    "full_text_length": 57535,
    "chunk_length": 1418
  },
  {
    "chunk_id": 3014,
    "paper_filename": "robert_2024_the_virtual_reference_radiologist.pdf",
    "paper_title": "Robert 2024 The Virtual Reference Radiologist",
    "chunk_index": 35,
    "total_chunks": 55,
    "text_content": "conditions like rheumatoid arthritis, there can be bone erosions, which can present as opacities on the x-ray image.Erosions do not cause \u2018opacities \u2019but\u2018lucencies \u2019. [Radiography] 31-year-old male patient with wrist pain. What are the most likely diagnoses (in sortedorder) for densi \ufb01cation of the os lunatum?[\u2026] Scaphoid cysts: These are relatively rare, but small \ufb02uid-\ufb01lled sacs could cause densi \ufb01cation in the x-ray image.Bone cysts are lucent compared to bone and, thus, not a possible differ",
    "full_text_length": 57535,
    "chunk_length": 1403
  },
  {
    "chunk_id": 3015,
    "paper_filename": "robert_2024_the_virtual_reference_radiologist.pdf",
    "paper_title": "Robert 2024 The Virtual Reference Radiologist",
    "chunk_index": 36,
    "total_chunks": 55,
    "text_content": "Vater. [Radiography] What are the most probabledifferential diagnoses for a 49-year-old male patientwith recurring bone fractures and infections?Compacted endplates of the vertebral bodies canbe seen in the lumbar spine.[\u2026] Multiple Myeloma: This is a form of bone marrow cancer that can lead to bone pain,fractures, and infections. An X-ray image canshow \u201cdensi \ufb01ed\u201dareas that are actually due to bone damage.Multiple myeloma typically presents with hyperlucent (instead of \u201cdensi \ufb01ed\u201d ) bone areas.",
    "full_text_length": 57535,
    "chunk_length": 1429
  },
  {
    "chunk_id": 3016,
    "paper_filename": "robert_2024_the_virtual_reference_radiologist.pdf",
    "paper_title": "Robert 2024 The Virtual Reference Radiologist",
    "chunk_index": 37,
    "total_chunks": 55,
    "text_content": "with osteopetrosis. [Radiography] Which diseases match \ufb01ne speckled- con\ufb02uent in \ufb01ltrates on both sides with sparing of the periphery?Potential differential diagnoses are [ \u2026] Pneumonia [ \u2026] Tuberculosis [ \u2026] Interstitial lung diseases [ \u2026] Pulmonary edema [ \u2026] Pneumonitis [ \u2026] Metastases [ \u2026]The differential diagnoses do not consider the sparing of the periphery (as stated in theprompt). [Radiography] The opacities are rather central and spare the periphery, the subpleural portions looknormal. ",
    "full_text_length": 57535,
    "chunk_length": 1445
  },
  {
    "chunk_id": 3017,
    "paper_filename": "robert_2024_the_virtual_reference_radiologist.pdf",
    "paper_title": "Robert 2024 The Virtual Reference Radiologist",
    "chunk_index": 38,
    "total_chunks": 55,
    "text_content": "periphery. [CT] What cardiac or vascular causes can intermittent chest pain and dyspnea in a youngmale patient have? What do these look like in a CTscan?[\u2026] Aortic Dissection: While this is rare in young people, it can occur in individuals with certaingenetic disorders or connective tissue diseases.It causes severe, sudden chest pain and can alsocause dyspnea. In a chest CT, you would see a\u201cfalse glade \u201dor\u201cintimal tumor \u201din the aorta. [ \u2026]Aortic dissection is usually described as a \ufb02ap- like tea",
    "full_text_length": 57535,
    "chunk_length": 1308
  },
  {
    "chunk_id": 3018,
    "paper_filename": "robert_2024_the_virtual_reference_radiologist.pdf",
    "paper_title": "Robert 2024 The Virtual Reference Radiologist",
    "chunk_index": 39,
    "total_chunks": 55,
    "text_content": "pulmonary trunk. Can thisexplain the symptoms? What is this variant called?The description of the right coronary artery running between the aorta and the pulmonarytrunk suggests a condition called \u201cAnomalous Origin of the Right Coronary Artery from theLeft Sinus \u201d(ARCAPA). However, the description of the coronary artery running between theaorta and the pulmonary trunk more closely matches another anomaly known as interarterial course or \u201ccoronary artery anomaly with interarterial course \u201d. In th",
    "full_text_length": 57535,
    "chunk_length": 1300
  },
  {
    "chunk_id": 3019,
    "paper_filename": "robert_2024_the_virtual_reference_radiologist.pdf",
    "paper_title": "Robert 2024 The Virtual Reference Radiologist",
    "chunk_index": 40,
    "total_chunks": 55,
    "text_content": "Left hepatic cholestasis and a gallstone can be identi \ufb01ed. What are the most likely diagnoses?[\u2026] A benign tumor such as a hemangioma or a liver adenoma would also be possible. The speci \ufb01c density values (HU) might suggest a lipoma or fatty degeneration.Lipoma or fatty liver lesions are suggested based on speci \ufb01c density values. The prompt indicates 20-30 Houns \ufb01eld Units, while fat has approximately -120 to -90 Houns \ufb01eld Units. [CT] What diseases should I consider with the following case de",
    "full_text_length": 57535,
    "chunk_length": 1350
  },
  {
    "chunk_id": 3020,
    "paper_filename": "robert_2024_the_virtual_reference_radiologist.pdf",
    "paper_title": "Robert 2024 The Virtual Reference Radiologist",
    "chunk_index": 41,
    "total_chunks": 55,
    "text_content": "pulmonary infections since childhood.The CT scan shows an almost non-aerated lungsegment that is supplied by an artery from theceliac trunk. What are the three most likelydifferential diagnoses?Differential diagnoses include [ \u2026] Bronchiectasis [ \u2026] Chronic Obstructive Pulmonary Disease (COPD) [ \u2026]Bronchiectasis and COPD are provided as differential diagnoses. However, they do notpresent with an unventilated lung segmentsupplied by an artery of the celiac trunk (asstated in the prompt). [CT] Wha",
    "full_text_length": 57535,
    "chunk_length": 1415
  },
  {
    "chunk_id": 3021,
    "paper_filename": "robert_2024_the_virtual_reference_radiologist.pdf",
    "paper_title": "Robert 2024 The Virtual Reference Radiologist",
    "chunk_index": 42,
    "total_chunks": 55,
    "text_content": "Adenoma: In an MRI, a liver adenoma often appears less intense in T1-weighted sequences, while it looks more intense in T2-weighted sequences. After contrast agentinjection, adenomas take up the contrast agentquickly in the early phase and then wash out inthe later phases, causing them to appear lessintense in later phases. [ \u2026]Hepatic adenoma is described as hypointense (T1-weighted images) and as displaying washout in the venous phase. Usually, hepaticadenomas are mildly hypointense orhyperint",
    "full_text_length": 57535,
    "chunk_length": 1406
  },
  {
    "chunk_id": 3022,
    "paper_filename": "robert_2024_the_virtual_reference_radiologist.pdf",
    "paper_title": "Robert 2024 The Virtual Reference Radiologist",
    "chunk_index": 43,
    "total_chunks": 55,
    "text_content": "\u2026] 3) Adenomyosis [ \u2026] 4) Cysts or masses in the pelvic area [ \u2026] 5) Interstitial Cystitis (Painful Bladder Syndrome) [ \u2026]6 ) Uterine prolapse [ \u2026]Endometriosis, adenomyosis, interstitial cystitis, and uterine prolapse are not primarilyassociated with dilated veins in the pelvis (asstated in the prompt). [MRI] What hepatic space-occupying lesions are there, how often do they occur in relatively young women, and what do they look like on an MRIscan?[\u2026] Focal Nodular Hyperplasias (FNH): These are ",
    "full_text_length": 57535,
    "chunk_length": 1302
  },
  {
    "chunk_id": 3023,
    "paper_filename": "robert_2024_the_virtual_reference_radiologist.pdf",
    "paper_title": "Robert 2024 The Virtual Reference Radiologist",
    "chunk_index": 44,
    "total_chunks": 55,
    "text_content": "within the effusion. Also, a septated cystic lesion is seen at the back of the knee. Whatare the three most likely differential diagnoses?[\u2026] Baker \u2019s Cyst: This is a \ufb02uid-\ufb01lled cyst that forms behind the knee joint and is typicallyassociated with knee joint effusions. It can be visible on MRI and may appear as a dark area within the effusion.Baker \u2019s cysts are usually hyperintense in T2- weighted images and hypointense in T1-weighted images and located in the popliteal fossa while communicating",
    "full_text_length": 57535,
    "chunk_length": 1294
  },
  {
    "chunk_id": 3024,
    "paper_filename": "robert_2024_the_virtual_reference_radiologist.pdf",
    "paper_title": "Robert 2024 The Virtual Reference Radiologist",
    "chunk_index": 45,
    "total_chunks": 55,
    "text_content": "was described as a \u201cspecial MRI technique \u201d. [Angiography] What does a liver hemangioma look like in the angiogram, and how can I differentiate itfrom focal nodular hyperplasia or other liver celltumors like hepatocellular carcinoma?[\u2026] In contrast, focal nodular hyperplasia (FNH) demonstrates a homogeneous contrastenhancement in the arterial phase with rapidwashout of the contrast in the portal venousand delayed phase, leading to an isointenseappearance compared to the surroundingnormal tissue.",
    "full_text_length": 57535,
    "chunk_length": 1345
  },
  {
    "chunk_id": 3025,
    "paper_filename": "robert_2024_the_virtual_reference_radiologist.pdf",
    "paper_title": "Robert 2024 The Virtual Reference Radiologist",
    "chunk_index": 46,
    "total_chunks": 55,
    "text_content": "the liver could cause fever and elevated infectionparameters. In an angiogram, an abscess wouldappear as a well-de \ufb01ned lesion with possible central necrosis or \ufb02uid collection.Abscess may not be directly visualized in an angiogram. Peripheral hyperemia may be seen. [Angiography] What arteries are in the thigh, and how do they run? How do they branch?[\u2026] The femoral artery runs along the front of the thigh and gives off several branches:Profunda femoris artery (deep artery of thethigh). [ \u2026] Aft",
    "full_text_length": 57535,
    "chunk_length": 1361
  },
  {
    "chunk_id": 3026,
    "paper_filename": "robert_2024_the_virtual_reference_radiologist.pdf",
    "paper_title": "Robert 2024 The Virtual Reference Radiologist",
    "chunk_index": 47,
    "total_chunks": 55,
    "text_content": "redacted for conciseness, readability, and comprehensionSiepmann et al .European Radiology (2024) 34:6652 \u20136666 6664 reproduced by future versions, given the rapid evolution and undulating performance of different chatGPT ver-sions [ 21]. Sixth, the GPT-4 utilization rates and patterns of general and subspecialty radiologists remain to be studied. Seventh, the LLM \u2019s response is closely related to how it is prompted [ 22]. If and how the quality of GPT-4 \u2019s responses and the frequency of its hal",
    "full_text_length": 57535,
    "chunk_length": 1366
  },
  {
    "chunk_id": 3027,
    "paper_filename": "robert_2024_the_virtual_reference_radiologist.pdf",
    "paper_title": "Robert 2024 The Virtual Reference Radiologist",
    "chunk_index": 48,
    "total_chunks": 55,
    "text_content": "mixed-effect model improved the model \u2019s inter- pretability, yet at the cost of reduced complexity. Inprioritizing the reduction of variables to mitigate the risk of multicollinearity, we aimed to improve clarity and comprehensibility; yet, this approach may have over-simpli \ufb01ed the complex interplay of factors affecting diagnostic accuracy and con \ufb01dence. Conclusion In conclusion, our study suggests that GPT-4 is a clini- cally useful adjunct tool that improves diagnostic accuracy slightly and ",
    "full_text_length": 57535,
    "chunk_length": 1506
  },
  {
    "chunk_id": 3028,
    "paper_filename": "robert_2024_the_virtual_reference_radiologist.pdf",
    "paper_title": "Robert 2024 The Virtual Reference Radiologist",
    "chunk_index": 49,
    "total_chunks": 55,
    "text_content": "version contains supplementary material available at https://doi. org/10.1007/s00330-024-10727-2 . Acknowledgements Following the COPE (Committee on Publication Ethics) position statement of February 13th, 2023 ( https://publicationethics.org/cope-position-statements/ ai-author ), the authors hereby disclose the use of the following arti \ufb01cial intelligence models during the writing of this article. GPT-4 (OpenAI) forchecking spelling and grammar. Funding This study has received funding as follow",
    "full_text_length": 57535,
    "chunk_length": 1492
  },
  {
    "chunk_id": 3029,
    "paper_filename": "robert_2024_the_virtual_reference_radiologist.pdf",
    "paper_title": "Robert 2024 The Virtual Reference Radiologist",
    "chunk_index": 50,
    "total_chunks": 55,
    "text_content": "to the subject matter of the article. Statistics and biometry One of the authors has signi \ufb01cant statistical expertise. Informed consent Only if the study is on human subjects: Written informed consent was waivedby the Institutional Review Board. Ethical approval Institutional Review Board approval was obtained. Study subjects or cohorts overlap None of the study subjects or cohorts have been previously reported. Methodology \u25cfRetrospective \u25cfDiagnostic or prognostic study \u25cfPerformed at one instit",
    "full_text_length": 57535,
    "chunk_length": 1317
  },
  {
    "chunk_id": 3030,
    "paper_filename": "robert_2024_the_virtual_reference_radiologist.pdf",
    "paper_title": "Robert 2024 The Virtual Reference Radiologist",
    "chunk_index": 51,
    "total_chunks": 55,
    "text_content": "cases for ChatGPT in radi- ology reporting. AJR Am J Roentgenol 221:373 \u2013376 4. Adams LC, Truhn D, Busch F et al (2023) Leveraging GPT-4 for post hoc transformation of free-text radiology reports into structured reporting: amultilingual feasibility study. Radiology 307:e230725 5. Rao A, Kim J, Kamineni M, Pang M, Lie W, Succi MD (2023) Evaluating ChatGPT as an adjunct for radiologic decision-making. medR-xiv:2023.2002. 2002.23285399 6. Nori H, King N, McKinney SM, Carignan D, Horvitz E (2023) Ca",
    "full_text_length": 57535,
    "chunk_length": 1392
  },
  {
    "chunk_id": 3031,
    "paper_filename": "robert_2024_the_virtual_reference_radiologist.pdf",
    "paper_title": "Robert 2024 The Virtual Reference Radiologist",
    "chunk_index": 52,
    "total_chunks": 55,
    "text_content": "ethical considerations,risks, and future directions. Diagn Interv Radiol 30:80 \u201390 10. Bera K, O \u2019Connor G, Jiang S, Tirumani SH, Ramaiya N (2023) Analysis of ChatGPT publications in radiology: literature so far. Curr Probl Diagn Radiol 53:215 \u2013225Siepmann et al .European Radiology (2024) 34:6652 \u20136666 6665 11. Suthar PP, Kounsal A, Chhetri L, Saini D, Dua SG (2023) Arti \ufb01cial Intelli- gence (AI) in Radiology: A Deep Dive Into ChatGPT 4.0 \u2019s Accuracy with the American Journal of Neuroradiology \u2019",
    "full_text_length": 57535,
    "chunk_length": 1285
  },
  {
    "chunk_id": 3032,
    "paper_filename": "robert_2024_the_virtual_reference_radiologist.pdf",
    "paper_title": "Robert 2024 The Virtual Reference Radiologist",
    "chunk_index": 53,
    "total_chunks": 55,
    "text_content": "al (2022) Faster and Better: How Anomaly Detection Can Accelerate and Improve Reporting of Head Computed Tomography. Diagnostics 12:452 15. Faul F, Erdfelder E, Lang AG, Buchner A (2007) G*Power 3: a \ufb02exible statistical power analysis program for the social, behavioral, and biome-dical sciences. Behav Res Methods 39:175 \u2013191 16. Kung TH, Cheatham M, Medenilla A et al (2023) Performance of ChatGPT on USMLE: Potential for AI-assisted medical education using large lan- guage models. PLoS Digital He",
    "full_text_length": 57535,
    "chunk_length": 1288
  },
  {
    "chunk_id": 3033,
    "paper_filename": "robert_2024_the_virtual_reference_radiologist.pdf",
    "paper_title": "Robert 2024 The Virtual Reference Radiologist",
    "chunk_index": 54,
    "total_chunks": 55,
    "text_content": "Engl J Med 388:1233 \u20131239 20. Lightman H, Kosaraju V, Burda Y et al (2023) Let \u2019s Verify Step by Step. arXiv:230520050 https://doi.org/10.48550/arXiv.2305.20050 21. Chen L, Zaharia M, Zou J (2023) How is ChatGPT \u2019s behavior changing over time? arXiv:230709009 https://doi.org/10.48550/arXiv.2307.09009 22. White J, Fu Q, Hays S et al (2023) A prompt pattern catalog to enhance prompt engineering with chatgpt. arXiv:230211382 https://doi.org/10. 48550/arXiv.2302.11382 Publisher \u2019s Note Springer Natu",
    "full_text_length": 57535,
    "chunk_length": 665
  },
  {
    "chunk_id": 3034,
    "paper_filename": "ruixin_2024_application_of_deep_learning_for_breast_ultrasound.pdf",
    "paper_title": "Ruixin 2024 Application Of Deep Learning For Breast Ultrasound",
    "chunk_index": 0,
    "total_chunks": 53,
    "text_content": "Journal Pre-proof Application of deep learning on automated breast ultrasound: Current developments, challenges, and opportunities Ruixin Wang, Zhiyuan Wang, Yuanming Xiao, Xiaohui Liu, Guoping Tan, Jun Liu PII: S2950-1628(25)00006-2 DOI: https://doi.org/10.1016/j.metrad.2025.100138 Reference: METRAD 100138 To appear in: Meta-Radiology Received Date: 16 January 2025 Revised Date: 19 February 2025 Accepted Date: 13 March 2025 Please cite this article as: Wang R, Wang Z, Xiao Y, Liu X, Tan G, Liu ",
    "full_text_length": 53099,
    "chunk_length": 1384
  },
  {
    "chunk_id": 3035,
    "paper_filename": "ruixin_2024_application_of_deep_learning_for_breast_ultrasound.pdf",
    "paper_title": "Ruixin 2024 Application Of Deep Learning For Breast Ultrasound",
    "chunk_index": 1,
    "total_chunks": 53,
    "text_content": "early visibility of the article. Please note that, during the production process, errors may be discovered which could affect the content, and all legal disclaimers that apply to the journal pertain. \u00a9 2025 The Authors. Publishing services by Elsevier B.V. on behalf of KeAi Communications Co. Ltd. Journal Pre-proof Application of deep learning on automated breast ultrasound: Current developments , challenges , and opportunities Ruixin Wang 1, Zhiyuan Wang 2*, Yuanming Xiao 3, Xiaohui Liu 4, Guop",
    "full_text_length": 53099,
    "chunk_length": 1519
  },
  {
    "chunk_id": 3036,
    "paper_filename": "ruixin_2024_application_of_deep_learning_for_breast_ultrasound.pdf",
    "paper_title": "Ruixin 2024 Application Of Deep Learning For Breast Ultrasound",
    "chunk_index": 2,
    "total_chunks": 53,
    "text_content": "Hohai University, N anjing, China; gptan@hhu.edu.cn 6Clinical Research Center for Medical Imaging in Hunan Province, Changsha, China 7Department of Radiology Quality Control Center in Hunan Province, Changsha, China *Correspondence: wangzhiyuan@hnca.org.cn (Z.W.); junliu123@c su.edu.cn (J.L.) Abstract: Breast cancer is a major disease threatening the health of women worldwide. The advent of automated breast ultrasound (ABUS) has provided new possibilities for the early screening and diagnosis of",
    "full_text_length": 53099,
    "chunk_length": 1428
  },
  {
    "chunk_id": 3037,
    "paper_filename": "ruixin_2024_application_of_deep_learning_for_breast_ultrasound.pdf",
    "paper_title": "Ruixin 2024 Application Of Deep Learning For Breast Ultrasound",
    "chunk_index": 3,
    "total_chunks": 53,
    "text_content": "comprehensive review of its applications in ABUS is still lacking. This paper provides a detailed analysis of the latest advancements, existing challenges, and future research opportunities in this rapidly evolving fie ld. Keywords: Automated breast ultrasound; Breast cancer; Deep learning; Computer -aided diagnosis 1 Introduction Breast cancer is the most common and deadliest cancer among women worldwide. As shown in Figure 1, according to the latest global cancer statistics by the Internationa",
    "full_text_length": 53099,
    "chunk_length": 1379
  },
  {
    "chunk_id": 3038,
    "paper_filename": "ruixin_2024_application_of_deep_learning_for_breast_ultrasound.pdf",
    "paper_title": "Ruixin 2024 Application Of Deep Learning For Breast Ultrasound",
    "chunk_index": 4,
    "total_chunks": 53,
    "text_content": "Pre-proof mammography (X -ray imaging of the breast), and magnetic resonance imaging (MRI) are currently the most widely utilized imaging modalities for breast evaluation, each with distinct advantages and limitations. The advantages of HHUS include the absence of radiation, real -time imaging capabilities, and relatively high specificity. However, it also has notable drawbacks: it p roduces two- dimensional ( 2D) images, which can lead to missed diagnoses; the image acquisition process lacks st",
    "full_text_length": 53099,
    "chunk_length": 1394
  },
  {
    "chunk_id": 3039,
    "paper_filename": "ruixin_2024_application_of_deep_learning_for_breast_ultrasound.pdf",
    "paper_title": "Ruixin 2024 Application Of Deep Learning For Breast Ultrasound",
    "chunk_index": 5,
    "total_chunks": 53,
    "text_content": "Compared to HHUS , ABUS provides automated, consistent imaging and 3D reconstruction, standardizing the ultrasound image acquisition process[2]. The 3D reconstructed images help reduce missed detection s, while ABUS also alleviate s operator fatigue and enhances examination efficiency . Furthermore, ABUS is particularly advantageous for high-risk or dense -breast patients, providing a more comprehensive and accurate evaluation compared to HHUS. Therefore, ABUS offers new opportunities for breast",
    "full_text_length": 53099,
    "chunk_length": 1357
  },
  {
    "chunk_id": 3040,
    "paper_filename": "ruixin_2024_application_of_deep_learning_for_breast_ultrasound.pdf",
    "paper_title": "Ruixin 2024 Application Of Deep Learning For Breast Ultrasound",
    "chunk_index": 6,
    "total_chunks": 53,
    "text_content": "been proposed to support the CAD across various imag ing modalities , with progress also made in relation to ABUS. However, due to the limited adoption and the unique imaging characteristic of ABUS, targeted efforts are Journal Pre-proof necessary to develop specialized DL model s for this modality . This article aims to provide a comprehensive analysis of the current state of DL for ABUS in the context of CAD , with a particular focus on DL methodolog ies. Additionally, the challenges encounter",
    "full_text_length": 53099,
    "chunk_length": 1236
  },
  {
    "chunk_id": 3041,
    "paper_filename": "ruixin_2024_application_of_deep_learning_for_breast_ultrasound.pdf",
    "paper_title": "Ruixin 2024 Application Of Deep Learning For Breast Ultrasound",
    "chunk_index": 7,
    "total_chunks": 53,
    "text_content": "breast area . (2) Initial set tings: The technician adjusts the imaging depth according to the patient \u2019s breast size and selects the optimal scanning position. (3) Automated scanning: The technician activates the scanning switch, and the probe is automatically moved by a motor, performing continuous scans of multiple axial slices. (4) Data processing: Once the scanning is complete, the ABUS system reconstructs the acquired 2D axial images into two additional planes , i.e., the coronal and sagit",
    "full_text_length": 53099,
    "chunk_length": 1264
  },
  {
    "chunk_id": 3042,
    "paper_filename": "ruixin_2024_application_of_deep_learning_for_breast_ultrasound.pdf",
    "paper_title": "Ruixin 2024 Application Of Deep Learning For Breast Ultrasound",
    "chunk_index": 8,
    "total_chunks": 53,
    "text_content": "plane refers to a transverse slice of the breast and is crucial for examining the tissue structure and evaluat ing the imaging characteristics of breast masses. The coronal plane is a slice perpendicular to the chest surface, offering a comprehensive view of the breast \u2019s structure. This plane is particularly useful for observing the anterior -posterior distribution of breast tissue and asses sing the relationship between breast masses and surrounding glandular tissue, revealing key signs such a",
    "full_text_length": 53099,
    "chunk_length": 1286
  },
  {
    "chunk_id": 3043,
    "paper_filename": "ruixin_2024_application_of_deep_learning_for_breast_ultrasound.pdf",
    "paper_title": "Ruixin 2024 Application Of Deep Learning For Breast Ultrasound",
    "chunk_index": 9,
    "total_chunks": 53,
    "text_content": "The images were acquired with a GE Invenia ABUS 2.0. The acquisition of ABUS images typically involves multiple standard scanning positions to ensure compr ehensive coverage of the whole breast. As shown in Figure 4, t he most common scanning positions are AP (Anterior -Posterior), LAT (Lateral), and MED (Medial). In the AP position, the probe scans from the front, primarily visualizing the anterior and upper a reas of the breast. In the LAT position , the probe scans from the lateral (outer) si",
    "full_text_length": 53099,
    "chunk_length": 1205
  },
  {
    "chunk_id": 3044,
    "paper_filename": "ruixin_2024_application_of_deep_learning_for_breast_ultrasound.pdf",
    "paper_title": "Ruixin 2024 Application Of Deep Learning For Breast Ultrasound",
    "chunk_index": 10,
    "total_chunks": 53,
    "text_content": "literature search across several databases, including PubMed, IEEE Xplore, Web of Science , and Google Scholar, to systematically identify relevant studies. The search utilized keywords such as \u201cdeep learning and automated breast ultrasound \u201d, \u201cautomated breast ultrasound \u201d, \u201cdeep learning and ABUS \u201d, and \u201cABUS \u201d to cover a broad range of research focused on the application of DL techniques in ABUS. We limited our search to papers published up to January 2025, to incl ude the most current and re",
    "full_text_length": 53099,
    "chunk_length": 1291
  },
  {
    "chunk_id": 3045,
    "paper_filename": "ruixin_2024_application_of_deep_learning_for_breast_ultrasound.pdf",
    "paper_title": "Ruixin 2024 Application Of Deep Learning For Breast Ultrasound",
    "chunk_index": 11,
    "total_chunks": 53,
    "text_content": "Journal Pre-proof 4 Results Table I presents an overview of the reviewed literature , where breast lesion classification, segmentation, and detection are the most widely applied areas of DL in ABUS. As shown in Figure 5, breast lesion classification is to categorize lesions (such as lumps or nodules) based on their features, usually into benign or malignant categories. Classification typically rel ies on imaging features such as shape, boundary, and echogenicity to make th e determination. Breas",
    "full_text_length": 53099,
    "chunk_length": 1315
  },
  {
    "chunk_id": 3046,
    "paper_filename": "ruixin_2024_application_of_deep_learning_for_breast_ultrasound.pdf",
    "paper_title": "Ruixin 2024 Application Of Deep Learning For Breast Ultrasound",
    "chunk_index": 12,
    "total_chunks": 53,
    "text_content": "Data volume Performance Lesion classification Wang et al.[4] CNN, Transfer learning 263 patients (316 breast lesions ) Sensitivity: 88.6% Specificity: 87.6% AUC: 0.9468 Hejduk et al.[5] CNN 113 patients (55 high malignancy probability) Accuracy: 90.9% AUC: 0.91 Kim et al.[6] Weak supervision, 3D CNN 363 patients (434 mass lesions) Sensitivity: 87.75% Specificity: 93.75% AUC: 0.9491. Lesion segmentation Cao et al.[7] U-Net, Dilated convolution, Focus loss 107 patients (170 volumes) DSC: 0.6902 Ja",
    "full_text_length": 53099,
    "chunk_length": 1341
  },
  {
    "chunk_id": 3047,
    "paper_filename": "ruixin_2024_application_of_deep_learning_for_breast_ultrasound.pdf",
    "paper_title": "Ruixin 2024 Application Of Deep Learning For Breast Ultrasound",
    "chunk_index": 13,
    "total_chunks": 53,
    "text_content": "124 patients (181 tumor instances) Sensitivity: 90% FP: 7.42 per volume Zhang et al.[13] YOLOv4, Monte Carlo Dropout 124 patients (170 volumes) Sensitivity: 88% FP: 0.19 per slice Zhang et al.[14] YOLOv5, 3D ResNet, Transformer encoder 741 patients (3114 breast lesions) Detection rate: 71.2% Oh et al.[15] Faster R -CNN, U-Net 28 patients (168 scans) Sensitivity: 93.65% FP: 8.6 per examination Li et al.[16] YOLOX, 124 patients Sensitivity: 90% Journal Pre-proof Deep mutual learning (238 volumes) ",
    "full_text_length": 53099,
    "chunk_length": 1277
  },
  {
    "chunk_id": 3048,
    "paper_filename": "ruixin_2024_application_of_deep_learning_for_breast_ultrasound.pdf",
    "paper_title": "Ruixin 2024 Application Of Deep Learning For Breast Ultrasound",
    "chunk_index": 14,
    "total_chunks": 53,
    "text_content": "a multi -view convolutional neural network (CNN) to address the multi -view nature of ABUS images and used transfer learning[19] for the classification of benign and malignant lesion s in ABUS images. After training and evaluating the model on 316 breast lesions (135 malignant and 181 benign), the method achieved an are a under the curve (AUC) value of 0.9468, with sensitivity and specificity reaching 88 .6% and 87 .6%, respectively. Furthermore, observer performance testing demonstrated that, b",
    "full_text_length": 53099,
    "chunk_length": 1271
  },
  {
    "chunk_id": 3049,
    "paper_filename": "ruixin_2024_application_of_deep_learning_for_breast_ultrasound.pdf",
    "paper_title": "Ruixin 2024 Application Of Deep Learning For Breast Ultrasound",
    "chunk_index": 15,
    "total_chunks": 53,
    "text_content": "4/5 categories . For the region of interest (ROI) determined using a sliding window, Model 1 is first applied to classify background or breast tissue . Next, Model 2 categorizes the tissue as either normal or lesioned , and finally, Model 3 is employed for benign or malignant classification. The study utilized 645 ABUS scans from 113 patients for training and validation. The results showed that the accuracy for single image classification containing lesions was 79.7%, with an AUC of 0.91. On the",
    "full_text_length": 53099,
    "chunk_length": 1329
  },
  {
    "chunk_id": 3050,
    "paper_filename": "ruixin_2024_application_of_deep_learning_for_breast_ultrasound.pdf",
    "paper_title": "Ruixin 2024 Application Of Deep Learning For Breast Ultrasound",
    "chunk_index": 16,
    "total_chunks": 53,
    "text_content": "the tumor feature information from radiology reports to generate a template mask, thereby enhan cing the performance of the existing neural network classifier without requiring additional annotations. Experimental results demonstrated that the branch network with the template mask significantly improved classification accuracy. On a dataset comprisin g 363 patients (286 with 434 BI -RADS 2 or higher tumor lesions and 77 without tumor lesions), the network achieved a sensitivity of 87.75%, a spec",
    "full_text_length": 53099,
    "chunk_length": 1353
  },
  {
    "chunk_id": 3051,
    "paper_filename": "ruixin_2024_application_of_deep_learning_for_breast_ultrasound.pdf",
    "paper_title": "Ruixin 2024 Application Of Deep Learning For Breast Ultrasound",
    "chunk_index": 17,
    "total_chunks": 53,
    "text_content": "dense blocks of D\u00b2U -Net. This design allows the network to handle mass features at different scales, enhancing its ability to learn effectively from small medical image datasets. A dditionally, the proposed uncertainty focus loss helps increase the model \u2019s attention on uncertain predictions, thereby improving the segmentation precision of blurry mass boundaries. The method was evaluated on a dataset of 170 volumes from 107 patients, with the experimental results show ing a Dice Similarity Coef",
    "full_text_length": 53099,
    "chunk_length": 1321
  },
  {
    "chunk_id": 3052,
    "paper_filename": "ruixin_2024_application_of_deep_learning_for_breast_ultrasound.pdf",
    "paper_title": "Ruixin 2024 Application Of Deep Learning For Breast Ultrasound",
    "chunk_index": 18,
    "total_chunks": 53,
    "text_content": "module with SC - attention captures the inter -slice relationships, leveraging contextual information from the 3D data to assist segmentation and reduce false positives. The method was evaluated on a private ABUS dataset containing 124 patients and 170 volumes, achieving a DSC of 0.8178 , a recall of 80.67% , and a precision of 82.92%. To address the challenges faced in breast tumor segmentation from ABUS images, such as low image quality, speckle noise, shadowing effects, and low contrast, Lei ",
    "full_text_length": 53099,
    "chunk_length": 1293
  },
  {
    "chunk_id": 3053,
    "paper_filename": "ruixin_2024_application_of_deep_learning_for_breast_ultrasound.pdf",
    "paper_title": "Ruixin 2024 Application Of Deep Learning For Breast Ultrasound",
    "chunk_index": 19,
    "total_chunks": 53,
    "text_content": "cross -validation and 30 cases for independent testing. The experi mental results demonstrated that, compared to manually delineated contours, the proposed method achieved an average DSC of 0.85\u00b10.10 in cross -validation, and 0.82\u00b10.15 in an independent test set. To address the challenges of lesion diversity, significant i maging artifacts , and blurred lesion boundaries , Cheng et al.[10] proposed a deepest semantically guided multi-scale feature fusion network (DSGMFFN). This method ensures th",
    "full_text_length": 53099,
    "chunk_length": 1348
  },
  {
    "chunk_id": 3054,
    "paper_filename": "ruixin_2024_application_of_deep_learning_for_breast_ultrasound.pdf",
    "paper_title": "Ruixin 2024 Application Of Deep Learning For Breast Ultrasound",
    "chunk_index": 20,
    "total_chunks": 53,
    "text_content": "Intersection over Union (IoU) of 0.7324. Luo et al.[11] addressed the challenge of balanc ing high sensitivity with low false positive s in existing CAD systems by designing and optimizing a CNN based on a 3D U -Net architecture to improve lesion segmentation efficiency and accuracy in ABUS images. The network incorporated optimization strategies such as densely deep supervision (DDS) mechanism and threshold map (TM) , and was further improved by utilizing a pre -trained C3D[26] model. The study",
    "full_text_length": 53099,
    "chunk_length": 1258
  },
  {
    "chunk_id": 3055,
    "paper_filename": "ruixin_2024_application_of_deep_learning_for_breast_ultrasound.pdf",
    "paper_title": "Ruixin 2024 Application Of Deep Learning For Breast Ultrasound",
    "chunk_index": 21,
    "total_chunks": 53,
    "text_content": "For lesions larger than 1 cm3, the sensitivity reached 98%, while for lesions less than or equal to 1 cm3, the sensitivity was 87%. Additionally, the study revealed no significant impact of breast tissue composit ion, lesion morphology, or echogenic patterns on sensitivity and false positive rates. 4.3 Lesion detection Compared to lesion segmentation, lesion detection in ABUS images focuses more on locating the lesion rather than delineating its exact boundaries. Li et al.[12] proposed a 3D tumo",
    "full_text_length": 53099,
    "chunk_length": 1348
  },
  {
    "chunk_id": 3056,
    "paper_filename": "ruixin_2024_application_of_deep_learning_for_breast_ultrasound.pdf",
    "paper_title": "Ruixin 2024 Application Of Deep Learning For Breast Ultrasound",
    "chunk_index": 22,
    "total_chunks": 53,
    "text_content": "the issue of positional differences and scoring inconsistencies in 2D detection results, the authors proposed a rescoring algorithm and constructed a 3D volume -forming mode l. This model differentiates real tumor volumes from false -positive areas and further eliminates false positives based on slice length and average area. The method was tested on an ABUS dataset consisting of 124 patients with 181 tumor instances. Experimental results showed a sensitivity of 90%, 85%, 80%, 75%, and 70% under",
    "full_text_length": 53099,
    "chunk_length": 1296
  },
  {
    "chunk_id": 3057,
    "paper_filename": "ruixin_2024_application_of_deep_learning_for_breast_ultrasound.pdf",
    "paper_title": "Ruixin 2024 Application Of Deep Learning For Breast Ultrasound",
    "chunk_index": 23,
    "total_chunks": 53,
    "text_content": "approach enhanced the de tection rate of challenging tumor regions while reduc ing false positives. The method was tested on a private dataset containing 170 ABUS tumor volumes, and the results showed a sensitivity of 88% and a false positive rate of 0.19 per slice, outperforming t he original YOLOv4 and other mainstream object detectors. Zhang et al.[14] developed an intelligent detection system called V -BUILDS (V olume -Breast Journal Pre-proof Ultrasound Intelligent Lesion Detection System) ",
    "full_text_length": 53099,
    "chunk_length": 1303
  },
  {
    "chunk_id": 3058,
    "paper_filename": "ruixin_2024_application_of_deep_learning_for_breast_ultrasound.pdf",
    "paper_title": "Ruixin 2024 Application Of Deep Learning For Breast Ultrasound",
    "chunk_index": 24,
    "total_chunks": 53,
    "text_content": "of 71.2%. Specifically, it demonstrated high detection rates of 96.5% for BI -RADS 4/5 lesions and 95.8% for malignant tumors. H owever, the detection performance for small lesions (<10 mm) and benign lesions classified as BI -RADS 2 or 3 remains suboptimal. Oh et al.[15] proposed a 3D breast nodule detection system to address the challenge of quickly interpreting the large volume of image data generated by ABUS . Their method combin es the Faster R-CNN[32] detector with U -Net[20]. This approac",
    "full_text_length": 53099,
    "chunk_length": 1258
  },
  {
    "chunk_id": 3059,
    "paper_filename": "ruixin_2024_application_of_deep_learning_for_breast_ultrasound.pdf",
    "paper_title": "Ruixin 2024 Application Of Deep Learning For Breast Ultrasound",
    "chunk_index": 25,
    "total_chunks": 53,
    "text_content": "per examination. Li et al.[16] proposed the DML -YOLOX model , which is based on the YOLOX detector[33] and deep mutual learning (DML)[34], to address the issues of false positives and f alse negatives in tumor detection in ABUS images. To tackle the problem of overconfidence in single -model predictions, the method introduces exploration loss and consistency loss to enhance collaborative learning between two parallel networks. Additionally , to resolve the issue of overestimating bounding box s",
    "full_text_length": 53099,
    "chunk_length": 1337
  },
  {
    "chunk_id": 3060,
    "paper_filename": "ruixin_2024_application_of_deep_learning_for_breast_ultrasound.pdf",
    "paper_title": "Ruixin 2024 Application Of Deep Learning For Breast Ultrasound",
    "chunk_index": 26,
    "total_chunks": 53,
    "text_content": "model achieved a sensitivity of 90 % and a false po sitive rate of 0.15 per slice. 4.4 Other Applications Besides the above DL applications for lesion diagnosis, Lei et al .[17] investigated breast anatomical layer segmentation in ABUS images. To address issues such as inherent speckle noise, posterior acoustic shadows, and boundary -blurring caused by overlapping echogenicity spectra between different layers, they designed a boundary -regularized convolutional encoder -decoder network (ConvEDNe",
    "full_text_length": 53099,
    "chunk_length": 1388
  },
  {
    "chunk_id": 3061,
    "paper_filename": "ruixin_2024_application_of_deep_learning_for_breast_ultrasound.pdf",
    "paper_title": "Ruixin 2024 Application Of Deep Learning For Breast Ultrasound",
    "chunk_index": 27,
    "total_chunks": 53,
    "text_content": "excelled in segmenting four anatomical structures: subcutaneous fat, breast parenchyma, muscle, and chest wall. The method effectively reduced false positives, thereby improving th e efficiency of clinical image Journal Pre-proof interpretation. Huang et al.[18] developed both a single -task model and a multi -task model based on 3D ResNet to non invasively estimate three breast cancer biomarkers \u2014estrogen receptor (ER), progesterone receptor (PR), and human epidermal growth factor receptor 2 (H",
    "full_text_length": 53099,
    "chunk_length": 1279
  },
  {
    "chunk_id": 3062,
    "paper_filename": "ruixin_2024_application_of_deep_learning_for_breast_ultrasound.pdf",
    "paper_title": "Ruixin 2024 Application Of Deep Learning For Breast Ultrasound",
    "chunk_index": 28,
    "total_chunks": 53,
    "text_content": "testing, the single -task and multi - task models achieved AUCs of 0.809 and 0.735 for ER, 0.688 and 0.767 for PR, and 0.626 and 0.697 for HER2, respectively. In the overall evaluation, the multi -task model outperformed the single -task model, achieving a higher macro AUC of 0.733 compared to 0.708 for the single -task model. 5 Discussions and conclusions 5.1 Challenges and suggestions 5.1.1 Limited patient volume In existing research, the number of patients involved in training DL models for A",
    "full_text_length": 53099,
    "chunk_length": 1281
  },
  {
    "chunk_id": 3063,
    "paper_filename": "ruixin_2024_application_of_deep_learning_for_breast_ultrasound.pdf",
    "paper_title": "Ruixin 2024 Application Of Deep Learning For Breast Ultrasound",
    "chunk_index": 29,
    "total_chunks": 53,
    "text_content": "suffer from overfitting , resulting in limited generalization in clinical practice . Furthermore, the limited sample size can lead to data imbalance, where certain diseases (such as malignant tumors or small calcificati ons) have fewer samples, creating a large disparity in sample sizes across different types of lesions. This, in turn, affects the overall performance of the model and its ability to identify rare lesions. To address th is challenge , in addition to common te chniques such as data",
    "full_text_length": 53099,
    "chunk_length": 1377
  },
  {
    "chunk_id": 3064,
    "paper_filename": "ruixin_2024_application_of_deep_learning_for_breast_ultrasound.pdf",
    "paper_title": "Ruixin 2024 Application Of Deep Learning For Breast Ultrasound",
    "chunk_index": 30,
    "total_chunks": 53,
    "text_content": "of DL models in segmenting or detecting small lesions in ABUS images requires further improvement. The characteristics of small lesions, including boundaries, morphology, and grayscale variations, are typically subtle and Journal Pre-proof difficult to differentiate from surrounding tissue, especially in high -noise, low -resolution images. ABUS images generally exhibit lower resolution compared to other imaging techniques , such as magnetic resonance imaging (MRI) or computed tomography (CT) , ",
    "full_text_length": 53099,
    "chunk_length": 1397
  },
  {
    "chunk_id": 3065,
    "paper_filename": "ruixin_2024_application_of_deep_learning_for_breast_ultrasound.pdf",
    "paper_title": "Ruixin 2024 Application Of Deep Learning For Breast Ultrasound",
    "chunk_index": 31,
    "total_chunks": 53,
    "text_content": "in ea rly-stage malignant tumors, microcalcifications, or small nodules in breast tissue, where lesions often lack clear boundaries or distinctive features, resulting in their potential oversight or misinterpretation as normal tissue in ABUS images. To address t he limitations in detecting small lesions in ABUS, techniques from the field of computer vision for small object recognition[38] can be leveraged . For instance, multi -scale learning[39] can enhance the model \u2019s ability to capture lesio",
    "full_text_length": 53099,
    "chunk_length": 1386
  },
  {
    "chunk_id": 3066,
    "paper_filename": "ruixin_2024_application_of_deep_learning_for_breast_ultrasound.pdf",
    "paper_title": "Ruixin 2024 Application Of Deep Learning For Breast Ultrasound",
    "chunk_index": 32,
    "total_chunks": 53,
    "text_content": "-weights easily classified samples during training and directs the model\u2019s attention towards hard-to-classify small lesions , could be another remedy for reducing false negatives and false positives. 5.1.3 False positives Due to the inherent limitations of ultrasound imaging, the resolution of breast tissue is generally lower, particularly in patients with dense glandular tissue, which may result in the misinterpretation of normal tissue as lesions. Additionally , breast lesions exhibit a broad ",
    "full_text_length": 53099,
    "chunk_length": 1378
  },
  {
    "chunk_id": 3067,
    "paper_filename": "ruixin_2024_application_of_deep_learning_for_breast_ultrasound.pdf",
    "paper_title": "Ruixin 2024 Application Of Deep Learning For Breast Ultrasound",
    "chunk_index": 33,
    "total_chunks": 53,
    "text_content": "several technical improvements can be implemented , such as d evelop ing more advanced network architectures (e.g., Vision Mamba[42]) and more effecti ve loss functions (e.g., exploration loss and consistency loss[16]). Additionally, incorporating clinical background information , such as age, family history, and medical history, can aid in distinguish ing between benign and malignant lesions. For example, i n patients with specific clinical profile s, lesions are more likely to be benign, allow",
    "full_text_length": 53099,
    "chunk_length": 1358
  },
  {
    "chunk_id": 3068,
    "paper_filename": "ruixin_2024_application_of_deep_learning_for_breast_ultrasound.pdf",
    "paper_title": "Ruixin 2024 Application Of Deep Learning For Breast Ultrasound",
    "chunk_index": 34,
    "total_chunks": 53,
    "text_content": "been a significant bottleneck in the development of DL applications of ultrasound imaging . Compared to other imaging modalities such as X-ray and CT, ultrasound image datasets are relatively limited , and there are currently no publicly available datasets for ABUS. This scarcity of public datasets complicates meaningful comparisons across studies and methods. When researchers train and test their models using proprietary datasets , they may encounter variations in data quality, annotation stand",
    "full_text_length": 53099,
    "chunk_length": 1378
  },
  {
    "chunk_id": 3069,
    "paper_filename": "ruixin_2024_application_of_deep_learning_for_breast_ultrasound.pdf",
    "paper_title": "Ruixin 2024 Application Of Deep Learning For Breast Ultrasound",
    "chunk_index": 35,
    "total_chunks": 53,
    "text_content": "comparisons across different models , thereby promoting the standardization and advancement of technologies in this field. 5.2 Opportunities 5.2.1 Extending DL beyond lesion diagnosis With the ongoing advancement of technology, the potential for integrating ultrasound and DL in breast cancer diagnosis is being explored across multiple domains . For instance, HHUS , in addition to being used for breast lesion diagnosis, has been applied in predict ing lymph node metastasis[43][44], molecular subt",
    "full_text_length": 53099,
    "chunk_length": 1364
  },
  {
    "chunk_id": 3070,
    "paper_filename": "ruixin_2024_application_of_deep_learning_for_breast_ultrasound.pdf",
    "paper_title": "Ruixin 2024 Application Of Deep Learning For Breast Ultrasound",
    "chunk_index": 36,
    "total_chunks": 53,
    "text_content": "data and clinical indicators, present new opportunities for the deve lopment of personalized treatment strategies , enabling clinicians to make more precise decisions tailored to individual patient conditions. 5.2.2 Utilizing DL in more phase s DL can also be applied to other phase s of ABUS -based breast examination. For example, in the quality control of ABUS images, DL models can assist in the automat ed identification of issues during the ABUS acquisition process , such as verifying proper n",
    "full_text_length": 53099,
    "chunk_length": 1376
  },
  {
    "chunk_id": 3071,
    "paper_filename": "ruixin_2024_application_of_deep_learning_for_breast_ultrasound.pdf",
    "paper_title": "Ruixin 2024 Application Of Deep Learning For Breast Ultrasound",
    "chunk_index": 37,
    "total_chunks": 53,
    "text_content": "can enhance diagnostic accuracy for breast cancer. Consequently , the development of multimodal AI systems represents a key direction for future research. In addition, the integration of ABUS with other ultrasound imaging techniques, such as Doppler and elastography[51] also warrants further exploration. Multimodal DL models can not only improve the detection performance for breast diseases but also expand the range of applications, as previously discussed. 5.2.4 Exploring DL -based large model ",
    "full_text_length": 53099,
    "chunk_length": 1434
  },
  {
    "chunk_id": 3072,
    "paper_filename": "ruixin_2024_application_of_deep_learning_for_breast_ultrasound.pdf",
    "paper_title": "Ruixin 2024 Application Of Deep Learning For Breast Ultrasound",
    "chunk_index": 38,
    "total_chunks": 53,
    "text_content": "and efficiency of disease diagnosis . With ongoing technological advancements , these emerging approaches are poised to significantly elevate the clinical utility of ABUS, improving the precision of breast cancer screening , diagnosis , and treatment. Funding : This work was supported by National Natural Science Foundation of China (61971451, U22A20303), Innovative Province special construction foundation of Hunan Province (2019SK2131), the Science and Technology Innovation Program of Hunan Prov",
    "full_text_length": 53099,
    "chunk_length": 1406
  },
  {
    "chunk_id": 3073,
    "paper_filename": "ruixin_2024_application_of_deep_learning_for_breast_ultrasound.pdf",
    "paper_title": "Ruixin 2024 Application Of Deep Learning For Breast Ultrasound",
    "chunk_index": 39,
    "total_chunks": 53,
    "text_content": "579\u2013592. [3] Sharma, B. P., & Purwar, R. K. (2024). Computer -aided detection and diagnosis of breast cancer: a review. ADCAIJ: Advances in Distributed Computing a nd Artificial Intelligence Journal, 13, e31412 -e31412. Journal Pre-proof [4] Wang, Y ., Choi, E. J., Choi, Y ., Zhang, H., Jin, G. Y ., & Ko, S. B. (2020). Breast cancer classification in automated breast ultrasound using multiview convolutional neural network with transfer learni ng. Ultrasound in Medicine & Biology, 46(5), 1119 -11",
    "full_text_length": 53099,
    "chunk_length": 1247
  },
  {
    "chunk_id": 3074,
    "paper_filename": "ruixin_2024_application_of_deep_learning_for_breast_ultrasound.pdf",
    "paper_title": "Ruixin 2024 Application Of Deep Learning For Breast Ultrasound",
    "chunk_index": 40,
    "total_chunks": 53,
    "text_content": "breast ultrasound. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (pp. 3912 -3919). [7] Cao, X., Chen, H., Li, Y ., Peng, Y ., Wang, S., & Cheng, L. (2021). Dilated densely connected U-Net with uncertainty focus loss for 3D ABUS mass segmentation. Comp uter Methods and Programs in Biomedicine, 209, 106313. [8] Pan, P., Chen, H., Li, Y ., Cai, N., Cheng, L., & Wang, S. (2021). Tumor segmentation in automated whole breast ultrasound using bidirectional LSTM neu",
    "full_text_length": 53099,
    "chunk_length": 1213
  },
  {
    "chunk_id": 3075,
    "paper_filename": "ruixin_2024_application_of_deep_learning_for_breast_ultrasound.pdf",
    "paper_title": "Ruixin 2024 Application Of Deep Learning For Breast Ultrasound",
    "chunk_index": 41,
    "total_chunks": 53,
    "text_content": "images. Computer Methods and Programs in Biomedicine, 221, 106891. [11] Luo, X., Xu, M., Tang, G., Wang, Y ., Wang, N., Ni, D., Lin, X, Li, A. H. (2022). The lesion detection efficacy of deep learning on automatic breast ultrasound and factors affecting its efficacy: a pilot study. The British Journal of Radiology, 95(1130), 20210438. [12] Li, Y ., Wu, W., Chen, H., Cheng, L., & Wang, S. (2020). 3D tumor detection in autom ated breast ultrasound using deep convolutional neural network. Medical P",
    "full_text_length": 53099,
    "chunk_length": 1195
  },
  {
    "chunk_id": 3076,
    "paper_filename": "ruixin_2024_application_of_deep_learning_for_breast_ultrasound.pdf",
    "paper_title": "Ruixin 2024 Application Of Deep Learning For Breast Ultrasound",
    "chunk_index": 42,
    "total_chunks": 53,
    "text_content": "Frontiers in Oncology, 12, 938413. [15] Oh, K., Lee, S. E., & Kim, E. K. (2023). 3 -D breast nodule detection on automated breast ultrasound using faster region -based convolutional neural networks and U -Net. Scientific Reports, 13(1), 22625. [16] Li, Y ., Zhang, Z., Sun, J., Chen, H., Chen, Z., & Wei, J. (2024). Tumor de tection based on deep mutual learning in automated breast ultrasound. Multimedia Tools and Applications, 1 -19. [17] Lei, B., Huang, S., Li, R., Bian, C., Li, H., Chou, Y . H.",
    "full_text_length": 53099,
    "chunk_length": 1243
  },
  {
    "chunk_id": 3077,
    "paper_filename": "ruixin_2024_application_of_deep_learning_for_breast_ultrasound.pdf",
    "paper_title": "Ruixin 2024 Application Of Deep Learning For Breast Ultrasound",
    "chunk_index": 43,
    "total_chunks": 53,
    "text_content": "whole breast ultrasound. Insights into Imaging, 15(1), 227. [19] Torrey, L., & S havlik, J. (2010). Transfer learning. In Handbook of research on machine learning applications and trends: algorithms, methods, and techniques (pp. 242 -264). IGI global. [20] Ronneberger, O., Fischer, P., & Brox, T. (2015). U -net: Convolutional networks for bio medical image segmentation. In Medical Image Computing and Computer -Assisted Intervention \u2013 MICCAI 2015: 18th international conference, Munich, Germany, O",
    "full_text_length": 53099,
    "chunk_length": 1341
  },
  {
    "chunk_id": 3078,
    "paper_filename": "ruixin_2024_application_of_deep_learning_for_breast_ultrasound.pdf",
    "paper_title": "Ruixin 2024 Application Of Deep Learning For Breast Ultrasound",
    "chunk_index": 44,
    "total_chunks": 53,
    "text_content": "pancreas. arXiv preprint arXiv:1804.03999. [24] He, K., Gkioxari, G., Doll\u00e1r, P., & Girshick, R. (2017). Mask r -cnn. In Proceedi ngs of the IEEE International Conference on Computer Vision (pp. 2961 -2969). [25] Vaswani, A. (2017). Attention is all you need. Advances in Neural Information Processing Systems. [26] Tran, D., Bourdev, L., Fergus, R., Torresani, L., & Paluri, M. (2015). Learning s patiotemporal features with 3d convolutional networks. In Proceedings of the IEEE International Confer",
    "full_text_length": 53099,
    "chunk_length": 1256
  },
  {
    "chunk_id": 3079,
    "paper_filename": "ruixin_2024_application_of_deep_learning_for_breast_ultrasound.pdf",
    "paper_title": "Ruixin 2024 Application Of Deep Learning For Breast Ultrasound",
    "chunk_index": 45,
    "total_chunks": 53,
    "text_content": "with Monte Carlo dropout. npj Digital Medicine, 5(1), 174. [30] Zhan, W., Sun, C., Wang, M., She, J., Zhang, Y ., Zhang, Z., & Sun, Y . (2022). An improved Yolov5 real -time detection method for small objects ca ptured by UA V . Soft Computing, 26, 361-373. [31] Al-Khater, W., & Al -Madeed, S. (2024). Using 3D -VGG -16 and 3D -Resnet -18 deep learning models and FABEMD techniques in the detection of malware. Alexandria Engineering Journal, 89, 39 -52. [32] Ren, S., He, K., Girshick , R., & Sun, ",
    "full_text_length": 53099,
    "chunk_length": 1167
  },
  {
    "chunk_id": 3080,
    "paper_filename": "ruixin_2024_application_of_deep_learning_for_breast_ultrasound.pdf",
    "paper_title": "Ruixin 2024 Application Of Deep Learning For Breast Ultrasound",
    "chunk_index": 46,
    "total_chunks": 53,
    "text_content": "IEEE Conference on Computer Vision and Pattern Recognition (pp. 4320 - 4328). [35] Lee, C. Y ., Xie, S., Gallagher, P., Zhang, Z., & Tu, Z. (2015) . Deeply -supervised nets. In Artificial Intelligence and Statistics (pp. 562 -570). Pmlr. [36] Parnami, A., & Lee, M. (2022). Learning from few examples: A summary of approaches to Journal Pre-proof few-shot learning. arxiv preprint arxiv:2203.04291. [37] Celard, P., Iglesias, E. L., Sorribes -Fdez, J. M., Romero, R., Vieira, A. S., & Borrajo, L. (20",
    "full_text_length": 53099,
    "chunk_length": 1221
  },
  {
    "chunk_id": 3081,
    "paper_filename": "ruixin_2024_application_of_deep_learning_for_breast_ultrasound.pdf",
    "paper_title": "Ruixin 2024 Application Of Deep Learning For Breast Ultrasound",
    "chunk_index": 47,
    "total_chunks": 53,
    "text_content": "Deep Learning for Detection and Recognition: A Comprehensive Survey. IEEE Transactions on Neural Networks and Learning Systems. [40] Guo, M. H., Lu, C. Z., Liu, Z. N., Cheng, M. M., & Hu, S. M. (2023). Visual attention network. Computational Visual Media, 9(4), 733 -752. [41] Ross, T. Y ., & Doll\u00e1r , G. K. H. P. (2017, July). Focal loss for dense object detection. In proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 2980 -2988). [42] Zhu, L., Liao, B., Zhang, Q.,",
    "full_text_length": 53099,
    "chunk_length": 1195
  },
  {
    "chunk_id": 3082,
    "paper_filename": "ruixin_2024_application_of_deep_learning_for_breast_ultrasound.pdf",
    "paper_title": "Ruixin 2024 Application Of Deep Learning For Breast Ultrasound",
    "chunk_index": 48,
    "total_chunks": 53,
    "text_content": "from Primary Breast Cancer US Images Using Deep Learning. Radiology, 294(1), 19\u201328. [44] Gu, J., Tong, T., Xu, D., et al . (2023). Deep learning radiomics of ultrasonography for comprehensively predicting tumor and axillary lymph node stat us after neoadjuvant chemotherapy in breast cancer patients: A multicenter study. Cancer, 129(3), 356 -366. [45] Ma, M., Liu, R., Wen, C., et al. (2022). Predicting the molecular subtype of breast cancer and identifying interpretable imaging features using mac",
    "full_text_length": 53099,
    "chunk_length": 1289
  },
  {
    "chunk_id": 3083,
    "paper_filename": "ruixin_2024_application_of_deep_learning_for_breast_ultrasound.pdf",
    "paper_title": "Ruixin 2024 Application Of Deep Learning For Breast Ultrasound",
    "chunk_index": 49,
    "total_chunks": 53,
    "text_content": "-Enhanced Ultrasound in Breast Cancer. Ultrasound in Medicine & Biology, 49(7), 1638 \u20131646. [48] Chwaab, J., Diez, Y ., Oliver, A., Mart\u00ed, R., Zelst, J. V ., Gubern -M\u00e9rida, A., Mourri, A. B., Gregori, J., & G\u00fcnther, M. (2016). Automated quality assessment in three -dimensional breast ultrasound images. Journal of Medi cal Imaging, 3(2), 027002. [49] Sch\u00e4fgen, B., Juskic, M., Radicke, M., Hertel, M., Barr, R., Pfob, A., Togawa, R., Nees, J., V on Au, A., Fastner, S., Harcos, A., Gomez, C., Stieb",
    "full_text_length": 53099,
    "chunk_length": 1274
  },
  {
    "chunk_id": 3084,
    "paper_filename": "ruixin_2024_application_of_deep_learning_for_breast_ultrasound.pdf",
    "paper_title": "Ruixin 2024 Application Of Deep Learning For Breast Ultrasound",
    "chunk_index": 50,
    "total_chunks": 53,
    "text_content": "-volume, single -center breast cancer Journal Pre-proof screening program. European Journal of Radiology, 85(9), 1554 \u20131563. [51] Zhou, B. Y ., Wang, L. F., Yin, H. H., et al. (2021 ). Decoding the molecular subtypes of breast cancer seen on multimodal ultrasound images using an assembled convolutional neural network model: A prospective and multicentre study. EBioMedicine, 74. [52] Tu, T., Azizi, S., Driess, D., et al. (2024). Towards gen eralist biomedical AI. NEJM AI, 1(3), AIoa2300138. [53] ",
    "full_text_length": 53099,
    "chunk_length": 1328
  },
  {
    "chunk_id": 3085,
    "paper_filename": "ruixin_2024_application_of_deep_learning_for_breast_ultrasound.pdf",
    "paper_title": "Ruixin 2024 Application Of Deep Learning For Breast Ultrasound",
    "chunk_index": 51,
    "total_chunks": 53,
    "text_content": "Ruixin Wang, Zhiyuan Wang, Yuanming Xiao, Xiaohui Liu, Guoping Tan, Jun Liu \u2022 Research highlights item 1\uff1aThis article introduce s the role of deep learning (DL) in the computer - aided diagnosis (CAD) for breast cancer based on Automated Breast Ultrasound (ABUS). \u2022 Research highlights item 2\uff1aThis article analyses the challenges faced with the DL-based CAD system for breast cancer when using ABUS . \u2022 Research highlights item 3\uff1aThe article explores future research opportunities in integrating DL w",
    "full_text_length": 53099,
    "chunk_length": 1240
  },
  {
    "chunk_id": 3086,
    "paper_filename": "ruixin_2024_application_of_deep_learning_for_breast_ultrasound.pdf",
    "paper_title": "Ruixin 2024 Application Of Deep Learning For Breast Ultrasound",
    "chunk_index": 52,
    "total_chunks": 53,
    "text_content": "decision to publish this article . \u2610The authors declare the following financial interests (e.g., any funding for the research project) /personal relationships (e.g., the author is an employee of a profitable company ) which may be considered as potential competing interest s: Click here to enter your full declaration Journal Pre-proof",
    "full_text_length": 53099,
    "chunk_length": 336
  },
  {
    "chunk_id": 3087,
    "paper_filename": "ruixin_2025_applicaiton_on_automated_breast_current_devolment.pdf",
    "paper_title": "Ruixin 2025 Applicaiton On Automated Breast Current Devolment",
    "chunk_index": 0,
    "total_chunks": 53,
    "text_content": "Journal Pre-proof Application of deep learning on automated breast ultrasound: Current developments, challenges, and opportunities Ruixin Wang, Zhiyuan Wang, Yuanming Xiao, Xiaohui Liu, Guoping Tan, Jun Liu PII: S2950-1628(25)00006-2 DOI: https://doi.org/10.1016/j.metrad.2025.100138 Reference: METRAD 100138 To appear in: Meta-Radiology Received Date: 16 January 2025 Revised Date: 19 February 2025 Accepted Date: 13 March 2025 Please cite this article as: Wang R, Wang Z, Xiao Y, Liu X, Tan G, Liu ",
    "full_text_length": 53099,
    "chunk_length": 1384
  },
  {
    "chunk_id": 3088,
    "paper_filename": "ruixin_2025_applicaiton_on_automated_breast_current_devolment.pdf",
    "paper_title": "Ruixin 2025 Applicaiton On Automated Breast Current Devolment",
    "chunk_index": 1,
    "total_chunks": 53,
    "text_content": "early visibility of the article. Please note that, during the production process, errors may be discovered which could affect the content, and all legal disclaimers that apply to the journal pertain. \u00a9 2025 The Authors. Publishing services by Elsevier B.V. on behalf of KeAi Communications Co. Ltd. Journal Pre-proof Application of deep learning on automated breast ultrasound: Current developments , challenges , and opportunities Ruixin Wang 1, Zhiyuan Wang 2*, Yuanming Xiao 3, Xiaohui Liu 4, Guop",
    "full_text_length": 53099,
    "chunk_length": 1519
  },
  {
    "chunk_id": 3089,
    "paper_filename": "ruixin_2025_applicaiton_on_automated_breast_current_devolment.pdf",
    "paper_title": "Ruixin 2025 Applicaiton On Automated Breast Current Devolment",
    "chunk_index": 2,
    "total_chunks": 53,
    "text_content": "Hohai University, N anjing, China; gptan@hhu.edu.cn 6Clinical Research Center for Medical Imaging in Hunan Province, Changsha, China 7Department of Radiology Quality Control Center in Hunan Province, Changsha, China *Correspondence: wangzhiyuan@hnca.org.cn (Z.W.); junliu123@c su.edu.cn (J.L.) Abstract: Breast cancer is a major disease threatening the health of women worldwide. The advent of automated breast ultrasound (ABUS) has provided new possibilities for the early screening and diagnosis of",
    "full_text_length": 53099,
    "chunk_length": 1428
  },
  {
    "chunk_id": 3090,
    "paper_filename": "ruixin_2025_applicaiton_on_automated_breast_current_devolment.pdf",
    "paper_title": "Ruixin 2025 Applicaiton On Automated Breast Current Devolment",
    "chunk_index": 3,
    "total_chunks": 53,
    "text_content": "comprehensive review of its applications in ABUS is still lacking. This paper provides a detailed analysis of the latest advancements, existing challenges, and future research opportunities in this rapidly evolving fie ld. Keywords: Automated breast ultrasound; Breast cancer; Deep learning; Computer -aided diagnosis 1 Introduction Breast cancer is the most common and deadliest cancer among women worldwide. As shown in Figure 1, according to the latest global cancer statistics by the Internationa",
    "full_text_length": 53099,
    "chunk_length": 1379
  },
  {
    "chunk_id": 3091,
    "paper_filename": "ruixin_2025_applicaiton_on_automated_breast_current_devolment.pdf",
    "paper_title": "Ruixin 2025 Applicaiton On Automated Breast Current Devolment",
    "chunk_index": 4,
    "total_chunks": 53,
    "text_content": "Pre-proof mammography (X -ray imaging of the breast), and magnetic resonance imaging (MRI) are currently the most widely utilized imaging modalities for breast evaluation, each with distinct advantages and limitations. The advantages of HHUS include the absence of radiation, real -time imaging capabilities, and relatively high specificity. However, it also has notable drawbacks: it p roduces two- dimensional ( 2D) images, which can lead to missed diagnoses; the image acquisition process lacks st",
    "full_text_length": 53099,
    "chunk_length": 1394
  },
  {
    "chunk_id": 3092,
    "paper_filename": "ruixin_2025_applicaiton_on_automated_breast_current_devolment.pdf",
    "paper_title": "Ruixin 2025 Applicaiton On Automated Breast Current Devolment",
    "chunk_index": 5,
    "total_chunks": 53,
    "text_content": "Compared to HHUS , ABUS provides automated, consistent imaging and 3D reconstruction, standardizing the ultrasound image acquisition process[2]. The 3D reconstructed images help reduce missed detection s, while ABUS also alleviate s operator fatigue and enhances examination efficiency . Furthermore, ABUS is particularly advantageous for high-risk or dense -breast patients, providing a more comprehensive and accurate evaluation compared to HHUS. Therefore, ABUS offers new opportunities for breast",
    "full_text_length": 53099,
    "chunk_length": 1357
  },
  {
    "chunk_id": 3093,
    "paper_filename": "ruixin_2025_applicaiton_on_automated_breast_current_devolment.pdf",
    "paper_title": "Ruixin 2025 Applicaiton On Automated Breast Current Devolment",
    "chunk_index": 6,
    "total_chunks": 53,
    "text_content": "been proposed to support the CAD across various imag ing modalities , with progress also made in relation to ABUS. However, due to the limited adoption and the unique imaging characteristic of ABUS, targeted efforts are Journal Pre-proof necessary to develop specialized DL model s for this modality . This article aims to provide a comprehensive analysis of the current state of DL for ABUS in the context of CAD , with a particular focus on DL methodolog ies. Additionally, the challenges encounter",
    "full_text_length": 53099,
    "chunk_length": 1236
  },
  {
    "chunk_id": 3094,
    "paper_filename": "ruixin_2025_applicaiton_on_automated_breast_current_devolment.pdf",
    "paper_title": "Ruixin 2025 Applicaiton On Automated Breast Current Devolment",
    "chunk_index": 7,
    "total_chunks": 53,
    "text_content": "breast area . (2) Initial set tings: The technician adjusts the imaging depth according to the patient \u2019s breast size and selects the optimal scanning position. (3) Automated scanning: The technician activates the scanning switch, and the probe is automatically moved by a motor, performing continuous scans of multiple axial slices. (4) Data processing: Once the scanning is complete, the ABUS system reconstructs the acquired 2D axial images into two additional planes , i.e., the coronal and sagit",
    "full_text_length": 53099,
    "chunk_length": 1264
  },
  {
    "chunk_id": 3095,
    "paper_filename": "ruixin_2025_applicaiton_on_automated_breast_current_devolment.pdf",
    "paper_title": "Ruixin 2025 Applicaiton On Automated Breast Current Devolment",
    "chunk_index": 8,
    "total_chunks": 53,
    "text_content": "plane refers to a transverse slice of the breast and is crucial for examining the tissue structure and evaluat ing the imaging characteristics of breast masses. The coronal plane is a slice perpendicular to the chest surface, offering a comprehensive view of the breast \u2019s structure. This plane is particularly useful for observing the anterior -posterior distribution of breast tissue and asses sing the relationship between breast masses and surrounding glandular tissue, revealing key signs such a",
    "full_text_length": 53099,
    "chunk_length": 1286
  },
  {
    "chunk_id": 3096,
    "paper_filename": "ruixin_2025_applicaiton_on_automated_breast_current_devolment.pdf",
    "paper_title": "Ruixin 2025 Applicaiton On Automated Breast Current Devolment",
    "chunk_index": 9,
    "total_chunks": 53,
    "text_content": "The images were acquired with a GE Invenia ABUS 2.0. The acquisition of ABUS images typically involves multiple standard scanning positions to ensure compr ehensive coverage of the whole breast. As shown in Figure 4, t he most common scanning positions are AP (Anterior -Posterior), LAT (Lateral), and MED (Medial). In the AP position, the probe scans from the front, primarily visualizing the anterior and upper a reas of the breast. In the LAT position , the probe scans from the lateral (outer) si",
    "full_text_length": 53099,
    "chunk_length": 1205
  },
  {
    "chunk_id": 3097,
    "paper_filename": "ruixin_2025_applicaiton_on_automated_breast_current_devolment.pdf",
    "paper_title": "Ruixin 2025 Applicaiton On Automated Breast Current Devolment",
    "chunk_index": 10,
    "total_chunks": 53,
    "text_content": "literature search across several databases, including PubMed, IEEE Xplore, Web of Science , and Google Scholar, to systematically identify relevant studies. The search utilized keywords such as \u201cdeep learning and automated breast ultrasound \u201d, \u201cautomated breast ultrasound \u201d, \u201cdeep learning and ABUS \u201d, and \u201cABUS \u201d to cover a broad range of research focused on the application of DL techniques in ABUS. We limited our search to papers published up to January 2025, to incl ude the most current and re",
    "full_text_length": 53099,
    "chunk_length": 1291
  },
  {
    "chunk_id": 3098,
    "paper_filename": "ruixin_2025_applicaiton_on_automated_breast_current_devolment.pdf",
    "paper_title": "Ruixin 2025 Applicaiton On Automated Breast Current Devolment",
    "chunk_index": 11,
    "total_chunks": 53,
    "text_content": "Journal Pre-proof 4 Results Table I presents an overview of the reviewed literature , where breast lesion classification, segmentation, and detection are the most widely applied areas of DL in ABUS. As shown in Figure 5, breast lesion classification is to categorize lesions (such as lumps or nodules) based on their features, usually into benign or malignant categories. Classification typically rel ies on imaging features such as shape, boundary, and echogenicity to make th e determination. Breas",
    "full_text_length": 53099,
    "chunk_length": 1315
  },
  {
    "chunk_id": 3099,
    "paper_filename": "ruixin_2025_applicaiton_on_automated_breast_current_devolment.pdf",
    "paper_title": "Ruixin 2025 Applicaiton On Automated Breast Current Devolment",
    "chunk_index": 12,
    "total_chunks": 53,
    "text_content": "Data volume Performance Lesion classification Wang et al.[4] CNN, Transfer learning 263 patients (316 breast lesions ) Sensitivity: 88.6% Specificity: 87.6% AUC: 0.9468 Hejduk et al.[5] CNN 113 patients (55 high malignancy probability) Accuracy: 90.9% AUC: 0.91 Kim et al.[6] Weak supervision, 3D CNN 363 patients (434 mass lesions) Sensitivity: 87.75% Specificity: 93.75% AUC: 0.9491. Lesion segmentation Cao et al.[7] U-Net, Dilated convolution, Focus loss 107 patients (170 volumes) DSC: 0.6902 Ja",
    "full_text_length": 53099,
    "chunk_length": 1341
  },
  {
    "chunk_id": 3100,
    "paper_filename": "ruixin_2025_applicaiton_on_automated_breast_current_devolment.pdf",
    "paper_title": "Ruixin 2025 Applicaiton On Automated Breast Current Devolment",
    "chunk_index": 13,
    "total_chunks": 53,
    "text_content": "124 patients (181 tumor instances) Sensitivity: 90% FP: 7.42 per volume Zhang et al.[13] YOLOv4, Monte Carlo Dropout 124 patients (170 volumes) Sensitivity: 88% FP: 0.19 per slice Zhang et al.[14] YOLOv5, 3D ResNet, Transformer encoder 741 patients (3114 breast lesions) Detection rate: 71.2% Oh et al.[15] Faster R -CNN, U-Net 28 patients (168 scans) Sensitivity: 93.65% FP: 8.6 per examination Li et al.[16] YOLOX, 124 patients Sensitivity: 90% Journal Pre-proof Deep mutual learning (238 volumes) ",
    "full_text_length": 53099,
    "chunk_length": 1277
  },
  {
    "chunk_id": 3101,
    "paper_filename": "ruixin_2025_applicaiton_on_automated_breast_current_devolment.pdf",
    "paper_title": "Ruixin 2025 Applicaiton On Automated Breast Current Devolment",
    "chunk_index": 14,
    "total_chunks": 53,
    "text_content": "a multi -view convolutional neural network (CNN) to address the multi -view nature of ABUS images and used transfer learning[19] for the classification of benign and malignant lesion s in ABUS images. After training and evaluating the model on 316 breast lesions (135 malignant and 181 benign), the method achieved an are a under the curve (AUC) value of 0.9468, with sensitivity and specificity reaching 88 .6% and 87 .6%, respectively. Furthermore, observer performance testing demonstrated that, b",
    "full_text_length": 53099,
    "chunk_length": 1271
  },
  {
    "chunk_id": 3102,
    "paper_filename": "ruixin_2025_applicaiton_on_automated_breast_current_devolment.pdf",
    "paper_title": "Ruixin 2025 Applicaiton On Automated Breast Current Devolment",
    "chunk_index": 15,
    "total_chunks": 53,
    "text_content": "4/5 categories . For the region of interest (ROI) determined using a sliding window, Model 1 is first applied to classify background or breast tissue . Next, Model 2 categorizes the tissue as either normal or lesioned , and finally, Model 3 is employed for benign or malignant classification. The study utilized 645 ABUS scans from 113 patients for training and validation. The results showed that the accuracy for single image classification containing lesions was 79.7%, with an AUC of 0.91. On the",
    "full_text_length": 53099,
    "chunk_length": 1329
  },
  {
    "chunk_id": 3103,
    "paper_filename": "ruixin_2025_applicaiton_on_automated_breast_current_devolment.pdf",
    "paper_title": "Ruixin 2025 Applicaiton On Automated Breast Current Devolment",
    "chunk_index": 16,
    "total_chunks": 53,
    "text_content": "the tumor feature information from radiology reports to generate a template mask, thereby enhan cing the performance of the existing neural network classifier without requiring additional annotations. Experimental results demonstrated that the branch network with the template mask significantly improved classification accuracy. On a dataset comprisin g 363 patients (286 with 434 BI -RADS 2 or higher tumor lesions and 77 without tumor lesions), the network achieved a sensitivity of 87.75%, a spec",
    "full_text_length": 53099,
    "chunk_length": 1353
  },
  {
    "chunk_id": 3104,
    "paper_filename": "ruixin_2025_applicaiton_on_automated_breast_current_devolment.pdf",
    "paper_title": "Ruixin 2025 Applicaiton On Automated Breast Current Devolment",
    "chunk_index": 17,
    "total_chunks": 53,
    "text_content": "dense blocks of D\u00b2U -Net. This design allows the network to handle mass features at different scales, enhancing its ability to learn effectively from small medical image datasets. A dditionally, the proposed uncertainty focus loss helps increase the model \u2019s attention on uncertain predictions, thereby improving the segmentation precision of blurry mass boundaries. The method was evaluated on a dataset of 170 volumes from 107 patients, with the experimental results show ing a Dice Similarity Coef",
    "full_text_length": 53099,
    "chunk_length": 1321
  },
  {
    "chunk_id": 3105,
    "paper_filename": "ruixin_2025_applicaiton_on_automated_breast_current_devolment.pdf",
    "paper_title": "Ruixin 2025 Applicaiton On Automated Breast Current Devolment",
    "chunk_index": 18,
    "total_chunks": 53,
    "text_content": "module with SC - attention captures the inter -slice relationships, leveraging contextual information from the 3D data to assist segmentation and reduce false positives. The method was evaluated on a private ABUS dataset containing 124 patients and 170 volumes, achieving a DSC of 0.8178 , a recall of 80.67% , and a precision of 82.92%. To address the challenges faced in breast tumor segmentation from ABUS images, such as low image quality, speckle noise, shadowing effects, and low contrast, Lei ",
    "full_text_length": 53099,
    "chunk_length": 1293
  },
  {
    "chunk_id": 3106,
    "paper_filename": "ruixin_2025_applicaiton_on_automated_breast_current_devolment.pdf",
    "paper_title": "Ruixin 2025 Applicaiton On Automated Breast Current Devolment",
    "chunk_index": 19,
    "total_chunks": 53,
    "text_content": "cross -validation and 30 cases for independent testing. The experi mental results demonstrated that, compared to manually delineated contours, the proposed method achieved an average DSC of 0.85\u00b10.10 in cross -validation, and 0.82\u00b10.15 in an independent test set. To address the challenges of lesion diversity, significant i maging artifacts , and blurred lesion boundaries , Cheng et al.[10] proposed a deepest semantically guided multi-scale feature fusion network (DSGMFFN). This method ensures th",
    "full_text_length": 53099,
    "chunk_length": 1348
  },
  {
    "chunk_id": 3107,
    "paper_filename": "ruixin_2025_applicaiton_on_automated_breast_current_devolment.pdf",
    "paper_title": "Ruixin 2025 Applicaiton On Automated Breast Current Devolment",
    "chunk_index": 20,
    "total_chunks": 53,
    "text_content": "Intersection over Union (IoU) of 0.7324. Luo et al.[11] addressed the challenge of balanc ing high sensitivity with low false positive s in existing CAD systems by designing and optimizing a CNN based on a 3D U -Net architecture to improve lesion segmentation efficiency and accuracy in ABUS images. The network incorporated optimization strategies such as densely deep supervision (DDS) mechanism and threshold map (TM) , and was further improved by utilizing a pre -trained C3D[26] model. The study",
    "full_text_length": 53099,
    "chunk_length": 1258
  },
  {
    "chunk_id": 3108,
    "paper_filename": "ruixin_2025_applicaiton_on_automated_breast_current_devolment.pdf",
    "paper_title": "Ruixin 2025 Applicaiton On Automated Breast Current Devolment",
    "chunk_index": 21,
    "total_chunks": 53,
    "text_content": "For lesions larger than 1 cm3, the sensitivity reached 98%, while for lesions less than or equal to 1 cm3, the sensitivity was 87%. Additionally, the study revealed no significant impact of breast tissue composit ion, lesion morphology, or echogenic patterns on sensitivity and false positive rates. 4.3 Lesion detection Compared to lesion segmentation, lesion detection in ABUS images focuses more on locating the lesion rather than delineating its exact boundaries. Li et al.[12] proposed a 3D tumo",
    "full_text_length": 53099,
    "chunk_length": 1348
  },
  {
    "chunk_id": 3109,
    "paper_filename": "ruixin_2025_applicaiton_on_automated_breast_current_devolment.pdf",
    "paper_title": "Ruixin 2025 Applicaiton On Automated Breast Current Devolment",
    "chunk_index": 22,
    "total_chunks": 53,
    "text_content": "the issue of positional differences and scoring inconsistencies in 2D detection results, the authors proposed a rescoring algorithm and constructed a 3D volume -forming mode l. This model differentiates real tumor volumes from false -positive areas and further eliminates false positives based on slice length and average area. The method was tested on an ABUS dataset consisting of 124 patients with 181 tumor instances. Experimental results showed a sensitivity of 90%, 85%, 80%, 75%, and 70% under",
    "full_text_length": 53099,
    "chunk_length": 1296
  },
  {
    "chunk_id": 3110,
    "paper_filename": "ruixin_2025_applicaiton_on_automated_breast_current_devolment.pdf",
    "paper_title": "Ruixin 2025 Applicaiton On Automated Breast Current Devolment",
    "chunk_index": 23,
    "total_chunks": 53,
    "text_content": "approach enhanced the de tection rate of challenging tumor regions while reduc ing false positives. The method was tested on a private dataset containing 170 ABUS tumor volumes, and the results showed a sensitivity of 88% and a false positive rate of 0.19 per slice, outperforming t he original YOLOv4 and other mainstream object detectors. Zhang et al.[14] developed an intelligent detection system called V -BUILDS (V olume -Breast Journal Pre-proof Ultrasound Intelligent Lesion Detection System) ",
    "full_text_length": 53099,
    "chunk_length": 1303
  },
  {
    "chunk_id": 3111,
    "paper_filename": "ruixin_2025_applicaiton_on_automated_breast_current_devolment.pdf",
    "paper_title": "Ruixin 2025 Applicaiton On Automated Breast Current Devolment",
    "chunk_index": 24,
    "total_chunks": 53,
    "text_content": "of 71.2%. Specifically, it demonstrated high detection rates of 96.5% for BI -RADS 4/5 lesions and 95.8% for malignant tumors. H owever, the detection performance for small lesions (<10 mm) and benign lesions classified as BI -RADS 2 or 3 remains suboptimal. Oh et al.[15] proposed a 3D breast nodule detection system to address the challenge of quickly interpreting the large volume of image data generated by ABUS . Their method combin es the Faster R-CNN[32] detector with U -Net[20]. This approac",
    "full_text_length": 53099,
    "chunk_length": 1258
  },
  {
    "chunk_id": 3112,
    "paper_filename": "ruixin_2025_applicaiton_on_automated_breast_current_devolment.pdf",
    "paper_title": "Ruixin 2025 Applicaiton On Automated Breast Current Devolment",
    "chunk_index": 25,
    "total_chunks": 53,
    "text_content": "per examination. Li et al.[16] proposed the DML -YOLOX model , which is based on the YOLOX detector[33] and deep mutual learning (DML)[34], to address the issues of false positives and f alse negatives in tumor detection in ABUS images. To tackle the problem of overconfidence in single -model predictions, the method introduces exploration loss and consistency loss to enhance collaborative learning between two parallel networks. Additionally , to resolve the issue of overestimating bounding box s",
    "full_text_length": 53099,
    "chunk_length": 1337
  },
  {
    "chunk_id": 3113,
    "paper_filename": "ruixin_2025_applicaiton_on_automated_breast_current_devolment.pdf",
    "paper_title": "Ruixin 2025 Applicaiton On Automated Breast Current Devolment",
    "chunk_index": 26,
    "total_chunks": 53,
    "text_content": "model achieved a sensitivity of 90 % and a false po sitive rate of 0.15 per slice. 4.4 Other Applications Besides the above DL applications for lesion diagnosis, Lei et al .[17] investigated breast anatomical layer segmentation in ABUS images. To address issues such as inherent speckle noise, posterior acoustic shadows, and boundary -blurring caused by overlapping echogenicity spectra between different layers, they designed a boundary -regularized convolutional encoder -decoder network (ConvEDNe",
    "full_text_length": 53099,
    "chunk_length": 1388
  },
  {
    "chunk_id": 3114,
    "paper_filename": "ruixin_2025_applicaiton_on_automated_breast_current_devolment.pdf",
    "paper_title": "Ruixin 2025 Applicaiton On Automated Breast Current Devolment",
    "chunk_index": 27,
    "total_chunks": 53,
    "text_content": "excelled in segmenting four anatomical structures: subcutaneous fat, breast parenchyma, muscle, and chest wall. The method effectively reduced false positives, thereby improving th e efficiency of clinical image Journal Pre-proof interpretation. Huang et al.[18] developed both a single -task model and a multi -task model based on 3D ResNet to non invasively estimate three breast cancer biomarkers \u2014estrogen receptor (ER), progesterone receptor (PR), and human epidermal growth factor receptor 2 (H",
    "full_text_length": 53099,
    "chunk_length": 1279
  },
  {
    "chunk_id": 3115,
    "paper_filename": "ruixin_2025_applicaiton_on_automated_breast_current_devolment.pdf",
    "paper_title": "Ruixin 2025 Applicaiton On Automated Breast Current Devolment",
    "chunk_index": 28,
    "total_chunks": 53,
    "text_content": "testing, the single -task and multi - task models achieved AUCs of 0.809 and 0.735 for ER, 0.688 and 0.767 for PR, and 0.626 and 0.697 for HER2, respectively. In the overall evaluation, the multi -task model outperformed the single -task model, achieving a higher macro AUC of 0.733 compared to 0.708 for the single -task model. 5 Discussions and conclusions 5.1 Challenges and suggestions 5.1.1 Limited patient volume In existing research, the number of patients involved in training DL models for A",
    "full_text_length": 53099,
    "chunk_length": 1281
  },
  {
    "chunk_id": 3116,
    "paper_filename": "ruixin_2025_applicaiton_on_automated_breast_current_devolment.pdf",
    "paper_title": "Ruixin 2025 Applicaiton On Automated Breast Current Devolment",
    "chunk_index": 29,
    "total_chunks": 53,
    "text_content": "suffer from overfitting , resulting in limited generalization in clinical practice . Furthermore, the limited sample size can lead to data imbalance, where certain diseases (such as malignant tumors or small calcificati ons) have fewer samples, creating a large disparity in sample sizes across different types of lesions. This, in turn, affects the overall performance of the model and its ability to identify rare lesions. To address th is challenge , in addition to common te chniques such as data",
    "full_text_length": 53099,
    "chunk_length": 1377
  },
  {
    "chunk_id": 3117,
    "paper_filename": "ruixin_2025_applicaiton_on_automated_breast_current_devolment.pdf",
    "paper_title": "Ruixin 2025 Applicaiton On Automated Breast Current Devolment",
    "chunk_index": 30,
    "total_chunks": 53,
    "text_content": "of DL models in segmenting or detecting small lesions in ABUS images requires further improvement. The characteristics of small lesions, including boundaries, morphology, and grayscale variations, are typically subtle and Journal Pre-proof difficult to differentiate from surrounding tissue, especially in high -noise, low -resolution images. ABUS images generally exhibit lower resolution compared to other imaging techniques , such as magnetic resonance imaging (MRI) or computed tomography (CT) , ",
    "full_text_length": 53099,
    "chunk_length": 1397
  },
  {
    "chunk_id": 3118,
    "paper_filename": "ruixin_2025_applicaiton_on_automated_breast_current_devolment.pdf",
    "paper_title": "Ruixin 2025 Applicaiton On Automated Breast Current Devolment",
    "chunk_index": 31,
    "total_chunks": 53,
    "text_content": "in ea rly-stage malignant tumors, microcalcifications, or small nodules in breast tissue, where lesions often lack clear boundaries or distinctive features, resulting in their potential oversight or misinterpretation as normal tissue in ABUS images. To address t he limitations in detecting small lesions in ABUS, techniques from the field of computer vision for small object recognition[38] can be leveraged . For instance, multi -scale learning[39] can enhance the model \u2019s ability to capture lesio",
    "full_text_length": 53099,
    "chunk_length": 1386
  },
  {
    "chunk_id": 3119,
    "paper_filename": "ruixin_2025_applicaiton_on_automated_breast_current_devolment.pdf",
    "paper_title": "Ruixin 2025 Applicaiton On Automated Breast Current Devolment",
    "chunk_index": 32,
    "total_chunks": 53,
    "text_content": "-weights easily classified samples during training and directs the model\u2019s attention towards hard-to-classify small lesions , could be another remedy for reducing false negatives and false positives. 5.1.3 False positives Due to the inherent limitations of ultrasound imaging, the resolution of breast tissue is generally lower, particularly in patients with dense glandular tissue, which may result in the misinterpretation of normal tissue as lesions. Additionally , breast lesions exhibit a broad ",
    "full_text_length": 53099,
    "chunk_length": 1378
  },
  {
    "chunk_id": 3120,
    "paper_filename": "ruixin_2025_applicaiton_on_automated_breast_current_devolment.pdf",
    "paper_title": "Ruixin 2025 Applicaiton On Automated Breast Current Devolment",
    "chunk_index": 33,
    "total_chunks": 53,
    "text_content": "several technical improvements can be implemented , such as d evelop ing more advanced network architectures (e.g., Vision Mamba[42]) and more effecti ve loss functions (e.g., exploration loss and consistency loss[16]). Additionally, incorporating clinical background information , such as age, family history, and medical history, can aid in distinguish ing between benign and malignant lesions. For example, i n patients with specific clinical profile s, lesions are more likely to be benign, allow",
    "full_text_length": 53099,
    "chunk_length": 1358
  },
  {
    "chunk_id": 3121,
    "paper_filename": "ruixin_2025_applicaiton_on_automated_breast_current_devolment.pdf",
    "paper_title": "Ruixin 2025 Applicaiton On Automated Breast Current Devolment",
    "chunk_index": 34,
    "total_chunks": 53,
    "text_content": "been a significant bottleneck in the development of DL applications of ultrasound imaging . Compared to other imaging modalities such as X-ray and CT, ultrasound image datasets are relatively limited , and there are currently no publicly available datasets for ABUS. This scarcity of public datasets complicates meaningful comparisons across studies and methods. When researchers train and test their models using proprietary datasets , they may encounter variations in data quality, annotation stand",
    "full_text_length": 53099,
    "chunk_length": 1378
  },
  {
    "chunk_id": 3122,
    "paper_filename": "ruixin_2025_applicaiton_on_automated_breast_current_devolment.pdf",
    "paper_title": "Ruixin 2025 Applicaiton On Automated Breast Current Devolment",
    "chunk_index": 35,
    "total_chunks": 53,
    "text_content": "comparisons across different models , thereby promoting the standardization and advancement of technologies in this field. 5.2 Opportunities 5.2.1 Extending DL beyond lesion diagnosis With the ongoing advancement of technology, the potential for integrating ultrasound and DL in breast cancer diagnosis is being explored across multiple domains . For instance, HHUS , in addition to being used for breast lesion diagnosis, has been applied in predict ing lymph node metastasis[43][44], molecular subt",
    "full_text_length": 53099,
    "chunk_length": 1364
  },
  {
    "chunk_id": 3123,
    "paper_filename": "ruixin_2025_applicaiton_on_automated_breast_current_devolment.pdf",
    "paper_title": "Ruixin 2025 Applicaiton On Automated Breast Current Devolment",
    "chunk_index": 36,
    "total_chunks": 53,
    "text_content": "data and clinical indicators, present new opportunities for the deve lopment of personalized treatment strategies , enabling clinicians to make more precise decisions tailored to individual patient conditions. 5.2.2 Utilizing DL in more phase s DL can also be applied to other phase s of ABUS -based breast examination. For example, in the quality control of ABUS images, DL models can assist in the automat ed identification of issues during the ABUS acquisition process , such as verifying proper n",
    "full_text_length": 53099,
    "chunk_length": 1376
  },
  {
    "chunk_id": 3124,
    "paper_filename": "ruixin_2025_applicaiton_on_automated_breast_current_devolment.pdf",
    "paper_title": "Ruixin 2025 Applicaiton On Automated Breast Current Devolment",
    "chunk_index": 37,
    "total_chunks": 53,
    "text_content": "can enhance diagnostic accuracy for breast cancer. Consequently , the development of multimodal AI systems represents a key direction for future research. In addition, the integration of ABUS with other ultrasound imaging techniques, such as Doppler and elastography[51] also warrants further exploration. Multimodal DL models can not only improve the detection performance for breast diseases but also expand the range of applications, as previously discussed. 5.2.4 Exploring DL -based large model ",
    "full_text_length": 53099,
    "chunk_length": 1434
  },
  {
    "chunk_id": 3125,
    "paper_filename": "ruixin_2025_applicaiton_on_automated_breast_current_devolment.pdf",
    "paper_title": "Ruixin 2025 Applicaiton On Automated Breast Current Devolment",
    "chunk_index": 38,
    "total_chunks": 53,
    "text_content": "and efficiency of disease diagnosis . With ongoing technological advancements , these emerging approaches are poised to significantly elevate the clinical utility of ABUS, improving the precision of breast cancer screening , diagnosis , and treatment. Funding : This work was supported by National Natural Science Foundation of China (61971451, U22A20303), Innovative Province special construction foundation of Hunan Province (2019SK2131), the Science and Technology Innovation Program of Hunan Prov",
    "full_text_length": 53099,
    "chunk_length": 1406
  },
  {
    "chunk_id": 3126,
    "paper_filename": "ruixin_2025_applicaiton_on_automated_breast_current_devolment.pdf",
    "paper_title": "Ruixin 2025 Applicaiton On Automated Breast Current Devolment",
    "chunk_index": 39,
    "total_chunks": 53,
    "text_content": "579\u2013592. [3] Sharma, B. P., & Purwar, R. K. (2024). Computer -aided detection and diagnosis of breast cancer: a review. ADCAIJ: Advances in Distributed Computing a nd Artificial Intelligence Journal, 13, e31412 -e31412. Journal Pre-proof [4] Wang, Y ., Choi, E. J., Choi, Y ., Zhang, H., Jin, G. Y ., & Ko, S. B. (2020). Breast cancer classification in automated breast ultrasound using multiview convolutional neural network with transfer learni ng. Ultrasound in Medicine & Biology, 46(5), 1119 -11",
    "full_text_length": 53099,
    "chunk_length": 1247
  },
  {
    "chunk_id": 3127,
    "paper_filename": "ruixin_2025_applicaiton_on_automated_breast_current_devolment.pdf",
    "paper_title": "Ruixin 2025 Applicaiton On Automated Breast Current Devolment",
    "chunk_index": 40,
    "total_chunks": 53,
    "text_content": "breast ultrasound. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (pp. 3912 -3919). [7] Cao, X., Chen, H., Li, Y ., Peng, Y ., Wang, S., & Cheng, L. (2021). Dilated densely connected U-Net with uncertainty focus loss for 3D ABUS mass segmentation. Comp uter Methods and Programs in Biomedicine, 209, 106313. [8] Pan, P., Chen, H., Li, Y ., Cai, N., Cheng, L., & Wang, S. (2021). Tumor segmentation in automated whole breast ultrasound using bidirectional LSTM neu",
    "full_text_length": 53099,
    "chunk_length": 1213
  },
  {
    "chunk_id": 3128,
    "paper_filename": "ruixin_2025_applicaiton_on_automated_breast_current_devolment.pdf",
    "paper_title": "Ruixin 2025 Applicaiton On Automated Breast Current Devolment",
    "chunk_index": 41,
    "total_chunks": 53,
    "text_content": "images. Computer Methods and Programs in Biomedicine, 221, 106891. [11] Luo, X., Xu, M., Tang, G., Wang, Y ., Wang, N., Ni, D., Lin, X, Li, A. H. (2022). The lesion detection efficacy of deep learning on automatic breast ultrasound and factors affecting its efficacy: a pilot study. The British Journal of Radiology, 95(1130), 20210438. [12] Li, Y ., Wu, W., Chen, H., Cheng, L., & Wang, S. (2020). 3D tumor detection in autom ated breast ultrasound using deep convolutional neural network. Medical P",
    "full_text_length": 53099,
    "chunk_length": 1195
  },
  {
    "chunk_id": 3129,
    "paper_filename": "ruixin_2025_applicaiton_on_automated_breast_current_devolment.pdf",
    "paper_title": "Ruixin 2025 Applicaiton On Automated Breast Current Devolment",
    "chunk_index": 42,
    "total_chunks": 53,
    "text_content": "Frontiers in Oncology, 12, 938413. [15] Oh, K., Lee, S. E., & Kim, E. K. (2023). 3 -D breast nodule detection on automated breast ultrasound using faster region -based convolutional neural networks and U -Net. Scientific Reports, 13(1), 22625. [16] Li, Y ., Zhang, Z., Sun, J., Chen, H., Chen, Z., & Wei, J. (2024). Tumor de tection based on deep mutual learning in automated breast ultrasound. Multimedia Tools and Applications, 1 -19. [17] Lei, B., Huang, S., Li, R., Bian, C., Li, H., Chou, Y . H.",
    "full_text_length": 53099,
    "chunk_length": 1243
  },
  {
    "chunk_id": 3130,
    "paper_filename": "ruixin_2025_applicaiton_on_automated_breast_current_devolment.pdf",
    "paper_title": "Ruixin 2025 Applicaiton On Automated Breast Current Devolment",
    "chunk_index": 43,
    "total_chunks": 53,
    "text_content": "whole breast ultrasound. Insights into Imaging, 15(1), 227. [19] Torrey, L., & S havlik, J. (2010). Transfer learning. In Handbook of research on machine learning applications and trends: algorithms, methods, and techniques (pp. 242 -264). IGI global. [20] Ronneberger, O., Fischer, P., & Brox, T. (2015). U -net: Convolutional networks for bio medical image segmentation. In Medical Image Computing and Computer -Assisted Intervention \u2013 MICCAI 2015: 18th international conference, Munich, Germany, O",
    "full_text_length": 53099,
    "chunk_length": 1341
  },
  {
    "chunk_id": 3131,
    "paper_filename": "ruixin_2025_applicaiton_on_automated_breast_current_devolment.pdf",
    "paper_title": "Ruixin 2025 Applicaiton On Automated Breast Current Devolment",
    "chunk_index": 44,
    "total_chunks": 53,
    "text_content": "pancreas. arXiv preprint arXiv:1804.03999. [24] He, K., Gkioxari, G., Doll\u00e1r, P., & Girshick, R. (2017). Mask r -cnn. In Proceedi ngs of the IEEE International Conference on Computer Vision (pp. 2961 -2969). [25] Vaswani, A. (2017). Attention is all you need. Advances in Neural Information Processing Systems. [26] Tran, D., Bourdev, L., Fergus, R., Torresani, L., & Paluri, M. (2015). Learning s patiotemporal features with 3d convolutional networks. In Proceedings of the IEEE International Confer",
    "full_text_length": 53099,
    "chunk_length": 1256
  },
  {
    "chunk_id": 3132,
    "paper_filename": "ruixin_2025_applicaiton_on_automated_breast_current_devolment.pdf",
    "paper_title": "Ruixin 2025 Applicaiton On Automated Breast Current Devolment",
    "chunk_index": 45,
    "total_chunks": 53,
    "text_content": "with Monte Carlo dropout. npj Digital Medicine, 5(1), 174. [30] Zhan, W., Sun, C., Wang, M., She, J., Zhang, Y ., Zhang, Z., & Sun, Y . (2022). An improved Yolov5 real -time detection method for small objects ca ptured by UA V . Soft Computing, 26, 361-373. [31] Al-Khater, W., & Al -Madeed, S. (2024). Using 3D -VGG -16 and 3D -Resnet -18 deep learning models and FABEMD techniques in the detection of malware. Alexandria Engineering Journal, 89, 39 -52. [32] Ren, S., He, K., Girshick , R., & Sun, ",
    "full_text_length": 53099,
    "chunk_length": 1167
  },
  {
    "chunk_id": 3133,
    "paper_filename": "ruixin_2025_applicaiton_on_automated_breast_current_devolment.pdf",
    "paper_title": "Ruixin 2025 Applicaiton On Automated Breast Current Devolment",
    "chunk_index": 46,
    "total_chunks": 53,
    "text_content": "IEEE Conference on Computer Vision and Pattern Recognition (pp. 4320 - 4328). [35] Lee, C. Y ., Xie, S., Gallagher, P., Zhang, Z., & Tu, Z. (2015) . Deeply -supervised nets. In Artificial Intelligence and Statistics (pp. 562 -570). Pmlr. [36] Parnami, A., & Lee, M. (2022). Learning from few examples: A summary of approaches to Journal Pre-proof few-shot learning. arxiv preprint arxiv:2203.04291. [37] Celard, P., Iglesias, E. L., Sorribes -Fdez, J. M., Romero, R., Vieira, A. S., & Borrajo, L. (20",
    "full_text_length": 53099,
    "chunk_length": 1221
  },
  {
    "chunk_id": 3134,
    "paper_filename": "ruixin_2025_applicaiton_on_automated_breast_current_devolment.pdf",
    "paper_title": "Ruixin 2025 Applicaiton On Automated Breast Current Devolment",
    "chunk_index": 47,
    "total_chunks": 53,
    "text_content": "Deep Learning for Detection and Recognition: A Comprehensive Survey. IEEE Transactions on Neural Networks and Learning Systems. [40] Guo, M. H., Lu, C. Z., Liu, Z. N., Cheng, M. M., & Hu, S. M. (2023). Visual attention network. Computational Visual Media, 9(4), 733 -752. [41] Ross, T. Y ., & Doll\u00e1r , G. K. H. P. (2017, July). Focal loss for dense object detection. In proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 2980 -2988). [42] Zhu, L., Liao, B., Zhang, Q.,",
    "full_text_length": 53099,
    "chunk_length": 1195
  },
  {
    "chunk_id": 3135,
    "paper_filename": "ruixin_2025_applicaiton_on_automated_breast_current_devolment.pdf",
    "paper_title": "Ruixin 2025 Applicaiton On Automated Breast Current Devolment",
    "chunk_index": 48,
    "total_chunks": 53,
    "text_content": "from Primary Breast Cancer US Images Using Deep Learning. Radiology, 294(1), 19\u201328. [44] Gu, J., Tong, T., Xu, D., et al . (2023). Deep learning radiomics of ultrasonography for comprehensively predicting tumor and axillary lymph node stat us after neoadjuvant chemotherapy in breast cancer patients: A multicenter study. Cancer, 129(3), 356 -366. [45] Ma, M., Liu, R., Wen, C., et al. (2022). Predicting the molecular subtype of breast cancer and identifying interpretable imaging features using mac",
    "full_text_length": 53099,
    "chunk_length": 1289
  },
  {
    "chunk_id": 3136,
    "paper_filename": "ruixin_2025_applicaiton_on_automated_breast_current_devolment.pdf",
    "paper_title": "Ruixin 2025 Applicaiton On Automated Breast Current Devolment",
    "chunk_index": 49,
    "total_chunks": 53,
    "text_content": "-Enhanced Ultrasound in Breast Cancer. Ultrasound in Medicine & Biology, 49(7), 1638 \u20131646. [48] Chwaab, J., Diez, Y ., Oliver, A., Mart\u00ed, R., Zelst, J. V ., Gubern -M\u00e9rida, A., Mourri, A. B., Gregori, J., & G\u00fcnther, M. (2016). Automated quality assessment in three -dimensional breast ultrasound images. Journal of Medi cal Imaging, 3(2), 027002. [49] Sch\u00e4fgen, B., Juskic, M., Radicke, M., Hertel, M., Barr, R., Pfob, A., Togawa, R., Nees, J., V on Au, A., Fastner, S., Harcos, A., Gomez, C., Stieb",
    "full_text_length": 53099,
    "chunk_length": 1274
  },
  {
    "chunk_id": 3137,
    "paper_filename": "ruixin_2025_applicaiton_on_automated_breast_current_devolment.pdf",
    "paper_title": "Ruixin 2025 Applicaiton On Automated Breast Current Devolment",
    "chunk_index": 50,
    "total_chunks": 53,
    "text_content": "-volume, single -center breast cancer Journal Pre-proof screening program. European Journal of Radiology, 85(9), 1554 \u20131563. [51] Zhou, B. Y ., Wang, L. F., Yin, H. H., et al. (2021 ). Decoding the molecular subtypes of breast cancer seen on multimodal ultrasound images using an assembled convolutional neural network model: A prospective and multicentre study. EBioMedicine, 74. [52] Tu, T., Azizi, S., Driess, D., et al. (2024). Towards gen eralist biomedical AI. NEJM AI, 1(3), AIoa2300138. [53] ",
    "full_text_length": 53099,
    "chunk_length": 1328
  },
  {
    "chunk_id": 3138,
    "paper_filename": "ruixin_2025_applicaiton_on_automated_breast_current_devolment.pdf",
    "paper_title": "Ruixin 2025 Applicaiton On Automated Breast Current Devolment",
    "chunk_index": 51,
    "total_chunks": 53,
    "text_content": "Ruixin Wang, Zhiyuan Wang, Yuanming Xiao, Xiaohui Liu, Guoping Tan, Jun Liu \u2022 Research highlights item 1\uff1aThis article introduce s the role of deep learning (DL) in the computer - aided diagnosis (CAD) for breast cancer based on Automated Breast Ultrasound (ABUS). \u2022 Research highlights item 2\uff1aThis article analyses the challenges faced with the DL-based CAD system for breast cancer when using ABUS . \u2022 Research highlights item 3\uff1aThe article explores future research opportunities in integrating DL w",
    "full_text_length": 53099,
    "chunk_length": 1240
  },
  {
    "chunk_id": 3139,
    "paper_filename": "ruixin_2025_applicaiton_on_automated_breast_current_devolment.pdf",
    "paper_title": "Ruixin 2025 Applicaiton On Automated Breast Current Devolment",
    "chunk_index": 52,
    "total_chunks": 53,
    "text_content": "decision to publish this article . \u2610The authors declare the following financial interests (e.g., any funding for the research project) /personal relationships (e.g., the author is an employee of a profitable company ) which may be considered as potential competing interest s: Click here to enter your full declaration Journal Pre-proof",
    "full_text_length": 53099,
    "chunk_length": 336
  },
  {
    "chunk_id": 3140,
    "paper_filename": "runqiu_2024_unterdisciplinary_medicine_ artificial_intelligence_driven_change_redefining_radiology.pdf",
    "paper_title": "Runqiu 2024 Unterdisciplinary Medicine  Artificial Intelligence Driven Change Redefining Radiology",
    "chunk_index": 0,
    "total_chunks": 44,
    "text_content": "Received: 5September 2024 -Revised: 23September 2024 -Accepted: 2October 2024 DOI:10.1002/INMD.20240063 REVIEW Artificial intelligence\u2010driven change redefining radiology through interdisciplinary innovation Runqiu Huang1|Xiaolin Meng2,3|Xiaoxuan Zhang4,5|Zhendong Luo6| LuCao7,8|Qianjin Feng4,5|Guolin Ma9|DiDong10,11|Yang Wang1 1Department ofRadiology, Zhujiang Hospital ofSouthern Medical University, Guangzhou, China 2SchoolofElectronic Information andElectrical Engineering, Shanghai JiaoTongUniv",
    "full_text_length": 89566,
    "chunk_length": 2513
  },
  {
    "chunk_id": 3141,
    "paper_filename": "runqiu_2024_unterdisciplinary_medicine_ artificial_intelligence_driven_change_redefining_radiology.pdf",
    "paper_title": "Runqiu 2024 Unterdisciplinary Medicine  Artificial Intelligence Driven Change Redefining Radiology",
    "chunk_index": 1,
    "total_chunks": 44,
    "text_content": "Prevention andControl, Grant/Award Number: 212977;Presidential Foundation of Zhujiang Hospital ofSouthern Medical University, Grant/Award Number: yzjj2022ms17; Scientific Research Funding ProjectoftheNanfang MedicalAbstract Artificial intelligence (AI)israpidlyadvancing, yetitsapplications inradi- ologyremainrelatively nascent. Fromaspatiotemporal perspective, thisre- viewexamines theforcesdrivingAIdevelopment anditsintegration with medicine andradiology, withaparticular focusonadvancements addr",
    "full_text_length": 89566,
    "chunk_length": 2874
  },
  {
    "chunk_id": 3142,
    "paper_filename": "runqiu_2024_unterdisciplinary_medicine_ artificial_intelligence_driven_change_redefining_radiology.pdf",
    "paper_title": "Runqiu 2024 Unterdisciplinary Medicine  Artificial Intelligence Driven Change Redefining Radiology",
    "chunk_index": 2,
    "total_chunks": 44,
    "text_content": "Inrecentyears,theartificial intelligence (AI)wavehas notonlytransformed sectorsacrossvariousfieldsbutalso profoundly impacted research andpractice inmedicine, Figure1.Thetraditional workstructures andframeworks ofhumansocietyarebeingtransformed byAI\u2010driven investments andtechnological innovations.1\u20133Several newAI\u2010related journals havebeenestablished withinthe academic realmtoreflectthewidespread application and penetration ofAIinmedicine. Theinterdisciplinary paradigm ofradiology hasevolvedwitht",
    "full_text_length": 89566,
    "chunk_length": 2938
  },
  {
    "chunk_id": 3143,
    "paper_filename": "runqiu_2024_unterdisciplinary_medicine_ artificial_intelligence_driven_change_redefining_radiology.pdf",
    "paper_title": "Runqiu 2024 Unterdisciplinary Medicine  Artificial Intelligence Driven Change Redefining Radiology",
    "chunk_index": 3,
    "total_chunks": 44,
    "text_content": "regarding thephilosophical principles thatgovernthedevelopment ofAI.Further, therearenocomprehensive reviews of AI'sspatiotemporal impactonmedicine andradiology or itspotential directions following thedevelopment of interdisciplinary medicine. Inthisreview,wefocuson thepotential andchallenges ofAIinradiology froma spatiotemporal perspective. 2|THE PHILOSOPHICAL FOUNDATIONS, PIONEERS, AND MILESTONES INAIDEVELOPMENT Overall, wefullyagreewiththesummary ofAIdevel- opmentanditscurrentobjective limita",
    "full_text_length": 89566,
    "chunk_length": 2722
  },
  {
    "chunk_id": 3144,
    "paper_filename": "runqiu_2024_unterdisciplinary_medicine_ artificial_intelligence_driven_change_redefining_radiology.pdf",
    "paper_title": "Runqiu 2024 Unterdisciplinary Medicine  Artificial Intelligence Driven Change Redefining Radiology",
    "chunk_index": 4,
    "total_chunks": 44,
    "text_content": "the applicable Creative Commons License FIGURE 1Spatiotemporal progress inAIdevelopment formedicine andradiology. Theclinicalapplicability ofAIhasbeentheresult ofcumulative scientific advancements andthecontributions ofpivotalcontributors andevents.Asignificant philosophical divergence in AIoccurred duringthe1980s,establishing thedifferentiation between symbolism andconnectionism. Thedevelopment trajectory ofAIhas beenpredominantly shapedbytheprinciples oftheConnectionism school,asindicated byth",
    "full_text_length": 89566,
    "chunk_length": 2915
  },
  {
    "chunk_id": 3145,
    "paper_filename": "runqiu_2024_unterdisciplinary_medicine_ artificial_intelligence_driven_change_redefining_radiology.pdf",
    "paper_title": "Runqiu 2024 Unterdisciplinary Medicine  Artificial Intelligence Driven Change Redefining Radiology",
    "chunk_index": 5,
    "total_chunks": 44,
    "text_content": "inthe reviewwithwhichwedisagree: \u201cwiththeadventofbig dataandcloudcomputing, AIhasseensignificant break- throughs andhasbecomeincreasingly integrated intoour dailylives.\u201dInthisdescription, thereseemstobeasu- perficial nature.18Bysearching for\u201cartificial intelligence\u201d inPubMed, wecanclearlyobserveanexplosive growthin significant AI\u2010related eventsandpublications starting around2017,forming the\u201c6\u201dshapetrendinAIdevelop- ment,asshowninFigure1.Asageneralrule,majorad- vancesinhumanhistoryareoftendrivenb",
    "full_text_length": 89566,
    "chunk_length": 3098
  },
  {
    "chunk_id": 3146,
    "paper_filename": "runqiu_2024_unterdisciplinary_medicine_ artificial_intelligence_driven_change_redefining_radiology.pdf",
    "paper_title": "Runqiu 2024 Unterdisciplinary Medicine  Artificial Intelligence Driven Change Redefining Radiology",
    "chunk_index": 6,
    "total_chunks": 44,
    "text_content": "development wasinadormant phase,asillustrated in Figure1.Afterthesepioneers, AIhasexperienced explo- sivegrowth,forming adistinctive \u201c6\u201dshapeinitsdevel- opmenttrajectory. Ourcuriosity alsodroveustosearchfor literature relatedtotheintegration ofAIandmedicine to exploreitsprogress andtrends. 3|AI's GENERALIZATION IN MEDICINE AND ITS POTENTIAL IN IMAGING WORKFLOWS Theintegration ofAIinmedicine exemplifies aninter- disciplinary research convergence. Byreviewing recent studiespublished intop\u2010tierjour",
    "full_text_length": 89566,
    "chunk_length": 2590
  },
  {
    "chunk_id": 3147,
    "paper_filename": "runqiu_2024_unterdisciplinary_medicine_ artificial_intelligence_driven_change_redefining_radiology.pdf",
    "paper_title": "Runqiu 2024 Unterdisciplinary Medicine  Artificial Intelligence Driven Change Redefining Radiology",
    "chunk_index": 7,
    "total_chunks": 44,
    "text_content": "fromdataintegration and personalized treatment inNEJM,tosurgical advance- mentsinNature Medicine ,innovations insurgery and4of22 / HUANG ETAL. 28326245, 2025, 1, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/INMD.20240063 by INASP/HINARI - PAKISTAN, Wiley Online Library on [19/03/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License path",
    "full_text_length": 89566,
    "chunk_length": 2710
  },
  {
    "chunk_id": 3148,
    "paper_filename": "runqiu_2024_unterdisciplinary_medicine_ artificial_intelligence_driven_change_redefining_radiology.pdf",
    "paper_title": "Runqiu 2024 Unterdisciplinary Medicine  Artificial Intelligence Driven Change Redefining Radiology",
    "chunk_index": 8,
    "total_chunks": 44,
    "text_content": "thecoreandworkflow ofradiology. Byfocusing onthesekeynodesasobservation points,AIcanoffera newperspective onintegration withtheentireimaging workflow (Figure3B).43Thisreviewwillprovideadetailed explanation ofthesefourkeynodes. Asaresultofourstudyoftheliterature, wefoundthat themajority ofcurrentresearch isfocused primarily on twoofthesepoints,thatis,(2)ImageReconstruction and Post\u2010Processing, aswellas(3)Imaging Diagnosis, Figure3B:redlight.Theformerispredominantly ledby expertsinbiomedical engin",
    "full_text_length": 89566,
    "chunk_length": 2717
  },
  {
    "chunk_id": 3149,
    "paper_filename": "runqiu_2024_unterdisciplinary_medicine_ artificial_intelligence_driven_change_redefining_radiology.pdf",
    "paper_title": "Runqiu 2024 Unterdisciplinary Medicine  Artificial Intelligence Driven Change Redefining Radiology",
    "chunk_index": 9,
    "total_chunks": 44,
    "text_content": "Online Library for rules of use; OA articles are governed by the applicable Creative Commons License FIGURE 3Legendonnextpage.6of22 / HUANG ETAL. 28326245, 2025, 1, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/INMD.20240063 by INASP/HINARI - PAKISTAN, Wiley Online Library on [19/03/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License n",
    "full_text_length": 89566,
    "chunk_length": 2364
  },
  {
    "chunk_id": 3150,
    "paper_filename": "runqiu_2024_unterdisciplinary_medicine_ artificial_intelligence_driven_change_redefining_radiology.pdf",
    "paper_title": "Runqiu 2024 Unterdisciplinary Medicine  Artificial Intelligence Driven Change Redefining Radiology",
    "chunk_index": 10,
    "total_chunks": 44,
    "text_content": "imagesforsubsequent processing anddiagnosis. Failure toachieve standardized, high\u2010 qualityimageacquisition directly impacts imaging de- cisionsandclinical accuracy. Without properquality control, datavariability canleadtodiagnostic discrep- ancies,affecting AImodeltraining andhindering AI clinicalapplication. Thus,advancing bothimageacqui- sitionandqualitycontrolisessential. Thisissimilarto thedevelopment offullself\u2010driving (FSD)capabilities in theelectricvehiclemarket. Although theenvironment f",
    "full_text_length": 89566,
    "chunk_length": 2929
  },
  {
    "chunk_id": 3151,
    "paper_filename": "runqiu_2024_unterdisciplinary_medicine_ artificial_intelligence_driven_change_redefining_radiology.pdf",
    "paper_title": "Runqiu 2024 Unterdisciplinary Medicine  Artificial Intelligence Driven Change Redefining Radiology",
    "chunk_index": 11,
    "total_chunks": 44,
    "text_content": "LARGE\u2010SCALE MULTIMODAL DATA Therapiddevelopment ofAIinradiology hasinjected newmomentum intoimageanalysis, particularly in processing andanalyzing large\u2010scale complex data.65\u201367 AIcanperform imaging tasksthatwerepreviously FIGURE 3Identifying promising integration pointsinAI\u2010enhanced imaging workflows fromaspatialperspective. (A)AI\u2010based interventions canenhance thetraditional oncological workflow byaugmenting diagnosis, decision\u2010making, andoutcome prediction at variousstages,withcontinuous feed",
    "full_text_length": 89566,
    "chunk_length": 2661
  },
  {
    "chunk_id": 3152,
    "paper_filename": "runqiu_2024_unterdisciplinary_medicine_ artificial_intelligence_driven_change_redefining_radiology.pdf",
    "paper_title": "Runqiu 2024 Unterdisciplinary Medicine  Artificial Intelligence Driven Change Redefining Radiology",
    "chunk_index": 12,
    "total_chunks": 44,
    "text_content": "Advantages Disadvantages Model relationshipsRadiology applications CNNs47,48\ufffdLocalfeature extraction \ufffdEfficient processing of high\u2010dimensional im- agedata \ufffdHierarchical feature learning\ufffdLimited capability in handling long\u2010range dependencies \ufffdRequirement forlarge\u2010 scaledatasetsThebasisforDeepLab, U\u2010net,and othernetworksClassification; detection; segmentation DeepLab49,50\ufffdMulti\u2010scale feature extraction \ufffdAtrousspatialpyramid pooling\ufffdComputational complexity \ufffdLimited segmentation performance forsmal",
    "full_text_length": 89566,
    "chunk_length": 2827
  },
  {
    "chunk_id": 3153,
    "paper_filename": "runqiu_2024_unterdisciplinary_medicine_ artificial_intelligence_driven_change_redefining_radiology.pdf",
    "paper_title": "Runqiu 2024 Unterdisciplinary Medicine  Artificial Intelligence Driven Change Redefining Radiology",
    "chunk_index": 13,
    "total_chunks": 44,
    "text_content": "ability \ufffdCapability inhandling sequential data \ufffdScalability\ufffdHighcomputational complexity andresource demand \ufffdSuboptimal performance onsmalldatasets \ufffdLongtraining timeand difficult hyperparameterIncreasingly combined withCNNs toformhybridmodelsforenhanced medical imageprocessing capabilitiesClassification; segmentation Abbreviations: CNNs,convolutional neuralnetworks; DeepLab, deepconvolutional neuralnetworks forSemantic imagesegmentation; FasterR\u2010CNN, Faster Region\u2010convolutional neuralnetwork; G",
    "full_text_length": 89566,
    "chunk_length": 2552
  },
  {
    "chunk_id": 3154,
    "paper_filename": "runqiu_2024_unterdisciplinary_medicine_ artificial_intelligence_driven_change_redefining_radiology.pdf",
    "paper_title": "Runqiu 2024 Unterdisciplinary Medicine  Artificial Intelligence Driven Change Redefining Radiology",
    "chunk_index": 14,
    "total_chunks": 44,
    "text_content": "data,including MR,CT,andPET,resulting ina morecomprehensive andaccurate diagnosis.71,72Multi- modaldataintegration notonlyimproves theefficiency ofmultidimensional dataprocessing butalsotransforms thetraditional medical imaging diagnostic modeland realizes interdisciplinary collaboration, contributing to thedevelopment ofdata\u2010driven automated medical imaging.73,74 6|AI\u2010POWERED EARLY DIAGNOSIS SCREENING AND RISK PREDICTION Theapplication ofAIinradiology hasgreatlyenhanced thecapability forearlydi",
    "full_text_length": 89566,
    "chunk_length": 2952
  },
  {
    "chunk_id": 3155,
    "paper_filename": "runqiu_2024_unterdisciplinary_medicine_ artificial_intelligence_driven_change_redefining_radiology.pdf",
    "paper_title": "Runqiu 2024 Unterdisciplinary Medicine  Artificial Intelligence Driven Change Redefining Radiology",
    "chunk_index": 15,
    "total_chunks": 44,
    "text_content": "forinstance, relatedAIproducts havebeenapproved bytheU.S.FoodandDrugAdmin- istration (FDA)fordiagnostic assistance.81Formammo- graphic imaging, Kimetal.evaluated thefeasibility of data\u2010driven biomarkers basedonweakly supervised learning. Onthebasisof29,107digitalmammograms, DIB\u2010MG achieved AUCof0.906and0.903inbothtestand validation sets,demonstrating itswideapplicability.82 Thisnotonlysolidifies AI'simportant roleinbreast cancerscreening butalsoprovides avaluable reference fordetecting othertype",
    "full_text_length": 89566,
    "chunk_length": 2788
  },
  {
    "chunk_id": 3156,
    "paper_filename": "runqiu_2024_unterdisciplinary_medicine_ artificial_intelligence_driven_change_redefining_radiology.pdf",
    "paper_title": "Runqiu 2024 Unterdisciplinary Medicine  Artificial Intelligence Driven Change Redefining Radiology",
    "chunk_index": 16,
    "total_chunks": 44,
    "text_content": "Commons License primarily focusedonsolvingdiagnostic needsrelatedto lungcancer.Forthediagnosis oflungcancer,thereisnowa growing trendtowards integrating datasets frommultiple modalities ormultiple centersthatgobeyondtraditional AI'sreliance onimaging. AstudybyKarimzadeh etal. demonstrated theuseofanAI\u2010based platform fortheearly detection ofnon\u2010small celllungcancer(NSCLC). Basedon theanalysis of64,379specific oncRNAs in540serum samples andotherdata,alogisticregression modelwas developed topredict",
    "full_text_length": 89566,
    "chunk_length": 3062
  },
  {
    "chunk_id": 3157,
    "paper_filename": "runqiu_2024_unterdisciplinary_medicine_ artificial_intelligence_driven_change_redefining_radiology.pdf",
    "paper_title": "Runqiu 2024 Unterdisciplinary Medicine  Artificial Intelligence Driven Change Redefining Radiology",
    "chunk_index": 17,
    "total_chunks": 44,
    "text_content": "biopsiesandfollow\u2010 ups,increasing theearlydetection rateofcancer.Ithas beenshownthatAI\u2010based DLscreening formammog- raphyimproved radiologists' specificity from93.5%to94.2%(p=0.002)in66,661caseswhilemaintaining almost thesamesensitivity astheoriginal diagnosis (90.1%vs. 90.6%)andreducing thenumber ofmammograms reviewed byapproximately 19.3%.89AsAIisgradually promoted, breastcancerscreening accuracy continues to improve, misdiagnosis ratesdecline,andclinicians receive morereliabledecision support",
    "full_text_length": 89566,
    "chunk_length": 2846
  },
  {
    "chunk_id": 3158,
    "paper_filename": "runqiu_2024_unterdisciplinary_medicine_ artificial_intelligence_driven_change_redefining_radiology.pdf",
    "paper_title": "Runqiu 2024 Unterdisciplinary Medicine  Artificial Intelligence Driven Change Redefining Radiology",
    "chunk_index": 18,
    "total_chunks": 44,
    "text_content": "enablesdoctorstoformulate moreprecisesurgical plans byanalyzing multimodal imaging data,improving tumor localization accuracy, andreducing surgical complica- tions.35,93,94Asanexample, Quetal.developed aradio- micmodelforpredicting tumorbudding gradein preoperative rectalcancerusingMRIT2WIandmultiple10of22 / HUANG ETAL. 28326245, 2025, 1, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/INMD.20240063 by INASP/HINARI - PAKISTAN, Wiley Online Library on [19/03/2025]. See the Terms and C",
    "full_text_length": 89566,
    "chunk_length": 2546
  },
  {
    "chunk_id": 3159,
    "paper_filename": "runqiu_2024_unterdisciplinary_medicine_ artificial_intelligence_driven_change_redefining_radiology.pdf",
    "paper_title": "Runqiu 2024 Unterdisciplinary Medicine  Artificial Intelligence Driven Change Redefining Radiology",
    "chunk_index": 19,
    "total_chunks": 44,
    "text_content": "commercial cardiovascular AIproducts havegreatlyenhanced clinicaldiagnostic ac- curacy.Artificial intelligence alsoplaysacrucialroleinthe earlyscreening andtreatment evaluation ofvascular dis- eases.Incontrast toconventional quantitative coronary angiography, AI\u2010assisted quantitative coronary CTangi- ography (AI\u2010QCT) iscapable ofrapidlyandaccurately detecting andexcluding high\u2010grade stenosis. Basedonthe analysis ofdatafrom303patients, AI\u2010QCT exhibited a sensitivity of94%,specificity of68%,andacc",
    "full_text_length": 89566,
    "chunk_length": 2747
  },
  {
    "chunk_id": 3160,
    "paper_filename": "runqiu_2024_unterdisciplinary_medicine_ artificial_intelligence_driven_change_redefining_radiology.pdf",
    "paper_title": "Runqiu 2024 Unterdisciplinary Medicine  Artificial Intelligence Driven Change Redefining Radiology",
    "chunk_index": 20,
    "total_chunks": 44,
    "text_content": "superior observer agreement (W=0.69) compared tomostexperts(W=0,67).101Furtherstudies indicate thatAIsystems basedontransformer models, usingRadImageNet pre\u2010trained andmultimodal data,can effectively diagnose fivetypesofILDandpredict3\u2010year survival ratesforpatients.102Clinicalresearch hasshown thatAIcanassistdoctorsindetecting respiratory lesionsat anearlystage. AmajorpublichealtheventsuchasCOVID\u201019 has proventheimportance ofAIinrespiratory infectious dis- eases.103,104Itispossible toaccurately ",
    "full_text_length": 89566,
    "chunk_length": 2708
  },
  {
    "chunk_id": 3161,
    "paper_filename": "runqiu_2024_unterdisciplinary_medicine_ artificial_intelligence_driven_change_redefining_radiology.pdf",
    "paper_title": "Runqiu 2024 Unterdisciplinary Medicine  Artificial Intelligence Driven Change Redefining Radiology",
    "chunk_index": 21,
    "total_chunks": 44,
    "text_content": "omicsisgradually emerging asan innovative direction forAIinradiology. Thereistremen- douspotential formultimodal omicsdevelopment, particularly indiagnosis, riskprediction, andpersonalized treatment.107,108Inrecentyears,multi\u2010omics integration hasbecomeessential forimproving cancerdiagnosis and personalized treatment. Bycombining genomics, tran- scriptomics, proteomics, andmetabolomics data,re- searchers canbetterunderstand cancermechanisms, whileAIalgorithms enhance earlydiagnosis andtreat- men",
    "full_text_length": 89566,
    "chunk_length": 2860
  },
  {
    "chunk_id": 3162,
    "paper_filename": "runqiu_2024_unterdisciplinary_medicine_ artificial_intelligence_driven_change_redefining_radiology.pdf",
    "paper_title": "Runqiu 2024 Unterdisciplinary Medicine  Artificial Intelligence Driven Change Redefining Radiology",
    "chunk_index": 22,
    "total_chunks": 44,
    "text_content": "gradually revolutionizing traditional medical imaging diagnostics byintegrating multimodal dataandimproving diagnostic andtherapeutic precision. However, thewidespread clinicalapplication ofAIstill facesnumerous overlooked issues,suchasethicalcon- cerns,dataprivacy, modelinterpretability, andadapt- abilityincomplex diseases. Theseissuesarealsocloselymonitored byNEJM.2Whiledeveloping AI'spowerful capabilities, itisessential topayattention tosafetyand ethicalconsiderations. 14 |GPT INTEGRATION RED",
    "full_text_length": 89566,
    "chunk_length": 2898
  },
  {
    "chunk_id": 3163,
    "paper_filename": "runqiu_2024_unterdisciplinary_medicine_ artificial_intelligence_driven_change_redefining_radiology.pdf",
    "paper_title": "Runqiu 2024 Unterdisciplinary Medicine  Artificial Intelligence Driven Change Redefining Radiology",
    "chunk_index": 23,
    "total_chunks": 44,
    "text_content": "curacyrateinactualconversions. Allfree\u2010text reportswere successfully converted intovalidJSONfileswithout any errors.117(b)Errordetection andcorrection inreports: GPT\u20104hasbeenusedtodetectandcorrecterrorsinradi- ologyreports,significantly improving thereportaccuracy to94%.118(c)generation andsummarization ofimaging reports: Ingenerating clinicalreportsforglioblastoma patients, GPT\u20104's reportswereconsistent withexpertas- sessments in91%ofcases.Theaccuracy ofessential in- formation reached 95%,andth",
    "full_text_length": 89566,
    "chunk_length": 2768
  },
  {
    "chunk_id": 3164,
    "paper_filename": "runqiu_2024_unterdisciplinary_medicine_ artificial_intelligence_driven_change_redefining_radiology.pdf",
    "paper_title": "Runqiu 2024 Unterdisciplinary Medicine  Artificial Intelligence Driven Change Redefining Radiology",
    "chunk_index": 24,
    "total_chunks": 44,
    "text_content": "layerofeachmodality, allowing theiterative learning ofenhanced featurerepresentations withinamultimodal context. Theunimodal datacanbefused(C)atasinglelevelor(D)progressively acrossdifferent layers.(E)Guided fusionenablesthemodeltoleverage information fromonemodality toinformandenhance featureextraction inanothermodality. (F)The keyforthesymbols usedisprovided. Reproduced withpermission.73Copyright 2022,CellPress.HUANG ETAL. /13of22 28326245, 2025, 1, Downloaded from https://onlinelibrary.wiley.",
    "full_text_length": 89566,
    "chunk_length": 2740
  },
  {
    "chunk_id": 3165,
    "paper_filename": "runqiu_2024_unterdisciplinary_medicine_ artificial_intelligence_driven_change_redefining_radiology.pdf",
    "paper_title": "Runqiu 2024 Unterdisciplinary Medicine  Artificial Intelligence Driven Change Redefining Radiology",
    "chunk_index": 25,
    "total_chunks": 44,
    "text_content": "inkeybenchmarks. Since2015\u20132016, DL\u2010driven machine visionsystemshavematched orexceeded humancapabilities, particularly invisualquestion answering (VQA)andmulti\u2010task language understanding (MMLU); (D)The development anddeployment offoundational modelframeworks havebeenledpredominantly byindustry, withlimitedacademic contributions; (E)Training costsforAImodelshaveincreased sharplyovertime;(F)Overthepast2\u20134years,LLMshavebeencategorized intoopen\u2010source, limitedopen\u2010source, andfullyclosed\u2010source, wit",
    "full_text_length": 89566,
    "chunk_length": 2710
  },
  {
    "chunk_id": 3166,
    "paper_filename": "runqiu_2024_unterdisciplinary_medicine_ artificial_intelligence_driven_change_redefining_radiology.pdf",
    "paper_title": "Runqiu 2024 Unterdisciplinary Medicine  Artificial Intelligence Driven Change Redefining Radiology",
    "chunk_index": 26,
    "total_chunks": 44,
    "text_content": "tasksbutalsobe- ginstoencroach uponsomeoftheradiologists' corere- sponsibilities (Figure 6).Theintegration ofGPTwith radiology islikelytoprogress inthefollowing directions: (a)Development andapplication ofautomated diagnostic assistance tools:Itisintended thatGPTwillbeusedto developmoreadvanced diagnostic assistance systemsasameansofimproving diagnostic efficiency andaccuracy, andreducing theworkload ofradiologists. (b)Real\u2010time clinical decision support: WithGPT,healthcare pro- fessionals canac",
    "full_text_length": 89566,
    "chunk_length": 2597
  },
  {
    "chunk_id": 3167,
    "paper_filename": "runqiu_2024_unterdisciplinary_medicine_ artificial_intelligence_driven_change_redefining_radiology.pdf",
    "paper_title": "Runqiu 2024 Unterdisciplinary Medicine  Artificial Intelligence Driven Change Redefining Radiology",
    "chunk_index": 27,
    "total_chunks": 44,
    "text_content": "INASP/HINARI - PAKISTAN, Wiley Online Library on [19/03/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License humans; however, inmorecomplex tasks,suchas competition levelmathematics, AIstillfallsshort.(b)The shiftinleadership ofAIfromacademia toindustry: Atotal of51AImodelswerelaunched bytheindustry in2023, whereas only15modelsweredeveloped byacademi",
    "full_text_length": 89566,
    "chunk_length": 2518
  },
  {
    "chunk_id": 3168,
    "paper_filename": "runqiu_2024_unterdisciplinary_medicine_ artificial_intelligence_driven_change_redefining_radiology.pdf",
    "paper_title": "Runqiu 2024 Unterdisciplinary Medicine  Artificial Intelligence Driven Change Redefining Radiology",
    "chunk_index": 28,
    "total_chunks": 44,
    "text_content": "ACKNOWLEDGMENTS Theresearch perspective wassupported bytheGuang- dong Natural Science Foundation (Grant No.2023A1515012499); Science andTechnology Pro- gramofGuangzhou: KeyProject Funds(GrantNo. 2024B03J0781); ChinaPostdoctoral Science Foundation SpecialFundforPneumonia Epidemic Prevention and Control(GrantNo.212977); thePresidential Foundation ofZhujiang Hospital ofSouthern Medical University (Grant No.yzjj2022ms17); theScientific Research Funding ProjectoftheNanfang Medical Imaging Alli- ance(",
    "full_text_length": 89566,
    "chunk_length": 2453
  },
  {
    "chunk_id": 3169,
    "paper_filename": "runqiu_2024_unterdisciplinary_medicine_ artificial_intelligence_driven_change_redefining_radiology.pdf",
    "paper_title": "Runqiu 2024 Unterdisciplinary Medicine  Artificial Intelligence Driven Change Redefining Radiology",
    "chunk_index": 29,
    "total_chunks": 44,
    "text_content": "Rev. Cancer 2018,18,500. 5.D.J.Mollura, M.P.Culp,E.Pollack, G.Battino, J.R.Scheel, V.L.Mango, A.Elahi,A.Schweitzer, F.Dako, Radiology 2020,297,513. 6.H.Xie,Y.Jia,S.Liu, Interdiscip. Med. 2024,2,e20230056. 7.GBD2019Diseases andInjuries Collaborators. Lancet 2020, 396,1204. 8.GBDChronic Respiratory Disease Collaborators. Lancet Respir. Med. 2020,8,585. 9.F.Bray,J.Ferlay,I.Soerjomataram, R.L.Siegel,L.A.Torre, A.Jemal, CACancer J.Clin. 2018,68,394. 10.C.J.L.Murray, Nat. Med. 2022,28,2019. 11.G.A.Rot",
    "full_text_length": 89566,
    "chunk_length": 2521
  },
  {
    "chunk_id": 3170,
    "paper_filename": "runqiu_2024_unterdisciplinary_medicine_ artificial_intelligence_driven_change_redefining_radiology.pdf",
    "paper_title": "Runqiu 2024 Unterdisciplinary Medicine  Artificial Intelligence Driven Change Redefining Radiology",
    "chunk_index": 30,
    "total_chunks": 44,
    "text_content": "D.Acharya, T.Adair,O.M.Adebayo, Z.Ademi, S.M.Advani, K.Afshari, A.Afshin,G.Agarwal, P.Agasthi, S.Ahmad, S.Ahmadi, M.B.Ahmed, B.Aji,Y.Akalu,W. Akande\u2010Sholabi, A.Aklilu,C.J.Akunna, F.Alahdab, A.Al\u2010 Eyadhy, K.F.Alhabib, S.M.Alif,V.Alipour, S.M.Aljunid, F.Alla,A.Almasi\u2010Hashiani, S.Almustanyir, R.M.Al\u2010 Raddadi, A.K.Amegah, S.Amini,A.Aminorroaya, H. Amu,D.A.Amugsi, R.Ancuceanu, D.Anderlini, T.Andrei, C.L.Andrei, A.Ansari\u2010Moghaddam, Z.A.Anteneh, I.C. Antonazzo, B.Antony, R.Anwer,L.T.Appiah, J.Arabloo, ",
    "full_text_length": 89566,
    "chunk_length": 2934
  },
  {
    "chunk_id": 3171,
    "paper_filename": "runqiu_2024_unterdisciplinary_medicine_ artificial_intelligence_driven_change_redefining_radiology.pdf",
    "paper_title": "Runqiu 2024 Unterdisciplinary Medicine  Artificial Intelligence Driven Change Redefining Radiology",
    "chunk_index": 31,
    "total_chunks": 44,
    "text_content": "Hartono, M.Hashemian, A.Hashi,S.Hassan, H.Y.Hassen, R.J.Havmoeller, S.I.Hay,K.Hayat,G.Heidari, C.Herteliu, R.Holla,M.Hosseini, M.Hosseinzadeh, M.Hostiuc, S. Hostiuc, M.Househ, J.Huang,A.Humayun, I.Iavicoli, C.U. Ibeneme, S.E.Ibitoye,O.S.Ilesanmi, I.M.Ilic,M.D.Ilic,U. Iqbal,S.S.N.Irvani,S.M.S.Islam,R.M.Islam,H.Iso,M. Iwagami, V.Jain,T.Javaheri, S.K.Jayapal, S.Jayaram, R. Jayawardena, P.Jeemon, R.P.Jha,J.B.Jonas,J.Jonna- gaddala, F.Joukar,J.J.Jozwiak, M.J\u00fcrisson, A.Kabir,T. Kahlon, R.Kalani, R.Kal",
    "full_text_length": 89566,
    "chunk_length": 2802
  },
  {
    "chunk_id": 3172,
    "paper_filename": "runqiu_2024_unterdisciplinary_medicine_ artificial_intelligence_driven_change_redefining_radiology.pdf",
    "paper_title": "Runqiu 2024 Unterdisciplinary Medicine  Artificial Intelligence Driven Change Redefining Radiology",
    "chunk_index": 32,
    "total_chunks": 44,
    "text_content": "Piradov, F.Pishgar, V.Podder,R.V.Polibin, A.Pourshams, D.R.Pribadi, N.Rabiee,M.Rabiee,A.Radfar,A.Rafiei,F. Rahim, V.Rahimi\u2010Movaghar, M.H.UrRahman, M.A. Rahman, A.M.Rahmani, I.Rakovac, P.Ram,S.Ram- alingam, J.Rana,P.Ranasinghe, S.J.Rao,P.Rathi,L.Rawal, W.F.Rawasia, R.Rawassizadeh, G.Remuzzi, A.M. Renzaho, A.Rezapour, S.M.Riahi,R.L.Roberts\u2010Thomson, L.Roever, P.Rohloff, M.Romoli, G.Roshandel, G.M. Rwegerera, S.Saadatagah, M.M.Saber\u2010Ayad, S.Sabour, S. Sacco,M.Sadeghi, S.SaeediMoghaddam, S.Safari,A. ",
    "full_text_length": 89566,
    "chunk_length": 2688
  },
  {
    "chunk_id": 3173,
    "paper_filename": "runqiu_2024_unterdisciplinary_medicine_ artificial_intelligence_driven_change_redefining_radiology.pdf",
    "paper_title": "Runqiu 2024 Unterdisciplinary Medicine  Artificial Intelligence Driven Change Redefining Radiology",
    "chunk_index": 33,
    "total_chunks": 44,
    "text_content": "articles are governed by the applicable Creative Commons License Yamagishi, L.L.Yan,S.Yandrapalli, Y.Yano,H.Yatsuya, T.Y.Yeheyis, Y.Yeshaw, C.S.Yilgwan, N.Yonemoto, C.Yu, H.Yusefzadeh, G.Zachariah, S.B.Zaman,M.S.Zaman,M. Zamanian, R.Zand,A.Zandifar, A.Zarghi,M.S.Zastrozhin, A.Zastrozhina, Z.J.Zhang,Y.Zhang,W.Zhang,C.Zhong, Z.Zou,Y.M.H.Zuniga, J.Am. Coll. Cardiol. 2020,76,2982. 12.M.Zhou,H.Wang,X.Zeng,P.Yin,J.Zhu,W.Chen,X.Li,L. Wang,L.Wang,Y.Liu,J.Liu,M.Zhang,J.Qi,S.Yu,A. Afshin,E.Gakidou, S.Glen",
    "full_text_length": 89566,
    "chunk_length": 2877
  },
  {
    "chunk_id": 3174,
    "paper_filename": "runqiu_2024_unterdisciplinary_medicine_ artificial_intelligence_driven_change_redefining_radiology.pdf",
    "paper_title": "Runqiu 2024 Unterdisciplinary Medicine  Artificial Intelligence Driven Change Redefining Radiology",
    "chunk_index": 34,
    "total_chunks": 44,
    "text_content": "2023,51,27. 29.W.Li,Z.Huang,Z.Chen,Y.Jiang,C.Zhou,X.Zhang,W. Fan,Y.Zhao,L.Zhang,L.Wan,Y.Yang,H.Zheng,D.Liang, Z.Hu,Eur. Radiol. 2024,34,5578. 30.J.S.Brownstein, B.Rader,C.M.Astley,H.Tian, N.Engl. J. Med. 2023,388,1597. 31.B.Gomes,E.A.Ashley, N.Engl. J.Med. 2023,388,2456. 32.D.J.Hunter, C.Holmes, N.Engl. J.Med. 2023,389,1211. 33.N.R.Sahni,B.Carrus, N.Engl. J.Med. 2023,389,348. 34.K.H.Yu,E.Healey,T.Y.Leong,I.S.Kohane, A.K.Manrai, N.Engl. J.Med. 2024,390,1895. 35.C.Varghese, E.M.Harrison, G.O'Grady",
    "full_text_length": 89566,
    "chunk_length": 3065
  },
  {
    "chunk_id": 3175,
    "paper_filename": "runqiu_2024_unterdisciplinary_medicine_ artificial_intelligence_driven_change_redefining_radiology.pdf",
    "paper_title": "Runqiu 2024 Unterdisciplinary Medicine  Artificial Intelligence Driven Change Redefining Radiology",
    "chunk_index": 35,
    "total_chunks": 44,
    "text_content": "O.Arnaout, C.Abbosh, I.F. Dunn,R.H.Mak,R.M.Tamimi, C.M.Tempany, C.Swan- ton,U.Hoffmann, L.H.Schwartz, R.J.Gillies,R.Y.Huang, H.J.W.L.Aerts, CACancer J.Clin. 2019,69,127. 43.A.S.Tejani,T.S.Cook,M.Hussain, T.S.SippelSchmidt, K.P.O'Donnell, S.Arzen, Radiology 2024,311,e232653. 44.Y.Wang,X.Lu,Y.Zhang,X.Zhang,K.Wang,J.Liu,X.Li,R. Hu,X.Meng,S.Dou,H.Hao,X.Zhao,W.Hu,C.Li,Y.Gao, Z.Wang,G.Lu,F.Yan,B.Zhang, eBioMedicine 2020,54, 102724. 45.Y.Wang,F.Yan,X.Lu,G.Zheng,X.Zhang,C.Wang,K. Zhou,Y.Zhang,H.Li,Q.Zha",
    "full_text_length": 89566,
    "chunk_length": 2737
  },
  {
    "chunk_id": 3176,
    "paper_filename": "runqiu_2024_unterdisciplinary_medicine_ artificial_intelligence_driven_change_redefining_radiology.pdf",
    "paper_title": "Runqiu 2024 Unterdisciplinary Medicine  Artificial Intelligence Driven Change Redefining Radiology",
    "chunk_index": 36,
    "total_chunks": 44,
    "text_content": "Comput. Methods Programs Biomed. 2021,200,105866. 56.X.Yi,E.Walia,P.Babyn, Med. Image Anal. 2019,58,101552. 57.A.Dash,J.Ye,G.Wang, IEEE Access 2023,12,18330. 58.F.Shamshad, S.Khan,S.W.Zamir,M.H.Khan,M.Hayat, F.S.Khan,H.Fu,Med. Image Anal. 2023,88,102802. 59.R.Azad,A.Kazerouni, M.Heidari, E.K.Aghdam, A.Molaei, Y.Jia,A.Jose,R.Roy,D.Merhof, Med. Image Anal. 2024,91, 103000. 60.A.Hata,M.L.Schiebler, D.A.Lynch,H.Hatabu, Radiology 2021,301,19. 61.M.S.Jochelson, M.B.I.Lobbes, Radiology 2021,299,36. 62.",
    "full_text_length": 89566,
    "chunk_length": 2916
  },
  {
    "chunk_id": 3177,
    "paper_filename": "runqiu_2024_unterdisciplinary_medicine_ artificial_intelligence_driven_change_redefining_radiology.pdf",
    "paper_title": "Runqiu 2024 Unterdisciplinary Medicine  Artificial Intelligence Driven Change Redefining Radiology",
    "chunk_index": 37,
    "total_chunks": 44,
    "text_content": "2020,39,204. 72.A.Sedghi,A.Mehrtash, A.Jamzad, A.Amalou, W.M.Wells, III,T.Kapur,J.T.Kwak,B.Turkbey, P.Choyke, P.Pinto,B. Wood,S.Xu,P.Abolmaesumi, P.Mousavi, Int.J.Comput. Assist. Radiol. Surg. 2020,15,1215. 73.J.Lipkova, R.J.Chen,B.Chen,M.Y.Lu,M.Barbieri, D. Shao,A.J.Vaidya,C.Chen,L.Zhuang, D.F.K.Williamson, M.Shaban, T.Y.Chen,F.Mahmood, Cancer Cell 2022,40, 1095. 74.F.Mohsen, H.Ali,N.ElHajj,Z.Shah, Sci.Rep. 2022,12, 17981. 75.S.H.Park,K.Han, Radiology 2018,286,800. 76.P.Sharma, M.Suehling, T.Fl",
    "full_text_length": 89566,
    "chunk_length": 2901
  },
  {
    "chunk_id": 3178,
    "paper_filename": "runqiu_2024_unterdisciplinary_medicine_ artificial_intelligence_driven_change_redefining_radiology.pdf",
    "paper_title": "Runqiu 2024 Unterdisciplinary Medicine  Artificial Intelligence Driven Change Redefining Radiology",
    "chunk_index": 38,
    "total_chunks": 44,
    "text_content": "Surg 2024,18,229.HUANG ETAL. /19of22 28326245, 2025, 1, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/INMD.20240063 by INASP/HINARI - PAKISTAN, Wiley Online Library on [19/03/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License 93.S.Xia,Q.Li,H.T.Zhu,X.Y.Zhang,Y.J.Shi,D.Yang,J.Wu, Z.Guan,Q.Lu,X.T.Li,Y.S.Sun, BMC Cancer 2024,24,315. 94.G.",
    "full_text_length": 89566,
    "chunk_length": 2947
  },
  {
    "chunk_id": 3179,
    "paper_filename": "runqiu_2024_unterdisciplinary_medicine_ artificial_intelligence_driven_change_redefining_radiology.pdf",
    "paper_title": "Runqiu 2024 Unterdisciplinary Medicine  Artificial Intelligence Driven Change Redefining Radiology",
    "chunk_index": 39,
    "total_chunks": 44,
    "text_content": "Deyer,A.Jacobi,M.Padilla, Z.A.Fayad,Y.Yang, Nat. Commun. 2023,14,2272. 103.Y.Wang,X.Lu,Y.Li,H.Chen,T.Chen,N.Su,F.Huang,J. Zhou,B.Zhang,F.Yan,J.Wang, Am. J.Respir. Crit. Care Med. 2020,201,1430. 104.S.H.R.Cheong, Y.J.X.Ng,Y.Lau,S.T.Lau, Prev. Med. 2022,162,107170. 105.A.Arian,M.M.Mehrabi Nejad,M.Zoorpaikar, N.Hasan- zadeh,S.Sotoudeh\u2010Paima, S.Kolahi,M.Gity,H.Soltanian\u2010 Zadeh, PLoS One 2023,18,e0294899. 106.X.Fang,U.Kruger, F.Homayounieh, H.Chao,J.Zhang, S.R.Digumarthy, C.D.Arru,M.K.Kalra,P.Yan, In",
    "full_text_length": 89566,
    "chunk_length": 2858
  },
  {
    "chunk_id": 3180,
    "paper_filename": "runqiu_2024_unterdisciplinary_medicine_ artificial_intelligence_driven_change_redefining_radiology.pdf",
    "paper_title": "Runqiu 2024 Unterdisciplinary Medicine  Artificial Intelligence Driven Change Redefining Radiology",
    "chunk_index": 40,
    "total_chunks": 44,
    "text_content": "C.Rubbert, N.C.Lehnen, C.Deuschl, M.Schla- mann,M.H.Sch\u00f6nfeld, J.Kottlors, Radiology 2024,312, e232640. 120.A.Cozzi,K.Pinker,A.Hidber,T.Zhang,L.Bonomo, R.Lo Gullo,B.Christianson, M.Curti,S.Rizzo,F.DelGrande, R.M.Mann,S.Schiaffino, A.Panzer, Radiology 2024,311, e232133. 121.F.Crim\u00ec,E.Quaia, Radiology 2023,308,e231701. 122.M.A.Fink,A.Bischoff, C.A.Fink,M.Moll,J.Kroschke, L. Dulz,C.P.Heu\u00dfel, H.U.Kauczor, T.F.Weber, Radiology 2023,308,e231362. 123.S.Krishna, N.Bhambra, R.Bleakney, R.Bhayana, S.Atzen",
    "full_text_length": 89566,
    "chunk_length": 2333
  },
  {
    "chunk_id": 3181,
    "paper_filename": "runqiu_2024_unterdisciplinary_medicine_ artificial_intelligence_driven_change_redefining_radiology.pdf",
    "paper_title": "Runqiu 2024 Unterdisciplinary Medicine  Artificial Intelligence Driven Change Redefining Radiology",
    "chunk_index": 41,
    "total_chunks": 44,
    "text_content": "devices, contributing tosignificant advancements inhealth- caretechnology. Currently, Meng'sresearch focuses onleveraging artificial intelligence (AI)andLarge Language Models(LLMs)torevolutionize theentire scanning workflow through intelligent automation, withtheaimofenhancing theprecision, efficiency, andreliability ofmedical imaging processes, thus playing apivotalroleintheintegration ofAIinto modern healthcare systems. Dr.Xiaoxuan Zhangobtained herPh. D.fromSouthern Medical University in 2023",
    "full_text_length": 89566,
    "chunk_length": 2424
  },
  {
    "chunk_id": 3182,
    "paper_filename": "runqiu_2024_unterdisciplinary_medicine_ artificial_intelligence_driven_change_redefining_radiology.pdf",
    "paper_title": "Runqiu 2024 Unterdisciplinary Medicine  Artificial Intelligence Driven Change Redefining Radiology",
    "chunk_index": 42,
    "total_chunks": 44,
    "text_content": "experience inmedical imageanalysis, including prostate medical imagesegmenta- tion,brainimageanalysis, cross\u2010modality image registration, cancerprognosis prediction basedon imageandgenetic data,andthedevelopment of medical imageapplication systems. Prof. Guolin MaisaChiefPhysician andPostdoctoral Supervisor atthe Department ofRadiology, China\u2010Japan Friendship Hospital. Healsoholds professorships andPh.D.supervisor rolesatPekingUniversity HealthSci- enceCenter, PekingUnionMedical College, andCapi",
    "full_text_length": 89566,
    "chunk_length": 2331
  },
  {
    "chunk_id": 3183,
    "paper_filename": "runqiu_2024_unterdisciplinary_medicine_ artificial_intelligence_driven_change_redefining_radiology.pdf",
    "paper_title": "Runqiu 2024 Unterdisciplinary Medicine  Artificial Intelligence Driven Change Redefining Radiology",
    "chunk_index": 43,
    "total_chunks": 44,
    "text_content": "the applicable Creative Commons License Dr. Yang Wanghasbeenservingas anAssociate ChiefPhysician inthe Department ofRadiology atZhujiang Hospital, Southern Medical University sincetheendof2021,wherehealso holdstheposition ofDeputyDirector oftheTranslational Research Center. Inaddition, hementors bothacademic andprofes- sionalmaster's students, guiding theirresearch and clinical development. Dr.Wangcompleted his postdoctoral research attheDepartment ofRadi- ologyatNanjing DrumTowerHospital. Heobt",
    "full_text_length": 89566,
    "chunk_length": 1232
  },
  {
    "chunk_id": 3184,
    "paper_filename": "shahriar_2023_review_of_CAD_systems_for_breast_mass_detection_in_mammography_based_on_deep Learning.pdf",
    "paper_title": "Shahriar 2023 Review Of Cad Systems For Breast Mass Detection In Mammography Based On Deep Learning",
    "chunk_index": 0,
    "total_chunks": 97,
    "text_content": "Review Not peer-reviewed version A Review of CAD systems for Breast Mass Detection in Mammography Based on Deep Learning Shahriar Mohammadi * and Mohammad Ahmadi Livani Posted Date: 26 May 2023 doi: 10.20944/preprints202305.1832.v1 Keywords: Breast cancer; convolution neural networks; computer-aided diagnosis systems; segmentation; classification Preprints.org is a free multidiscipline platform providing preprint service that is dedicated to making early versions of research outputs permanently ",
    "full_text_length": 96734,
    "chunk_length": 1491
  },
  {
    "chunk_id": 3185,
    "paper_filename": "shahriar_2023_review_of_CAD_systems_for_breast_mass_detection_in_mammography_based_on_deep Learning.pdf",
    "paper_title": "Shahriar 2023 Review Of Cad Systems For Breast Mass Detection In Mammography Based On Deep Learning",
    "chunk_index": 1,
    "total_chunks": 97,
    "text_content": "Iran ; mohammad.ahmadilivani@email.kntu.ac.ir * Correspondence: mohammadi@kntu.ac.ir Abstract: Early detection of breast cancer, one of the most common cancers in women worl dwide, raises survival rates and lowers the cost of treatment. Although breast cancer detection and classification (CAD) tools have improved, some issues and restrictions still require further research. The creation of breast cancer CAD systems, particular ly with deep learning models, was significantly impacted by recent ad",
    "full_text_length": 96734,
    "chunk_length": 1388
  },
  {
    "chunk_id": 3186,
    "paper_filename": "shahriar_2023_review_of_CAD_systems_for_breast_mass_detection_in_mammography_based_on_deep Learning.pdf",
    "paper_title": "Shahriar 2023 Review Of Cad Systems For Breast Mass Detection In Mammography Based On Deep Learning",
    "chunk_index": 2,
    "total_chunks": 97,
    "text_content": "breast cancer. We point out research gaps a nd make suggestions for further study. This systematic review may be beneficial for clinicians using CAD systems to diagnose breast cancer early on and for researchers looking for knowledge gaps and making more contribu tions to the field. Keywords: Breast cancer; convolution neural networks; computer -aided diagnosis systems; segmentation; classific ation 1. Introduction The introduction should briefly place the study in a broad context and highlight ",
    "full_text_length": 96734,
    "chunk_length": 1282
  },
  {
    "chunk_id": 3187,
    "paper_filename": "shahriar_2023_review_of_CAD_systems_for_breast_mass_detection_in_mammography_based_on_deep Learning.pdf",
    "paper_title": "Shahriar 2023 Review Of Cad Systems For Breast Mass Detection In Mammography Based On Deep Learning",
    "chunk_index": 3,
    "total_chunks": 97,
    "text_content": "in order of appearance and indicated by a numeral or numerals in square brackets \u2014e.g., [1] or [2,3], or [4 \u20136]. See the end of the document for further details on references. Breast cancer is the most prevalent cancer and the second cause of cancer -related deaths among women. Early - diagnosis of this disease can significantly help the treatment process. Unfortu nately, devel oped countries have a higher infection rate [1]. Screening significantly impacts early diagnosis and decreases the brea",
    "full_text_length": 96734,
    "chunk_length": 1330
  },
  {
    "chunk_id": 3188,
    "paper_filename": "shahriar_2023_review_of_CAD_systems_for_breast_mass_detection_in_mammography_based_on_deep Learning.pdf",
    "paper_title": "Shahriar 2023 Review Of Cad Systems For Breast Mass Detection In Mammography Based On Deep Learning",
    "chunk_index": 4,
    "total_chunks": 97,
    "text_content": "and enhancing the discovery of subtle but significant variations in tissues and anatomical structures, which are essentia l for the timely treatment of diseases [5]. Deep learn ing models, especially Convolutional Neural Networks (CNNs), have significantly advanced computer vision, including medical image analysis [6\u20138]. The reason for this success is the capability to directly learn hierarchical representatio ns of features from data rather than utilizing handcrafted properties in terms of doma",
    "full_text_length": 96734,
    "chunk_length": 1398
  },
  {
    "chunk_id": 3189,
    "paper_filename": "shahriar_2023_review_of_CAD_systems_for_breast_mass_detection_in_mammography_based_on_deep Learning.pdf",
    "paper_title": "Shahriar 2023 Review Of Cad Systems For Breast Mass Detection In Mammography Based On Deep Learning",
    "chunk_index": 5,
    "total_chunks": 97,
    "text_content": "This work presents state -of-the-art breast cancer CAD systems studies based on deep learning in mammography images. To meet the study's objective, we tried to respond t o the research questions as follows: \u2022 What are the breast cancer screening methods, and what are public databases for mammography images applied in CAD systems? \u2022 What steps are in developing CAD systems for breast cancer detection? \u2022 What are the deep learnin g methods used to develop CAD systems? \u2022 What measurements are used ",
    "full_text_length": 96734,
    "chunk_length": 1251
  },
  {
    "chunk_id": 3190,
    "paper_filename": "shahriar_2023_review_of_CAD_systems_for_breast_mass_detection_in_mammography_based_on_deep Learning.pdf",
    "paper_title": "Shahriar 2023 Review Of Cad Systems For Breast Mass Detection In Mammography Based On Deep Learning",
    "chunk_index": 6,
    "total_chunks": 97,
    "text_content": "learning. In Section 4, we briefly review the CAD systems. The current literature for segmentation of breast mass in mammography is presented. Then classification of breast mass literature is presented. We emphasize the strengths and limitations of studies. Section 6 gives the asses sment metrics for breast cancer CAD systems. Then, we discuss future trends and remaining problems in breast cancer in Section 7. Finally, we provide a conclusion in section 8. 2. Imaging Modalities By screening, abn",
    "full_text_length": 96734,
    "chunk_length": 1311
  },
  {
    "chunk_id": 3191,
    "paper_filename": "shahriar_2023_review_of_CAD_systems_for_breast_mass_detection_in_mammography_based_on_deep Learning.pdf",
    "paper_title": "Shahriar 2023 Review Of Cad Systems For Breast Mass Detection In Mammography Based On Deep Learning",
    "chunk_index": 7,
    "total_chunks": 97,
    "text_content": "most common breast cancer screening tests is mammography, usi ng uses X -rays to image the breast and can identify and diagnose tiny and unpalpa ble masses. Mammography can detect abnormal cells around the breast duct, known as Ductal Carcino ma in Situ (DCIS). Factors like the skill and experience of the radiologist, tumor size, and breast tissue d ensity can affect mammography sensitivity. Mammography is unsuitable for dense breasts because both dense tissue and breast abnormalities, such as c",
    "full_text_length": 96734,
    "chunk_length": 1273
  },
  {
    "chunk_id": 3192,
    "paper_filename": "shahriar_2023_review_of_CAD_systems_for_breast_mass_detection_in_mammography_based_on_deep Learning.pdf",
    "paper_title": "Shahriar 2023 Review Of Cad Systems For Breast Mass Detection In Mammography Based On Deep Learning",
    "chunk_index": 8,
    "total_chunks": 97,
    "text_content": "ultrasound can direct the biopsy operation at the assembly area. Ultrasou nd is available extensively and relatively easy to perform while not giving radiation to a pe rson. It is also less expensive than other imaging options. MRI use for women with a higher risk of breast cancer. MRI is a method that us es magnets, a computer, and radio waves to form detailed images of breast tissue. Since X -rays do not operate in this method, the patient will not expose to radiation. MRI can detect some canc",
    "full_text_length": 96734,
    "chunk_length": 1191
  },
  {
    "chunk_id": 3193,
    "paper_filename": "shahriar_2023_review_of_CAD_systems_for_breast_mass_detection_in_mammography_based_on_deep Learning.pdf",
    "paper_title": "Shahriar 2023 Review Of Cad Systems For Breast Mass Detection In Mammography Based On Deep Learning",
    "chunk_index": 9,
    "total_chunks": 97,
    "text_content": "women with dense breasts, which are not visible on a mammography, Easy to use, cheaper, and painless, It can utilize for women with dense breasts and pregnant women It depends on the operator, Higher rate of false positives MRI The highest sensitivity, Us ed for patients at high risk Unable to identify calcifications or tiny calcium deposits, Expensive compared to US and DM DBT Better detail, less imaging time , Increased recall rate, higher interpretation time. PET Able to detect diseases befor",
    "full_text_length": 96734,
    "chunk_length": 1316
  },
  {
    "chunk_id": 3194,
    "paper_filename": "shahriar_2023_review_of_CAD_systems_for_breast_mass_detection_in_mammography_based_on_deep Learning.pdf",
    "paper_title": "Shahriar 2023 Review Of Cad Systems For Breast Mass Detection In Mammography Based On Deep Learning",
    "chunk_index": 10,
    "total_chunks": 97,
    "text_content": "for further examination Histopathology Instead of diagnosing malignancy because of multi -colored images, diagnose different types of cancer, A comprehensive study of tissues, Detect cancer in the early stage. Require high skill, Misdiagnosis due to color variations and different staining approaches Histopathology is the microscopic examination of whole tissue samples. A histopathologist can view potentially cancerous or atypical tissues and assist other medical spec ialists in making diagnoses ",
    "full_text_length": 96734,
    "chunk_length": 1359
  },
  {
    "chunk_id": 3195,
    "paper_filename": "shahriar_2023_review_of_CAD_systems_for_breast_mass_detection_in_mammography_based_on_deep Learning.pdf",
    "paper_title": "Shahriar 2023 Review Of Cad Systems For Breast Mass Detection In Mammography Based On Deep Learning",
    "chunk_index": 11,
    "total_chunks": 97,
    "text_content": "and body temperature changes. In the breast t hermogra phy screening method, infrared rays display a thermal image of the breast tissues. Preprints (www.preprints.org) | NOT PEER-REVIEWED | Posted: 26 May 2023 doi:10.20944/preprints202305.1832.v1 4 3. Public Mammography Datasets Mammography is the most popular screening test for breast cancer [10]. Various datasets are available to the public that differs in size, resolution, image format, image typ e, and abnormalities included in the database.",
    "full_text_length": 96734,
    "chunk_length": 1282
  },
  {
    "chunk_id": 3196,
    "paper_filename": "shahriar_2023_review_of_CAD_systems_for_breast_mass_detection_in_mammography_based_on_deep Learning.pdf",
    "paper_title": "Shahriar 2023 Review Of Cad Systems For Breast Mass Detection In Mammography Based On Deep Learning",
    "chunk_index": 12,
    "total_chunks": 97,
    "text_content": "analysis. The Magic -5 database [11] is an Italian database that provides 3369 images from 967 patients. These images, digitized with 12 -bit resolution and saved in DICOM format, include several CC, MLO, and l ateral views. Like MIAS, ground truth characterizes by centers of MCCs and masses and circles surrounding ROIs. The patient\u2019s age is available as supplementary information , but there is no BI - RADS classification according to the ACR standard. Heterogeneity is the limitation of Magic -5",
    "full_text_length": 96734,
    "chunk_length": 1320
  },
  {
    "chunk_id": 3197,
    "paper_filename": "shahriar_2023_review_of_CAD_systems_for_breast_mass_detection_in_mammography_based_on_deep Learning.pdf",
    "paper_title": "Shahriar 2023 Review Of Cad Systems For Breast Mass Detection In Mammography Based On Deep Learning",
    "chunk_index": 13,
    "total_chunks": 97,
    "text_content": "distortions, and asymmetries). XML format pr ovides precise contours created by specialists. INbreast is powerful as it is made with full -field digital mammograms (ag ainst digitized mammograms) and offers various cases. Moreover, it is available pu blicly, together with accurate annotations. The Digital Database for Screening Mammography (DDSM) [14] contains 2,620 scan -film mammography studies. Normal, malignant, and benign cases with validated p athological information included. Extensive da",
    "full_text_length": 96734,
    "chunk_length": 1416
  },
  {
    "chunk_id": 3198,
    "paper_filename": "shahriar_2023_review_of_CAD_systems_for_breast_mass_detection_in_mammography_based_on_deep Learning.pdf",
    "paper_title": "Shahriar 2023 Review Of Cad Systems For Breast Mass Detection In Mammography Based On Deep Learning",
    "chunk_index": 14,
    "total_chunks": 97,
    "text_content": "of lesions but without accurate segmentation. The Breast Cancer Digital Repository (BCDR) [16] is a comprehensive collection of digital content (digitized film mammography images) and related metadata suc h as segmented lesions, medical history, BI -RADS classification, image -based descriptors, and confirmed biopsies. The database includes anonymized cases of patients, which annotate by expe rt radiologists compri sing lesion outlines, clinical data, and image -based properties computed from Me",
    "full_text_length": 96734,
    "chunk_length": 1382
  },
  {
    "chunk_id": 3199,
    "paper_filename": "shahriar_2023_review_of_CAD_systems_for_breast_mass_detection_in_mammography_based_on_deep Learning.pdf",
    "paper_title": "Shahriar 2023 Review Of Cad Systems For Breast Mass Detection In Mammography Based On Deep Learning",
    "chunk_index": 15,
    "total_chunks": 97,
    "text_content": "a distributed setting with grid services heterogeneity BancoWeb LAPIMO [12] (2011) 320 1400 12 MLO, CC TIFF Contain BI -RADs category Include additional information (patient age, hormone replacement therapy status, and scanner brand). Requires administrator approval Limited in size. INbreast [13] (2012) 115 410 14 MLO, CC DICOM Commonly cited by the literature. The database is now restricted; DDSM [14] (1999) 2620 10480 8-16 MLO, CC LJPEG Commonly cited by the literature. Consists of outdated fi",
    "full_text_length": 96734,
    "chunk_length": 1336
  },
  {
    "chunk_id": 3200,
    "paper_filename": "shahriar_2023_review_of_CAD_systems_for_breast_mass_detection_in_mammography_based_on_deep Learning.pdf",
    "paper_title": "Shahriar 2023 Review Of Cad Systems For Breast Mass Detection In Mammography Based On Deep Learning",
    "chunk_index": 16,
    "total_chunks": 97,
    "text_content": "data, They are limited in size. VICTRE [17] (2018) 217913 - MLO, CC DICOM Accurate mammographic lesions. Wholly synthetic. Fully synthetic. OPTIMUM (2020 ) NA 2889312 12-16 MLO, CC DICOM Extremely Large Dataset, Open -source API for simple image retrieval in Require administrator approval, Data only come from patients in Preprints (www.preprints.org) | NOT PEER-REVIEWED | Posted: 26 May 2023 doi:10.20944/preprints202305.1832.v1 6 [18] Python the UK. A new paradigm was developed [17] to evaluate ",
    "full_text_length": 96734,
    "chunk_length": 1352
  },
  {
    "chunk_id": 3201,
    "paper_filename": "shahriar_2023_review_of_CAD_systems_for_breast_mass_detection_in_mammography_based_on_deep Learning.pdf",
    "paper_title": "Shahriar 2023 Review Of Cad Systems For Breast Mass Detection In Mammography Based On Deep Learning",
    "chunk_index": 17,
    "total_chunks": 97,
    "text_content": "conducted per modality, breast type, and compressed breast thickness are added as the DIC OM tags to allow reproducibility. The OPTIMAM Medical Image Database (OMI -DB) [18] is an extensive database of mammogram ima ges and related clinical data from multiple NHS Breast Cancer screening site s in the UK. The automated image collection, storage, and processing process fa cilitates easy expansion across novel imaging sites and the constant supply of new dat a. The unproces sed and processed medica",
    "full_text_length": 96734,
    "chunk_length": 1335
  },
  {
    "chunk_id": 3202,
    "paper_filename": "shahriar_2023_review_of_CAD_systems_for_breast_mass_detection_in_mammography_based_on_deep Learning.pdf",
    "paper_title": "Shahriar 2023 Review Of Cad Systems For Breast Mass Detection In Mammography Based On Deep Learning",
    "chunk_index": 18,
    "total_chunks": 97,
    "text_content": "and interpret large quantities of data. Through deep learning, this process becomes easier and fas ter. The convolutional neural network (CNN) architectures are the most com mon deep learning framework utilized for various applications, from computer vision to n atural language processing [19]. In this section, we will review the different CNN architecture types that f requently employ for object detection and image classification. AlexNet [20] is a famous CNN architecture, and its results have ",
    "full_text_length": 96734,
    "chunk_length": 1345
  },
  {
    "chunk_id": 3203,
    "paper_filename": "shahriar_2023_review_of_CAD_systems_for_breast_mass_detection_in_mammography_based_on_deep Learning.pdf",
    "paper_title": "Shahriar 2023 Review Of Cad Systems For Breast Mass Detection In Mammography Based On Deep Learning",
    "chunk_index": 19,
    "total_chunks": 97,
    "text_content": "while Softmax uses the activation function in the output layer. About 60 millio n parameters include in this architecture. The AlexNet architecture is presented in Figure 2. Figure 2. Alexnet architecture . Google uses GoogLeNet, also call ed Inception -V1, as the CNN architecture to conquest ILSVRC 2014 classification task [21]. It has a remarkably decreased error rate compared to former winners ZF - Net (Ilsvrc 2013 winner) and AlexNet (Ilsvrc 2012 winner). The error is less si gnificant than ",
    "full_text_length": 96734,
    "chunk_length": 1307
  },
  {
    "chunk_id": 3204,
    "paper_filename": "shahriar_2023_review_of_CAD_systems_for_breast_mass_detection_in_mammography_based_on_deep Learning.pdf",
    "paper_title": "Shahriar 2023 Review Of Cad Systems For Breast Mass Detection In Mammography Based On Deep Learning",
    "chunk_index": 20,
    "total_chunks": 97,
    "text_content": "7 connections within the first two convolutional layers before adding new f ilters in later CNN layers. The basic architecture of the inception module is display in Figure 3, along with the concepts of integration and division. Figure 3. Inception block architecture . VGGNet was presented by Andrew Zisserman et al. and Karen Simonyan [22] at Oxford University. As a 16 -layer CNN, VGGNet is trained on over one billion images( 1000 classes) and h as over 95 million parameters. It can take large in",
    "full_text_length": 96734,
    "chunk_length": 1234
  },
  {
    "chunk_id": 3205,
    "paper_filename": "shahriar_2023_review_of_CAD_systems_for_breast_mass_detection_in_mammography_based_on_deep Learning.pdf",
    "paper_title": "Shahriar 2023 Review Of Cad Systems For Breast Mass Detection In Mammography Based On Deep Learning",
    "chunk_index": 21,
    "total_chunks": 97,
    "text_content": "tasks, including object detection, the VGG CN N model is computationally efficient and is an excellent foundation for many comp uter vision applications. Its deep feature representations are used in YOLO and other neural network archit ectures. Figure 4 represents the standard VGG16 network architecture diagram. Figure 4. VGG -19 architecture . Kaiming He et al. developed ResNet as the CNN architecture [24] to obtain the ILSVRC 2015 sorting task with a top -five error of only 15.43%. About 152 l",
    "full_text_length": 96734,
    "chunk_length": 1301
  },
  {
    "chunk_id": 3206,
    "paper_filename": "shahriar_2023_review_of_CAD_systems_for_breast_mass_detection_in_mammography_based_on_deep Learning.pdf",
    "paper_title": "Shahriar 2023 Review Of Cad Systems For Breast Mass Detection In Mammography Based On Deep Learning",
    "chunk_index": 22,
    "total_chunks": 97,
    "text_content": "It was utilized by the Microsoft Research Asia team (2016, 2017 ). Figure 5 represents the residual block architecture. In conclusion, ResNet overcomes the \u201cvanishing gradi ent\u201d problem, thus constructing networks with thousands of convolutional layers outperforming shallower net works. During backpropagation, a vanishing gradient happens. Preprints (www.preprints.org) | NOT PEER-REVIEWED | Posted: 26 May 2023 doi:10.20944/preprints202305.1832.v1 8 Figure 5. Residual block architecture . A Dense",
    "full_text_length": 96734,
    "chunk_length": 1450
  },
  {
    "chunk_id": 3207,
    "paper_filename": "shahriar_2023_review_of_CAD_systems_for_breast_mass_detection_in_mammography_based_on_deep Learning.pdf",
    "paper_title": "Shahriar 2023 Review Of Cad Systems For Breast Mass Detection In Mammography Based On Deep Learning",
    "chunk_index": 23,
    "total_chunks": 97,
    "text_content": "compelling benefits, such as reducing the vanishing -gradient problem, encouraging feature reuse, bolstering feature propagation, and significantly reduci ng parameter count. Figure 6 . DenseNet architecture . Google researchers developed Xception [26] as an architecture for deep convolutional neural networks, including Depthwise Separable Convolutions. According to G oogle's interpretation of Inception modules, they are an intermediate step in convolutional neural networks between regular convo",
    "full_text_length": 96734,
    "chunk_length": 1430
  },
  {
    "chunk_id": 3208,
    "paper_filename": "shahriar_2023_review_of_CAD_systems_for_breast_mass_detection_in_mammography_based_on_deep Learning.pdf",
    "paper_title": "Shahriar 2023 Review Of Cad Systems For Breast Mass Detection In Mammography Based On Deep Learning",
    "chunk_index": 24,
    "total_chunks": 97,
    "text_content": "separable convolutions th at should be more effective based on computation time. Ronneberger et al. proposed UNet as the first high -performance encoder -decoder structure [27], widely used for medical image segmentation. The U -Net architecture is shown in Figure 7. One of the notable points in the architecture of this network is the lack of fully connected layers, which reduces the complexity of the network. The idea hidden in this algorithm is to create a sequential contracting path that happ",
    "full_text_length": 96734,
    "chunk_length": 1264
  },
  {
    "chunk_id": 3209,
    "paper_filename": "shahriar_2023_review_of_CAD_systems_for_breast_mass_detection_in_mammography_based_on_deep Learning.pdf",
    "paper_title": "Shahriar 2023 Review Of Cad Systems For Breast Mass Detection In Mammography Based On Deep Learning",
    "chunk_index": 25,
    "total_chunks": 97,
    "text_content": "whatever is removed in the contraction path is restored in the expansion path. U -Net has become the target for most medical image segmentation tasks, stimulating many significant improvements. Figure 7. UNet architecture [27]. Mask R -CNN [28] was developed on top of Faster R -CNN as a state -of-the-art model, for instance, segmentation [29]. Faster R -CNNs are convolutional neural networks based on regions that return bounding boxes for each object and its class label with a confide nce score.",
    "full_text_length": 96734,
    "chunk_length": 1204
  },
  {
    "chunk_id": 3210,
    "paper_filename": "shahriar_2023_review_of_CAD_systems_for_breast_mass_detection_in_mammography_based_on_deep Learning.pdf",
    "paper_title": "Shahriar 2023 Review Of Cad Systems For Breast Mass Detection In Mammography Based On Deep Learning",
    "chunk_index": 26,
    "total_chunks": 97,
    "text_content": "one of the best neural network architectures to create higher accuracy and overall processing speed. A single neural network is used in the algorithm for the fu ll image. Then, the image is divided into regions and bounding boxes to pre dict probabilities for each area. The estimated probabilities weight these bounding boxes. Figure 9 represents the YOLO architecture. Preprints (www.preprints.org) | NOT PEER-REVIEWED | Posted: 26 May 2023 doi:10.20944/preprints202305.1832.v1 10 Figure 9. YOLO ar",
    "full_text_length": 96734,
    "chunk_length": 1360
  },
  {
    "chunk_id": 3211,
    "paper_filename": "shahriar_2023_review_of_CAD_systems_for_breast_mass_detection_in_mammography_based_on_deep Learning.pdf",
    "paper_title": "Shahriar 2023 Review Of Cad Systems For Breast Mass Detection In Mammography Based On Deep Learning",
    "chunk_index": 27,
    "total_chunks": 97,
    "text_content": "the network ResNet 2014 224 \u00d7224 22 1,3,5,6 Data augmentation, Dropout softmax An Inception module with the concept of division and integration GoogleNet 2015 224 \u00d7224 50/101/152 1,3,7 Data augmentation, Dropout softmax Residual block concatenation DenseNet 2017 224\u00d7224 201 Cross -layer information flow Xception 2017 299 \u00d7299 126 3 Data augmentation, Dropout softmax Depth separable convolution layers U-Net 2015 256\u00d7256 23 Encoder - decoder structure Mask RCNN (2017). 1024\u00d71024 YOLO 2015 448 \u00d7448",
    "full_text_length": 96734,
    "chunk_length": 1378
  },
  {
    "chunk_id": 3212,
    "paper_filename": "shahriar_2023_review_of_CAD_systems_for_breast_mass_detection_in_mammography_based_on_deep Learning.pdf",
    "paper_title": "Shahriar 2023 Review Of Cad Systems For Breast Mass Detection In Mammography Based On Deep Learning",
    "chunk_index": 28,
    "total_chunks": 97,
    "text_content": "networks, using a massive deal of parameters and memory ResNet Overcame the \"vanishing gradient.\" Long training time, overfitting of hyperparameters GoogleNet Using multi -scale filters in layers, using bottleneck layer to reduce the number of parameters The bottleneck layer's intricate structure and information loss DenseNet It lets each layer access the features of all former layers, optimizing the gradient flow during training and allowing the network to acquire knowledge more effectively. Th",
    "full_text_length": 96734,
    "chunk_length": 1282
  },
  {
    "chunk_id": 3213,
    "paper_filename": "shahriar_2023_review_of_CAD_systems_for_breast_mass_detection_in_mammography_based_on_deep Learning.pdf",
    "paper_title": "Shahriar 2023 Review Of Cad Systems For Breast Mass Detection In Mammography Based On Deep Learning",
    "chunk_index": 29,
    "total_chunks": 97,
    "text_content": "o the poor segmentation of small targets and blurred bounding box of the target image YOLO Fast inference speed (which allows it to process images in real -time), provides end-to-end training It struggles to detect smaller images within a group of images, unable to detect new or unusual shapes successfully 5. CAD Systems Several effective CAD systems have been developed with considerable progress in com puter - aided diagnosis (CAD). Novel avenues have been created by machine learning (ML) advan",
    "full_text_length": 96734,
    "chunk_length": 1318
  },
  {
    "chunk_id": 3214,
    "paper_filename": "shahriar_2023_review_of_CAD_systems_for_breast_mass_detection_in_mammography_based_on_deep Learning.pdf",
    "paper_title": "Shahriar 2023 Review Of Cad Systems For Breast Mass Detection In Mammography Based On Deep Learning",
    "chunk_index": 30,
    "total_chunks": 97,
    "text_content": "or malignant. Figure 10 shows the CAde/CADx pipeline based on deep learning and machine learning. As illustrated, in traditional machine learning, the classificat ion and feature extraction process is done in two separate stages. Still, in deep understanding, the feature ex traction process is compl etely automatic and combined with classification. CAde systems, the proce ss continues to the stage of mass detection and cannot classify the masses. Preprints (www.preprints.org) | NOT PEER-REVIEWED",
    "full_text_length": 96734,
    "chunk_length": 1349
  },
  {
    "chunk_id": 3215,
    "paper_filename": "shahriar_2023_review_of_CAD_systems_for_breast_mass_detection_in_mammography_based_on_deep Learning.pdf",
    "paper_title": "Shahriar 2023 Review Of Cad Systems For Breast Mass Detection In Mammography Based On Deep Learning",
    "chunk_index": 31,
    "total_chunks": 97,
    "text_content": "feature extraction [36\u201339], and classification [40\u201342]. Preprocessing can significantly limit the searches for abnormalities on only th e breast area with no effect from the background. By preprocessing, mammogram size is reduced, thu s improving the images' quality to make a more reliable feature extraction stage. By the presence of artifacts and no ises, breast cancer detection can be disturbed, thus reducing the accuracy rate in analysis . Preprocessing removes all irrelevant information, exc",
    "full_text_length": 96734,
    "chunk_length": 1464
  },
  {
    "chunk_id": 3216,
    "paper_filename": "shahriar_2023_review_of_CAD_systems_for_breast_mass_detection_in_mammography_based_on_deep Learning.pdf",
    "paper_title": "Shahriar 2023 Review Of Cad Systems For Breast Mass Detection In Mammography Based On Deep Learning",
    "chunk_index": 32,
    "total_chunks": 97,
    "text_content": "can be quite adjustable in appearance and size. Therefore, outlin ing their margins and extent precisely on a mammography image can be effective in both monito ring treatment response or tumor progression and treatment planning. In the segmenta tion stage, a picture is partitioned into multiple parts or regions, often based on the character istics of the pixels in the image, to identify a tumor's spatial location. Compared to traditional seg mentation methods, CNN -based segmentation methods are",
    "full_text_length": 96734,
    "chunk_length": 1290
  },
  {
    "chunk_id": 3217,
    "paper_filename": "shahriar_2023_review_of_CAD_systems_for_breast_mass_detection_in_mammography_based_on_deep Learning.pdf",
    "paper_title": "Shahriar 2023 Review Of Cad Systems For Breast Mass Detection In Mammography Based On Deep Learning",
    "chunk_index": 33,
    "total_chunks": 97,
    "text_content": "discussed the breast c ancer mass classification approaches. Figure 11. Techniques in different stages of breast cancer CAD systems . 5.1. Breast Masses Segmentation Segmentation is a vital part of medical image analysis and part of CAD and pathology research. Medical image segmentation involves anatomical structure and locatin g lesions, masses, or any abnormalities in a medical image. Radiologists can segment masses manually, which is expensive and time -consuming, or they can fully automate t",
    "full_text_length": 96734,
    "chunk_length": 1423
  },
  {
    "chunk_id": 3218,
    "paper_filename": "shahriar_2023_review_of_CAD_systems_for_breast_mass_detection_in_mammography_based_on_deep Learning.pdf",
    "paper_title": "Shahriar 2023 Review Of Cad Systems For Breast Mass Detection In Mammography Based On Deep Learning",
    "chunk_index": 34,
    "total_chunks": 97,
    "text_content": "mammograms. They use tree re -weighted (TRW) belief propagation to discover the marginal of the CRF model and minimize the mass segmentation error. Breast Cancer CAD System PhasePreprocessingData Augmentation Background and Artifact Removal ROI Extraction Removal Normalization and Enhancement SegmentationEdge -based Thresholding based Region -based ML based DL based ClassificationTexture Morphological Statistical BI-RADSPreprints (www.preprints.org) | NOT PEER-REVIEWED | Posted: 26 May 2023 doi:",
    "full_text_length": 96734,
    "chunk_length": 1387
  },
  {
    "chunk_id": 3219,
    "paper_filename": "shahriar_2023_review_of_CAD_systems_for_breast_mass_detection_in_mammography_based_on_deep Learning.pdf",
    "paper_title": "Shahriar 2023 Review Of Cad Systems For Breast Mass Detection In Mammography Based On Deep Learning",
    "chunk_index": 35,
    "total_chunks": 97,
    "text_content": "a dee p convolutional neural network (CNN), the mass is recognized and classified as malignant or benign. Li et al. [47] improved th e performance of U -Net segmentation for mass segmentation by Conditional Residual U -Net (CRU -Net) by integrating the merits of probabilistic graphical modeling and residual learning. The complex shape variation of pectoral muscle boundary and overlap of th e breast tissue make Mammography image segmentation a challenge. Rampun et al. [48] developed a method usin",
    "full_text_length": 96734,
    "chunk_length": 1336
  },
  {
    "chunk_id": 3220,
    "paper_filename": "shahriar_2023_review_of_CAD_systems_for_breast_mass_detection_in_mammography_based_on_deep Learning.pdf",
    "paper_title": "Shahriar 2023 Review Of Cad Systems For Breast Mass Detection In Mammography Based On Deep Learning",
    "chunk_index": 36,
    "total_chunks": 97,
    "text_content": "based on a conditional generative adversarial network (cGAN) to enhance breast mass segmentation performance. In their meth od, mapping between images and equivalent segmentation masks learns the mass image's distribution. Next, using cGAN produces several lesion images. Then, the created adversarial spec imens are concatenated with the original samples to generate a dataset with incremented diversity. Also, they introduced an enhanced U -net and trained it on former augmented datasets for breas",
    "full_text_length": 96734,
    "chunk_length": 1337
  },
  {
    "chunk_id": 3221,
    "paper_filename": "shahriar_2023_review_of_CAD_systems_for_breast_mass_detection_in_mammography_based_on_deep Learning.pdf",
    "paper_title": "Shahriar 2023 Review Of Cad Systems For Breast Mass Detection In Mammography Based On Deep Learning",
    "chunk_index": 37,
    "total_chunks": 97,
    "text_content": "a dense U -Net. A dense convolutional network comprises the encoder structure, and a U -Net decoder integrated with AGs serves as the decoding structure. Shen et al. [52] combined joint classification and segmentation using a mixed -supervision - guided and residual -aided classification U -Net model (ResCU - Net). Abdelhafiz et al. [53] improved the U -Net structure by adding residual attention modules (RU - Net) to segment mass lesions in mammogram images. Subsequently, the ResNet classifier (",
    "full_text_length": 96734,
    "chunk_length": 1270
  },
  {
    "chunk_id": 3222,
    "paper_filename": "shahriar_2023_review_of_CAD_systems_for_breast_mass_detection_in_mammography_based_on_deep Learning.pdf",
    "paper_title": "Shahriar 2023 Review Of Cad Systems For Breast Mass Detection In Mammography Based On Deep Learning",
    "chunk_index": 38,
    "total_chunks": 97,
    "text_content": "capturing multi -scale image information for precise breast mass segmentation. To defeat the unbalanced class problem, they used weighted cross -entropy loss. Abdelhafiz et al. [57] presented a model based on U -Net architecture for automatically segmenting mass lesions in mammography images. Their model is trained on four data bases, CBIS - DDSM, BCDR -01, INbreast, and UCHC, and an adaptive median filter was used to reduce noise in their model. Yu et al. [58] proposed a small -scale target det",
    "full_text_length": 96734,
    "chunk_length": 1316
  },
  {
    "chunk_id": 3223,
    "paper_filename": "shahriar_2023_review_of_CAD_systems_for_breast_mass_detection_in_mammography_based_on_deep Learning.pdf",
    "paper_title": "Shahriar 2023 Review Of Cad Systems For Breast Mass Detection In Mammography Based On Deep Learning",
    "chunk_index": 39,
    "total_chunks": 97,
    "text_content": "is predicted at various levels of spatial resolution. Then the found border is recovered using the shortest pathway problem on a graph designed specia lly. Min [60] developed a framework for breast mass detection and segmentation from mammograms regarding Mask R -CNN and pseudo -color mammogr ams. They utilized a multi -scale morphological sifter (MMS) to improve the detection performance of Mas k R-CNN. Al-Antari et al. [61] proposed a wholly integrated CAD system oriented by deep learning for ",
    "full_text_length": 96734,
    "chunk_length": 1343
  },
  {
    "chunk_id": 3224,
    "paper_filename": "shahriar_2023_review_of_CAD_systems_for_breast_mass_detection_in_mammography_based_on_deep Learning.pdf",
    "paper_title": "Shahriar 2023 Review Of Cad Systems For Breast Mass Detection In Mammography Based On Deep Learning",
    "chunk_index": 40,
    "total_chunks": 97,
    "text_content": "Generative Adversarial Networks (GCN) and then classifying them based on convolutional neural networks. Ahmed et al. [63] applied two deep learning -based semantic segmentation frameworks, Mask RCNN and DeepLab, to segment the cancerous region in mammogram images. For breast mass segmentation in whole mammograms, a new attention -guided dense - upsampling network( AUNet) was suggested in [64]. Asymmetrical encoder -decoder architecture in AUNet makes combining low - and high -level features easi",
    "full_text_length": 96734,
    "chunk_length": 1280
  },
  {
    "chunk_id": 3225,
    "paper_filename": "shahriar_2023_review_of_CAD_systems_for_breast_mass_detection_in_mammography_based_on_deep Learning.pdf",
    "paper_title": "Shahriar 2023 Review Of Cad Systems For Breast Mass Detection In Mammography Based On Deep Learning",
    "chunk_index": 41,
    "total_chunks": 97,
    "text_content": "introduced that adds a term to the standa rd cross -entropy, intending to direct the attention of the network to the dense region and penali ze the activation of stable features based on the ir location. A deeply supervised U -Net (DS U -Net) model with dense conditional random fields( CRFs) was proposed by Ravitha et al. [68]. They combined deep supervision with U -net to increase the network's focus on the edge of suspicious areas. Salama et al . [69] provided a CCN mammogram image classificat",
    "full_text_length": 96734,
    "chunk_length": 1210
  },
  {
    "chunk_id": 3226,
    "paper_filename": "shahriar_2023_review_of_CAD_systems_for_breast_mass_detection_in_mammography_based_on_deep Learning.pdf",
    "paper_title": "Shahriar 2023 Review Of Cad Systems For Breast Mass Detection In Mammography Based On Deep Learning",
    "chunk_index": 42,
    "total_chunks": 97,
    "text_content": "to segment the masses. Table 5. Studies on th e segmentation of breast cancer . References Year Methods Dataset Model Performance Dhungel et al. [44] 2015 Tree Re -weighted Belief Propagation INbreast and DDSM - BCRP Dice: 89% Zhu et al. [45] 2018 End- to-End Adversarial FCN - CRF Network INbreast, DDSM - BCRP Dice: INbreast -90.97% DDSM - BCRP -91.30% Al-Antari et al. [46] 2018 YOLO, FrCN, DCNN INbreast Mass segmentation: Accuracy: 92.97%, MCC: 85.93%, F1-score: 92.69%, Jaccard: 86.37% Li et al",
    "full_text_length": 96734,
    "chunk_length": 1263
  },
  {
    "chunk_id": 3227,
    "paper_filename": "shahriar_2023_review_of_CAD_systems_for_breast_mass_detection_in_mammography_based_on_deep Learning.pdf",
    "paper_title": "Shahriar 2023 Review Of Cad Systems For Breast Mass Detection In Mammography Based On Deep Learning",
    "chunk_index": 43,
    "total_chunks": 97,
    "text_content": "DDSM : 98.1% Shen et al. [49] 2019 cGAN, Unet INbreast, Private Private: 88.82% ; Accuracy: INbreast: 92% Wang et al. [50] 2019 PyramidNet (Multi -level Nested Pyramid Network) INbreast, DDSM Dice: DDSM: 91.10%, INbreast: 91.69% Li et al. [51] 2019 Densely Connected U -Net with Attention Gates (AGs) DDSM F1-score:82.24\u00b10.06 Sensitivity 77.89\u00b10.08 Shen et al. [52] 2019 Residual -aided Classification U-Net Model (MS -ResCU -Net) and Mixed -Supervision -guided INbreast Accuracy: 94.16% Dice: 91.78%",
    "full_text_length": 96734,
    "chunk_length": 1227
  },
  {
    "chunk_id": 3228,
    "paper_filename": "shahriar_2023_review_of_CAD_systems_for_breast_mass_detection_in_mammography_based_on_deep Learning.pdf",
    "paper_title": "Shahriar 2023 Review Of Cad Systems For Breast Mass Detection In Mammography Based On Deep Learning",
    "chunk_index": 44,
    "total_chunks": 97,
    "text_content": "2020 Vanilla U -Net Private Accuracy: 92.6%, Dice:95.1%, IOU: 90.9 % Yu et al. [58] 2020 Dense Mask -RCNN CBIS -DDSM Precision: 65% Soleimani et al. [59] 2020 End to End CNN CBIS -DDSM, INbreast, MIAS Average Dice: 97.22 \u00b1 1.96%, Average Accuracy: 99.64\u00b1.27% Min et al. [60] 2020 Mask R -CNN INbreast Dice: 88% Al-Antari et al. [61] 2020 YOLO Full Resolution Convolutional Network (FrCN) Regular Feed -forward CNN, ResNet -50, and InceptionResNet -V2 INbreast Accuracy: 92.97%, MCC: 85.93%, F1-score:",
    "full_text_length": 96734,
    "chunk_length": 1260
  },
  {
    "chunk_id": 3229,
    "paper_filename": "shahriar_2023_review_of_CAD_systems_for_breast_mass_detection_in_mammography_based_on_deep Learning.pdf",
    "paper_title": "Shahriar 2023 Review Of Cad Systems For Breast Mass Detection In Mammography Based On Deep Learning",
    "chunk_index": 45,
    "total_chunks": 97,
    "text_content": "et al. [65] 2020 Masked Regional Convolutional Neural Network fixed with Feature Pyramid Network (Mask RCNN -FPN) DDSM and INbreast Precision: 84% Accuracy: 91% Zeiser et al. [66] 2020 Modified U -net DDSM Sensitivity: 92.32% Specificity: 80.47%, Accuracy: 85.95% Dice: 79.39%, AUC: 86.40% Tsochatzidis et al. [67] 2021 Modified U -Net DDSM -400 and CBIS -DDSM AUC: DDSM -400: 88%, CBIS - DDSM: 860% Accuracy: DDSM -400: 73.8%, CBIS -DDSM:77.4% Ravitha et al. [68] 2021 Deeply supervised U -Net model",
    "full_text_length": 96734,
    "chunk_length": 1301
  },
  {
    "chunk_id": 3230,
    "paper_filename": "shahriar_2023_review_of_CAD_systems_for_breast_mass_detection_in_mammography_based_on_deep Learning.pdf",
    "paper_title": "Shahriar 2023 Review Of Cad Systems For Breast Mass Detection In Mammography Based On Deep Learning",
    "chunk_index": 46,
    "total_chunks": 97,
    "text_content": "to its performance and ability to generalize across diverse data. Mass detection c an generally be divided Preprints (www.preprints.org) | NOT PEER-REVIEWED | Posted: 26 May 2023 doi:10.20944/preprints202305.1832.v1 17 into two stages: mass localization and mass classification. These two ste ps can be done more easily and efficiently through deep CNNs, This section reviews a CAD system oriented by a convolutional neural netwo rk (CNN) using deep learning for classifying mammogram images into nor",
    "full_text_length": 96734,
    "chunk_length": 1324
  },
  {
    "chunk_id": 3231,
    "paper_filename": "shahriar_2023_review_of_CAD_systems_for_breast_mass_detection_in_mammography_based_on_deep Learning.pdf",
    "paper_title": "Shahriar 2023 Review Of Cad Systems For Breast Mass Detection In Mammography Based On Deep Learning",
    "chunk_index": 47,
    "total_chunks": 97,
    "text_content": "-tuned pre -trained deep learning classifiers were applied for classification. Kooi et al. [72] compared different CAD systems for detecting and classifying mammograp hy via a feature set designed manually and a convolutional neural network. T he findings of DCNN were compared with a group of experts in medical imaging, i ndicating almost the same performance for DCNN and the human reader. A graph -based semi -supervised learning scheme was developed by Sun et al. [73] utilizing a deep convoluti",
    "full_text_length": 96734,
    "chunk_length": 1254
  },
  {
    "chunk_id": 3232,
    "paper_filename": "shahriar_2023_review_of_CAD_systems_for_breast_mass_detection_in_mammography_based_on_deep Learning.pdf",
    "paper_title": "Shahriar 2023 Review Of Cad Systems For Breast Mass Detection In Mammography Based On Deep Learning",
    "chunk_index": 48,
    "total_chunks": 97,
    "text_content": "was presented by Al -Antari et al. [46] that includes all detection, classification, and segmentation tasks. In this work, the YOLO approach is used to detect the mass. The segmentation is done using the full convolutiona l network (FrCN). A deep convolutional neural network (DCNN) was utilized to detect the mass angroupassify it as malignant or benign. Ribli et al. [75] use the Faster R -CNN method to detect and classify masses. The suspicious region produced by this method is improved by fine ",
    "full_text_length": 96734,
    "chunk_length": 1259
  },
  {
    "chunk_id": 3233,
    "paper_filename": "shahriar_2023_review_of_CAD_systems_for_breast_mass_detection_in_mammography_based_on_deep Learning.pdf",
    "paper_title": "Shahriar 2023 Review Of Cad Systems For Breast Mass Detection In Mammography Based On Deep Learning",
    "chunk_index": 49,
    "total_chunks": 97,
    "text_content": "a fully connect ed layer to classify benign and malignant m asses using average pooling classification. To minimize annotation efforts, Shen et al. [78] proposed a new learning framework for mass detection by incorporating self -paced learning (SPL) and deep active learning (DAL). The annotation efforts of radiologi sts can be significantly reduced by DAL, thus improving the model training efficiency by achieving better performance with fewer overall annot ated specimens. The SPL can lessen the ",
    "full_text_length": 96734,
    "chunk_length": 1232
  },
  {
    "chunk_id": 3234,
    "paper_filename": "shahriar_2023_review_of_CAD_systems_for_breast_mass_detection_in_mammography_based_on_deep Learning.pdf",
    "paper_title": "Shahriar 2023 Review Of Cad Systems For Breast Mass Detection In Mammography Based On Deep Learning",
    "chunk_index": 50,
    "total_chunks": 97,
    "text_content": "discover and trace abnormaliti es in the images through the surrounding context of a lesion and the local features. A new approach was developed by Bru no et al. [80] based on a combination of two methods, scale -invariant feature transform (SIFT) key points and transfer learning wit h pre -trained CNNs like AlexNet fine-tuned and PyramidNet on digital mammograms to detect suspicious regions in mammograms. T he SIFT method includes selection, preprocessing, and feature extraction s teps. Then SI",
    "full_text_length": 96734,
    "chunk_length": 1345
  },
  {
    "chunk_id": 3235,
    "paper_filename": "shahriar_2023_review_of_CAD_systems_for_breast_mass_detection_in_mammography_based_on_deep Learning.pdf",
    "paper_title": "Shahriar 2023 Review Of Cad Systems For Breast Mass Detection In Mammography Based On Deep Learning",
    "chunk_index": 51,
    "total_chunks": 97,
    "text_content": "for breast lesion detection. Then, modified CNN architecture, n amely ResNet - 50, feed -forward CNN, and InceptionResNet -V2, was applied for breast lesion classification. Li et al. [83] have developed a two -way mass detection method, including two registration networks and the Siamese -Faster -RCNN network. Registration network for registration of bilateral mammography images and Siamese -Faster -RCNN network for mass detection utilizing ima ges registered by registration network. First, a se",
    "full_text_length": 96734,
    "chunk_length": 1333
  },
  {
    "chunk_id": 3236,
    "paper_filename": "shahriar_2023_review_of_CAD_systems_for_breast_mass_detection_in_mammography_based_on_deep Learning.pdf",
    "paper_title": "Shahriar 2023 Review Of Cad Systems For Breast Mass Detection In Mammography Based On Deep Learning",
    "chunk_index": 52,
    "total_chunks": 97,
    "text_content": "with well -annotated features. It was indicated that the proposed training converges rapidly. This method has limitations in the ca se of close masses; it cannot have good accur acy with small masses close to each other. An end -to-end CAD system was proposed by Aly et al. [85] based on YOLO. The YOLO is used to detect and classify processes, and their performance is compared. Inception an d ResNet are utilized as feature extractors for comparing their classification performance against YOLO. Xi",
    "full_text_length": 96734,
    "chunk_length": 1302
  },
  {
    "chunk_id": 3237,
    "paper_filename": "shahriar_2023_review_of_CAD_systems_for_breast_mass_detection_in_mammography_based_on_deep Learning.pdf",
    "paper_title": "Shahriar 2023 Review Of Cad Systems For Breast Mass Detection In Mammography Based On Deep Learning",
    "chunk_index": 53,
    "total_chunks": 97,
    "text_content": "of Rahman et al. [88] were oriented by modified versions of the pre -trained Inception V3 and ResNet50 for classifying pre -segmented mammogram mass tumors as malignant or benign. Zhang et al. [89], including ResNet and Alexnet, developed an d investigated [89]different convolutional neural network -based models for whole mammogram classification. They found that CNN- based models have been optimized via transfer learning. They also found g ood potential for data augmentation for automatic breas",
    "full_text_length": 96734,
    "chunk_length": 1297
  },
  {
    "chunk_id": 3238,
    "paper_filename": "shahriar_2023_review_of_CAD_systems_for_breast_mass_detection_in_mammography_based_on_deep Learning.pdf",
    "paper_title": "Shahriar 2023 Review Of Cad Systems For Breast Mass Detection In Mammography Based On Deep Learning",
    "chunk_index": 54,
    "total_chunks": 97,
    "text_content": "properties to achieve a good prediction and scale to massive datasets [91]. The proposed CNN model contains four max -pooling, eight convolutional, and two fully connected layers. The model presented better resul ts than the pre - trained nets, such as VGG16 and AlexNet. Two pooling struct ures were proposed [92], and deep CNNs oriented an end -to-end architecture. A global group -max pooling (GGP) structure and region -based group max pooling (RGP) structure were explored to solve the challenge",
    "full_text_length": 96734,
    "chunk_length": 1277
  },
  {
    "chunk_id": 3239,
    "paper_filename": "shahriar_2023_review_of_CAD_systems_for_breast_mass_detection_in_mammography_based_on_deep Learning.pdf",
    "paper_title": "Shahriar 2023 Review Of Cad Systems For Breast Mass Detection In Mammography Based On Deep Learning",
    "chunk_index": 55,
    "total_chunks": 97,
    "text_content": "an also overcome challenges in breast cancer cases, like the mass in dense regions or pectoral muscles. Preprints (www.preprints.org) | NOT PEER-REVIEWED | Posted: 26 May 2023 doi:10.20944/preprints202305.1832.v1 19 Gardezi et al. [94] pres ented a classification technique for abnormal and normal mammogram tissues via a VGG -16 CNN. To assess the results, 10 -fold cross -validation was used on SVM, simple logistics binary trees, and KNN (with k=1, 3, 5 classifiers). Tsochatzidis et al. [95] exam",
    "full_text_length": 96734,
    "chunk_length": 1351
  },
  {
    "chunk_id": 3240,
    "paper_filename": "shahriar_2023_review_of_CAD_systems_for_breast_mass_detection_in_mammography_based_on_deep Learning.pdf",
    "paper_title": "Shahriar 2023 Review Of Cad Systems For Breast Mass Detection In Mammography Based On Deep Learning",
    "chunk_index": 56,
    "total_chunks": 97,
    "text_content": "this same model to classify mammogram exams. They also revealed that it is optional to register the segmentation maps and multi -view mammograms to create precise classification results via such a fine -tuned deep -learning model. Table 1 . Summary of Studies on breast cancer classification . References Year Methods Task performed Dataset Model Performance Dhungel et al. [71] 2017 Multi -scale Deep Belief nets (M - DBN), R -CNN, Random Forest Classifier Detection, Classification INbreast Detecti",
    "full_text_length": 96734,
    "chunk_length": 1392
  },
  {
    "chunk_id": 3241,
    "paper_filename": "shahriar_2023_review_of_CAD_systems_for_breast_mass_detection_in_mammography_based_on_deep Learning.pdf",
    "paper_title": "Shahriar 2023 Review Of Cad Systems For Breast Mass Detection In Mammography Based On Deep Learning",
    "chunk_index": 57,
    "total_chunks": 97,
    "text_content": "99.24% MCC: 85.93%, F1 -score: 92.69%, Segmentation: accuracy: 92.97%, Jaccard: 86.37% Classification: accuracy: 95.64%, AUC: 94.78%, MCC of 89.91%, F1 -score;e96.84% Ribli et al. [75] 2018 Faster R -CNN Detection, Classification INbreast Accuracy: 95% Diniz et al. [76] 2018 DCNN Detection DDSM Accuracy for Non -dense regions: 95.6% Accuracy for Dense area: 97.72% Khan et al. [77] 2019 GoogleNet, VGGNet, ResNet Detection Classification Private Accuracy: 97.67% Shen et al. [78] 2019 Self-Paced Le",
    "full_text_length": 96734,
    "chunk_length": 1408
  },
  {
    "chunk_id": 3242,
    "paper_filename": "shahriar_2023_review_of_CAD_systems_for_breast_mass_detection_in_mammography_based_on_deep Learning.pdf",
    "paper_title": "Shahriar 2023 Review Of Cad Systems For Breast Mass Detection In Mammography Based On Deep Learning",
    "chunk_index": 58,
    "total_chunks": 97,
    "text_content": "at 1.69 FPI FPI for benign masses FPI for malignant 0.85\u00b10.08 at 1.0 Al-antari et al. [82] 2020 YOLO, feed - forward CNN, InceptionResNet - V2, and ResNet - 50 Detection Classification DDSM, INbreast Detection: Accuracy: DDSM: 99.17%, INbreast: 97.27% F1-scores: DDSM 99.28%, INbreast 98.02% Classification: Accuracies DDSM: 94.50%, CNN: 95.83%, , ResNet -50: InceptionResNet -V2: 97.50%, INbreast: CNN: 88.74%, InceptionResNet -V2: 95.32%, ResNet -50: 92.55% Li et al. [83] 2020 Convolution Neural N",
    "full_text_length": 96734,
    "chunk_length": 1390
  },
  {
    "chunk_id": 3243,
    "paper_filename": "shahriar_2023_review_of_CAD_systems_for_breast_mass_detection_in_mammography_based_on_deep Learning.pdf",
    "paper_title": "Shahriar 2023 Review Of Cad Systems For Breast Mass Detection In Mammography Based On Deep Learning",
    "chunk_index": 59,
    "total_chunks": 97,
    "text_content": "ResNet, AlexNet, GoogleNet Detection CBIS - DDSM Accuracy: GoogleNet: 91.10% ResNet:91.80% VGGNet: 92.53%, AlexNet: 91.23% Deb et al. [87] 2020 Pre-trained CNN models with Global Average Pooling DCNN Detection, Classification CBIS - DDSM AUC: VGG16: 0.70 InceptionV3: 0.74 InceptionResNetV2: 0.76 Xception: 0.75 NasNet: 0.73 MobileNet: 0.76 Rahman et al. [88] 2020 ResNet50, InceptionV3 Classification DDSM Accuracy: ResNet50: 85.71 InceptionV3: 79.6 Preprints (www.preprints.org) | NOT PEER-REVIEWED",
    "full_text_length": 96734,
    "chunk_length": 1436
  },
  {
    "chunk_id": 3244,
    "paper_filename": "shahriar_2023_review_of_CAD_systems_for_breast_mass_detection_in_mammography_based_on_deep Learning.pdf",
    "paper_title": "Shahriar 2023 Review Of Cad Systems For Breast Mass Detection In Mammography Based On Deep Learning",
    "chunk_index": 60,
    "total_chunks": 97,
    "text_content": "0.934 CBIS - DDSM: 0.838 Al-masni et al. [93] 2017 YOLO Detection Classification DDSM Detection Accuracy: 96.33% Classification Accuracy: 85.52% Gardezi et al. [94] 2017 VGGNet Classification IRMA AUC:1.0 Tsochatzidis et al. [95] 2019 AlexNet, GoogleNet, VGGNet, InceptionV2, ResNet Classification DDSM -400 CBIS - DDSM ResNet outperforms the rest of the pre -trained networks with an accuracy of DDSM - 400: 0.785 and CBIS -DDSM: 0.755 Carneiro et al. [96] 2017 Pre-trained CNN Models Classification",
    "full_text_length": 96734,
    "chunk_length": 1286
  },
  {
    "chunk_id": 3245,
    "paper_filename": "shahriar_2023_review_of_CAD_systems_for_breast_mass_detection_in_mammography_based_on_deep Learning.pdf",
    "paper_title": "Shahriar 2023 Review Of Cad Systems For Breast Mass Detection In Mammography Based On Deep Learning",
    "chunk_index": 61,
    "total_chunks": 97,
    "text_content": "metrics t o evaluate these methods' performance. A confusion matrix is an N \u00d7 N matrix to assess a classification model's performance, in which N represents the number of target groups. The actual target values are compared with the matrix a nd those estimated by the model. For a binary classification problem, a 2 x 2 matr ix will exist ( Figure 11) with four values True True False False Positive Negative Positive Negative Actual PredictePreprints (www.preprints.org) | NOT PEER-REVIEWED | Posted",
    "full_text_length": 96734,
    "chunk_length": 1282
  },
  {
    "chunk_id": 3246,
    "paper_filename": "shahriar_2023_review_of_CAD_systems_for_breast_mass_detection_in_mammography_based_on_deep Learning.pdf",
    "paper_title": "Shahriar 2023 Review Of Cad Systems For Breast Mass Detection In Mammography Based On Deep Learning",
    "chunk_index": 62,
    "total_chunks": 97,
    "text_content": "problems which are not skewed and are well balanced , with no class imbalance. Precision can measure the correctness obtained in true prediction. In other words, it shows the number of positive predictions from all the total positive predictions a s follows: ,ive FalsePosit ve TruePositive TruePositiPrecision+= (2) Precision is a useful metric when there is a deeper concern about False Positi ve than False Negatives. The recall is a measure of corr ectly predicted actual observations, i.e., the ",
    "full_text_length": 96734,
    "chunk_length": 1289
  },
  {
    "chunk_id": 3247,
    "paper_filename": "shahriar_2023_review_of_CAD_systems_for_breast_mass_detection_in_mammography_based_on_deep Learning.pdf",
    "paper_title": "Shahriar 2023 Review Of Cad Systems For Breast Mass Detection In Mammography Based On Deep Learning",
    "chunk_index": 63,
    "total_chunks": 97,
    "text_content": "be defined as ,ive FalseNegat ve TrueNegative TrueNegatiy Specificit+= (4) Sensitivity measures the correct identification of instances of the positive c lass by the model, while specificity measures how well the model can correctly identify cases of th e negative type. The F1 -Score shows the harmonic mean of precision and recall as a number between 0 and 1. Unlike simple averages, the harmonic mean is sued due to non -sensitivity to extremely large values. , 2 1Recall PrecisionRecall Precision",
    "full_text_length": 96734,
    "chunk_length": 1260
  },
  {
    "chunk_id": 3248,
    "paper_filename": "shahriar_2023_review_of_CAD_systems_for_breast_mass_detection_in_mammography_based_on_deep Learning.pdf",
    "paper_title": "Shahriar 2023 Review Of Cad Systems For Breast Mass Detection In Mammography Based On Deep Learning",
    "chunk_index": 64,
    "total_chunks": 97,
    "text_content": "misleading results are obtained by this for the smaller class representation within the image, as the measure will be biased mainly in identifying negative cases (fo r lack of class). Dice score can assess the similarity between the ground truth (GT) segmentation mask a nd an estimated segmentati on mask. The Dice score ranges from 0 (no overlap) to 1 ( perfect overlap). ,2 B ABADice+=\uf049 (7) Where A and B represent GT and predicted segmentation maps, respectively. Volumetric overlap error (VOE) i",
    "full_text_length": 96734,
    "chunk_length": 1266
  },
  {
    "chunk_id": 3249,
    "paper_filename": "shahriar_2023_review_of_CAD_systems_for_breast_mass_detection_in_mammography_based_on_deep Learning.pdf",
    "paper_title": "Shahriar 2023 Review Of Cad Systems For Breast Mass Detection In Mammography Based On Deep Learning",
    "chunk_index": 65,
    "total_chunks": 97,
    "text_content": "diagnosing and screening breast cancer, which have outstanding performan ce. However , there are still challenges in research other than class imbalance and small data s ets requiring further investigation. This section discusses potential future research directions to improve t he results of the CAD system for breast cancer diagnosis. Network architecture design : Improving network structure design affects CAD systems' performance and can be extended to other applications. The manual design of ",
    "full_text_length": 96734,
    "chunk_length": 1333
  },
  {
    "chunk_id": 3250,
    "paper_filename": "shahriar_2023_review_of_CAD_systems_for_breast_mass_detection_in_mammography_based_on_deep Learning.pdf",
    "paper_title": "Shahriar 2023 Review Of Cad Systems For Breast Mass Detection In Mammography Based On Deep Learning",
    "chunk_index": 66,
    "total_chunks": 97,
    "text_content": "the future trend needs to include manual design and NAS technology. Initially, a ba ckbone network is manually designed, and then NAS, before training, searches the small network modu les. Transfer learning: Limited access to annotated medical image data is a considerable problem explaining the restrictions of developing robust deep learning models. However, building a labeled dataset of sufficient size can be very expensive. Therefore, using pre-trained deep learning models on natural images is",
    "full_text_length": 96734,
    "chunk_length": 1336
  },
  {
    "chunk_id": 3251,
    "paper_filename": "shahriar_2023_review_of_CAD_systems_for_breast_mass_detection_in_mammography_based_on_deep Learning.pdf",
    "paper_title": "Shahriar 2023 Review Of Cad Systems For Breast Mass Detection In Mammography Based On Deep Learning",
    "chunk_index": 67,
    "total_chunks": 97,
    "text_content": "the number of user interactions owing to the superior performance of deep learni ng, which shows wider application prospects. Graph Convolutional Neural Network: CNN has attained significant success in the last decade, especially in breast cancer diagnosis. However, many real -world data have underlying graph structures, which are non -Euclidean, and CNN has some limitations in non -Euclidean data. The non - regularity of data structures has resulted in the current advancements in Graph Convolut",
    "full_text_length": 96734,
    "chunk_length": 1378
  },
  {
    "chunk_id": 3252,
    "paper_filename": "shahriar_2023_review_of_CAD_systems_for_breast_mass_detection_in_mammography_based_on_deep Learning.pdf",
    "paper_title": "Shahriar 2023 Review Of Cad Systems For Breast Mass Detection In Mammography Based On Deep Learning",
    "chunk_index": 68,
    "total_chunks": 97,
    "text_content": "improved accuracy with lower b randed instances. Despite all recent methods, detecting and classifying breast cancer in ma mmography with minimal needed annotated data while considering the pattern and relatio nship of the te xture properties is still challenging. There is no semi -supervised or self -supervised graph -based method for processing the high -resolution mammogram images and performing multi -classification of the anomalous areas with less annotated data for the trainin g procedure.",
    "full_text_length": 96734,
    "chunk_length": 1442
  },
  {
    "chunk_id": 3253,
    "paper_filename": "shahriar_2023_review_of_CAD_systems_for_breast_mass_detection_in_mammography_based_on_deep Learning.pdf",
    "paper_title": "Shahriar 2023 Review Of Cad Systems For Breast Mass Detection In Mammography Based On Deep Learning",
    "chunk_index": 69,
    "total_chunks": 97,
    "text_content": "24 encode long -range dependencies by replacing convolution oper ations, leveraging the self -attention mechanism in the encoder -decoder structure, and learning highly expressive representations. Though, if transformers substitute all convolution operators in machine vision tasks, several problems occur, including high memory usage and computational cost. Considering that the local information of the image is extracted by the convolution block and the trans former depicts the complex relations ",
    "full_text_length": 96734,
    "chunk_length": 1367
  },
  {
    "chunk_id": 3254,
    "paper_filename": "shahriar_2023_review_of_CAD_systems_for_breast_mass_detection_in_mammography_based_on_deep Learning.pdf",
    "paper_title": "Shahriar 2023 Review Of Cad Systems For Breast Mass Detection In Mammography Based On Deep Learning",
    "chunk_index": 70,
    "total_chunks": 97,
    "text_content": "the paper based on the configuration and main characteristics of this model. The limitations and strengths of these pre -trained architectures direct researchers in this field in choosing the right model for their applications. This work highl ights the main challenges of C NNs for mammography image training. Methods for improving model generaliz ation are also presented. Although CNNs have achieved state -of-the-art achievements in breast cancer diagnosis and other image -processing tasks, seve",
    "full_text_length": 96734,
    "chunk_length": 1246
  },
  {
    "chunk_id": 3255,
    "paper_filename": "shahriar_2023_review_of_CAD_systems_for_breast_mass_detection_in_mammography_based_on_deep Learning.pdf",
    "paper_title": "Shahriar 2023 Review Of Cad Systems For Breast Mass Detection In Mammography Based On Deep Learning",
    "chunk_index": 71,
    "total_chunks": 97,
    "text_content": "a review on mammograms analysis techniques,\" in In 10th International Multi -Conferences on Systems, Signals & Devices 2013 (SSD13), Hammamet, Tunisia. , 2013. 3. S. Misra, N. L. Solomon, F. L. Moffat and L. G. Koniaris, \"Screening Cri teria for Breast Cancer,\" Advances in Surgery, vol. 4, no. 1, pp. 87 -100, 2010. 4. H. G. Welch,. C. Prorok, . J. O'Malley and . S. Kramer, \"Breast -Cancer Tumor Size, Overdiagnosis, and Mammography Screening Effectiveness,\" New England Journal of Medicine ( NEJM)",
    "full_text_length": 96734,
    "chunk_length": 1143
  },
  {
    "chunk_id": 3256,
    "paper_filename": "shahriar_2023_review_of_CAD_systems_for_breast_mass_detection_in_mammography_based_on_deep Learning.pdf",
    "paper_title": "Shahriar 2023 Review Of Cad Systems For Breast Mass Detection In Mammography Based On Deep Learning",
    "chunk_index": 72,
    "total_chunks": 97,
    "text_content": "cancer automatic diagnosis using deep learning: Taxonomy, survey and insights,\" Neurocompu ting, vol. 375, pp. 9 -24, 2020. 7. E. Wulczyn, D. F. Steiner, Z. Xu, A. Sadhwani, H. Wang, I. Flament -Auvigne, C. H. Mermel, P. -H. C. Chen, Y. Liu and M. C. Stumpe, \"Deep learning -based survival prediction for multiple cancer types using histopathology images vol. 15, no. 6, p,\" PLoS One, vol. 15, no. 6, p. p. e 0233678, 2020.. 8. D. F. Y. L. P. -H. C. C. E. W. F. T. N. O. J. L. S. A. M. J. H. W. e. a.",
    "full_text_length": 96734,
    "chunk_length": 1061
  },
  {
    "chunk_id": 3257,
    "paper_filename": "shahriar_2023_review_of_CAD_systems_for_breast_mass_detection_in_mammography_based_on_deep Learning.pdf",
    "paper_title": "Shahriar 2023 Review Of Cad Systems For Breast Mass Detection In Mammography Based On Deep Learning",
    "chunk_index": 73,
    "total_chunks": 97,
    "text_content": "\"Computer- aided detection/ diagnosis of breast cancer in mammography and ultrasou nd: a review,\" Cliniacl Imaging, vol. 37, no. 3, pp. 420 -426, 2013. 10. V. M., \"Breast cancer screening methods: a review of the evidence,\" Healt h Care Women Int. , vol. 24, no. 9, p. 773-793, 2003. 11. S. Tangaro, R. Bellotti, F. D. Carlo, G. Gargano, E. Lattanzio, P. Mo nno, R. Massafra, P. Delogu, M. E. Fantacci, A. Retico, M. Bazzocchi, S. Bagnasco, P. Cerello, S. C. Cheran, E . L. Torres, E. Zanon, A. Lauri",
    "full_text_length": 96734,
    "chunk_length": 1171
  },
  {
    "chunk_id": 3258,
    "paper_filename": "shahriar_2023_review_of_CAD_systems_for_breast_mass_detection_in_mammography_based_on_deep Learning.pdf",
    "paper_title": "Shahriar 2023 Review Of Cad Systems For Breast Mass Detection In Mammography Based On Deep Learning",
    "chunk_index": 74,
    "total_chunks": 97,
    "text_content": "no. 3, p. 500-506, 2011. 13. I. C. Moreira, I. Amaral, I. Domingues, A. Cardoso, M. J. Cardoso and J. S. Cardoso , \"INbreast: toward a full-field digital mammographic database,\" Acad Radiol., vol. 19, no. 2, p. 236-248, 2012. Preprints (www.preprints.org) | NOT PEER-REVIEWED | Posted: 26 May 2023 doi:10.20944/preprints202305.1832.v1 25 14. M. H. KB, K. D and R. M. P. Jr., \"The digital database for screening mammogr aphy,\" in In Proceedings of the 5th international workshop on digital mammography",
    "full_text_length": 96734,
    "chunk_length": 1163
  },
  {
    "chunk_id": 3259,
    "paper_filename": "shahriar_2023_review_of_CAD_systems_for_breast_mass_detection_in_mammography_based_on_deep Learning.pdf",
    "paper_title": "Shahriar 2023 Review Of Cad Systems For Breast Mass Detection In Mammography Based On Deep Learning",
    "chunk_index": 75,
    "total_chunks": 97,
    "text_content": "Loureiro and T. C. Fernandes, \"BCDR : A Breast Cancer Digital Repository,\" in In 15th International conference on experimental mechanics (Vol . 1215), Porto, Portugal, 2017. 17. Aldo Badano, C. G. Graff, Andreu Badal and e. al., \"Evaluation of Digital Breast Tomosynthes is as Replacement of Full -Field Digital Mammography Using an In Silico Imaging Trial,\" JAMA Netw ork Ope, vol. 1, no. 7, p. e185474, 2018. 18. M. D. Halling -Brown, L. M. Warren, D. Ward, E. Lewis, A. Mackenzie, M. G. Wallis, L.",
    "full_text_length": 96734,
    "chunk_length": 1199
  },
  {
    "chunk_id": 3260,
    "paper_filename": "shahriar_2023_review_of_CAD_systems_for_breast_mass_detection_in_mammography_based_on_deep Learning.pdf",
    "paper_title": "Shahriar 2023 Review Of Cad Systems For Breast Mass Detection In Mammography Based On Deep Learning",
    "chunk_index": 76,
    "total_chunks": 97,
    "text_content": "4, p. 611 \u2013629, 2018. 20. A. Krizhevsky, I. Sutskever and G. E. Hinton, \"Imagenet classifi cation with deep convolutional neural networks,\" Advances in Ne ural Information Processing Systems, vol. 25, p. 1097 \u20131105, 2012. 21. C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan , V. Vanhoucke and A. Rabinovich, \"Going deeper with convolutions,\" in In Proceedings of the IEEE Conf erence on Comput er Vision and Pattern Recognition, 2015. 22. K. Simonyan and A. Zisserman, \"Very d",
    "full_text_length": 96734,
    "chunk_length": 1205
  },
  {
    "chunk_id": 3261,
    "paper_filename": "shahriar_2023_review_of_CAD_systems_for_breast_mass_detection_in_mammography_based_on_deep Learning.pdf",
    "paper_title": "Shahriar 2023 Review Of Cad Systems For Breast Mass Detection In Mammography Based On Deep Learning",
    "chunk_index": 77,
    "total_chunks": 97,
    "text_content": "the IEEE conference on compute r vision and pattern recognition, Las Vegas, NV, USA, 2016. 25. G. Huang, Z. Liu, L. v. d. Maaten and K. Q. Weinberger, \"Densely Connecte d Convolutional Networks,\" in Proceedings of the IEEE Conference on Computer Vision and Pattern Recogni tion, , 21 -26 July 2017,, Honolulu, 2017. 26. F. Chollet, \"Xception: deep learning with depthwise separable convol utions,\" in Proceedings of the IEEE conference on computer vision and pattern recognition pp, 2017. 27. O. Ronn",
    "full_text_length": 96734,
    "chunk_length": 1233
  },
  {
    "chunk_id": 3262,
    "paper_filename": "shahriar_2023_review_of_CAD_systems_for_breast_mass_detection_in_mammography_based_on_deep Learning.pdf",
    "paper_title": "Shahriar 2023 Review Of Cad Systems For Breast Mass Detection In Mammography Based On Deep Learning",
    "chunk_index": 78,
    "total_chunks": 97,
    "text_content": "r -cnn: towards real -time object detection with region proposal networks,\" IEEE Transactions on Pattern Analysis and Machine In telligence, vol. 39, no. 6, pp. 1137 -1149, 2015. 30. J. Redmon, S. Divvala, R. Girshick and A. Farhadi, \"You Only Look Once : Unified, Real -Time Object Detection,\" in 2016 IEEE Conference on Computer Vision and Pattern Rec ognition (CVPR), Las Vegas, NV, USA, 2016. 31. K. Ganesan, U. R. Acharya, K. C. Chua, L. C. Min and K. T. Abraham, \"Pectoral m uscle segmentation:",
    "full_text_length": 96734,
    "chunk_length": 1217
  },
  {
    "chunk_id": 3263,
    "paper_filename": "shahriar_2023_review_of_CAD_systems_for_breast_mass_detection_in_mammography_based_on_deep Learning.pdf",
    "paper_title": "Shahriar 2023 Review Of Cad Systems For Breast Mass Detection In Mammography Based On Deep Learning",
    "chunk_index": 79,
    "total_chunks": 97,
    "text_content": "methods,\" Advances in Breast Cancer Research, vol. 2, pp. 72 -77, 2013. 34. M. J. George and S. P. Sankar, \"Efficient preprocessing filters and m ass segmentation t echniques for mammogram images,\" in IEEE international conference on circuits and s ystems (ICCS), Thiruvananthapuram, India, 2017. 35. L. S. Varughese and A. J, \"A study of region based segmentation metho ds for mammograms,\" IJRET: International Journal of Research in Engineering and Technology, vol. 02, p. 421 \u2013425, 2013. 36. M. M.",
    "full_text_length": 96734,
    "chunk_length": 1283
  },
  {
    "chunk_id": 3264,
    "paper_filename": "shahriar_2023_review_of_CAD_systems_for_breast_mass_detection_in_mammography_based_on_deep Learning.pdf",
    "paper_title": "Shahriar 2023 Review Of Cad Systems For Breast Mass Detection In Mammography Based On Deep Learning",
    "chunk_index": 80,
    "total_chunks": 97,
    "text_content": "no. 1, pp. 123 -128, 2012. Preprints (www.preprints.org) | NOT PEER-REVIEWED | Posted: 26 May 2023 doi:10.20944/preprints202305.1832.v1 26 38. D. Kulkarni, S. M. Bhagyashree and G. R. Udupi, \"Texture Analysis o f Mammographic images,\" International Journal of Computer Applications, vol. 5, no. 6, p. 1 2\u201317, 2010. 39. R. Llobet, R. Paredes and J. C. P\u00e9rez -Cort\u00e9s , \"Comparison of Feature Extraction Methods for Breast Cancer Detection,\" Pattern Recognition and Image Analysis, vol. 3523, p. 495 \u201350",
    "full_text_length": 96734,
    "chunk_length": 1236
  },
  {
    "chunk_id": 3265,
    "paper_filename": "shahriar_2023_review_of_CAD_systems_for_breast_mass_detection_in_mammography_based_on_deep Learning.pdf",
    "paper_title": "Shahriar 2023 Review Of Cad Systems For Breast Mass Detection In Mammography Based On Deep Learning",
    "chunk_index": 81,
    "total_chunks": 97,
    "text_content": "156, pp. 25 -45, 2018. 42. H. Li, X. Meng, T. Wang, Y. Tang and Y. Yin, \"Breast masses in mammography cla ssification with local contour features,\" BioMedical Engineering OnLine, vol. 16, no. 1, pp . 44-55, 2017. 43. K. Zuidervel d, \"Contrast limited adaptive histogram equalization,\" Graphics gems , pp. 474 -485, 1994. 44. N. Dhungel, G. Carneiro and A. P. Bradley, \"Tree reweighted belief propagation using deep learning potentials for mass segmentation from mammograms,\" in IEEE 12th Int ernation",
    "full_text_length": 96734,
    "chunk_length": 1205
  },
  {
    "chunk_id": 3266,
    "paper_filename": "shahriar_2023_review_of_CAD_systems_for_breast_mass_detection_in_mammography_based_on_deep Learning.pdf",
    "paper_title": "Shahriar 2023 Review Of Cad Systems For Breast Mass Detection In Mammography Based On Deep Learning",
    "chunk_index": 82,
    "total_chunks": 97,
    "text_content": "fully integrated computer - aided diagnosis system for digital X -ray mammograms via deep learning detection, segmentation, and classification, \" International Journal of Medical Informatics, vol. 117, p. 44 \u201354, 2018. 47. H. Li, D. Chen, B. Nailon, M. Davies and D. Laurenson, \"Improved brea st mass segmentation in mammograms with conditional residual u -net,\" Image Analysis for Moving Organ, Breast, and Thoracic Images, vol. 11040, p. 81 \u201389, 2018. 48. A. Rampun, K. L\u00f3pez -Linares, P. J. Morrow",
    "full_text_length": 96734,
    "chunk_length": 1188
  },
  {
    "chunk_id": 3267,
    "paper_filename": "shahriar_2023_review_of_CAD_systems_for_breast_mass_detection_in_mammography_based_on_deep Learning.pdf",
    "paper_title": "Shahriar 2023 Review Of Cad Systems For Breast Mass Detection In Mammography Based On Deep Learning",
    "chunk_index": 83,
    "total_chunks": 97,
    "text_content": "Programs in Biomedicine, vol. 180, p. 105012, 2019. 50. R. Wang, Y. Ma, W. Sun, Y. Guo, W. Wang, Y. Qi and X. Gong, \"Multi -level nested pyramid network for mass segmentation in mammograms,\" Neurocomputing, vol. 363, pp. 313 -320, 2019. 51. S. Li, M. Dong, G. Du and X. Mu, \" Attention dense -u-net for automatic breast mass segmentation in digital mammogram,\" IEEE Access, vol. 7, p. 59037 \u201359047, 2019. 52. T. Shen, C. Gou, J. Wang and F. -Y. Wang, \"Simultaneous segmentation and classification of ",
    "full_text_length": 96734,
    "chunk_length": 1216
  },
  {
    "chunk_id": 3268,
    "paper_filename": "shahriar_2023_review_of_CAD_systems_for_breast_mass_detection_in_mammography_based_on_deep Learning.pdf",
    "paper_title": "Shahriar 2023 Review Of Cad Systems For Breast Mass Detection In Mammography Based On Deep Learning",
    "chunk_index": 84,
    "total_chunks": 97,
    "text_content": "Health In formatics, Niagara Falls; NY; USA, 2019. 54. S. K. Ghosh, A. Mitra and A. Ghosh, \"A novel intuitionistic fuzzy soft s et entrenched mammogram segmentation u nder Multigranulation approximation for breast cancer detection in early stages,\" Expert Systems with Applications, vol. 169, p. 114329, 2021. 55. V. K. Singh, H. A. Rashwan, S. Romani, F. Akram, N. Pandey, M. M. K. Sarker, A. S aleh, M. Arenas, M. Arquez, D. Puig and J. Torrents -Barrena, \" Breast tumor segmentation and shape clas",
    "full_text_length": 96734,
    "chunk_length": 1232
  },
  {
    "chunk_id": 3269,
    "paper_filename": "shahriar_2023_review_of_CAD_systems_for_breast_mass_detection_in_mammography_based_on_deep Learning.pdf",
    "paper_title": "Shahriar 2023 Review Of Cad Systems For Breast Mass Detection In Mammography Based On Deep Learning",
    "chunk_index": 85,
    "total_chunks": 97,
    "text_content": "for automated mass segmentation in mammography,\" BMC Bioinformatics, vol. 21, no. 1, pp. 1 -19, 2020. 58. H. Yu, R. Bai, J. An and R. Cao, \"Deep learning -based fully automated detection and segmentation of breast mass,\" in 13th International Congress on Im age and Signal Processing, BioMedical Engineering and Informatics (CISP -BMEI), Chengdu, China, 2020. 59. H. Soleimani and O. V. Michailovich, \"On segmentation of pectoral muscl e in digital mammograms by means of deep learning,\" IEEE Access,",
    "full_text_length": 96734,
    "chunk_length": 1239
  },
  {
    "chunk_id": 3270,
    "paper_filename": "shahriar_2023_review_of_CAD_systems_for_breast_mass_detection_in_mammography_based_on_deep Learning.pdf",
    "paper_title": "Shahriar 2023 Review Of Cad Systems For Breast Mass Detection In Mammography Based On Deep Learning",
    "chunk_index": 86,
    "total_chunks": 97,
    "text_content": "A. Al -masni and T. -S. Kim, \"Deep Learning Computer -Aided Diagnosis for Breast Lesion in Digital Mammogram,\" Advances in Experimental Medicine and Biology, Springer, Cham, vol. 1213, p. 59 \u201372, 2020. 62. N. Saffa ri, H. A. Rashwan, M. Abdel -Nasser, V. K. Singh, M. Arenas, E. Mangina, B. Herrera and D. Puig, \"Fully Automated Breast Density Segmentation and Classification Using De ep Learning,\" Diagnostics; vol. 10; no. 11; p. 988; 2020., vol. 10, no. 11, pp. 998 -, 2020. 63. L. Ahmed, M. M. Iq",
    "full_text_length": 96734,
    "chunk_length": 1165
  },
  {
    "chunk_id": 3271,
    "paper_filename": "shahriar_2023_review_of_CAD_systems_for_breast_mass_detection_in_mammography_based_on_deep Learning.pdf",
    "paper_title": "Shahriar 2023 Review Of Cad Systems For Breast Mass Detection In Mammography Based On Deep Learning",
    "chunk_index": 87,
    "total_chunks": 97,
    "text_content": "mass segmentation in whole mammog rams,\" Physics in Medicine & Biology, vol. 65, no. 5, p. 55005, 2020. 65. H. M. A. Bhatti, J. Li, S. Siddeeq, A. Rehman and A. Manzoor, \"Multi -detection and segmentation of breast lesions based on mask rcnn -fpn,\" in IEEE International Conference on Bioinformatics and Biomed icine (BIBM), Seoul, Korea (South), 2020. 66. F. A. Zeiser, C. A. d. Costa, T. Zonta, N. M. C. Marques, A. V. Roehe, M. Moreno and R . d. R. Righi, \"Segmentation of masses on mammograms usi",
    "full_text_length": 96734,
    "chunk_length": 1192
  },
  {
    "chunk_id": 3272,
    "paper_filename": "shahriar_2023_review_of_CAD_systems_for_breast_mass_detection_in_mammography_based_on_deep Learning.pdf",
    "paper_title": "Shahriar 2023 Review Of Cad Systems For Breast Mass Detection In Mammography Based On Deep Learning",
    "chunk_index": 88,
    "total_chunks": 97,
    "text_content": "and N. Ramesh, \"Deeply s upervised u -net for mass segmentation in digital mammograms,\" International Journal of Ima ging Systems and Technology, vol. 31, no. 1, pp. 59 -71, 2021. 69. W. M. Salama and M. H. Aly, \"Deep learning in mammography images segme ntation and classification: automat ed cnn approach,\" Alexandria Engineering Journal, vol. 60, no. 5, pp . 4701 -4701, 2021. 70. Q. Yu, Y. Shi, Y. Zheng, Y. Gao, J. Zhu and Y. Da, \"Crossover -Net: leveraging vertical -hori- zontal crossover rela",
    "full_text_length": 96734,
    "chunk_length": 1185
  },
  {
    "chunk_id": 3273,
    "paper_filename": "shahriar_2023_review_of_CAD_systems_for_breast_mass_detection_in_mammography_based_on_deep Learning.pdf",
    "paper_title": "Shahriar 2023 Review Of Cad Systems For Breast Mass Detection In Mammography Based On Deep Learning",
    "chunk_index": 89,
    "total_chunks": 97,
    "text_content": "d. Heeten and N. Karssemeijer, \"Large scale deep learning for computer aided detec tion of mammographic lesions,\" Medical Image Analysis, vol. 35, pp. 303 -312, 2017. 73. W. Sun, T. -L. (. Tseng, J. Zh ang and W. Qian, \"Enhancing deep convolutional neural network scheme for breast cancer diagnosis with unlabeled data,\" Computerized Medi cal Imaging and Graphics, vol. 57, pp. 4 - 9, 2017. 74. M. A. Al -masnia, M. A. Al -antaria, J. -M. Park, G. Gi, T. -Y. Kim, P. R ivera, E. Valarezo, M. -T. Choi",
    "full_text_length": 96734,
    "chunk_length": 1126
  },
  {
    "chunk_id": 3274,
    "paper_filename": "shahriar_2023_review_of_CAD_systems_for_breast_mass_detection_in_mammography_based_on_deep Learning.pdf",
    "paper_title": "Shahriar 2023 Review Of Cad Systems For Breast Mass Detection In Mammography Based On Deep Learning",
    "chunk_index": 90,
    "total_chunks": 97,
    "text_content": "learning,\" Scientific Reports, vol. 8, no. 1, pp. 1 -7, 2018. 76. J. O. B. Diniz, P. H. B. Diniz, T. L. A. Valente, A. C. Silva, A. C. d. Paiva and M. Gattass, \"Detection of mass regions in mammograms by bilateral analysis adapted to breast density using similarity indexes and convolutional neural networks,\" Computer Methods and Programs in Biom edicine, vol. 156, pp. 191 -207, 2018. 77. S. Khan, N. I slam, Z. Jan, I. U. Din and J. J. P. C. Rodrigues, \"A novel deep learni ng based framework for ",
    "full_text_length": 96734,
    "chunk_length": 1145
  },
  {
    "chunk_id": 3275,
    "paper_filename": "shahriar_2023_review_of_CAD_systems_for_breast_mass_detection_in_mammography_based_on_deep Learning.pdf",
    "paper_title": "Shahriar 2023 Review Of Cad Systems For Breast Mass Detection In Mammography Based On Deep Learning",
    "chunk_index": 91,
    "total_chunks": 97,
    "text_content": "vol. 101, pp. 668 -679, 2019. 79. B. Savelli, A. Bria, M. Molinara, C. Ma rrocco and F. Tortorella, \"A multi -context CNN ensemble for small lesion detection,\" Artificial Intelligence in Medicine, vol. 103, p. 1017 49, 2020. 80. A. Bruno, E. Ardizzone, S. Vitabile and M. Midiri, \"A Novel Solution B ased on Scale Invariant Feature Tran sform Descriptors and Deep Learning for the Detection of Suspicious Reg ions in Mammogram Images,\" Journal of Medical Signals and Sensors, vol. 10, no. 3, pp. 15 8",
    "full_text_length": 96734,
    "chunk_length": 1228
  },
  {
    "chunk_id": 3276,
    "paper_filename": "shahriar_2023_review_of_CAD_systems_for_breast_mass_detection_in_mammography_based_on_deep Learning.pdf",
    "paper_title": "Shahriar 2023 Review Of Cad Systems For Breast Mass Detection In Mammography Based On Deep Learning",
    "chunk_index": 92,
    "total_chunks": 97,
    "text_content": "towards c omputer - aided diagnosis of breast lesions in digital X-ray mammograms,\" Computer Methods and Programs in Biomedicine, vol. 196, p. 105584, 2020. 83. Y. Li, L. Zhang, H. Chen and L. Cheng, \"Mass detection in mammogram s by bilateral analysis using convolution neural network,\" Computer Methods and Programs in Biomedic ine, vol. 195, p. 105518, 2020. 84. R. Shen, J. Yao, K. Yan, K. Tian, C. Jiang and K. Zhou, \"Unsupervised domain adaptation with adversarial learning for mass detection i",
    "full_text_length": 96734,
    "chunk_length": 1247
  },
  {
    "chunk_id": 3277,
    "paper_filename": "shahriar_2023_review_of_CAD_systems_for_breast_mass_detection_in_mammography_based_on_deep Learning.pdf",
    "paper_title": "Shahriar 2023 Review Of Cad Systems For Breast Mass Detection In Mammography Based On Deep Learning",
    "chunk_index": 93,
    "total_chunks": 97,
    "text_content": "Symposium on Medical Measur ements and Applications (MeMeA), Rome, Italy, 2018. 87. S. Deep Deb, M. A. Rahman and R. K. Jha, \"Breast cancer detection and class ification using global pooling,\" in 11th International Conference on Computing, Communication and Net working Technologies (ICCCN T), Kharagpur, India, 2020. 88. A. S. Abdel Rahman, S. B. Belhaouari, A. Bouzerdoum, H. Baali, T. Alam and A . M. Eldaraa, \"Breast mass tumor classification using deep learning,\" in IEEE Internationa l Conferen",
    "full_text_length": 96734,
    "chunk_length": 1286
  },
  {
    "chunk_id": 3278,
    "paper_filename": "shahriar_2023_review_of_CAD_systems_for_breast_mass_detection_in_mammography_based_on_deep Learning.pdf",
    "paper_title": "Shahriar 2023 Review Of Cad Systems For Breast Mass Detection In Mammography Based On Deep Learning",
    "chunk_index": 94,
    "total_chunks": 97,
    "text_content": "of tumors in mammograms,,\" in 6th International C onference on Image and Signal Processing and their Applications (ISP A), Mostaganem, Algeria, 2019. 91. V. S. Gnanasekaran, S. Joypaul, P. Meenakshi Sundaram and D. D. Chairma n, \"Deep learning algorithm for breast masses classification in mammograms,\" IET Image Processi ng, vol. 14, no. 12, p. 2860 \u20132868, 2020. 92. X. Shu, L. Zhang, Z. Wang, Q. Lv and Z. Yi, \"Deep neural networks with reg ion-based pooling structures for mammographic image class",
    "full_text_length": 96734,
    "chunk_length": 1237
  },
  {
    "chunk_id": 3279,
    "paper_filename": "shahriar_2023_review_of_CAD_systems_for_breast_mass_detection_in_mammography_based_on_deep Learning.pdf",
    "paper_title": "Shahriar 2023 Review Of Cad Systems For Breast Mass Detection In Mammography Based On Deep Learning",
    "chunk_index": 95,
    "total_chunks": 97,
    "text_content": "Medici ne and Biology Society (EMBC), Jeju, Korea (South), 2017. 94. S. J. S. Gardezi, M. Awais, I. Faye and F. Meriaudeau, \"Mammogram classification using deep learning features,\" in IEEE International Conference on Signal and Image Processing Applications (ICSIPA) , Kuching, Malaysia, 2017. 95. L. Tsochatzidis, L. Costaridou and I. Pratikakis, \"Deep learning f or breast cancer diagnosis from mammograms -a comparative study,\" Journal of Imaging, vol. 5, no. 3, p. 37, 2019 . 96. G. Carneiro, J. ",
    "full_text_length": 96734,
    "chunk_length": 1319
  },
  {
    "chunk_id": 3280,
    "paper_filename": "shahriar_2023_review_of_CAD_systems_for_breast_mass_detection_in_mammography_based_on_deep Learning.pdf",
    "paper_title": "Shahriar 2023 Review Of Cad Systems For Breast Mass Detection In Mammography Based On Deep Learning",
    "chunk_index": 96,
    "total_chunks": 97,
    "text_content": "(CVPR), 2018. Disclaimer/Publisher\u2019s Note: The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/o r the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resu lting from any ideas, m ethods, instructions or products referred to in the content. Preprints (www.preprints.org) | NOT PEER-REVIEWED | Posted: 26 May 2023 doi:10.20944/preprints202305.1832.v1",
    "full_text_length": 96734,
    "chunk_length": 495
  },
  {
    "chunk_id": 3281,
    "paper_filename": "stefano_2024_semiautonomous_deep_learning_system_to_reduce_false_positives_in_screening_mammography.pdf",
    "paper_title": "Stefano 2024 Semiautonomous Deep Learning System To Reduce False Positives In Screening Mammography",
    "chunk_index": 0,
    "total_chunks": 53,
    "text_content": "ORIGINAL RESEARCH Globally, breast cancer is the most common cancer among female individuals and is predicted to result in the most cancer deaths for this population (1). Screening mammograms allow early detection, improve prognosis, and reduce mortality (2\u20136). Many nations have developed screening programs (7,8); the United States alone per- forms more than 38 million examinations yearly (9,10). False positives are a concern, as over 50% of individuals undergoing 10 screening examinations will ",
    "full_text_length": 52687,
    "chunk_length": 1411
  },
  {
    "chunk_id": 3282,
    "paper_filename": "stefano_2024_semiautonomous_deep_learning_system_to_reduce_false_positives_in_screening_mammography.pdf",
    "paper_title": "Stefano 2024 Semiautonomous Deep Learning System To Reduce False Positives In Screening Mammography",
    "chunk_index": 1,
    "total_chunks": 53,
    "text_content": "50 years of age (16). However, recent updated recommendations suggest screening should begin at 40 years of age. Most existing computer-aided detection and diagnosis software for cancer screening attempt to balance sensitivity and specificity by having roughly equal rates of false posi- tives and false negatives. For example, computer-aided triage (17,18), computer-aided detection (17,19), and automated second interpretation (17,18,20) in double-reading settings (eg, United Kingdom and Europe) (",
    "full_text_length": 52687,
    "chunk_length": 1418
  },
  {
    "chunk_id": 3283,
    "paper_filename": "stefano_2024_semiautonomous_deep_learning_system_to_reduce_false_positives_in_screening_mammography.pdf",
    "paper_title": "Stefano 2024 Semiautonomous Deep Learning System To Reduce False Positives In Screening Mammography",
    "chunk_index": 2,
    "total_chunks": 53,
    "text_content": "(23\u201325). Rule-out devices operate at This copy is for personal use only. To order printed copies, contact reprints@rsna.orgPurpose: To evaluate the ability of a semiautonomous artificial intelligence (AI) model to identify screening mammograms not suspicious for breast cancer and reduce the number of false-positive examinations. Materials and Methods: The deep learning algorithm was trained using 123 248 two-dimensional digital mammograms (6161 cancers) and a retrospective study was performed on",
    "full_text_length": 52687,
    "chunk_length": 1361
  },
  {
    "chunk_id": 3284,
    "paper_filename": "stefano_2024_semiautonomous_deep_learning_system_to_reduce_false_positives_in_screening_mammography.pdf",
    "paper_title": "Stefano 2024 Semiautonomous Deep Learning System To Reduce False Positives In Screening Mammography",
    "chunk_index": 3,
    "total_chunks": 53,
    "text_content": "to the cancer detection rate with use of the AI device (noninferiority margin of 0.25 cancers per 1000 examinations: U.S. dataset 1, P = .02; U.S. dataset 2, P < .001; U.K. dataset, P < .001). On U.S. dataset 1 (11 592 mammograms; 101 cancers; 3810 female patients; mean age, 57.3 years \u00b1 10.0 [SD]), the device reduced screening examinations requiring radiologist interpretation by 41.6% (95% CI: 40.6%, 42.4%; P < .001), diagnostic examinations callbacks by 31.1% (95% CI: 28.7%, 33.4%; P < .001), ",
    "full_text_length": 52687,
    "chunk_length": 1155
  },
  {
    "chunk_id": 3285,
    "paper_filename": "stefano_2024_semiautonomous_deep_learning_system_to_reduce_false_positives_in_screening_mammography.pdf",
    "paper_title": "Stefano 2024 Semiautonomous Deep Learning System To Reduce False Positives In Screening Mammography",
    "chunk_index": 4,
    "total_chunks": 53,
    "text_content": "female patients; mean age, 63.5 years \u00b1 7.1) was reduced by 36.8% (95% CI: 34.4%, 39.7%; P < .001), 17.1% (95% CI: 5.9%, 30.1%: P < .001), and 5.9% (95% CI: 2.9%, 11.5%; P < .001), respecti vely. Conclusion: This work demonstrates the potential of a semiautonomous breast cancer screening system to reduce false positives, unnecessary procedures, patient anxiety, and medical expenses. Supplemental material is available for this article. Published under a CC BY 4.0 license.A Semiautonomous Deep Lea",
    "full_text_length": 52687,
    "chunk_length": 1252
  },
  {
    "chunk_id": 3286,
    "paper_filename": "stefano_2024_semiautonomous_deep_learning_system_to_reduce_false_positives_in_screening_mammography.pdf",
    "paper_title": "Stefano 2024 Semiautonomous Deep Learning System To Reduce False Positives In Screening Mammography",
    "chunk_index": 5,
    "total_chunks": 53,
    "text_content": "Clara, CA 95054 (S.P ., T.T., B.M., Y.N.T.V., T.M., R.M.H., M.S., N.G., N.Z.D., J.S.); Onsite Women\u2019s Health, Westfield, Mass (S.H.); SSM Health, St Louis, Mo (C.M.A.); and Mallinckrodt Institute of Radiology, Washington University School of Medicine, St Louis, Mo (R.L.W.). Received February 2, 2023; revision requested March 23; revision received February 16, 2024; accepted March 19. Address correspondence to T.T. (email: research@whiterabbit.ai). Supported by Whiterabbit.ai. Conflicts of intere",
    "full_text_length": 52687,
    "chunk_length": 1447
  },
  {
    "chunk_id": 3287,
    "paper_filename": "stefano_2024_semiautonomous_deep_learning_system_to_reduce_false_positives_in_screening_mammography.pdf",
    "paper_title": "Stefano 2024 Semiautonomous Deep Learning System To Reduce False Positives In Screening Mammography",
    "chunk_index": 6,
    "total_chunks": 53,
    "text_content": "has equity interests in Whiterabbit.ai and may receive royalty income and milestone payments according to an agreement with Whiterabbit.ai to develop the technology in this research. The inclusion of data and analyses was controlled by R.L.W., who is not an employee or consultant of Whiterabbit.ai. Data T wo-dimensional full-field digital mammography examina- tions were gathered from three institutions: two U.S. institu- tions (U.S. dataset 1 from 2008 through 2017 and U.S. dataset 2 from 2014 t",
    "full_text_length": 52687,
    "chunk_length": 1269
  },
  {
    "chunk_id": 3288,
    "paper_filename": "stefano_2024_semiautonomous_deep_learning_system_to_reduce_false_positives_in_screening_mammography.pdf",
    "paper_title": "Stefano 2024 Semiautonomous Deep Learning System To Reduce False Positives In Screening Mammography",
    "chunk_index": 7,
    "total_chunks": 53,
    "text_content": "Hologic Selenia Dimensions (SED) scanner models. U.S. dataset 1 was interpreted by 28 breast radiologists with experience ranging from 2 to 30 years from 2008 to 2017 (av- erage number of reads, 392.8 \u00b1 602.2 [SD]; range, 1\u20131765). U.S. dataset 2 was interpreted by 59 radiologists, 16 of whom were fellowship trained, with experience ranging from 1 to 37 years from 2014 to 2019 (average number of reads, 24.7 \u00b1 37.5; range, 1\u2013190). U.K. dataset 3 was interpreted by 210 readers of unknown experience",
    "full_text_length": 52687,
    "chunk_length": 1229
  },
  {
    "chunk_id": 3289,
    "paper_filename": "stefano_2024_semiautonomous_deep_learning_system_to_reduce_false_positives_in_screening_mammography.pdf",
    "paper_title": "Stefano 2024 Semiautonomous Deep Learning System To Reduce False Positives In Screening Mammography",
    "chunk_index": 8,
    "total_chunks": 53,
    "text_content": "pathology benign (P) for benign pathology assessments, high risk (H) for nonupstaged high-risk pathology assessments, malignant (M) for malig- nant pathology assessments, and interval cancers (I) for a BI- RADS 1 or 2 with a malignant pathology assessment within the screening interval (12 months for the U.S. dataset and 36 months for the U.K. dataset) and prior to the subsequent screening examination (Appendix S1). The N, S, and D la- bels needed a follow-up examination at least 2 years later an",
    "full_text_length": 52687,
    "chunk_length": 1227
  },
  {
    "chunk_id": 3290,
    "paper_filename": "stefano_2024_semiautonomous_deep_learning_system_to_reduce_false_positives_in_screening_mammography.pdf",
    "paper_title": "Stefano 2024 Semiautonomous Deep Learning System To Reduce False Positives In Screening Mammography",
    "chunk_index": 9,
    "total_chunks": 53,
    "text_content": "of normal or benign were labeled BI-RADS 1 or BI-RADS an extreme point of high sensitivity, near 100%. This operat- ing point ensures that nearly all cases marked as nonsuspicious are truly cancer free and can be removed from the radiologist\u2019s workflow. Previous works have proposed rule-out devices that can automatically declare 17.0% (23), 19.3% (24), 60.0% (25), 34.3% (26), and 30.9% (27) of the mammograms as nonsuspi- cious with a sensitivity of 99.0%, 99.0%, 80.1%, 99.0%, and 97.8%, respecti",
    "full_text_length": 52687,
    "chunk_length": 1316
  },
  {
    "chunk_id": 3291,
    "paper_filename": "stefano_2024_semiautonomous_deep_learning_system_to_reduce_false_positives_in_screening_mammography.pdf",
    "paper_title": "Stefano 2024 Semiautonomous Deep Learning System To Reduce False Positives In Screening Mammography",
    "chunk_index": 10,
    "total_chunks": 53,
    "text_content": "rule-out devices affect radiologist workflow and performance in the hu- man plus artificial intelligence (AI) paradigm. This scheme al- lows us to highlight potential downstream benefits of rule-out devices, such as the reduction of invasive biopsy procedures for patients without cancer. Materials and Methods This study used retrospective anonymized data to train and evaluate a deep learning system that identifies mammograms not suspicious for breast cancer. The objective was to assess the poten",
    "full_text_length": 52687,
    "chunk_length": 1409
  },
  {
    "chunk_id": 3292,
    "paper_filename": "stefano_2024_semiautonomous_deep_learning_system_to_reduce_false_positives_in_screening_mammography.pdf",
    "paper_title": "Stefano 2024 Semiautonomous Deep Learning System To Reduce False Positives In Screening Mammography",
    "chunk_index": 11,
    "total_chunks": 53,
    "text_content": "rate unaffected. Key Points \u25a0The semiautonomous artificial intelligence (AI) breast cancer screening model reduced the number of screening mammographic examinations requiring radiologist interpretation (U.S. dataset 1, 41.6% [95% CI: 40.6%, 42.4%]; P < .001; U.S. dataset 2, 19.5% [95% CI: 16.9%, 22.1%]; P < .001; U.K. dataset 3, 36.8% [95% CI: 34.4%, 39.7%]; P < .001) with minimal effect to sensitivity (U.S. dataset 1, P = .02; U.S. dataset 2, P < .001; U.K. dataset 3, P < .001). \u25a0A new labeling",
    "full_text_length": 52687,
    "chunk_length": 1270
  },
  {
    "chunk_id": 3293,
    "paper_filename": "stefano_2024_semiautonomous_deep_learning_system_to_reduce_false_positives_in_screening_mammography.pdf",
    "paper_title": "Stefano 2024 Semiautonomous Deep Learning System To Reduce False Positives In Screening Mammography",
    "chunk_index": 12,
    "total_chunks": 53,
    "text_content": "1, 7.4%; P < .001; U.S. dataset 2, 6.5%; P = .08; U.K. dataset 3, 5.9%; P < .001). Keywords Artificial Intelligence, Semiautonomous Deep Learning, Breast Cancer, Screening Mammography Radiology: Artificial Intelligence Volume 6: Number 3\u20142024 \u25a0 radiology-ai.rsna.org 3 Pedemonte et alvision model (metamodel) that combines the low-level informa- tion to compute a final examination malignancy probability. This architecture enables the algorithm to (a ) utilize multiview, bilateral, and prior imagin",
    "full_text_length": 52687,
    "chunk_length": 1410
  },
  {
    "chunk_id": 3294,
    "paper_filename": "stefano_2024_semiautonomous_deep_learning_system_to_reduce_false_positives_in_screening_mammography.pdf",
    "paper_title": "Stefano 2024 Semiautonomous Deep Learning System To Reduce False Positives In Screening Mammography",
    "chunk_index": 13,
    "total_chunks": 53,
    "text_content": "which the device reads the examinations before radiologists, examinations with predic- tions lower than the rule-out operating threshold are assigned a negative prediction (BI-RADS 1). For all other examinations, the original clinical assessment is assigned, modeling the sce- nario in which radiologists\u2019 interpretations are unaffected by 2, respectively. Screening examinations with a final reader\u2019s opinion other than normal or benign were labeled BI-RADS 0. Other examinations with nonscreening o",
    "full_text_length": 52687,
    "chunk_length": 1315
  },
  {
    "chunk_id": 3295,
    "paper_filename": "stefano_2024_semiautonomous_deep_learning_system_to_reduce_false_positives_in_screening_mammography.pdf",
    "paper_title": "Stefano 2024 Semiautonomous Deep Learning System To Reduce False Positives In Screening Mammography",
    "chunk_index": 14,
    "total_chunks": 53,
    "text_content": "class contained the examinations with outcomes N, S, D, P , and H (Appendix S7). Development of the Rule-out Algorithm The algorithm is composed of a low-level vision (deep learning) model that analyzes each image independently and a high-level Figure 1: Flow diagram for the exclusion of mammography examinations for U.S. dataset 1, U.S. dataset 2, and U.K. dataset 3. Only U.S. dataset 1 had diagnostic examinations which were used only during model development to increase the number of cancers av",
    "full_text_length": 52687,
    "chunk_length": 1349
  },
  {
    "chunk_id": 3296,
    "paper_filename": "stefano_2024_semiautonomous_deep_learning_system_to_reduce_false_positives_in_screening_mammography.pdf",
    "paper_title": "Stefano 2024 Semiautonomous Deep Learning System To Reduce False Positives In Screening Mammography",
    "chunk_index": 15,
    "total_chunks": 53,
    "text_content": "rebalanced the datasets when computing the values and CIs for metrics dependent on the prevalence of the subclasses N, S, D, P , H, M, and I. These prevalence-adjusted metrics were the area under the receiver operating characteristic curve, CDR, rule-out rate, false-positive callback reduction rate, and benign-biopsy reduction rate. Metrics for radiologist perfor- mance are reported for individual radiologists or for all radiologists in the dataset (collective radiologist performance). For colle",
    "full_text_length": 52687,
    "chunk_length": 1313
  },
  {
    "chunk_id": 3297,
    "paper_filename": "stefano_2024_semiautonomous_deep_learning_system_to_reduce_false_positives_in_screening_mammography.pdf",
    "paper_title": "Stefano 2024 Semiautonomous Deep Learning System To Reduce False Positives In Screening Mammography",
    "chunk_index": 16,
    "total_chunks": 53,
    "text_content": "S6) (30). A P value of .05 was chosen as the threshold for significance. Results Dataset Demographics We included female patients from three datasets: U.S. dataset 1 (38 451 patients; mean age, 57.3 years \u00b1 10.0), U.K. dataset 3 (15 025 patients; mean age, 63.5 years \u00b1 7.2), and U.S. da- taset 2 (1293 patients; mean age, 55.4 years \u00b1 10.5) (Table 2). The study began with a total of 1 037 543 mammograms. After applying the exclusion criteria, 163 828 examinations from all the fact that the device",
    "full_text_length": 52687,
    "chunk_length": 1257
  },
  {
    "chunk_id": 3298,
    "paper_filename": "stefano_2024_semiautonomous_deep_learning_system_to_reduce_false_positives_in_screening_mammography.pdf",
    "paper_title": "Stefano 2024 Semiautonomous Deep Learning System To Reduce False Positives In Screening Mammography",
    "chunk_index": 17,
    "total_chunks": 53,
    "text_content": "reduction rate, and benign biopsy reduction rate. We character- ized the stand-alone device by the absolute and relative sensitivity, rule-out rate, reduction of false-positive callbacks, and reduction of benign biopsies. The radiologists\u2019 true positives were cancers with BI-RADS 0, 4, 5, and 6 assessments. The device\u2019s true posi- tives were the cancers with predicted scores greater than or equal to the rule-out operating threshold. The device plus radiologists true positives were cancers with B",
    "full_text_length": 52687,
    "chunk_length": 1327
  },
  {
    "chunk_id": 3299,
    "paper_filename": "stefano_2024_semiautonomous_deep_learning_system_to_reduce_false_positives_in_screening_mammography.pdf",
    "paper_title": "Stefano 2024 Semiautonomous Deep Learning System To Reduce False Positives In Screening Mammography",
    "chunk_index": 18,
    "total_chunks": 53,
    "text_content": "operating threshold. Statistical Analysis Performance is reported on the test datasets of U.S. dataset 1, U.S. dataset 2, and U.K. dataset 3. Analysis was conducted using Py- thon (Python version 3.6 [Python Software Foundation], scikit-Table 1: Data Used for Model Development and the Evaluation Study CharacteristicU.S. 1 (Full Set)U.S. 2 (Full Set)U.K. 3 (Full Set)Model Development (U.S. 1 and U.K. 3, T raining and Validation Sets)Retrospective Study, U.S. 1 (Test Set)Retrospective Study, U.S. ",
    "full_text_length": 52687,
    "chunk_length": 1188
  },
  {
    "chunk_id": 3300,
    "paper_filename": "stefano_2024_semiautonomous_deep_learning_system_to_reduce_false_positives_in_screening_mammography.pdf",
    "paper_title": "Stefano 2024 Semiautonomous Deep Learning System To Reduce False Positives In Screening Mammography",
    "chunk_index": 19,
    "total_chunks": 53,
    "text_content": "78 NA 466 498 (7)6 NA 40 Total 117 844 1362 18 873 123 248 (8617)11 592 1362 1877 Note.\u2014Data are the numbers of examinations, and data in parentheses are findings annotated with bounding boxes used for model devel- opment. The datasets comprised a total of 138 079 screening examinations. Of these, 123 248 from the United States and United Kingdom were used for training and tuning the model (model development), and 14 831 were used in the evaluation study that evaluated the effect of the rule-out",
    "full_text_length": 52687,
    "chunk_length": 1158
  },
  {
    "chunk_id": 3301,
    "paper_filename": "stefano_2024_semiautonomous_deep_learning_system_to_reduce_false_positives_in_screening_mammography.pdf",
    "paper_title": "Stefano 2024 Semiautonomous Deep Learning System To Reduce False Positives In Screening Mammography",
    "chunk_index": 20,
    "total_chunks": 53,
    "text_content": "tivity (the percent of radiologist-detected cancers that were detected by the device) of 100% (95 of 95) in U.S. dataset 1, 100% (322 of 322) in U.S. dataset 2, and 99.6% (550 of 552) in U.K. dataset 3. T wo cancers were missed across the three datasets that would have otherwise been detected without the device (Appendix S10). The sensitivity of the screening rule-out workflow (calculated by multiplying the sensitivity of the standard workflow, which is the radiologist sensitivity, and the relat",
    "full_text_length": 52687,
    "chunk_length": 1185
  },
  {
    "chunk_id": 3302,
    "paper_filename": "stefano_2024_semiautonomous_deep_learning_system_to_reduce_false_positives_in_screening_mammography.pdf",
    "paper_title": "Stefano 2024 Semiautonomous Deep Learning System To Reduce False Positives In Screening Mammography",
    "chunk_index": 21,
    "total_chunks": 53,
    "text_content": ".001; U.K. dataset 3, P < .001). The device marked the following (prevalence-adjusted) percentages of mammograms as nonsuspicious: 41.6% (95% CI: 40.6%, 42.4%) in U.S. dataset 1, 19.5% (95% CI: 16.9%, 22.1%) in U.S. dataset 2, and 36.8% (95% CI: 34.4%, 39.7%) in U.K. dataset 3. The device also marked several clinical false positives as nonsuspicious, reducing groups (U.S. dataset 1, 143 593; U.K. dataset 3, 18 873; U.S. dataset 2, 1362) (Fig 1) were included in the final analyses. Race was self-",
    "full_text_length": 52687,
    "chunk_length": 1292
  },
  {
    "chunk_id": 3303,
    "paper_filename": "stefano_2024_semiautonomous_deep_learning_system_to_reduce_false_positives_in_screening_mammography.pdf",
    "paper_title": "Stefano 2024 Semiautonomous Deep Learning System To Reduce False Positives In Screening Mammography",
    "chunk_index": 22,
    "total_chunks": 53,
    "text_content": "average perfor- mance of radiologists in the United States (31) and United King- dom (32). Instead, for our simulations of changes in practice, the rule-out device is operated at a point on the right side of the receiver operating characteristic curves, near 100% sensitivity. Effect of the Rule-out Device on Quality of Screening for Patients The main results are for target sensitivities of 99% and 97% (Ta- bles 3, 4). Results are reported for the 12-month and 24-month prediction window for the U",
    "full_text_length": 52687,
    "chunk_length": 1160
  },
  {
    "chunk_id": 3304,
    "paper_filename": "stefano_2024_semiautonomous_deep_learning_system_to_reduce_false_positives_in_screening_mammography.pdf",
    "paper_title": "Stefano 2024 Semiautonomous Deep Learning System To Reduce False Positives In Screening Mammography",
    "chunk_index": 23,
    "total_chunks": 53,
    "text_content": "886 11 592 1362 15 160 1836 1877 No. of patients 30 807 3834 3810 1293 12 072 1462 1491 Age (y) Less than 40 961 113 109 20 4 0 0 40\u201349 21 936 2888 2593 442 120 14 11 50\u201359 33 799 4367 4376 435 5160 593 633 60\u201369 24 754 3167 2959 315 6372 792 801 70\u201379 11 093 1192 1368 133 3300 410 409 Greater than 80 1823 159 187 17 204 27 23 Mean 57.4 (10.2) 56.8 (9.8) 57.3 (10.0) 55.4 (10.5) 63.3 (7.5) 63.6 (7.2) 63.5 (7.1) Median 57.0 56.0 57.0 54.0 63.0 63.0 63.0 Race or ethnicity Asian 677 85 95 0 640 88 8",
    "full_text_length": 52687,
    "chunk_length": 866
  },
  {
    "chunk_id": 3305,
    "paper_filename": "stefano_2024_semiautonomous_deep_learning_system_to_reduce_false_positives_in_screening_mammography.pdf",
    "paper_title": "Stefano 2024 Semiautonomous Deep Learning System To Reduce False Positives In Screening Mammography",
    "chunk_index": 24,
    "total_chunks": 53,
    "text_content": "10 652 1360 1241 38 0 0 0 B 49 273 6072 6108 305 0 0 0 C 29 645 3876 3708 354 0 0 0 D 3650 443 393 79 0 0 0 Unknown 1146 135 142 586 15 160 1836 1877 Note.\u2014Data are numbers, and values in parentheses are SDs. \u201cOther\u201d race includes American Indian or Alaska Native, Native Hawaiian or Other Pacific Islander, as well as persons who explicitly selected \u201cOther Race.\u201d BI-RADS = Breast Imaging Reporting and Data System. 6 radiology-ai.rsna.org \u25a0 Radiology: Artificial Intelligence Volume 6: Number 3\u2014202",
    "full_text_length": 52687,
    "chunk_length": 1105
  },
  {
    "chunk_id": 3306,
    "paper_filename": "stefano_2024_semiautonomous_deep_learning_system_to_reduce_false_positives_in_screening_mammography.pdf",
    "paper_title": "Stefano 2024 Semiautonomous Deep Learning System To Reduce False Positives In Screening Mammography",
    "chunk_index": 25,
    "total_chunks": 53,
    "text_content": "U.K. dataset 3, P < .001). Similarly, the device reduced the number of negative biopsies by 7.4% (95% CI: 4.1%, 12.4%) in U.S. dataset 1, 6.5% (95% CI: 0.0%, 19.0%) in U.S. dataset 2, and 5.9% (95% CI: 2.9%, 11.5%) in U.K. dataset 3. These reduction rates were significantly larger than 0 for U.S. dataset 1 and U.K. dataset 3 (U.S. dataset 1, P < .001; U.K. dataset 3, P < .001; U.S. dataset 2, P = .08) (Fig 3). We also stratified the results according to the scanner models (HSE and SED) used. For",
    "full_text_length": 52687,
    "chunk_length": 1230
  },
  {
    "chunk_id": 3307,
    "paper_filename": "stefano_2024_semiautonomous_deep_learning_system_to_reduce_false_positives_in_screening_mammography.pdf",
    "paper_title": "Stefano 2024 Semiautonomous Deep Learning System To Reduce False Positives In Screening Mammography",
    "chunk_index": 26,
    "total_chunks": 53,
    "text_content": "model HSE and 1294 for SED (Appendices S1 and S11). Effect of the Rule-out Device on Radiologists\u2019 Performance We evaluated the potential effect of the device on the indi- vidual radiologists and on their collective performance. This analysis included both scanner models. Figure 4 compares individual and collective radiologists\u2019 sensitivity and false- positive rates with and without the device. For the double- reading system in U.K. dataset 3, the device improved speci- ficity from 94.7% to 96.0",
    "full_text_length": 52687,
    "chunk_length": 1173
  },
  {
    "chunk_id": 3308,
    "paper_filename": "stefano_2024_semiautonomous_deep_learning_system_to_reduce_false_positives_in_screening_mammography.pdf",
    "paper_title": "Stefano 2024 Semiautonomous Deep Learning System To Reduce False Positives In Screening Mammography",
    "chunk_index": 27,
    "total_chunks": 53,
    "text_content": "89.6%, 94.3%) (P < .001) for the last reader. For the single-reader system in U.S. dataset 1, the device had a more pronounced effect. The average U.S. dataset 1 radiologist\u2019s specificity increased (P < .001) from 88.9% to 92.4% (95% CI: 92.1%, 92.7%) while maintaining sensitivity (P = .01 with 5% noninferiority margin) at 94.1% (95 of 101) (95% CI: 87.5%, 97.8%). For U.S. dataset 2, the rule-out device increased specificity (P < .001) from 88.8% to 90.2% (95% CI: 89.8%, 90.6%) and maintained se",
    "full_text_length": 52687,
    "chunk_length": 1227
  },
  {
    "chunk_id": 3309,
    "paper_filename": "stefano_2024_semiautonomous_deep_learning_system_to_reduce_false_positives_in_screening_mammography.pdf",
    "paper_title": "Stefano 2024 Semiautonomous Deep Learning System To Reduce False Positives In Screening Mammography",
    "chunk_index": 28,
    "total_chunks": 53,
    "text_content": ".001) and benign biopsies by 7.4% (95% CI: 4.1%, 12.4%; P < .001). While previous works have shown a reduction in benign biopsies given lo- calizations from radiologists (34), to our knowledge, this is the first work that has shown a fully autonomous reduction in benign biopsies in simulations. To accomplish this, we in- troduced a labeling scheme that categorized examinations as malignant, high risk, pathology benign, diagnostic benign, Figure 2: Receiver operating characteristic curves show in",
    "full_text_length": 52687,
    "chunk_length": 1374
  },
  {
    "chunk_id": 3310,
    "paper_filename": "stefano_2024_semiautonomous_deep_learning_system_to_reduce_false_positives_in_screening_mammography.pdf",
    "paper_title": "Stefano 2024 Semiautonomous Deep Learning System To Reduce False Positives In Screening Mammography",
    "chunk_index": 29,
    "total_chunks": 53,
    "text_content": "curve (AUC) values are presented with 95% CIs in brackets. Since U.S. dataset 2 performs primarily screening examinations, the radiologists\u2019 sensitivity could not be measured as the true extent of false negatives and interval cancers is unknown. The device would perform at a level of performance close to, but not superior to, average radiologists if operated as a stand-alone device at an oper- ating point of balanced sensitivity and specificity. Instead, in this study, the cancer detector is ope",
    "full_text_length": 52687,
    "chunk_length": 1341
  },
  {
    "chunk_id": 3311,
    "paper_filename": "stefano_2024_semiautonomous_deep_learning_system_to_reduce_false_positives_in_screening_mammography.pdf",
    "paper_title": "Stefano 2024 Semiautonomous Deep Learning System To Reduce False Positives In Screening Mammography",
    "chunk_index": 30,
    "total_chunks": 53,
    "text_content": "device\u2019s performance was lower than the radiologists at a similar operating point, the rule-out device is designed to complement the radiologist and operate at a point of high sensitivity, leading to improvements for the potential human plus AI paradigm. Reduced false positives may increase screening compli- ance as false positives and anxiety are linked to lower screen- ing compliance rates (14). This reduction also prevents the financial burden of follow-up examinations and treatment. Shortage",
    "full_text_length": 52687,
    "chunk_length": 1368
  },
  {
    "chunk_id": 3312,
    "paper_filename": "stefano_2024_semiautonomous_deep_learning_system_to_reduce_false_positives_in_screening_mammography.pdf",
    "paper_title": "Stefano 2024 Semiautonomous Deep Learning System To Reduce False Positives In Screening Mammography",
    "chunk_index": 31,
    "total_chunks": 53,
    "text_content": "of reducing sensitivity and the number of deaths averted by 30% (16). Consequently, many individuals would die of breast cancers that would have oth- erwise been found by more frequent screening. However, our study showed that algorithms may achieve at least half of the reduction of the false-positive rate achieved by the USPSTF guidelines update while reducing the sensitivity by only 1%.Table 3: Stand-alone Rule-out Device, Radiologists, and Rule-out Device plus Radiologists Cancer Detection Pe",
    "full_text_length": 52687,
    "chunk_length": 1440
  },
  {
    "chunk_id": 3313,
    "paper_filename": "stefano_2024_semiautonomous_deep_learning_system_to_reduce_false_positives_in_screening_mammography.pdf",
    "paper_title": "Stefano 2024 Semiautonomous Deep Learning System To Reduce False Positives In Screening Mammography",
    "chunk_index": 32,
    "total_chunks": 53,
    "text_content": "[40.6, 42.4]31.1 [28.7, 33.4]7.4 [4.1, 12.4]101 95 All U.S. dataset 2100.0 [98.9, 100.0] (322/322)100.0 [98.9, 100.0] (330/330)5.76 [5.21, 6.33]5.76 [5.21, 6.33]19.5 [16.9, 22.1]11.9 [8.6, 15.7]6.5 [0.0, 19.0]330 322 All U.K. dataset 399.6 [98.7, 100.0] (550/552)98.7 [97.4, 99.4] (587/595)9.74 [9.06, 10.44]9.71 [9.03, 10.41]36.8 [34.4, 39.7]17.1 [5.9, 30.1]5.9 [2.9, 11.5]595 552 Selenia U.S. dataset 1100.0 [76.8, 100.0] (14/14)100.0 [76.8, 100.0] (14/14)5.90 [3.23, 9.89]5.90 [3.23, 9.89]54.0 [52",
    "full_text_length": 52687,
    "chunk_length": 1539
  },
  {
    "chunk_id": 3314,
    "paper_filename": "stefano_2024_semiautonomous_deep_learning_system_to_reduce_false_positives_in_screening_mammography.pdf",
    "paper_title": "Stefano 2024 Semiautonomous Deep Learning System To Reduce False Positives In Screening Mammography",
    "chunk_index": 33,
    "total_chunks": 53,
    "text_content": "12.8]4.1 [2.0, 7.7]4.3 [0.0, 25.0]262 255 Selenia Dimensions U.K. dataset 397.6 [87.4, 99.9] (41/42)95.7 [85.2, 99.5] (44/46)9.59 [7.52, 11.59]9.36 [7.30, 11.37]13.8 [4.3, 34.8]0.0* 0.0* 46 42 Note.\u2014Data are numbers or percentages, with 95% CIs in brackets and numerators and denominators in parentheses. The prediction win- dows are 12 months in the United States and 24 months in the United Kingdom. The cancer detection rate (CDR), rule-out rate, decrease in false-positive callbacks, and decrease",
    "full_text_length": 52687,
    "chunk_length": 1382
  },
  {
    "chunk_id": 3315,
    "paper_filename": "stefano_2024_semiautonomous_deep_learning_system_to_reduce_false_positives_in_screening_mammography.pdf",
    "paper_title": "Stefano 2024 Semiautonomous Deep Learning System To Reduce False Positives In Screening Mammography",
    "chunk_index": 34,
    "total_chunks": 53,
    "text_content": "models are interpreted differently by the model. Additionally, when selecting our operating point, we targeted 99% and 97% sensitivity from the vali- dation set. The SED examinations had lower rule-out, false- positive reduction, and benign biopsy reduction rates than HSE examinations. Instead, evaluating SED examinations at the 97% target sensitivity was more comparable to HSE examinations at the target sensitivity of 99%. This pattern was also mirrored when comparing U.S. dataset 2 and U.K. da",
    "full_text_length": 52687,
    "chunk_length": 1320
  },
  {
    "chunk_id": 3316,
    "paper_filename": "stefano_2024_semiautonomous_deep_learning_system_to_reduce_false_positives_in_screening_mammography.pdf",
    "paper_title": "Stefano 2024 Semiautonomous Deep Learning System To Reduce False Positives In Screening Mammography",
    "chunk_index": 35,
    "total_chunks": 53,
    "text_content": "introduces the concept of relative sensitivity, meaning that the device still detects the cancers that radiologists detect. We observe that even when the absolute sensitivity is 97.0%, the relative sen- sitivity can remain at 100%, suggesting that no new cancers would be lost in this paradigm. There were limitations to this study. Our evaluation is a retrospective simulation that assumes that radiologist behav- ior is unaffected by the removal of examinations from their worklist. Thus, reader st",
    "full_text_length": 52687,
    "chunk_length": 1340
  },
  {
    "chunk_id": 3317,
    "paper_filename": "stefano_2024_semiautonomous_deep_learning_system_to_reduce_false_positives_in_screening_mammography.pdf",
    "paper_title": "Stefano 2024 Semiautonomous Deep Learning System To Reduce False Positives In Screening Mammography",
    "chunk_index": 36,
    "total_chunks": 53,
    "text_content": "Detection Perfor - mance Metrics for U.S. Dataset 1, U.S. Dataset 2, and U.K. Dataset 3 Test Datasets for the Target 97% Sensitivity Target Sens. 97% Scanner and DatasetDevice Rela- tive Sens. (%)Device Abso- lute Sens. (%)Radiologists CDR (Per 1000 Exami- nations)Device plus Radiologists CDR (Per 1000 Exami- nations)Rule-out Rate (%)Decrease in False-Positive Callbacks (%)Decrease in Benign Biopsies (%)No. of CancersNo. of Radiologist- detected Cancers All U.S. dataset 299.4 [97.8, 99.9] (320/3",
    "full_text_length": 52687,
    "chunk_length": 1524
  },
  {
    "chunk_id": 3318,
    "paper_filename": "stefano_2024_semiautonomous_deep_learning_system_to_reduce_false_positives_in_screening_mammography.pdf",
    "paper_title": "Stefano 2024 Semiautonomous Deep Learning System To Reduce False Positives In Screening Mammography",
    "chunk_index": 37,
    "total_chunks": 53,
    "text_content": "U.K. dataset 399.2 [98.0, 99.8] (506/510)97.4 [95.8, 98.6] (535/549)9.75 [9.04, 10.49]9.68 [8.97, 10.41]52.2 [49.2, 55.0]31.1 [18.9, 47.5]7.6 [3.7, 13.2]549 510 Selenia Dimensions U.S. dataset 197.5 [91.4, 99.7] (79/81)94.3 [87.1, 98.1] (82/87)5.49 [4.37, 6.82]5.36 [4.25, 6.67]50.1 [48.9, 51.2]35.9 [32.4, 38.7]12.8 [7.5, 19.4]87 81 Selenia Dimensions U.S. dataset 2100.0 [98.6, 100.0] (255/255)100.0 [98.6, 100.0] (262/262)5.74 [5.15, 6.37]5.74 [5.15, 6.37]22.3 [19.3, 26.7]11.6 [8.0, 16.8]4.3 [0.0",
    "full_text_length": 52687,
    "chunk_length": 1406
  },
  {
    "chunk_id": 3319,
    "paper_filename": "stefano_2024_semiautonomous_deep_learning_system_to_reduce_false_positives_in_screening_mammography.pdf",
    "paper_title": "Stefano 2024 Semiautonomous Deep Learning System To Reduce False Positives In Screening Mammography",
    "chunk_index": 38,
    "total_chunks": 53,
    "text_content": "bootstraps with the 95% CIs. Sens. = sensitivity. * Indicates less than 10 samples. Radiology: Artificial Intelligence Volume 6: Number 3\u20142024 \u25a0 radiology-ai.rsna.org 9 Pedemonte et ala performance higher than that of SED but lower than that of HSE. The cancer cases with SED scanners were limited in training and testing (except for the U.S. dataset 2 testing dataset), and there was about one-fourth of the number of cancer cases with HSE scanners. Thus, the performance can appear better when not ",
    "full_text_length": 52687,
    "chunk_length": 1293
  },
  {
    "chunk_id": 3320,
    "paper_filename": "stefano_2024_semiautonomous_deep_learning_system_to_reduce_false_positives_in_screening_mammography.pdf",
    "paper_title": "Stefano 2024 Semiautonomous Deep Learning System To Reduce False Positives In Screening Mammography",
    "chunk_index": 39,
    "total_chunks": 53,
    "text_content": "negatives were defined without discerning between missed cancers and true interval cancers not visible at the examination. Also, these false negatives may not be tracked by the clinics where the examinations were performed. In conclusion, rule-out devices promise to have several ben- efits. The elimination of incorrect follow-up examinations and biopsies, which constitute major limitations of breast cancer screening today, benefits patients directly and is the most criti- cal advantage of cancer",
    "full_text_length": 52687,
    "chunk_length": 1341
  },
  {
    "chunk_id": 3321,
    "paper_filename": "stefano_2024_semiautonomous_deep_learning_system_to_reduce_false_positives_in_screening_mammography.pdf",
    "paper_title": "Stefano 2024 Semiautonomous Deep Learning System To Reduce False Positives In Screening Mammography",
    "chunk_index": 40,
    "total_chunks": 53,
    "text_content": "NHS Foundation T rust who developed the OPTIMAM database used for U.K. dataset 3 and Cancer Research UK who funded the creation and maintenance of the database. The authors would also like to thank Daniel Marcus, PhD, Jenny Gurney, MS, David Maffit, and Stephen Moore, MS, for the coordination efforts for data acquisition, and Nate Dolley for the technical assistance. We thank Andrea R. Gwosdow, PhD, of Gwosdow Associates Science Consultants, for assisting in preparing this manuscript. Author con",
    "full_text_length": 52687,
    "chunk_length": 1299
  },
  {
    "chunk_id": 3322,
    "paper_filename": "stefano_2024_semiautonomous_deep_learning_system_to_reduce_false_positives_in_screening_mammography.pdf",
    "paper_title": "Stefano 2024 Semiautonomous Deep Learning System To Reduce False Positives In Screening Mammography",
    "chunk_index": 41,
    "total_chunks": 53,
    "text_content": "for the United States and 24 months for the United Kingdom. In each node, the small font represents the stand-alone radiologists\u2019 workflow and the large font the rule-out device plus ra- diologists workflow. By marking a subset of the screening examinations as nonsuspicious, the rule-out device has the downstream effect of reducing false-positive callbacks (reduction greater than 0; U.S. dataset 1, P < .001; U.S. dataset 2, P < .001; U.K. dataset 3, P < .001) and biopsies (reduction greater than",
    "full_text_length": 52687,
    "chunk_length": 1256
  },
  {
    "chunk_id": 3323,
    "paper_filename": "stefano_2024_semiautonomous_deep_learning_system_to_reduce_false_positives_in_screening_mammography.pdf",
    "paper_title": "Stefano 2024 Semiautonomous Deep Learning System To Reduce False Positives In Screening Mammography",
    "chunk_index": 42,
    "total_chunks": 53,
    "text_content": "and collective radiologists\u2019 sensitivity and false-positive rates with and without the rule-out device. Small circular marks report the effect of the rule-out device on the cancer detection performance of individual radiologists. Large circular marks report the effect on their collective performance. In the U.S. dataset 1 and U.S. dataset 2 results, large square and triangular marks report the effect of the rule-out device on radiologists\u2019 collective performance when reading examinations from th",
    "full_text_length": 52687,
    "chunk_length": 1361
  },
  {
    "chunk_id": 3324,
    "paper_filename": "stefano_2024_semiautonomous_deep_learning_system_to_reduce_false_positives_in_screening_mammography.pdf",
    "paper_title": "Stefano 2024 Semiautonomous Deep Learning System To Reduce False Positives In Screening Mammography",
    "chunk_index": 43,
    "total_chunks": 53,
    "text_content": "the first, second, or arbitrating reader. In the U.S. results, the gray rectangle represents the region of acceptable performance as defined by Lehman et al (31). The U.K. National Health System considers acceptable false-positive rates between 3% and 9% and does not define explicit thresholds of acceptable performance for sensitivity, focusing instead on performance thresholds for the cancer detection rate (33). Outcomes are derived from examinations within 12 months from the screening examinat",
    "full_text_length": 52687,
    "chunk_length": 1284
  },
  {
    "chunk_id": 3325,
    "paper_filename": "stefano_2024_semiautonomous_deep_learning_system_to_reduce_false_positives_in_screening_mammography.pdf",
    "paper_title": "Stefano 2024 Semiautonomous Deep Learning System To Reduce False Positives In Screening Mammography",
    "chunk_index": 44,
    "total_chunks": 53,
    "text_content": ".; clinical studies, N.G., R.L.W .; experi- mental studies, S.P ., T. T. , B.M., M.S., R.L.W .; statistical analysis, S.P ., T. T. , B.M., Y.N.T .V ., T .M., M.S.; and manuscript editing, S.P ., T. T. , B.M., T .M., R.M.H., N.Z.D., S.H., C.M.A., J.S., R.L.W . Disclosures of conflicts of interest: S.P . Whiterabbit.ai employee; travel support from Whiterabbit.ai; patents with Whiterabbit.ai; stock options in Whiterabbit.ai. T. T. Whiterabbit.ai employee; patents with Whiterabbit.ai; stock options",
    "full_text_length": 52687,
    "chunk_length": 1507
  },
  {
    "chunk_id": 3326,
    "paper_filename": "stefano_2024_semiautonomous_deep_learning_system_to_reduce_false_positives_in_screening_mammography.pdf",
    "paper_title": "Stefano 2024 Semiautonomous Deep Learning System To Reduce False Positives In Screening Mammography",
    "chunk_index": 45,
    "total_chunks": 53,
    "text_content": "from Whiterabbit.ai; stock options in Whiterabbit.ai. S.H. Intermittent consultant for Whiterabbit.ai and Therapixel regarding AI in breast imaging. C.M.A. General payments for product devel- opment, quality control considerations, and providing general subject matter expertise from Whiterabbit.ai; volunteer board member for Pink Ribbon Good; stock options in Whiterabbit.ai. J.S. Whiterabbit.ai employee; travel support from Whiterabbit.ai; in- ventor on filed patents with Whiterabbit.ai; chief t",
    "full_text_length": 52687,
    "chunk_length": 1454
  },
  {
    "chunk_id": 3327,
    "paper_filename": "stefano_2024_semiautonomous_deep_learning_system_to_reduce_false_positives_in_screening_mammography.pdf",
    "paper_title": "Stefano 2024 Semiautonomous Deep Learning System To Reduce False Positives In Screening Mammography",
    "chunk_index": 46,
    "total_chunks": 53,
    "text_content": "M, Soerjomataram I, et al. Estimating the global cancer incidence and mortality in 2018: GLOBOCAN sources and methods. Int J Cancer 2019;144(8):1941\u20131953. 2. Shapiro S. Periodic screening for breast cancer: the HIP Randomized Controlled Trial. Health Insurance Plan. J Natl Cancer Inst Monogr 1997;1997(22):27\u201330. 3. Andersson I, Aspegren K, Janzon L, et al. Mammographic screening and mortality from breast cancer: the Malm\u00f6 mammographic screening trial. BMJ 1988;297(6654):943\u2013948. 4. Frisell J, Li",
    "full_text_length": 52687,
    "chunk_length": 1399
  },
  {
    "chunk_id": 3328,
    "paper_filename": "stefano_2024_semiautonomous_deep_learning_system_to_reduce_false_positives_in_screening_mammography.pdf",
    "paper_title": "Stefano 2024 Semiautonomous Deep Learning System To Reduce False Positives In Screening Mammography",
    "chunk_index": 47,
    "total_chunks": 53,
    "text_content": "MB, Moore S, Polk S, Shtatland E, Elmore JG, Fletcher SW. Increased patient concern after false-positive mammograms: clinician documentation and subsequent ambulatory visits. J Gen Intern Med 2001;16(3):150\u2013156. 8. Barton MB, Morley DS, Moore S, et al. Decreasing women\u2019s anxiet- ies after abnormal mammograms: a controlled trial. J Natl Cancer Inst 2004;96(7):529\u2013538. 9. Dolan NC, Feinglass J, Priyanath A, Haviley C, Sorensen AV, Venta LA. Measuring satisfaction with mammography results reporting",
    "full_text_length": 52687,
    "chunk_length": 1416
  },
  {
    "chunk_id": 3329,
    "paper_filename": "stefano_2024_semiautonomous_deep_learning_system_to_reduce_false_positives_in_screening_mammography.pdf",
    "paper_title": "Stefano 2024 Semiautonomous Deep Learning System To Reduce False Positives In Screening Mammography",
    "chunk_index": 48,
    "total_chunks": 53,
    "text_content": "screening for breast cancer raise anxiety? Results from a three wave prospective study in England. J Epidemiol Community Health 1995;49(4):413\u2013418. 13. Bolejko A. Psychosocial consequences of false-positive mammography among women attending breast cancer screening. Assessment, prediction, and coping. Department of Health Sciences, Lund University, 2014. 14. Chubak J, Boudreau DM, Fishman PA, Elmore JG. Cost of breast-related care in the year following false positive screening mammograms. Med Car",
    "full_text_length": 52687,
    "chunk_length": 1445
  },
  {
    "chunk_id": 3330,
    "paper_filename": "stefano_2024_semiautonomous_deep_learning_system_to_reduce_false_positives_in_screening_mammography.pdf",
    "paper_title": "Stefano 2024 Semiautonomous Deep Learning System To Reduce False Positives In Screening Mammography",
    "chunk_index": 49,
    "total_chunks": 53,
    "text_content": "Assessment to Interpret Screening Mammograms. JAMA Netw Open 2020;3(3):e200265. 18. Harvey H, Karpati E, Khara G, et al. The Role of Deep Learning in Breast Screening. Curr Breast Cancer Rep 2019;11(1):17\u201322. 19. Cole EB, Zhang Z, Marques HS, et al. Assessing the stand-alone sensitivity of computer-aided detection with cancer cases from the Digital Mammographic Imaging Screening Trial. AJR Am J Roentgenol 2012;199(3):W392\u2013W401. 20. Gromet M. Comparison of computer-aided detection to double readi",
    "full_text_length": 52687,
    "chunk_length": 1445
  },
  {
    "chunk_id": 3331,
    "paper_filename": "stefano_2024_semiautonomous_deep_learning_system_to_reduce_false_positives_in_screening_mammography.pdf",
    "paper_title": "Stefano 2024 Semiautonomous Deep Learning System To Reduce False Positives In Screening Mammography",
    "chunk_index": 50,
    "total_chunks": 53,
    "text_content": "reduce the workload of mammographic screening by automatic identification of normal exams with artificial intelligence? A feasibility study. Eur Radiol 2019;29(9):4825\u20134832. 24. Yala A, Schuster T, Miles R, Barzilay R, Lehman C. A Deep Learning Model to Triage Screening Mammograms: A Simulation Study. Radiology 2019;293(1):38\u201346. 25. Dembrower K, W\u00e5hlin E, Liu Y, et al. Effect of artificial intelligence-based triaging of breast cancer screening mammograms on cancer detection and radiologist work",
    "full_text_length": 52687,
    "chunk_length": 1463
  },
  {
    "chunk_id": 3332,
    "paper_filename": "stefano_2024_semiautonomous_deep_learning_system_to_reduce_false_positives_in_screening_mammography.pdf",
    "paper_title": "Stefano 2024 Semiautonomous Deep Learning System To Reduce False Positives In Screening Mammography",
    "chunk_index": 51,
    "total_chunks": 53,
    "text_content": "Database: A Large-Scale Resource of Mammography Images and Clinical Data. Radiol Artif Intell 2020;3(1):e200103. 29. Liu JP, Hsueh HM, Hsieh E, Chen JJ. Tests for equivalence or non-inferiority for paired binary data. Stat Med 2002;21(2):231\u2013245. 30. Thulin M. Modern Statistics with R: From wrangling and exploring data to inference and predictive modelling. BoD - Books on Demand, 2021. 31. Lehman CD, Arao RF, Sprague BL, et al. National performance benchmarks for modern screening digital mammogr",
    "full_text_length": 52687,
    "chunk_length": 1338
  },
  {
    "chunk_id": 3333,
    "paper_filename": "stefano_2024_semiautonomous_deep_learning_system_to_reduce_false_positives_in_screening_mammography.pdf",
    "paper_title": "Stefano 2024 Semiautonomous Deep Learning System To Reduce False Positives In Screening Mammography",
    "chunk_index": 52,
    "total_chunks": 53,
    "text_content": "al. Reducing false-positive biopsies using deep neural networks that utilize local and global image context of screening mammograms. J Digit Imaging 2021;34(6):1414\u20131423.",
    "full_text_length": 52687,
    "chunk_length": 170
  },
  {
    "chunk_id": 3334,
    "paper_filename": "sudan_2021_ensemble_learning_based_hybrid_segmentation_of_mammography_images_for_breast_cancer_risk_prediction.pdf",
    "paper_title": "Sudan 2021 Ensemble Learning Based Hybrid Segmentation Of Mammography Images For Breast Cancer Risk Prediction",
    "chunk_index": 0,
    "total_chunks": 45,
    "text_content": "Research Article Ensemble Learning-Based Hybrid Segmentation of Mammographic Images for Breast Cancer Risk Prediction Using Fuzzy C-Means and CNN Model Sudan Jha ,1Sultan Ahmad ,2,3Anoopa Arya,4,5Bader Alouf ,6Abdullah Alharbi,7 Meshal Alharbi ,2and Surender Singh 4 1DepartmentofComputerScience andEngineering,SchoolofEngineering,KathmanduUniversity,Banepa,Kathmandu, Nepal 2Department of Computer Science, College of Computer Engineering and Sciences, Prince Sattam Bin Abdulaziz University, P.O. B",
    "full_text_length": 55315,
    "chunk_length": 1616
  },
  {
    "chunk_id": 3335,
    "paper_filename": "sudan_2021_ensemble_learning_based_hybrid_segmentation_of_mammography_images_for_breast_cancer_risk_prediction.pdf",
    "paper_title": "Sudan 2021 Ensemble Learning Based Hybrid Segmentation Of Mammography Images For Breast Cancer Risk Prediction",
    "chunk_index": 1,
    "total_chunks": 45,
    "text_content": "1 November 2021; Revised 23 July 2022; Accepted 25 November 2022; Published 31 January 2023 Academic Editor: Agostino ForestieroCopyright \u00a92023 Sudan Jha et al. Tis is an open access article distributed under the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited. Teresearchinterestinthisfeldisthatfemalesarenotawareoftheirhealthconditionsuntiltheydeveloptumour,especiallywhen breastcancer",
    "full_text_length": 55315,
    "chunk_length": 1917
  },
  {
    "chunk_id": 3336,
    "paper_filename": "sudan_2021_ensemble_learning_based_hybrid_segmentation_of_mammography_images_for_breast_cancer_risk_prediction.pdf",
    "paper_title": "Sudan 2021 Ensemble Learning Based Hybrid Segmentation Of Mammography Images For Breast Cancer Risk Prediction",
    "chunk_index": 2,
    "total_chunks": 45,
    "text_content": "Images from the MIAS database havebeenusedtoobtainreadingsforparameters:threshold,accuracy,sensitivity,specifcityrate,biopsyrate,oracombinationof all the parameters and many others under study. 1.Introduction Cancer is a disease that causes abnormal changes in the body\u2019s tissues and cells, as well as growth that is out ofcontrol. One of the types of cancer is breast cancer. Te prognosis assessment of breast cancer can help patients withbreastcancerimprovetheirchancesofsurvival.Teidea behind the ",
    "full_text_length": 55315,
    "chunk_length": 1951
  },
  {
    "chunk_id": 3337,
    "paper_filename": "sudan_2021_ensemble_learning_based_hybrid_segmentation_of_mammography_images_for_breast_cancer_risk_prediction.pdf",
    "paper_title": "Sudan 2021 Ensemble Learning Based Hybrid Segmentation Of Mammography Images For Breast Cancer Risk Prediction",
    "chunk_index": 3,
    "total_chunks": 45,
    "text_content": "is one ofthe modalities to detect masses, especially in densebreasts; it is the best suitable technique for the same [2].Despite a few shortcomings, mammography holdsa sensitivity of approximately 90% in the detection of tumours [3]. Segmentation becomes robust in noisy, blurred images, and low contrast images. Images need to be preprocessed before segmentation. Multiple techniques for segmentationtosegmentoutmasses,microcalcifcation,pectoralmuscles,and lesions have been discussed in the paper. ",
    "full_text_length": 55315,
    "chunk_length": 1584
  },
  {
    "chunk_id": 3338,
    "paper_filename": "sudan_2021_ensemble_learning_based_hybrid_segmentation_of_mammography_images_for_breast_cancer_risk_prediction.pdf",
    "paper_title": "Sudan 2021 Ensemble Learning Based Hybrid Segmentation Of Mammography Images For Breast Cancer Risk Prediction",
    "chunk_index": 4,
    "total_chunks": 45,
    "text_content": "as \u201c2D medianflter, CLAHE, FCM on images, removing con- nected components having less than xpixels.\u201d (ii) To improve the segmentation accuracy by de- veloping the algorithm which optimized the threshold value and specifcity of each thresholdbetween the data points. (iii) For displaying the relevant captions, calculate the best value for threshold position, sensitivity, spec-ifcity, area under curve, accuracy, and all false andtrue positives and negatives. (iv) To make use of the same algorithm f",
    "full_text_length": 55315,
    "chunk_length": 1593
  },
  {
    "chunk_id": 3339,
    "paper_filename": "sudan_2021_ensemble_learning_based_hybrid_segmentation_of_mammography_images_for_breast_cancer_risk_prediction.pdf",
    "paper_title": "Sudan 2021 Ensemble Learning Based Hybrid Segmentation Of Mammography Images For Breast Cancer Risk Prediction",
    "chunk_index": 5,
    "total_chunks": 45,
    "text_content": "and the ratio can be improved. Te best modality for early detectionismammography,especiallyinlow-contrastanddensebreastimages. Diferent authors have worked in this feld for theearly detection of cancer using various modalities and seg-mentationtechniquesthathavebeenlistedinthissectionforbetterfutureresearchandimplementation.TestudybyBicket al. [6] implements diferent procedures, such as thresh-olding, fltering, and region-growing. Te mammogramreduces noise from the image, improves the contrast, ",
    "full_text_length": 55315,
    "chunk_length": 1836
  },
  {
    "chunk_id": 3340,
    "paper_filename": "sudan_2021_ensemble_learning_based_hybrid_segmentation_of_mammography_images_for_breast_cancer_risk_prediction.pdf",
    "paper_title": "Sudan 2021 Ensemble Learning Based Hybrid Segmentation Of Mammography Images For Breast Cancer Risk Prediction",
    "chunk_index": 6,
    "total_chunks": 45,
    "text_content": "Seed selection is made using auto-mation along with histogram peak analysis. Te appropri-atenessofthesegmentedregionisstudied.Tepreprocessingof the image is done before carrying out segmentation. Tesensitivity is primarily focused upon during the work.GLCM-based sum average features learned to fetch the seedpointautomatically,whichisconsideredfarbetterthanotherGLCM-based texture features. Te paper also discussed the importance of extracting the mass boundary more precisely to understand the seve",
    "full_text_length": 55315,
    "chunk_length": 1808
  },
  {
    "chunk_id": 3341,
    "paper_filename": "sudan_2021_ensemble_learning_based_hybrid_segmentation_of_mammography_images_for_breast_cancer_risk_prediction.pdf",
    "paper_title": "Sudan 2021 Ensemble Learning Based Hybrid Segmentation Of Mammography Images For Breast Cancer Risk Prediction",
    "chunk_index": 7,
    "total_chunks": 45,
    "text_content": "KNN andSVM techniques are used to classify the attributes amongbenign and malignant tissue. Te data mining technique iswidelyusedinthepaper.Terigorousnessofthecancerstagepredicted, which may further help in the early detection ofcancer. Vala and Baxi [11] discussed the benefts of the Otsu2 JournalofHealthcareEngineering 7158, 2023, 1, Downloaded from https://onlinelibrary.wiley.com/doi/10.1155/2023/1491955 by INASP/HINARI - PAKISTAN, Wiley Online Library on [11/08/2025]. See the Terms and Condit",
    "full_text_length": 55315,
    "chunk_length": 1590
  },
  {
    "chunk_id": 3342,
    "paper_filename": "sudan_2021_ensemble_learning_based_hybrid_segmentation_of_mammography_images_for_breast_cancer_risk_prediction.pdf",
    "paper_title": "Sudan 2021 Ensemble Learning Based Hybrid Segmentation Of Mammography Images For Breast Cancer Risk Prediction",
    "chunk_index": 8,
    "total_chunks": 45,
    "text_content": "automatic pick of seed point is done Gupta and Tiwari, 2017 [14]HM-GRA and CLAHETe histogram of the mammographic image was generated and the selection of parameters for enhancement was performed.Temodifcationofthe histogram is done using the uniform histogram. Te grey relational analysis was used to improve the contrast and further normalizationandsegmentationof ROI was presentedMini-MIAS 322 samplesTe contrast of the image is enhanced to improve minute calcifcation by decreasing the ratio of fa",
    "full_text_length": 55315,
    "chunk_length": 1460
  },
  {
    "chunk_id": 3343,
    "paper_filename": "sudan_2021_ensemble_learning_based_hybrid_segmentation_of_mammography_images_for_breast_cancer_risk_prediction.pdf",
    "paper_title": "Sudan 2021 Ensemble Learning Based Hybrid Segmentation Of Mammography Images For Breast Cancer Risk Prediction",
    "chunk_index": 9,
    "total_chunks": 45,
    "text_content": "removed from original image further segmentation are done based on pixel-wise clustering,followedbydetectionof the boundary of breast muscles. Finally,a local texture flteris used to detect calcifcationMIAS, BCDR, INbreast, 322, 100, and 201 samplesItis immuneto noisesand can detect calcifcation in dense breasts too. With few settings,FFDMimagescanalsoanalyze efectivelyTe distinction of skin air boundary marked by the proposed algorithm gradient weight map in compassion to another threshold-base",
    "full_text_length": 55315,
    "chunk_length": 1724
  },
  {
    "chunk_id": 3344,
    "paper_filename": "sudan_2021_ensemble_learning_based_hybrid_segmentation_of_mammography_images_for_breast_cancer_risk_prediction.pdf",
    "paper_title": "Sudan 2021 Ensemble Learning Based Hybrid Segmentation Of Mammography Images For Breast Cancer Risk Prediction",
    "chunk_index": 10,
    "total_chunks": 45,
    "text_content": "INbreastTe precision of segmentation is higher in comparison to other existing algorithmsTe classifcation between acceptable, unacceptable, and successful.Tesensitivitywasthen checked for unacceptable samples Hazarika andMahanta, 2018 [18]Pectoral muscle removal using region growingA suppression algorithm is applied, and further, the samples whose results come comparable and close with the hand-drawn segmented mask are distinguished as acceptedMini-mias and 150 samples86.67% is the accuracy of a",
    "full_text_length": 55315,
    "chunk_length": 1600
  },
  {
    "chunk_id": 3345,
    "paper_filename": "sudan_2021_ensemble_learning_based_hybrid_segmentation_of_mammography_images_for_breast_cancer_risk_prediction.pdf",
    "paper_title": "Sudan 2021 Ensemble Learning Based Hybrid Segmentation Of Mammography Images For Breast Cancer Risk Prediction",
    "chunk_index": 11,
    "total_chunks": 45,
    "text_content": "mammogram is done then ROI, whichissegmentedoutusingfuzzy C-means clustering segmentation methodDDSM and 300 samples94%segmentation precisionin terms of sensitivityAs it is based on an intelligent system, i.e., fuzzy clustering, it provides high precision Touil and Kalti, 2016 [21]IFBS (iterative fuzzy breast segmentation algorithm)Teimageisdividedinto kclusters to remove the over-segmentation background region and extract perfect ROIMIAS and 200 samplesAs compared to the manual ROI curve, its p",
    "full_text_length": 55315,
    "chunk_length": 1653
  },
  {
    "chunk_id": 3346,
    "paper_filename": "sudan_2021_ensemble_learning_based_hybrid_segmentation_of_mammography_images_for_breast_cancer_risk_prediction.pdf",
    "paper_title": "Sudan 2021 Ensemble Learning Based Hybrid Segmentation Of Mammography Images For Breast Cancer Risk Prediction",
    "chunk_index": 12,
    "total_chunks": 45,
    "text_content": "known before as it does not work on images with edgesAlso, work where the variance is diferent Aggarwal and Chatha,2019[23]Edge detection algorithm is designedon8-bitgrayscaleimageBinarization is done to reduce the data reduction step using an edge detection algorithmMIAS 50 random samplesIt reduced the diference between region of interest and backgroundData reduction leads to loss ofinformation can be reduced by using multilevel thresholding Tembhurne et al.,2021 [24]Computer-aided transfer lea",
    "full_text_length": 55315,
    "chunk_length": 1623
  },
  {
    "chunk_id": 3347,
    "paper_filename": "sudan_2021_ensemble_learning_based_hybrid_segmentation_of_mammography_images_for_breast_cancer_risk_prediction.pdf",
    "paper_title": "Sudan 2021 Ensemble Learning Based Hybrid Segmentation Of Mammography Images For Breast Cancer Risk Prediction",
    "chunk_index": 13,
    "total_chunks": 45,
    "text_content": "medical CT image restoration and recovers the reconstructed image quality, efciency, and speed through sparse transform Fang et al., 2021 [26]Confguration of the multilayer perceptron (MLP) neural network multilayer perceptron network using backpropagation networkA new training algorithm is proposed based on whale optimization for MLP networkMIAS 332 digitized mammography imagesDetection performance is detected using detection rate and identifcation with the false percentageAccuracy is achieved ",
    "full_text_length": 55315,
    "chunk_length": 1610
  },
  {
    "chunk_id": 3348,
    "paper_filename": "sudan_2021_ensemble_learning_based_hybrid_segmentation_of_mammography_images_for_breast_cancer_risk_prediction.pdf",
    "paper_title": "Sudan 2021 Ensemble Learning Based Hybrid Segmentation Of Mammography Images For Breast Cancer Risk Prediction",
    "chunk_index": 14,
    "total_chunks": 45,
    "text_content": "threshold values like a balanced histogram, k-means, and otsu maximum varianceNoprerequisitesarerequiredaboutimageand computation is fast with less complexityInformation with low peaks is not considered and ignored; hence, continuous value is not obtained. In the presence of noise and poor contrast, performance is not up to mark Region-based [16, 27]Identical regions are grouped using techniques like region growing, splitting, merging, etcBetter than edge detection about noise immunity. It works",
    "full_text_length": 55315,
    "chunk_length": 1555
  },
  {
    "chunk_id": 3349,
    "paper_filename": "sudan_2021_ensemble_learning_based_hybrid_segmentation_of_mammography_images_for_breast_cancer_risk_prediction.pdf",
    "paper_title": "Sudan 2021 Ensemble Learning Based Hybrid Segmentation Of Mammography Images For Breast Cancer Risk Prediction",
    "chunk_index": 15,
    "total_chunks": 45,
    "text_content": "many edges or improperly defned boundaries and noisy images Contour-based segmentation [25]Inthisworkcomputer-aideddiagnosis(CAD)and fusion via CNN is done for recognition, analysis, and further treatment with the help of RF giving maximumaccuracyof97.51%andminimumerror via CNN classifer 95.65%. After segmentation using sparse transform, the algorithm eliminates the attributes of the point spread functionDiagnosis of cancer and accuracy detection is now upgraded with machine learning techniques.",
    "full_text_length": 55315,
    "chunk_length": 1729
  },
  {
    "chunk_id": 3350,
    "paper_filename": "sudan_2021_ensemble_learning_based_hybrid_segmentation_of_mammography_images_for_breast_cancer_risk_prediction.pdf",
    "paper_title": "Sudan 2021 Ensemble Learning Based Hybrid Segmentation Of Mammography Images For Breast Cancer Risk Prediction",
    "chunk_index": 16,
    "total_chunks": 45,
    "text_content": "other deep learningmodels,itdoesnotrequireannotated photosTe method is easily simulated by AI, ML, or DNN persons rather than physicians or biologists6 JournalofHealthcareEngineering 7158, 2023, 1, Downloaded from https://onlinelibrary.wiley.com/doi/10.1155/2023/1491955 by INASP/HINARI - PAKISTAN, Wiley Online Library on [11/08/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the appli",
    "full_text_length": 55315,
    "chunk_length": 1651
  },
  {
    "chunk_id": 3351,
    "paper_filename": "sudan_2021_ensemble_learning_based_hybrid_segmentation_of_mammography_images_for_breast_cancer_risk_prediction.pdf",
    "paper_title": "Sudan 2021 Ensemble Learning Based Hybrid Segmentation Of Mammography Images For Breast Cancer Risk Prediction",
    "chunk_index": 17,
    "total_chunks": 45,
    "text_content": "grey level cooccurrence matrix are utilized to classify to extract the features from enhanced images using the hybrid SVM-ANNA better algorithm can be made for enhanced feature detection Supervised segmentation [32]Te work proposes to constrain the segmentation output when morphological operations to measure performance which uses top-hat and closing operations to evaluate on high-resolution images from the INBreast datasetItachievesanincreaseinF1andintherecallif comparedtothetrainingwithoutmorp",
    "full_text_length": 55315,
    "chunk_length": 1653
  },
  {
    "chunk_id": 3352,
    "paper_filename": "sudan_2021_ensemble_learning_based_hybrid_segmentation_of_mammography_images_for_breast_cancer_risk_prediction.pdf",
    "paper_title": "Sudan 2021 Ensemble Learning Based Hybrid Segmentation Of Mammography Images For Breast Cancer Risk Prediction",
    "chunk_index": 18,
    "total_chunks": 45,
    "text_content": "radiation Graph segmentation [34]Te automatic segmentation of stained tissue images aid in the detection of malignant disease, and this is done after the separation of contacting cells. Traditional segmentation algorithms face numerous challenges. Tey present a novel automatic approach for segmenting clustered cancercellsinthiswork.Inthefrststage,alsouse Chan-Vese energy functional to determine cell areas by a modifed geometric active contourBydetermininghighconcavitylocationsalongwith the cell ",
    "full_text_length": 55315,
    "chunk_length": 1819
  },
  {
    "chunk_id": 3353,
    "paper_filename": "sudan_2021_ensemble_learning_based_hybrid_segmentation_of_mammography_images_for_breast_cancer_risk_prediction.pdf",
    "paper_title": "Sudan 2021 Ensemble Learning Based Hybrid Segmentation Of Mammography Images For Breast Cancer Risk Prediction",
    "chunk_index": 19,
    "total_chunks": 45,
    "text_content": "on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License Table2: Continued. Segmentation techniques Overview Advantages Drawbacks Region \u2013growing algorithm [36]TeworkprovidesanewCRG(conditionalregion growth) approach for determining correct MC bounds beginning from seed points selected, and theyaredeterminedbydetectingregionalmaxima and analyzing superpixels. Te region-growing stageisthenmaintainedbythecriteriasettunedto MC detection for contr",
    "full_text_length": 55315,
    "chunk_length": 1711
  },
  {
    "chunk_id": 3354,
    "paper_filename": "sudan_2021_ensemble_learning_based_hybrid_segmentation_of_mammography_images_for_breast_cancer_risk_prediction.pdf",
    "paper_title": "Sudan 2021 Ensemble Learning Based Hybrid Segmentation Of Mammography Images For Breast Cancer Risk Prediction",
    "chunk_index": 20,
    "total_chunks": 45,
    "text_content": "computational complexity to design an algorithm for the measurement of breast density using machine learning or deep learningBreast density measurement via machine learning and deep learning is increasing the rapid developmentIf the density of the breast increases, then chances of breast cancer also increase which reduces the sensitivity of measuring mammographic density Watershed algorithm [38]A novel approach for detection of breast cancer automatically from histopathological images that are c",
    "full_text_length": 55315,
    "chunk_length": 1476
  },
  {
    "chunk_id": 3355,
    "paper_filename": "sudan_2021_ensemble_learning_based_hybrid_segmentation_of_mammography_images_for_breast_cancer_risk_prediction.pdf",
    "paper_title": "Sudan 2021 Ensemble Learning Based Hybrid Segmentation Of Mammography Images For Breast Cancer Risk Prediction",
    "chunk_index": 21,
    "total_chunks": 45,
    "text_content": "used for automated breast cancer detection with high performances Fuzzy \u2013C means [39]Tis method uses computer-aided cancer diagnosis in its various stages. Te frst stage performs a reduction of noise and preprocessing contrast enhancement, while the 2nd stage segments the ROI and, the kernel fuzzy C-meansmethod is used for segmentation, and features are extracted and feature selection is employed. For fnal identifcation, these selected features are fed into an SVMTe proposed version of the metah",
    "full_text_length": 55315,
    "chunk_length": 1598
  },
  {
    "chunk_id": 3356,
    "paper_filename": "sudan_2021_ensemble_learning_based_hybrid_segmentation_of_mammography_images_for_breast_cancer_risk_prediction.pdf",
    "paper_title": "Sudan 2021 Ensemble Learning Based Hybrid Segmentation Of Mammography Images For Breast Cancer Risk Prediction",
    "chunk_index": 22,
    "total_chunks": 45,
    "text_content": "Continued. Segmentation techniques Overview Advantages Drawbacks Otsu\u2019s optimal thresholding [40]Tepapercreatesaframeworkthatscansthestage of cancer by using the optimized kernel fuzzy clustering algorithm to determine cancer and identify segmented regions in mammogram images. Te mammographic images are preprocessed noise-free images obtained by using the hybrid denoising flter algorithm. Data clustering is done to classify data of similar types in one group and of dissimilar types in another gr",
    "full_text_length": 55315,
    "chunk_length": 1510
  },
  {
    "chunk_id": 3357,
    "paper_filename": "sudan_2021_ensemble_learning_based_hybrid_segmentation_of_mammography_images_for_breast_cancer_risk_prediction.pdf",
    "paper_title": "Sudan 2021 Ensemble Learning Based Hybrid Segmentation Of Mammography Images For Breast Cancer Risk Prediction",
    "chunk_index": 23,
    "total_chunks": 45,
    "text_content": "run length matrix, texture feature: Local binary pattern, and scale-invariant feature transformAn amalgamation of local and global features with an SVM classifer diferentiates benevolent or malignant images and an accuracy of 96% is achieved by GLCM and LBPTe existence rate after detection of cancer afected person cannot be sometimes predictedJournalofHealthcareEngineering 9 7158, 2023, 1, Downloaded from https://onlinelibrary.wiley.com/doi/10.1155/2023/1491955 by INASP/HINARI - PAKISTAN, Wiley ",
    "full_text_length": 55315,
    "chunk_length": 1529
  },
  {
    "chunk_id": 3358,
    "paper_filename": "sudan_2021_ensemble_learning_based_hybrid_segmentation_of_mammography_images_for_breast_cancer_risk_prediction.pdf",
    "paper_title": "Sudan 2021 Ensemble Learning Based Hybrid Segmentation Of Mammography Images For Breast Cancer Risk Prediction",
    "chunk_index": 24,
    "total_chunks": 45,
    "text_content": "for segmenting outROI by merging the region and global thresholding appliedto the mammographic images. To eliminate Gaussian noise,Wiener flters were used, and then the resulting image wasnormalized using the histogram to enhance the quality ofinput images. Among the above two technologies, a globalthreshold isused tosegment ROI,andthe segmentedregionis extracted by region merging. Te implementation andtesting was done on 50 mammographic images and thespecifcity of the research was 82%. Te relat",
    "full_text_length": 55315,
    "chunk_length": 1348
  },
  {
    "chunk_id": 3359,
    "paper_filename": "sudan_2021_ensemble_learning_based_hybrid_segmentation_of_mammography_images_for_breast_cancer_risk_prediction.pdf",
    "paper_title": "Sudan 2021 Ensemble Learning Based Hybrid Segmentation Of Mammography Images For Breast Cancer Risk Prediction",
    "chunk_index": 25,
    "total_chunks": 45,
    "text_content": "binary imageApply FCM on I2 to initiate segmentation Calculate image vector of input I and segmented image I3Calculate threshold values, sensibility and specificity of each of each threshold between data pointsCalculate Area under curve, accuracy, PPV, NPV, FNR, FPR, FOR, MCC, Onformedness, and MarkednessFigure1: Proposed method for doing segmentation on mammograms. STARTClassified images using Deep Learning (CNN Model)Apply Activation function in the output layerflattening the data Visualized i",
    "full_text_length": 55315,
    "chunk_length": 1655
  },
  {
    "chunk_id": 3360,
    "paper_filename": "sudan_2021_ensemble_learning_based_hybrid_segmentation_of_mammography_images_for_breast_cancer_risk_prediction.pdf",
    "paper_title": "Sudan 2021 Ensemble Learning Based Hybrid Segmentation Of Mammography Images For Breast Cancer Risk Prediction",
    "chunk_index": 26,
    "total_chunks": 45,
    "text_content": "the green channel on input image IM. GRC \ufffdimgcomplement(GR); axxes(handlesi.greenimg_channel); set(imgshow(GRC)); CLH \ufffdadaptohisteq(GRC);//To apply CLAHE on GRC to receiveCLH % contrast limited image. set(imshow(CLH));SE\ufffdstrel(\u201cball\u201d,8,8);//Perform structuring of an element with the specifed neighbourhood (8). mgopen \ufffdimgopen(CLH,SE);//Do morphological on binary image CLH with structuring element \u2018sse\u2019. gordisk \ufffdCLH - gimgopen;//Replace optic disk by appling a 2D Median Filter. medfllt \ufffdmedfllt2",
    "full_text_length": 55315,
    "chunk_length": 1846
  },
  {
    "chunk_id": 3361,
    "paper_filename": "sudan_2021_ensemble_learning_based_hybrid_segmentation_of_mammography_images_for_breast_cancer_risk_prediction.pdf",
    "paper_title": "Sudan 2021 Ensemble Learning Based Hybrid Segmentation Of Mammography Images For Breast Cancer Risk Prediction",
    "chunk_index": 27,
    "total_chunks": 45,
    "text_content": "errors set all default parameters //Evaluate the threshold values among the data points. % Sort data points % ss_data \ufffdunique(sort([class_1; class_2])); % Del NaN values % ss_data(isnan(ss_data)) \ufffd[]; % Cal diference between consecutive points % dd_data \ufffddif(ss_data); % Cal last point % dd_data(length(d_data)+1,1) \ufffddd_data(length(d_data)); % Cal frst point % thresh(1,1) \ufffdss_data(1) - dd_data(1); % Cal Treshold % thres(2:len(s_data)+1,1) \ufffds_data+d_data./2; cur\ufffdzeross(sizeof(thresh,1),2);//Find se",
    "full_text_length": 55315,
    "chunk_length": 2033
  },
  {
    "chunk_id": 3362,
    "paper_filename": "sudan_2021_ensemble_learning_based_hybrid_segmentation_of_mammography_images_for_breast_cancer_risk_prediction.pdf",
    "paper_title": "Sudan 2021 Ensemble Learning Based Hybrid Segmentation Of Mammography Images For Breast Cancer Risk Prediction",
    "chunk_index": 28,
    "total_chunks": 45,
    "text_content": "Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License 3.ComparisionofSegmentationTechniquesfor Mammographic Images Tere are many works that follow segmentation techniques of masses in mammographic images. Table 2 is highlightingthe key-points and overview and advantages and majordrawbacksofvariousworks.Tekeyobjectiveistopointouttheadvantagesanddisadvantagesofthevariousapproaches. 4.Proposed Methodology Image segmentation refers to the techniqu",
    "full_text_length": 55315,
    "chunk_length": 1601
  },
  {
    "chunk_id": 3363,
    "paper_filename": "sudan_2021_ensemble_learning_based_hybrid_segmentation_of_mammography_images_for_breast_cancer_risk_prediction.pdf",
    "paper_title": "Sudan 2021 Ensemble Learning Based Hybrid Segmentation Of Mammography Images For Breast Cancer Risk Prediction",
    "chunk_index": 29,
    "total_chunks": 45,
    "text_content": "images thatare having noisy and low contrast. We have proposed al-gorithm could be applied on the mammographic imagesmore efectively in such condition. Te proposed method developed to conduct the seg- mentation of mammograms is detailed in the fowchartshown in Figures 1 and 2.Te algorithm of the implemented work, is as below (Algorithm 1): 5.Experiment and Results Te mean based region growing segmentation (MRGS)method [44] is presented which has the improvement overordinary region growing (RG) m",
    "full_text_length": 55315,
    "chunk_length": 1570
  },
  {
    "chunk_id": 3364,
    "paper_filename": "sudan_2021_ensemble_learning_based_hybrid_segmentation_of_mammography_images_for_breast_cancer_risk_prediction.pdf",
    "paper_title": "Sudan 2021 Ensemble Learning Based Hybrid Segmentation Of Mammography Images For Breast Cancer Risk Prediction",
    "chunk_index": 30,
    "total_chunks": 45,
    "text_content": "0 25 5075 100125150175200 05 0 100 150 200Benign 0 25 5075 100125150175200 0 50 100 150 200Benign 0 25 50 75 100 125150175200 05 0 100 150 200Benign Figure7: Image of the MIAS database undergoing classifcation for fnding out the benign and malignant images. Figure8:Malignantimages,aswellasbenignimagesofallimagesetsoftheMIASdatabaseundergoingclassifcationtofndthebenignand malignant image.14 JournalofHealthcareEngineering 7158, 2023, 1, Downloaded from https://onlinelibrary.wiley.com/doi/10.1155/2",
    "full_text_length": 55315,
    "chunk_length": 1543
  },
  {
    "chunk_id": 3365,
    "paper_filename": "sudan_2021_ensemble_learning_based_hybrid_segmentation_of_mammography_images_for_breast_cancer_risk_prediction.pdf",
    "paper_title": "Sudan 2021 Ensemble Learning Based Hybrid Segmentation Of Mammography Images For Breast Cancer Risk Prediction",
    "chunk_index": 31,
    "total_chunks": 45,
    "text_content": "0.1083 0.9057 mdb003.pgm 0.9193 212 0.0807 0.9970 0.3636 0.9210 0.7113 0.9230 0.9193 0.0030 0.2887 0.0770 0.1449 0.2220 0.0777 0.6343 mdb004.pgm 0.8771 217 0.1229 0.9998 0.4006 0.9270 0.9826 0.9265 0.8771 0.0002 0.0174 0.0735 0.2184 0.3339 0.1227 0.9091 mdb005.pgm 0.7749 216 0.2251 0.9996 0.4808 0.9354 0.9822 0.9345 0.7749 0.0004 0.0178 0.0655 0.3662 0.4539 0.2247 0.9166 mdb006.pgm 0.7525 216 0.2475 0.9996 0.4930 0.9372 0.9837 0.9362 0.7525 0.0004 0.0163 0.0638 0.3954 0.4768 0.2471 0.9200 mdb007",
    "full_text_length": 55315,
    "chunk_length": 1411
  },
  {
    "chunk_id": 3366,
    "paper_filename": "sudan_2021_ensemble_learning_based_hybrid_segmentation_of_mammography_images_for_breast_cancer_risk_prediction.pdf",
    "paper_title": "Sudan 2021 Ensemble Learning Based Hybrid Segmentation Of Mammography Images For Breast Cancer Risk Prediction",
    "chunk_index": 32,
    "total_chunks": 45,
    "text_content": "0.0713 0.2635 0.3741 0.1518 0.9219 mdb012.pgm 0.8531 216 0.1469 0.9996 0.4142 0.9289 0.9729 0.9283 0.8531 0.0004 0.0271 0.0717 0.2553 0.3634 0.1466 0.9012 mdb013.pgm 0.8569 211 0.1431 0.9958 0.4111 0.9251 0.7557 0.9228 0.8569 0.0042 0.2443 0.0722 0.2406 0.3081 0.1389 0.6835 mdb014.pgm 0.8517 216 0.1483 0.9996 0.4179 0.9290 0.9732 0.9284 0.8517 0.0004 0.0268 0.0716 0.2574 0.3653 0.1480 0.9016 mdb015.pgm 0.8825 214 0.1175 0.9987 0.3879 0.9256 0.8929 0.9260 0.8825 0.0013 0.1071 0.0740 0.2076 0.3084",
    "full_text_length": 55315,
    "chunk_length": 1552
  },
  {
    "chunk_id": 3367,
    "paper_filename": "sudan_2021_ensemble_learning_based_hybrid_segmentation_of_mammography_images_for_breast_cancer_risk_prediction.pdf",
    "paper_title": "Sudan 2021 Ensemble Learning Based Hybrid Segmentation Of Mammography Images For Breast Cancer Risk Prediction",
    "chunk_index": 33,
    "total_chunks": 45,
    "text_content": "input to the developed method. Figure 5 shows the third image of the MIAS database \u201cmdb003.pgm\u201d been given as an input to the developed method. Figure 6 shows the fourth image of the MIAS database \u201cmdb004.pgm\u201d been given as an input to the developedmethod. TeimagesinFigures7and8areclassifedimagesoutof thepixels,combinedusingPYPLOTandbypassingtheinputdata through 3 layered CNN models with alternated maxpool layers to combine the pixels of similar density. Tey used ReLu activation function in the ",
    "full_text_length": 55315,
    "chunk_length": 1575
  },
  {
    "chunk_id": 3368,
    "paper_filename": "sudan_2021_ensemble_learning_based_hybrid_segmentation_of_mammography_images_for_breast_cancer_risk_prediction.pdf",
    "paper_title": "Sudan 2021 Ensemble Learning Based Hybrid Segmentation Of Mammography Images For Breast Cancer Risk Prediction",
    "chunk_index": 34,
    "total_chunks": 45,
    "text_content": "performing the seg-mentation of mammograms. More than ffteen images of theMIASdatabasearetestedtoassuretheworthoftheconductedresearchwork.Teundertakenresearchworkprovedthatthecombined approaches provide improved segmentation ac-curacy. Accuracy related to segmentation has a vital role incategorizing cancer as benign or malignant. Te adoptedpreprocessing methods assist in procuring enhanced seg- mentation outcomes. In future work, images from diferent databasesareusedtoperformsegmentation,andthen",
    "full_text_length": 55315,
    "chunk_length": 1783
  },
  {
    "chunk_id": 3369,
    "paper_filename": "sudan_2021_ensemble_learning_based_hybrid_segmentation_of_mammography_images_for_breast_cancer_risk_prediction.pdf",
    "paper_title": "Sudan 2021 Ensemble Learning Based Hybrid Segmentation Of Mammography Images For Breast Cancer Risk Prediction",
    "chunk_index": 35,
    "total_chunks": 45,
    "text_content": "re-duces the need for breast removal and also the need ofchemotherapy, saving the lives at earlier stage.Data Availability Te data used to support the fndings of this study areavailable from the corresponding author upon reasonable request. Conflicts of Interest Te authors declare that there are no conficts of interest. Acknowledgments Tis study is supported via funding from Prince Sattam Bin AbdulazizUniversityprojectnumber(PSAU/2023/R/1444). References [1] R.L.Siegel,K.D.Miller,andA.Jemal,\u201cCan",
    "full_text_length": 55315,
    "chunk_length": 1533
  },
  {
    "chunk_id": 3370,
    "paper_filename": "sudan_2021_ensemble_learning_based_hybrid_segmentation_of_mammography_images_for_breast_cancer_risk_prediction.pdf",
    "paper_title": "Sudan 2021 Ensemble Learning Based Hybrid Segmentation Of Mammography Images For Breast Cancer Risk Prediction",
    "chunk_index": 36,
    "total_chunks": 45,
    "text_content": "of the 2019 9th International Con- ference on Cloud Computing, Data Science & Engineering(Confuence), pp. 658\u2013663, Noida, India, January 2019. [5] A.U.Haq,J.P.Li,A.Sabooretal.,\u201cDetectionofbreastcancer through clinical data using supervised and unsupervised feature selection techniques,\u201d IEEE Access, vol. 9, Article ID 22090, 2021. [6] U. Bick, M. L. Giger, R. A. Schmidt, R. M. Nishikawa, D. E. Wolverton, and K. Doi, \u201cAutomated segmentation ofdigitized mammograms,\u201d Academic Radiology, vol. 2, no.",
    "full_text_length": 55315,
    "chunk_length": 1513
  },
  {
    "chunk_id": 3371,
    "paper_filename": "sudan_2021_ensemble_learning_based_hybrid_segmentation_of_mammography_images_for_breast_cancer_risk_prediction.pdf",
    "paper_title": "Sudan 2021 Ensemble Learning Based Hybrid Segmentation Of Mammography Images For Breast Cancer Risk Prediction",
    "chunk_index": 37,
    "total_chunks": 45,
    "text_content": "no. 4, pp. 269\u2013276, 2010. [10] V.Hariraj,K.Wan,Vikneswaran,andI.Zunaidi,\u201cAnefcient data mining approaches for breast cancer detection andsegmentation in a mammogram,\u201d J. Adv. Res. Dyn. Control Syst.vol. 9, no. 3, pp. 185\u2013194, 2017. [11] H.J.ValaandA.Baxi,\u201cAreviewonOtsuimagesegmentation algorithm,\u201d International Journal of Advanced Research in Computer Engineering & Technology (IJARCET), vol. 2, no. 2, pp. 387\u2013389, 2013. [12] B. L. Agbley, J. Li, A. U. Haq et al., \u201cMultimodal melanoma detection w",
    "full_text_length": 55315,
    "chunk_length": 1598
  },
  {
    "chunk_id": 3372,
    "paper_filename": "sudan_2021_ensemble_learning_based_hybrid_segmentation_of_mammography_images_for_breast_cancer_risk_prediction.pdf",
    "paper_title": "Sudan 2021 Ensemble Learning Based Hybrid Segmentation Of Mammography Images For Breast Cancer Risk Prediction",
    "chunk_index": 38,
    "total_chunks": 45,
    "text_content": "on Computer Science and Engineering, vol. 6, no. 12, pp. 292\u2013297, 2018. [14] B. Gupta and M. Tiwari, \u201cA tool supported approach for brightness preserving contrast enhancement and mass seg- mentation of mammogram images using histogram modifedgreyrelationalanalysis,\u201d MultidimensionalSystemsandSignal Processing, vol. 28, no. 4, pp. 1549\u20131567, 2017. [15] S. A. Taghanaki, Y. Liu, B. Miles, and G. Hamarneh, \u201cGeometry-based pectoral muscle segmentation from MLOmammogram views,\u201d IEEE Transactions on Bi",
    "full_text_length": 55315,
    "chunk_length": 1363
  },
  {
    "chunk_id": 3373,
    "paper_filename": "sudan_2021_ensemble_learning_based_hybrid_segmentation_of_mammography_images_for_breast_cancer_risk_prediction.pdf",
    "paper_title": "Sudan 2021 Ensemble Learning Based Hybrid Segmentation Of Mammography Images For Breast Cancer Risk Prediction",
    "chunk_index": 39,
    "total_chunks": 45,
    "text_content": "and L. B. Mahanta, \u201cA novel region growing based method to remove pectoral muscle from MLO mam- mogram images,\u201d Lecture Notes in Electrical Engineering, vol. 443, pp. 307\u2013316, 2018. [19] N. Alam, A. Oliver, E. R. E. Denton, and R. Zwiggelaar, \u201cAutomatic segmentation of microcalcifcation clusters,\u201d Communications in Computer and Information Science,vol. 894, pp. 251\u2013261, 2018. [20] J. Anitha and J. D. Peter, \u201cMass segmentation in mammo- grams using a kernel-based fuzzy level set method,\u201d In- tern",
    "full_text_length": 55315,
    "chunk_length": 1540
  },
  {
    "chunk_id": 3374,
    "paper_filename": "sudan_2021_ensemble_learning_based_hybrid_segmentation_of_mammography_images_for_breast_cancer_risk_prediction.pdf",
    "paper_title": "Sudan 2021 Ensemble Learning Based Hybrid Segmentation Of Mammography Images For Breast Cancer Risk Prediction",
    "chunk_index": 40,
    "total_chunks": 45,
    "text_content": "an artifcial neural network(ANN),\u201d International Journal of Distributed Artifcial In- telligence (IJDAI), vol. 11, no. 1, pp. 34\u201343, 2019. [24] J.V.Tembhurne,A.Hazarika,andT.Diwan,\u201cBrC-MCDLM: breast Cancer detection using Multi-Channel deep learning model,\u201dMultimediaToolsandApplications,vol.80,no.21-23, Article ID 31647, 2021. [25] M. Malathi, P. Sinthia, F. Farzana, and G. Aloy Anuja Mary, \u201cBreast cancer detection using active contour and classif- cation by deep belief network,\u201d Materials Today",
    "full_text_length": 55315,
    "chunk_length": 1651
  },
  {
    "chunk_id": 3375,
    "paper_filename": "sudan_2021_ensemble_learning_based_hybrid_segmentation_of_mammography_images_for_breast_cancer_risk_prediction.pdf",
    "paper_title": "Sudan 2021 Ensemble Learning Based Hybrid Segmentation Of Mammography Images For Breast Cancer Risk Prediction",
    "chunk_index": 41,
    "total_chunks": 45,
    "text_content": "vol. 74, no. 1, pp. 2179\u20132194, 2023. [29] E. Michael, H. Ma, H. Li, F. Kulwa, and J. Li, \u201cBreast cancer segmentation methods: current status and future potentials,\u201d BioMedResearchInternational,vol.2021,ArticleID9962109,29 pages, 2021. [30] E. Shivhare and V. N. Saxena, \u201cBreast cancer diagnosis from mammographicimagesusingoptimizedfeatureselectionand neural network architecture,\u201d International Journal of Im- aging Systems and Technology, vol. 31, no. 1, pp. 253\u2013269, 2021. [31] T.S.Lim,K.G.Tay,A. ",
    "full_text_length": 55315,
    "chunk_length": 1640
  },
  {
    "chunk_id": 3376,
    "paper_filename": "sudan_2021_ensemble_learning_based_hybrid_segmentation_of_mammography_images_for_breast_cancer_risk_prediction.pdf",
    "paper_title": "Sudan 2021 Ensemble Learning Based Hybrid Segmentation Of Mammography Images For Breast Cancer Risk Prediction",
    "chunk_index": 42,
    "total_chunks": 45,
    "text_content": "F. Fnaiech, \u201cAutomatic seg- mentationofclusteredbreastcancercellsusingwatershedandconcave vertex graph,\u201d in Proceedings of the 2011 In- ternational Conference on Communications, Computing andControl Applications (CCCA), pp. 1\u20136, Hammamet, Tunisia,March 2011. [35] J.-P. Villemin, C. Lorenzi, M. S. Cabrillac, A. Oldfeld, W. Ritchie, and R. F. Luco, \u201cA cell-to-patient machinelearning transfer approach uncovers novel basal-like breast cancer prognostic markers amongst alternative splice vari- ants,\u201d",
    "full_text_length": 55315,
    "chunk_length": 1593
  },
  {
    "chunk_id": 3377,
    "paper_filename": "sudan_2021_ensemble_learning_based_hybrid_segmentation_of_mammography_images_for_breast_cancer_risk_prediction.pdf",
    "paper_title": "Sudan 2021 Ensemble Learning Based Hybrid Segmentation Of Mammography Images For Breast Cancer Risk Prediction",
    "chunk_index": 43,
    "total_chunks": 45,
    "text_content": "Engineering, vol. 41, 2021. [39] J. Huaping, Z. Junlong, and A. M. Norouzzadeh Gil Molk, \u201cSkin cancer detection using kernel fuzzy C-means and im- proved neural network optimization algorithm,\u201d Computa- tional Intelligence and Neuroscience, vol. 2021, Article ID9651957, 12 pages, 2021. [40] S. Safdar, M. Rizwan, T. R. Gadekallu et al., \u201cBio-imaging- based machine learning algorithm for breast cancer de-tection,\u201dDiagnostics, vol. 12, no. 5, p. 1134, 2022. [41] A.Ashok,D.Vijayan,andR.Lavanya,\u201cComp",
    "full_text_length": 55315,
    "chunk_length": 1588
  },
  {
    "chunk_id": 3378,
    "paper_filename": "sudan_2021_ensemble_learning_based_hybrid_segmentation_of_mammography_images_for_breast_cancer_risk_prediction.pdf",
    "paper_title": "Sudan 2021 Ensemble Learning Based Hybrid Segmentation Of Mammography Images For Breast Cancer Risk Prediction",
    "chunk_index": 44,
    "total_chunks": 45,
    "text_content": "\u201cIssues of clinical identity verifcation for healthcare applications over mobile terminal platform,\u201d Wireless Communications and Mobile Computing, vol. 2022, Article ID 6245397, 10 pages, 2022. [43] J. Srivastava, S. Routray, S. Ahmad, and M. M. Waris, \u201cIn- ternet of medical things (IoMT)-based smart healthcare system: trends and progress,\u201d Computational Intelligence and Neuroscience, vol. 2022, Article ID 7218113, 17 pages, 2022. [44] S. Ahmad, S. Khan, M. Fahad AlAjmi et al., \u201cDeep learning en",
    "full_text_length": 55315,
    "chunk_length": 1010
  },
  {
    "chunk_id": 3379,
    "paper_filename": "sughoon_2024_multimodal_deep_learning_models_in_breast_cancer.pdf",
    "paper_title": "Sughoon 2024 Multimodal Deep Learning Models In Breast Cancer",
    "chunk_index": 0,
    "total_chunks": 43,
    "text_content": "1 Vol.:(0123456789) Scientific Reports | (2021) 11:18800 | https://doi.org/10.1038/s41598-021-98408-8 www.nature.com/scientificreportsMultimodal deep learning models for the prediction of pathologic response to neoadjuvant chemotherapy in breast cancer Sunghoon Joo1,4,5, Eun Sook Ko2,5, Soonhwan Kwon1, Eunjoo Jeon1, Hyungsik Jung1, Ji\u2011Yeon Kim3, Myung Jin Chung & Young\u2011Hyuck Im2,3* The achievement of the pathologic complete response (pCR) has been considered a metric for the success of neoadjuva",
    "full_text_length": 42810,
    "chunk_length": 1421
  },
  {
    "chunk_id": 3380,
    "paper_filename": "sughoon_2024_multimodal_deep_learning_models_in_breast_cancer.pdf",
    "paper_title": "Sughoon 2024 Multimodal Deep Learning Models In Breast Cancer",
    "chunk_index": 1,
    "total_chunks": 43,
    "text_content": "deep learning model trained on all datasets as clinical information, T1\u2011weighted subtraction images, and T2\u2011weighted images shows better performance with area under the curve (AUC) of 0.888 as compared to the model using only clinical information (AUC = 0.827, P < 0.05). Our results demonstrate that the multimodal fusion approach using deep learning with both clinical information and MR images achieve higher prediction performance compared to the deep learning model without the fusion approach. ",
    "full_text_length": 42810,
    "chunk_length": 1385
  },
  {
    "chunk_id": 3381,
    "paper_filename": "sughoon_2024_multimodal_deep_learning_models_in_breast_cancer.pdf",
    "paper_title": "Sughoon 2024 Multimodal Deep Learning Models In Breast Cancer",
    "chunk_index": 2,
    "total_chunks": 43,
    "text_content": "considered a metric for the success of NAC and a powerful surrogate indicator of the risk of recurrence and long-term survival5,6. Recently, many studies have been conducted to find clinical and pathologic features to predict pCR prior to the NAC7\u20139. As a monitoring tool, breast magnetic resonance (MR) imaging has been shown to be most effec - tive in predicting response to NAC10. Recently, several studies using radiomics methodology in which features were extracted in the computational method f",
    "full_text_length": 42810,
    "chunk_length": 1348
  },
  {
    "chunk_id": 3382,
    "paper_filename": "sughoon_2024_multimodal_deep_learning_models_in_breast_cancer.pdf",
    "paper_title": "Sughoon 2024 Multimodal Deep Learning Models In Breast Cancer",
    "chunk_index": 3,
    "total_chunks": 43,
    "text_content": "representation with minimal human intervention14. In medical image analysis, convolutional neural network (CNN) has shown high performance in classification and segmentation tasks15\u201318. From breast MR images, several attempts with deep learning methods have been conducted to predict pCR to NAC in patients with breast cancer19\u201321. Recent studies have reported that the per - formance of the deep learning model is improved when the fusion approach that integrates features from MR images which have ",
    "full_text_length": 42810,
    "chunk_length": 1516
  },
  {
    "chunk_id": 3383,
    "paper_filename": "sughoon_2024_multimodal_deep_learning_models_in_breast_cancer.pdf",
    "paper_title": "Sughoon 2024 Multimodal Deep Learning Models In Breast Cancer",
    "chunk_index": 4,
    "total_chunks": 43,
    "text_content": "Joo and Eun Sook Ko. *email: imyh00@skku.edu 2 Vol:.(1234567890) Scientific Reports | (2021) 11:18800 | https://doi.org/10.1038/s41598-021-98408-8 www.nature.com/scientificreports/in diagnosing breast cancer15,22\u201324. However, previous studies were conducted with a relatively small number of patients or limited clinical information or using just a few select images not covering the entire tumor. This provokes the need of an in-depth research of the application of deep learning to fuse MR images w",
    "full_text_length": 42810,
    "chunk_length": 1330
  },
  {
    "chunk_id": 3384,
    "paper_filename": "sughoon_2024_multimodal_deep_learning_models_in_breast_cancer.pdf",
    "paper_title": "Sughoon 2024 Multimodal Deep Learning Models In Breast Cancer",
    "chunk_index": 5,
    "total_chunks": 43,
    "text_content": "in this study. Overall, there were 133 (24.8%) and 403 (75.2%) patients with and without pCR, respectively. More than half of the patients were ER negative (n = 299, 55.8%), PR negative (n = 342, 63.8%), and HER2 negative (n = 359, 67.0%). Most patients were treated with AC-T regimen (n = 367, 68.5%). Model performance for predicting pCR. To determine the training and validation sets, the entire patient group was randomly divided into the training set (n = 429) and validation set (n = 107). Ther",
    "full_text_length": 42810,
    "chunk_length": 1180
  },
  {
    "chunk_id": 3385,
    "paper_filename": "sughoon_2024_multimodal_deep_learning_models_in_breast_cancer.pdf",
    "paper_title": "Sughoon 2024 Multimodal Deep Learning Models In Breast Cancer",
    "chunk_index": 6,
    "total_chunks": 43,
    "text_content": "= 403 patients) P value Age (y) Mean (\u00b1 standard deviation) 45.22 (\u00b1 9.91) 46.13 (\u00b1 10.13) 44.92 (\u00b1 9.82) 0.222 CA 15\u20133 (U/mL) 0.024 Mean (\u00b1 standard deviation) 17.52 (\u00b1 23.59) 13.53 (\u00b1 15.41) 18.84 (\u00b1 25.61) ER (n, %)* Positive 266 (49.63) 51 (19.17) 215 (80.83) 0.002 Negative 270 (50.37) 82 (30.37) 188 (69.63) PR (n, %)* Positive 209 (38.99) 31 (14.83) 178 (85.17) < 0.001 Negative 327 (61.01) 102 (31.19) 225 (68.81) HER2 (n, %) Positive 177 (33.02) 65 (36.72) 112 (63.28) < 0.001 Negative 359 (",
    "full_text_length": 42810,
    "chunk_length": 1104
  },
  {
    "chunk_id": 3386,
    "paper_filename": "sughoon_2024_multimodal_deep_learning_models_in_breast_cancer.pdf",
    "paper_title": "Sughoon 2024 Multimodal Deep Learning Models In Breast Cancer",
    "chunk_index": 7,
    "total_chunks": 43,
    "text_content": "at diagnosis (n, %) cT1 26 (4.85) 12 (46.15) 14 (53.85) < 0.001cT2 288 (53.73) 86 (29.86) 202 (70.14) cT3 178 (33.21) 25 (14.04) 153 (85.96) cT4 44 (8.21) 10 (22.73) 34 (77.27) Clinical N-stage at diagnosis (n, %) cN0 42 (7.84) 12 (28.57) 30 (71.43) 0.032cN1 108 (20.15) 38 (35.19) 70 (64.81) cN2 230 (42.91) 50 (21.74) 180 (78.26) cN3 156 (29.10) 33 (21.15) 123 (78.85) NAC regimen (n, %) AC-T 367 (68.47) 75 (20.44) 292 (79.56) < 0.001ACTH 131 (24.44) 51 (38.93) 80 (61.07) AC-T & Platinum 15 (2.80",
    "full_text_length": 42810,
    "chunk_length": 1265
  },
  {
    "chunk_id": 3387,
    "paper_filename": "sughoon_2024_multimodal_deep_learning_models_in_breast_cancer.pdf",
    "paper_title": "Sughoon 2024 Multimodal Deep Learning Models In Breast Cancer",
    "chunk_index": 8,
    "total_chunks": 43,
    "text_content": "in the validation set (area under the curve [AUC] = 0.888). The clinical information-trained deep learning model achieved a validation AUC of 0.827. Comparison of AUC showed a significant difference between the two models (P < 0.05). We investigated the relative importance of each clini- cal information as input on the performance of the deep learning model. Supplementary Figure S1 shows the importance of each clinical information to the deep learning model. The type of NAC regimen and Ki-67 lev",
    "full_text_length": 42810,
    "chunk_length": 1272
  },
  {
    "chunk_id": 3388,
    "paper_filename": "sughoon_2024_multimodal_deep_learning_models_in_breast_cancer.pdf",
    "paper_title": "Sughoon 2024 Multimodal Deep Learning Models In Breast Cancer",
    "chunk_index": 9,
    "total_chunks": 43,
    "text_content": "type of data used for training. Confusion matrices include information about actual and predicted clas- sifications for models (Supplementary Figure S2). Figure 1B shows that ROC curves for the deep learning models in the validation set according to different scenarios. The deep learning model trained on T1W subtraction images and that on T2W images achieved an AUC of 0.725 and 0.663, respectively. When using cropped T1W subtraction images for the lesion as the dataset, the performance reduced w",
    "full_text_length": 42810,
    "chunk_length": 1319
  },
  {
    "chunk_id": 3389,
    "paper_filename": "sughoon_2024_multimodal_deep_learning_models_in_breast_cancer.pdf",
    "paper_title": "Sughoon 2024 Multimodal Deep Learning Models In Breast Cancer",
    "chunk_index": 10,
    "total_chunks": 43,
    "text_content": "difference (p < 0.05) compared to Clinical information. # Statistically significant difference (P < 0.05) compared to T1W subtraction image.Input modality AUC (S.E.) Accuracy Sensitivity Specificity PPV NPV Clinical information 0.827 (0.027) 0.785 0.848 0.757 0.609 0.918 T1W subtraction image 0.725 (0.031) 0.718 0.314 0.907 0.601 0.748 T2W image 0.663 (0.033) 0.709 0.457 0.824 0.537 0.773 Cropped T1W subtraction image for lesion (56 \u00d7 56 \u00d7 12) 0.624# (0.033) 0.700 0.429 0.813 0.506 0.762 T1W sub",
    "full_text_length": 42810,
    "chunk_length": 1306
  },
  {
    "chunk_id": 3390,
    "paper_filename": "sughoon_2024_multimodal_deep_learning_models_in_breast_cancer.pdf",
    "paper_title": "Sughoon 2024 Multimodal Deep Learning Models In Breast Cancer",
    "chunk_index": 11,
    "total_chunks": 43,
    "text_content": "trained on clinical information and MR images or only clinical information pCR classifiers. T1 + T2 + C: subtracted-T1W images, T2W images, and clinical information. T1 + C: T1W subtraction images and clinical information. C: clinical information. (B ) ROC curves for prediction of pretreatment pCR based on different deep learning models trained on the dataset in the combinations of MR images. T1 + T2: T1W subtraction images and T2W images. T1: T1W subtraction images. T2: T2W images. T1 (lesion):",
    "full_text_length": 42810,
    "chunk_length": 1355
  },
  {
    "chunk_id": 3391,
    "paper_filename": "sughoon_2024_multimodal_deep_learning_models_in_breast_cancer.pdf",
    "paper_title": "Sughoon 2024 Multimodal Deep Learning Models In Breast Cancer",
    "chunk_index": 12,
    "total_chunks": 43,
    "text_content": "than using only clinical information or MR images. Our experimental results show that the multimodal fusion approach that combined clinical information, T1W subtraction images, and T2W images is the best method for pretreatment pCR prediction (AUC = 0.888). Considering that the performance improvement in the deep learning model as increase in number of dataset, it is relevant to previous findings that multiparametric MR images can reflect various information about the tumor25. Although our resul",
    "full_text_length": 42810,
    "chunk_length": 1335
  },
  {
    "chunk_id": 3392,
    "paper_filename": "sughoon_2024_multimodal_deep_learning_models_in_breast_cancer.pdf",
    "paper_title": "Sughoon 2024 Multimodal Deep Learning Models In Breast Cancer",
    "chunk_index": 13,
    "total_chunks": 43,
    "text_content": "the axilla and chest wall to include information outside the lesion. To the best of our knowledge, this is the first application of the 3D-CNN model to extract the features from the 3D-bilateral whole MR images. Further, we confirmed that the entire image-trained model had better performance than cropped MR image-trained model adopted in previous studies. It may be due to the inclusion of invisible to human eyes or missed abnormal findings in the axilla or other organs identified in the entire i",
    "full_text_length": 42810,
    "chunk_length": 1238
  },
  {
    "chunk_id": 3393,
    "paper_filename": "sughoon_2024_multimodal_deep_learning_models_in_breast_cancer.pdf",
    "paper_title": "Sughoon 2024 Multimodal Deep Learning Models In Breast Cancer",
    "chunk_index": 14,
    "total_chunks": 43,
    "text_content": "of the pCR prediction model with breast MR images. Regarding the impact of sample size on the accuracy and reliability, Cho et al. studied the scale of the training dataset to achieve high performance of the medical image deep learning system26. They trained CNN to classify axial Computed Tomog - raphy (CT) images into six anatomical classes (brain, neck, shoulder, chest, abdomen, pelvis). They present the trend that increasing the training dataset size increases the performance of the deep lear",
    "full_text_length": 42810,
    "chunk_length": 1289
  },
  {
    "chunk_id": 3394,
    "paper_filename": "sughoon_2024_multimodal_deep_learning_models_in_breast_cancer.pdf",
    "paper_title": "Sughoon 2024 Multimodal Deep Learning Models In Breast Cancer",
    "chunk_index": 15,
    "total_chunks": 43,
    "text_content": "MR image to improve pCR prediction performance, it is difficult to understand how out- put features were extracted from deep learning model effects to classify pCR. This has been frequently included as a problem when applying deep learning methods to clinical settings27. Recently, there are several attention techniques using the backward pass or response of a feedforward propagation on the deep learning algorithm, such as Grad-CAM and attention branch network28,29, to provide explanations for th",
    "full_text_length": 42810,
    "chunk_length": 1291
  },
  {
    "chunk_id": 3395,
    "paper_filename": "sughoon_2024_multimodal_deep_learning_models_in_breast_cancer.pdf",
    "paper_title": "Sughoon 2024 Multimodal Deep Learning Models In Breast Cancer",
    "chunk_index": 16,
    "total_chunks": 43,
    "text_content": "we have MR images from only two types of MR scanners purchased from a single vendor (Philips); therefore, it might be difficult to generalize our proposed deep learning models. Further studies validating our model using MR images from multiple institutions are needed. Third, more study will need to improve the generalizability of our model for clinical implications. Even though the prediction performance of an MRI-clinical information fused model is best in our experiments, it is difficult to ex",
    "full_text_length": 42810,
    "chunk_length": 1336
  },
  {
    "chunk_id": 3396,
    "paper_filename": "sughoon_2024_multimodal_deep_learning_models_in_breast_cancer.pdf",
    "paper_title": "Sughoon 2024 Multimodal Deep Learning Models In Breast Cancer",
    "chunk_index": 17,
    "total_chunks": 43,
    "text_content": "the sensitivity was higher in the clinical information-only model and the specificity was higher in all the models trained with MR images (Table 2). The clinical information-only model is supposed to predict pCR relatively consistently by learning standardized clinical information such as subtype or receptor status. However, in the model that was trained with MR images, it seems that the sensitivity decreases due to the increase of false nega- tives (interpreted as non-pCR but actually pCR) beca",
    "full_text_length": 42810,
    "chunk_length": 1288
  },
  {
    "chunk_id": 3397,
    "paper_filename": "sughoon_2024_multimodal_deep_learning_models_in_breast_cancer.pdf",
    "paper_title": "Sughoon 2024 Multimodal Deep Learning Models In Breast Cancer",
    "chunk_index": 18,
    "total_chunks": 43,
    "text_content": "functional images such as a diffusion MR images are added to the training dataset30. Further study is needed. We selected T2W and T1W subtraction images as MR sequences for this study because they are most com - monly used by radiologists to diagnose breast cancer and reflect tumor biology. If we additionally apply contrast- enhanced images from multiple post-contrast time points that were not used in this study, we can use kinetic information as leverage for predicting pCR31. Moreover, addition",
    "full_text_length": 42810,
    "chunk_length": 1346
  },
  {
    "chunk_id": 3398,
    "paper_filename": "sughoon_2024_multimodal_deep_learning_models_in_breast_cancer.pdf",
    "paper_title": "Sughoon 2024 Multimodal Deep Learning Models In Breast Cancer",
    "chunk_index": 19,
    "total_chunks": 43,
    "text_content": "in our study, sensitivity of MR image-based model was lower than clinical variable-only model. We anticipate that this could be improved when 5 Vol.:(0123456789) Scientific Reports | (2021) 11:18800 | https://doi.org/10.1038/s41598-021-98408-8 www.nature.com/scientificreports/functional images such as a DWI are added to the training dataset for further study although strict standardiza- tion for obtaining consistent ADC value would be necessary. After further studies with larger numbers of patie",
    "full_text_length": 42810,
    "chunk_length": 1343
  },
  {
    "chunk_id": 3399,
    "paper_filename": "sughoon_2024_multimodal_deep_learning_models_in_breast_cancer.pdf",
    "paper_title": "Sughoon 2024 Multimodal Deep Learning Models In Breast Cancer",
    "chunk_index": 20,
    "total_chunks": 43,
    "text_content": "survival5,6. Through the pCR prediction result of the deep learning model, the clinician may change treatment plans for breast cancer during NAC to improve survival of patients. In summary, we conducted a study on the performance improvement of predictive model of pCR to NAC using both clinical information and pretreatment breast MR images by a multimodal fusion approach of the deep learning method. Based on the results presented in this study, we believe that a multimodal fusion approach for pC",
    "full_text_length": 42810,
    "chunk_length": 1285
  },
  {
    "chunk_id": 3400,
    "paper_filename": "sughoon_2024_multimodal_deep_learning_models_in_breast_cancer.pdf",
    "paper_title": "Sughoon 2024 Multimodal Deep Learning Models In Breast Cancer",
    "chunk_index": 21,
    "total_chunks": 43,
    "text_content": "Review Board of ethics committee from Samsung medical center (Seoul, Korea). The requirement for informed consent waived (IRB No. 2019-04-021). We reviewed patients with breast cancer who were treated with NAC at SMC between January 2010 and August 2018. The inclusion criteria for this study were as follows: (1) NAC with no prior therapy; (2) unilateral biopsy-proven primary breast cancer; (3) surgery after completion of NAC at our institution; and (4) breast MR images within 1 month prior to in",
    "full_text_length": 42810,
    "chunk_length": 1273
  },
  {
    "chunk_id": 3401,
    "paper_filename": "sughoon_2024_multimodal_deep_learning_models_in_breast_cancer.pdf",
    "paper_title": "Sughoon 2024 Multimodal Deep Learning Models In Breast Cancer",
    "chunk_index": 22,
    "total_chunks": 43,
    "text_content": "followed by docetaxel (AC-T), AC-T plus platinum, and AC-T with trastuzumab. We treated breast cancer patients with standard NAC regimen as below: (1) adriamycin (60 mg/m2) with cyclophosphamide (600 mg/m2) iv every 3 weeks for six cycles; (2) adriamycin (60 mg/m2) with cyclophosphamide (600 mg/m2) iv every 3 weeks for four cycles followed by taxane iv every 3 weeks for another four cycles; or 3) docetaxel (75 mg/m2), and carboplatin (area under the curve 5.5). In HER2-positive breast cancer, ad",
    "full_text_length": 42810,
    "chunk_length": 1276
  },
  {
    "chunk_id": 3402,
    "paper_filename": "sughoon_2024_multimodal_deep_learning_models_in_breast_cancer.pdf",
    "paper_title": "Sughoon 2024 Multimodal Deep Learning Models In Breast Cancer",
    "chunk_index": 23,
    "total_chunks": 43,
    "text_content": "of NAC. MR image acquisition and preprocessing. The breast MR examination consisted of turbo spin-echo T1W and T2W sequences and 3D dynamic contrast-enhanced (DCE) sequence for each patient. T2W and contrast-enhanced T1W subtraction MR images were retrieved from the picture archiving communication sys- tem and loaded onto a workstation for further analysis. Subtraction images from contrast-enhanced images at 90 s after contrast injection to pre-enhanced images were selected in this study. The de",
    "full_text_length": 42810,
    "chunk_length": 1286
  },
  {
    "chunk_id": 3403,
    "paper_filename": "sughoon_2024_multimodal_deep_learning_models_in_breast_cancer.pdf",
    "paper_title": "Sughoon 2024 Multimodal Deep Learning Models In Breast Cancer",
    "chunk_index": 24,
    "total_chunks": 43,
    "text_content": "of view, 32 \u00d7 32 cm. DCE-MRI was performed with axial imaging with one pre-contrast and six post-contrast dynamic series. Contrast-enhanced images were acquired at 0.5, 1.5, 2.5, 3.5, 4.5, and 5.5 min after contrast injection. The DCE-MRI scans on a 3.0 T scanner (n = 282) were acquired using the fol- lowing parameters: TR/TE, 5.5/2.8; slice thickness, 3 mm; flip angle, 12\u00b0; matrix size, 500 \u00d7 237; and field of view, 30 \u00d7 30 cm. DCE-MRI was performed with axial imaging, with one pre-contrast and",
    "full_text_length": 42810,
    "chunk_length": 1273
  },
  {
    "chunk_id": 3404,
    "paper_filename": "sughoon_2024_multimodal_deep_learning_models_in_breast_cancer.pdf",
    "paper_title": "Sughoon 2024 Multimodal Deep Learning Models In Breast Cancer",
    "chunk_index": 25,
    "total_chunks": 43,
    "text_content": "mmol/kg bolus of gadobutrol (Gadovist; Bayer HealthCare Pharmaceuticals, Berlin, Germany) was injected, followed by a 20 mL saline flush. We applied MR image preprocessing as follows: (1) interpolating different voxel dimensions of MR images to isotropic spacing (1.2 \u00d7 1.2 \u00d7 1.2 mm3) using nearest neighbor interpolation34; (2) normalizing various intensity of MR images, DCE T1W and T2W , using histogram-matching algorithm35; and (3) converting normalized MR images from 12-bit to 8-bit grayscale ",
    "full_text_length": 42810,
    "chunk_length": 1417
  },
  {
    "chunk_id": 3405,
    "paper_filename": "sughoon_2024_multimodal_deep_learning_models_in_breast_cancer.pdf",
    "paper_title": "Sughoon 2024 Multimodal Deep Learning Models In Breast Cancer",
    "chunk_index": 26,
    "total_chunks": 43,
    "text_content": "experience of reading breast MR images. From the center point measured from the lesion boundary, 3D cropped images were obtained. 6 Vol:.(1234567890) Scientific Reports | (2021) 11:18800 | https://doi.org/10.1038/s41598-021-98408-8 www.nature.com/scientificreports/Clinicopathologic characteristics of patients. Pretreatment core biopsies were used to determine the receptor status of the tumor. pCR was defined as no residual invasive tumor in the breast and ipsilateral axilla (pT0/is N0). Patients",
    "full_text_length": 42810,
    "chunk_length": 1409
  },
  {
    "chunk_id": 3406,
    "paper_filename": "sughoon_2024_multimodal_deep_learning_models_in_breast_cancer.pdf",
    "paper_title": "Sughoon 2024 Multimodal Deep Learning Models In Breast Cancer",
    "chunk_index": 27,
    "total_chunks": 43,
    "text_content": "2 + requiring further evaluation by silver in-situ hybridization. CA 15-3 levels before NAC were reviewed from the medical records. Clinical T and N stages were determined according to TNM staging system by the American Joint Committee on Cancer 7th edition35. For convenience, histologic subtypes of breast cancer were divided into two groups: invasive ductal carcinoma (IDC) and others. For clinical information preprocessing, we normalized numeric features, such as age, BMI, ER score, PR score, T",
    "full_text_length": 42810,
    "chunk_length": 1317
  },
  {
    "chunk_id": 3407,
    "paper_filename": "sughoon_2024_multimodal_deep_learning_models_in_breast_cancer.pdf",
    "paper_title": "Sughoon 2024 Multimodal Deep Learning Models In Breast Cancer",
    "chunk_index": 28,
    "total_chunks": 43,
    "text_content": "layers in a network and tends to benefit from a reduced number of parameters37. It enables the trainable deeper structure and adds nonlinearity to the model for achieving adequate performance in classification. The proposed model consists of three parts: (1) 3D volumetric CNNs for feature extraction from MR images; (2) FC layers for clinical information and fea- ture concatenation; and (3) final FC layer to allow prediction of pCR (Fig. 2). To achieve feature extraction from volumetric MR images",
    "full_text_length": 42810,
    "chunk_length": 1260
  },
  {
    "chunk_id": 3408,
    "paper_filename": "sughoon_2024_multimodal_deep_learning_models_in_breast_cancer.pdf",
    "paper_title": "Sughoon 2024 Multimodal Deep Learning Models In Breast Cancer",
    "chunk_index": 29,
    "total_chunks": 43,
    "text_content": "six nodes with dropout rate of 0.1 (Fig. 2B). To fuse multimodal features, the two 6D vectors from T1W subtraction and T2W images and the 6D vector from clini- cal information were concatenated into an 18D vector. Finally, the concatenated output was passed through the final sigmoid layer, allowing a binary classification output. From the experiment, six dimensions were selected as feature vector size to achieve the best model performance. We elaborate here about the training procedure of our mo",
    "full_text_length": 42810,
    "chunk_length": 1278
  },
  {
    "chunk_id": 3409,
    "paper_filename": "sughoon_2024_multimodal_deep_learning_models_in_breast_cancer.pdf",
    "paper_title": "Sughoon 2024 Multimodal Deep Learning Models In Breast Cancer",
    "chunk_index": 30,
    "total_chunks": 43,
    "text_content": "rotating, and flipping to improve generalization of the model. The random cropping was parameterized to minimize the background in axial views in the crops Figure 2. Deep learning architectures for the multimodal pCR prediction model. (A ) The feature extractors for contrast-enhanced T1W subtraction MR images and T2W MR images were used in two 3D ResNet-50. The MR images for the input were subjected to isotropic transformation and cropped to a 3D form of 224 \u00d7 224 \u00d7 64. (B ) FC layer was used fo",
    "full_text_length": 42810,
    "chunk_length": 1269
  },
  {
    "chunk_id": 3410,
    "paper_filename": "sughoon_2024_multimodal_deep_learning_models_in_breast_cancer.pdf",
    "paper_title": "Sughoon 2024 Multimodal Deep Learning Models In Breast Cancer",
    "chunk_index": 31,
    "total_chunks": 43,
    "text_content": "left and right with probability of 50%. To address the concern of class imbalance issue, focal loss38 was used. We use the PyTorch39 (version 1.1.0) open-source deep learning framework as the main tool for generating the soft- ware implementation of the presented architecture. The training was conducted for 12 h on a DGX Station with a Tesla V100 GPU with the Rectified Adam optimizer40. The model also trained with a learning rate of 0.0354, weight decay of 0.00001, batch size of 8, and early sto",
    "full_text_length": 42810,
    "chunk_length": 1305
  },
  {
    "chunk_id": 3411,
    "paper_filename": "sughoon_2024_multimodal_deep_learning_models_in_breast_cancer.pdf",
    "paper_title": "Sughoon 2024 Multimodal Deep Learning Models In Breast Cancer",
    "chunk_index": 32,
    "total_chunks": 43,
    "text_content": "T2W images to analyze the effects of each dataset. The seven datasets to train each model consisted of the following: (1) dataset containing clinical information; (2) dataset containing T1W subtraction images; (3) dataset containing T2W images; (4) dataset containing cropped image for lesions in T1W subtraction images; (5) combined dataset containing T1W subtraction and T2W images; (6) combined dataset containing T1W subtraction images and clinical information; and (7) combined dataset containin",
    "full_text_length": 42810,
    "chunk_length": 1373
  },
  {
    "chunk_id": 3412,
    "paper_filename": "sughoon_2024_multimodal_deep_learning_models_in_breast_cancer.pdf",
    "paper_title": "Sughoon 2024 Multimodal Deep Learning Models In Breast Cancer",
    "chunk_index": 33,
    "total_chunks": 43,
    "text_content": "M. et al. Recommendations from an international expert panel on the use of neoadjuvant (primary) systemic treatment of operable breast cancer: an update. J. Clin. Oncol. 24, 1940\u20131949 (2006). 3. Wang-Lopez, Q. et al. Can pathologic complete response (pCR) be used as a surrogate marker of survival after neoadjuvant therapy for breast cancer?. Crit. Rev. Oncol. Hematol. 95, 88\u2013104 (2015). 4. Mougalian, S. S. et al. Use of neoadjuvant chemotherapy for patients with stage I to III breast cancer in t",
    "full_text_length": 42810,
    "chunk_length": 1323
  },
  {
    "chunk_id": 3413,
    "paper_filename": "sughoon_2024_multimodal_deep_learning_models_in_breast_cancer.pdf",
    "paper_title": "Sughoon 2024 Multimodal Deep Learning Models In Breast Cancer",
    "chunk_index": 34,
    "total_chunks": 43,
    "text_content": "Shukla, H. S. Predictive markers of response to neoadjuvant chemotherapy in breast cancer. Surg. Oncol. 17, 301\u2013311 (2008). 8. Rouzier, R. et al. Nomograms to predict pathologic complete response and metastasis-free survival after preoperative chemotherapy for breast cancer. J. Clin. Oncol. 23, 8331\u20138339 (2005). 9. Fern\u00e1ndez-S\u00e1nchez, M. et al. Clinical and pathological predictors of the response to neoadjuvant anthracycline chemotherapy in locally advanced breast cancer. Med. Oncol. 23, 171\u2013183 ",
    "full_text_length": 42810,
    "chunk_length": 1390
  },
  {
    "chunk_id": 3414,
    "paper_filename": "sughoon_2024_multimodal_deep_learning_models_in_breast_cancer.pdf",
    "paper_title": "Sughoon 2024 Multimodal Deep Learning Models In Breast Cancer",
    "chunk_index": 35,
    "total_chunks": 43,
    "text_content": "a study using an independent validation set. Breast Cancer Res. Treat. 173, 455\u2013463 (2019). 13. Liu, Z. et al. Radiomics of multiparametric MRI for pretreatment prediction of pathologic complete response to neoadjuvant chemotherapy in breast cancer: a multicenter study. Clin. Cancer Res. 25, 3538\u20133547 (2019). 14. LeCun, Y ., Bengio, Y . & Hinton, G. Deep learning. Nature 521, 436\u2013444 (2015). 15. Xi, I. L. et al. Deep Learning to distinguish benign from malignant renal lesions based on routine MR",
    "full_text_length": 42810,
    "chunk_length": 1339
  },
  {
    "chunk_id": 3415,
    "paper_filename": "sughoon_2024_multimodal_deep_learning_models_in_breast_cancer.pdf",
    "paper_title": "Sughoon 2024 Multimodal Deep Learning Models In Breast Cancer",
    "chunk_index": 36,
    "total_chunks": 43,
    "text_content": "carcinoma. Clin. Cancer Res. 25, 4271\u20134279 (2019). 19. El Adoui, M., Drisis, S. & Benjelloun, M. Predict Breast Tumor Response to Chemotherapy Using a 3D Deep Learning Architecture Applied to DCE-MRI Data. in Bioinformatics and Biomedical Engineering. IWBBIO 2019. Lecture Notes in Computer Science vol. 2 33\u201340 (Springer International Publishing, 2019). 20. Ravichandran, K., Braman, N., Janowczyk, A. & Madabhushi, A. A deep learning classifier for prediction of pathological complete response to n",
    "full_text_length": 42810,
    "chunk_length": 1334
  },
  {
    "chunk_id": 3416,
    "paper_filename": "sughoon_2024_multimodal_deep_learning_models_in_breast_cancer.pdf",
    "paper_title": "Sughoon 2024 Multimodal Deep Learning Models In Breast Cancer",
    "chunk_index": 37,
    "total_chunks": 43,
    "text_content": "MRI based on multimodal convolutional neural networks. Phys. Med. Biol. 62, 6497\u20136514 (2017). 23. Nie, K. et al. Rectal cancer: assessment of neoadjuvant chemoradiation outcome based on radiomics of multiparametric MRI. Clin. Cancer Res. 22, 5256\u20135264 (2016). 24. Nie, D. et al. Multi-channel 3D deep feature learning for survival time prediction of brain tumor patients using multi-modal neuroimages. Sci. Rep. 9, 1103 (2019). 25. Gillies, R. J., Kinahan, P . E. & Hricak, H. Radiomics: images are m",
    "full_text_length": 42810,
    "chunk_length": 1367
  },
  {
    "chunk_id": 3417,
    "paper_filename": "sughoon_2024_multimodal_deep_learning_models_in_breast_cancer.pdf",
    "paper_title": "Sughoon 2024 Multimodal Deep Learning Models In Breast Cancer",
    "chunk_index": 38,
    "total_chunks": 43,
    "text_content": "T., Y amashita, T. & Fujiyoshi, H. Attention Branch Network: Learning of Attention Mechanism for Visual Explanation (2018). 29. Selvaraju, R. R. et al. Grad-CAM: Visual Explanations from Deep Networks via Gradient-Based Localization. in Proceedings of the IEEE International Conference on Computer Vision vols 2017-Octob 618\u2013626 (2017). 30. ElDaly, M. M., Moustafa, A. F. I., Abdel-Meguid, S. M. S., Shokry, A. M. & Abd El Wahab, N. Can MRI diffusion-weighted imaging identify postoperative residual/",
    "full_text_length": 42810,
    "chunk_length": 1293
  },
  {
    "chunk_id": 3418,
    "paper_filename": "sughoon_2024_multimodal_deep_learning_models_in_breast_cancer.pdf",
    "paper_title": "Sughoon 2024 Multimodal Deep Learning Models In Breast Cancer",
    "chunk_index": 39,
    "total_chunks": 43,
    "text_content": "cancer patients. Invest. Radiol. 54, 110\u2013117 (2019). 33. Dorrius, M. D., Jansen-van der Weide, M. C., van Ooijen, P . M. A., Pijnappel, R. M. & Oudkerk, M. Computer-aided detection in breast MRI: a systematic review and meta-analysis. Eur. Radiol. 21, 1600\u20131608 (2011). 34. Manj\u00f3n, J. V . et al. Non-local MRI upsampling. Med. Image Anal. 14, 784\u2013792 (2010). 35. Ny\u00fal, L. G., Udupa, J. K. & Zhang, X. New variants of a method of MRI scale standardization. IEEE Trans. Med. Imaging 19, 143\u2013150 (2000).",
    "full_text_length": 42810,
    "chunk_length": 1201
  },
  {
    "chunk_id": 3419,
    "paper_filename": "sughoon_2024_multimodal_deep_learning_models_in_breast_cancer.pdf",
    "paper_title": "Sughoon 2024 Multimodal Deep Learning Models In Breast Cancer",
    "chunk_index": 40,
    "total_chunks": 43,
    "text_content": "doi:https:// doi. org/ 10. 1109/ CVPR. 2018. 00685. 38. Lin, T. Y ., Goyal, P ., Girshick, R., He, K. & Dollar, P . Focal Loss for Dense Object Detection. in Proceedings of the IEEE International Conference on Computer Vision vols 2017-Octob 2999\u20133007 (2017). 39. Paszke, A. et al. PyTorch: An Imperative Style, High-Performance Deep Learning Library (2019). 40. Liu, L. et al. On the Variance of the Adaptive Learning Rate and Beyond (2019). 41. Lin, J., Camoriano, R. & Rosasco, L. Generalization P",
    "full_text_length": 42810,
    "chunk_length": 1283
  },
  {
    "chunk_id": 3420,
    "paper_filename": "sughoon_2024_multimodal_deep_learning_models_in_breast_cancer.pdf",
    "paper_title": "Sughoon 2024 Multimodal Deep Learning Models In Breast Cancer",
    "chunk_index": 41,
    "total_chunks": 43,
    "text_content": "guidelines for authorship and publishing. Author contributions S.J., E.S.K., E.J., M.J.J., and Y .-H.I.: designed research. E.S.K., J.-Y .K., M.J.J., and Y .-H.I.: contributed in data acquisition. E.S.K. and J.-Y .K.: reviewed medical records. S.J., S.K., E.J., and H.J.: processed the data. S.J. and S.K.: built prediction models. S.J.: conducted the experiments. S.J. and E.S.K.: the main manuscript text and prepared all figures. All authors reviewed the manuscript. Funding This study was support",
    "full_text_length": 42810,
    "chunk_length": 1436
  },
  {
    "chunk_id": 3421,
    "paper_filename": "sughoon_2024_multimodal_deep_learning_models_in_breast_cancer.pdf",
    "paper_title": "Sughoon 2024 Multimodal Deep Learning Models In Breast Cancer",
    "chunk_index": 42,
    "total_chunks": 43,
    "text_content": "This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article\u2019s Creative Commons licence, unless indicated otherwise in a credit ",
    "full_text_length": 42810,
    "chunk_length": 867
  },
  {
    "chunk_id": 3422,
    "paper_filename": "suheer_2024_critical_analysis_of_benchmarks_techniques_and_models_in_medical_visual_question_answering.pdf",
    "paper_title": "Suheer 2024 Critical Analysis Of Benchmarks Techniques And Models In Medical Visual Question Answering",
    "chunk_index": 0,
    "total_chunks": 146,
    "text_content": "Received 14 October 2023, accepted 4 November 2023, date of publication 20 November 2023, date of current version 8 December 2023. Digital Object Identifier 10.1 109/ACCESS.2023.3335216 A Critical Analysis of Benchmarks, Techniques, and Models in Medical Visual Question Answering SUHEER AL-HADHRAMI 1,2, MOHAMED EL BACHIR MENAI 1, SAAD AL-AHMADI 1, AND AHMED ALNAFESSAH 3,4 1Computer Science Department, King Saud University, Riyadh 12371, Saudi Arabia 2Department of Computer Engineering, Hadhramou",
    "full_text_length": 142579,
    "chunk_length": 1437
  },
  {
    "chunk_id": 3423,
    "paper_filename": "suheer_2024_critical_analysis_of_benchmarks_techniques_and_models_in_medical_visual_question_answering.pdf",
    "paper_title": "Suheer 2024 Critical Analysis Of Benchmarks Techniques And Models In Medical Visual Question Answering",
    "chunk_index": 1,
    "total_chunks": 146,
    "text_content": "Deanship of Scientific Research at King Saud University for funding and supporting this Research through the Initiative of Deanship of Scientific Research (DSR) Graduate Students Research Support (GSR). ABSTRACT This paper comprehensively reviews medical VQA models, structures, and datasets, focusing on combining vision and language. Over 75 models and their statistical and SWOT (Strengths, Weaknesses, Opportunities, Threats) analyses were compared and analyzed. The study highlights whether the ",
    "full_text_length": 142579,
    "chunk_length": 1374
  },
  {
    "chunk_id": 3424,
    "paper_filename": "suheer_2024_critical_analysis_of_benchmarks_techniques_and_models_in_medical_visual_question_answering.pdf",
    "paper_title": "Suheer 2024 Critical Analysis Of Benchmarks Techniques And Models In Medical Visual Question Answering",
    "chunk_index": 2,
    "total_chunks": 146,
    "text_content": "the primary approaches in medical VQA, with 18% and 15% usage rates, respectively. The statistical analysis of medical VQA from 2018 to 2023 and individual yearly analyses reveals consistent preferences for LSTM and VGGNet, except in 2018 when ResNet was more commonly used. The SWOT analysis provides insights into the strengths and weaknesses of medical VQA research, highlighting areas for future exploration. These areas include addressing limited dataset sizes, enhancing question diversity, mit",
    "full_text_length": 142579,
    "chunk_length": 1436
  },
  {
    "chunk_id": 3425,
    "paper_filename": "suheer_2024_critical_analysis_of_benchmarks_techniques_and_models_in_medical_visual_question_answering.pdf",
    "paper_title": "Suheer 2024 Critical Analysis Of Benchmarks Techniques And Models In Medical Visual Question Answering",
    "chunk_index": 3,
    "total_chunks": 146,
    "text_content": "natural language. When these questions focus on visual information, the process is called visual question answering (VQA). The VQA about non-medical images is called general VQA, whereas the VQA about The associate editor coordinating the review of this manuscript and approving it for publication was Wenbing Zhao .medical images is called medical VQA. This paper focuses on medical images and related VQA questions. VQA is a multidisciplinary task that involves natural language processing (NLP), c",
    "full_text_length": 142579,
    "chunk_length": 1375
  },
  {
    "chunk_id": 3426,
    "paper_filename": "suheer_2024_critical_analysis_of_benchmarks_techniques_and_models_in_medical_visual_question_answering.pdf",
    "paper_title": "Suheer 2024 Critical Analysis Of Benchmarks Techniques And Models In Medical Visual Question Answering",
    "chunk_index": 4,
    "total_chunks": 146,
    "text_content": "Benchmarks, Techniques, and Models in Medical VQA Recently, given the high significance of deep learning and the use of transfer learning in building vision and NLP models, VQA has become a challenge for artificial intelligence (AI) researchers. VQA helps achieve a visual dialog AI-dream to make a computer as efficient as a human in understanding, analyzing, and answering questions about a visual scene [1]. Providing an explanation for answering selection is essential in VQA [2]. Although signif",
    "full_text_length": 142579,
    "chunk_length": 1233
  },
  {
    "chunk_id": 3427,
    "paper_filename": "suheer_2024_critical_analysis_of_benchmarks_techniques_and_models_in_medical_visual_question_answering.pdf",
    "paper_title": "Suheer 2024 Critical Analysis Of Benchmarks Techniques And Models In Medical Visual Question Answering",
    "chunk_index": 5,
    "total_chunks": 146,
    "text_content": "a question on tumor types according to specific properties, such as size, shape, and texture, requires more than object detection. When asking about a tumor larger than 5 mm, the model must detect all tumors in the image and all sizes of existing tumors, make a comparison, and answer the question. \u2022Many studies do not focus on the deep relationships between the expressed ideas in the text and image contents [5]. Many studies do not show whether the results are based on correct reasoning or coinc",
    "full_text_length": 142579,
    "chunk_length": 1225
  },
  {
    "chunk_id": 3428,
    "paper_filename": "suheer_2024_critical_analysis_of_benchmarks_techniques_and_models_in_medical_visual_question_answering.pdf",
    "paper_title": "Suheer 2024 Critical Analysis Of Benchmarks Techniques And Models In Medical Visual Question Answering",
    "chunk_index": 6,
    "total_chunks": 146,
    "text_content": "there are limitations related to this field. Although recent data has been made available by ImageCLEF-Med yearly and researchers enhanced existing data or generated new data, the information still has limitations and is insufficient in developing a robust and practical model used in the real world [8]. For example, a limitation appears in data size, which needs to be large to handle various questions and answers. Data with insufficient information about the images or patient history limit the r",
    "full_text_length": 142579,
    "chunk_length": 1339
  },
  {
    "chunk_id": 3429,
    "paper_filename": "suheer_2024_critical_analysis_of_benchmarks_techniques_and_models_in_medical_visual_question_answering.pdf",
    "paper_title": "Suheer 2024 Critical Analysis Of Benchmarks Techniques And Models In Medical Visual Question Answering",
    "chunk_index": 7,
    "total_chunks": 146,
    "text_content": "models. Researchers proposed different solutions and multi-models to exceed those borders and enhance the overall performance, which is still considered low.Medical images, such as CT, MRI, Mammogram, and ultrasound, are affected during acquisition and transmission. These noisy images require a robust model that can pass this noisy limitation [11]. Although researchers, such as Nguyen et al. [12]and Zhan et al. [13], proposed models to exceed this problem and significantly enhance performance, t",
    "full_text_length": 142579,
    "chunk_length": 1325
  },
  {
    "chunk_id": 3430,
    "paper_filename": "suheer_2024_critical_analysis_of_benchmarks_techniques_and_models_in_medical_visual_question_answering.pdf",
    "paper_title": "Suheer 2024 Critical Analysis Of Benchmarks Techniques And Models In Medical Visual Question Answering",
    "chunk_index": 8,
    "total_chunks": 146,
    "text_content": "the medical field. The accurate models help doctors and ray specialists acquire more information by asking questions about ambiguous objects in the image. Medical VQA is a new and underexplored AI field. Researchers designed various multi-model VQAs to improve performance. These multi-models require further study to detect their pros and cons and overcome limitations. An in- depth analysis of the most recent models is needed to select a novel model structure that significantly enhances performan",
    "full_text_length": 142579,
    "chunk_length": 1218
  },
  {
    "chunk_id": 3431,
    "paper_filename": "suheer_2024_critical_analysis_of_benchmarks_techniques_and_models_in_medical_visual_question_answering.pdf",
    "paper_title": "Suheer 2024 Critical Analysis Of Benchmarks Techniques And Models In Medical Visual Question Answering",
    "chunk_index": 9,
    "total_chunks": 146,
    "text_content": "not have sections on medical VQA, their pros and cons, or the open challenges in the medical field except Lin et al. [24], Noor Mohammed and Srinivasan [26], and Lin et al. [27]. Lin et al. [24] and Mohamed and Srinivasan [26] propose a VQA survey study in the medical field. Those survey studies survey the methods designed for the 2018, 2019, and 2020 ImageCLEF challenges and two extra models on the VQA-RAD dataset. The Lin et al. [24]study also surveys the public medical VQA datasets except VQA",
    "full_text_length": 142579,
    "chunk_length": 1191
  },
  {
    "chunk_id": 3432,
    "paper_filename": "suheer_2024_critical_analysis_of_benchmarks_techniques_and_models_in_medical_visual_question_answering.pdf",
    "paper_title": "Suheer 2024 Critical Analysis Of Benchmarks Techniques And Models In Medical Visual Question Answering",
    "chunk_index": 10,
    "total_chunks": 146,
    "text_content": "[27], is more comprehensive than the previous two surveys [24], [26] where it covers 44 studies with 47 models. The present survey study is an analytical review with 60 studies in medical VQA. It would be more comprehensive than the previous study. It proposes and compares more than 75 medical VQA models. Regarding a dataset, the existing reviews [24], [26], [27] surveyed eight, five, and eight datasets, respectively, whereas, in the proposed review, 16 datasets are analyzed. Moreover, 136508 VO",
    "full_text_length": 142579,
    "chunk_length": 1280
  },
  {
    "chunk_id": 3433,
    "paper_filename": "suheer_2024_critical_analysis_of_benchmarks_techniques_and_models_in_medical_visual_question_answering.pdf",
    "paper_title": "Suheer 2024 Critical Analysis Of Benchmarks Techniques And Models In Medical Visual Question Answering",
    "chunk_index": 11,
    "total_chunks": 146,
    "text_content": "[22]is done to show which techniques are primarily used in the general field and check whether those influence the researchers in the medical field. Furthermore, the Strengths, Weaknesses, Opportunities, and Threats (SWOT) analysis technique provides a clear view of a subject that helps the researchers understand what has already been done and their weaknesses. They also receive opportunities to consider new research about the threats they may face. This research has utilized SWOT to comprehensi",
    "full_text_length": 142579,
    "chunk_length": 1364
  },
  {
    "chunk_id": 3434,
    "paper_filename": "suheer_2024_critical_analysis_of_benchmarks_techniques_and_models_in_medical_visual_question_answering.pdf",
    "paper_title": "Suheer 2024 Critical Analysis Of Benchmarks Techniques And Models In Medical Visual Question Answering",
    "chunk_index": 12,
    "total_chunks": 146,
    "text_content": "VQA, based on Sharma and Jalal [22], are compared with those in the medical field to check whether general VQA influences the researcher. \u2022We propose the challenges and give recommendations that may help the researcher start new research in the field. The rest of this paper proposes the review assumptions and methodology in the second and third sections, followed by the VQA question types in the fourth section. Surveys of medical benchmark datasets have been proposed in the fifth section, follow",
    "full_text_length": 142579,
    "chunk_length": 1276
  },
  {
    "chunk_id": 3435,
    "paper_filename": "suheer_2024_critical_analysis_of_benchmarks_techniques_and_models_in_medical_visual_question_answering.pdf",
    "paper_title": "Suheer 2024 Critical Analysis Of Benchmarks Techniques And Models In Medical Visual Question Answering",
    "chunk_index": 13,
    "total_chunks": 146,
    "text_content": "on statistics and SWOT analysis for the dataset and methods in the field of med-VQA. These assumptions could consist of: VOLUME 11, 2023 136509 S. Al-Hadhrami et al.: Critical Analysis of Benchmarks, Techniques, and Models in Medical VQA \u2022Representative Dataset: The chosen datasets for the study are presumed to be typical of the broader field of medical VQA and include various medical diseases, image kinds, and question categories. Methodological Consistency: The presumption that methodology use",
    "full_text_length": 142579,
    "chunk_length": 1356
  },
  {
    "chunk_id": 3436,
    "paper_filename": "suheer_2024_critical_analysis_of_benchmarks_techniques_and_models_in_medical_visual_question_answering.pdf",
    "paper_title": "Suheer 2024 Critical Analysis Of Benchmarks Techniques And Models In Medical Visual Question Answering",
    "chunk_index": 14,
    "total_chunks": 146,
    "text_content": "scenarios. \u2022SWOT framework applicability: Assumption that the SWOT analysis framework is a suitable and valuable tool for assessing the advantages and disadvantages of the dataset and the methods used in the med-VQA area. \u2022Validity of Statistical Analysis: The validity of the presented results is predicated on the assumption that the statistical analyses carried out in the examined research were adequately planned, carried out, and interpreted. \u2022Publication Bias: The presumption that the studies",
    "full_text_length": 142579,
    "chunk_length": 1323
  },
  {
    "chunk_id": 3437,
    "paper_filename": "suheer_2024_critical_analysis_of_benchmarks_techniques_and_models_in_medical_visual_question_answering.pdf",
    "paper_title": "Suheer 2024 Critical Analysis Of Benchmarks Techniques And Models In Medical Visual Question Answering",
    "chunk_index": 15,
    "total_chunks": 146,
    "text_content": "inspiration by the VQA in the general field based on Sharma and Jalal [22] that focuses on studies published between 2014 and 2021, and since the field of Visual Question Answering (VQA) in the medical domain emerged in 2018, the most relevant studies published between 2018 and 2021 were included in our survey. In order to stay up to date, we also considered some studies published in 2022 and 2023. Additionally, we included all studies from the imageCLEF challenges conducted between 2018 and 202",
    "full_text_length": 142579,
    "chunk_length": 1254
  },
  {
    "chunk_id": 3438,
    "paper_filename": "suheer_2024_critical_analysis_of_benchmarks_techniques_and_models_in_medical_visual_question_answering.pdf",
    "paper_title": "Suheer 2024 Critical Analysis Of Benchmarks Techniques And Models In Medical Visual Question Answering",
    "chunk_index": 16,
    "total_chunks": 146,
    "text_content": "providing insights into their gen- eration methods, sizes, validation procedures, question types, image types, and limitations. \u2022Since one criterion of the comparison between models is a performance metric, and it is one of the VQA gaps, those metrics are discussed. \u2022The review explores VQA components and techniques to provide researchers with a clear overview of VQA before delving into med-VQA models. \u2022The models are classified into sections and discussed based on their methods. Statistical and",
    "full_text_length": 142579,
    "chunk_length": 1346
  },
  {
    "chunk_id": 3439,
    "paper_filename": "suheer_2024_critical_analysis_of_benchmarks_techniques_and_models_in_medical_visual_question_answering.pdf",
    "paper_title": "Suheer 2024 Critical Analysis Of Benchmarks Techniques And Models In Medical Visual Question Answering",
    "chunk_index": 17,
    "total_chunks": 146,
    "text_content": "the general field. \u2022The SWOT analysis addresses several key questions: What significant research aspects exist in the field that can contribute to significant progress? What are the limitations of existing research? What opportunities do these limitations present for researchers? Lastly, what aspects do researchers need to be aware of? \u2022Finally, the review concludes with a discussion of challenges in the med-VQA field and provides recom- mendations to guide future research. This methodology ensu",
    "full_text_length": 142579,
    "chunk_length": 1323
  },
  {
    "chunk_id": 3440,
    "paper_filename": "suheer_2024_critical_analysis_of_benchmarks_techniques_and_models_in_medical_visual_question_answering.pdf",
    "paper_title": "Suheer 2024 Critical Analysis Of Benchmarks Techniques And Models In Medical Visual Question Answering",
    "chunk_index": 18,
    "total_chunks": 146,
    "text_content": "DATASET GENERATION Generating questions and answer pairs in natural languages based on images is a new process known as visual question generation (VQG) [29]. The primary motivation behind this task is to provide a large-scale dataset to create practical VQA agents [30], [31]. There are three methods for visual question-answer pairs: manual, automatic (VQG), and semi- automatic. Figure 3 shows the dataset generation types. Manual VQA dataset generation is based on specialists creating the questi",
    "full_text_length": 142579,
    "chunk_length": 1324
  },
  {
    "chunk_id": 3441,
    "paper_filename": "suheer_2024_critical_analysis_of_benchmarks_techniques_and_models_in_medical_visual_question_answering.pdf",
    "paper_title": "Suheer 2024 Critical Analysis Of Benchmarks Techniques And Models In Medical Visual Question Answering",
    "chunk_index": 19,
    "total_chunks": 146,
    "text_content": "of VQA dataset generation is semi-automatic, based on automatic generation for question-answer pairs and authenticated by specialists, such as VQA-Med 2019. VQG refers to the automatic visual question-answer pair generation methods based on generating question-answer pairs with no human authentications. RedVisDial [9], Tools [7], BACH [7], and IDiRD [7] datasets are examples of VQG. The VQG datasets have two primary problems: noise and having no sense question-answer pairs [10], [33]. A generate",
    "full_text_length": 142579,
    "chunk_length": 1352
  },
  {
    "chunk_id": 3442,
    "paper_filename": "suheer_2024_critical_analysis_of_benchmarks_techniques_and_models_in_medical_visual_question_answering.pdf",
    "paper_title": "Suheer 2024 Critical Analysis Of Benchmarks Techniques And Models In Medical Visual Question Answering",
    "chunk_index": 20,
    "total_chunks": 146,
    "text_content": "al. proposed a comprehensive state-of-the-art survey for VQG [30]. Table 2shows a comparison between the three dataset generation types in terms of size, authentication, errors, question sense, cost, and trust. Eleven question types and four answer types (\u2018\u2018yes\u2019\u2019 or \u2018\u2018no\u2019\u2019, numbers, categories, and locations) were used. B. EXISTING VQA DATASETS In 2018, the ImageCLEF-Med challenge [3]called on researchers for a medical VQA challenge. They provided the VQA-Med v1 dataset with 2,866 radiology imag",
    "full_text_length": 142579,
    "chunk_length": 1377
  },
  {
    "chunk_id": 3443,
    "paper_filename": "suheer_2024_critical_analysis_of_benchmarks_techniques_and_models_in_medical_visual_question_answering.pdf",
    "paper_title": "Suheer 2024 Critical Analysis Of Benchmarks Techniques And Models In Medical Visual Question Answering",
    "chunk_index": 21,
    "total_chunks": 146,
    "text_content": "bias. In the same year, theVQA-RAD [10] dataset was made available publicly. The VQA-RAD was the first manual dataset included questions 136512 VOLUME 11, 2023 S. Al-Hadhrami et al.: Critical Analysis of Benchmarks, Techniques, and Models in Medical VQA FIGURE 7. PathVQA dataset. FIGURE 8. SLAKE dataset. FIGURE 9. DME dataset. answered by clinicians. It has 315 radiology anatomical medical images; thus, one limitation of this dataset is its small size. In 2019 and 2020, ImageCLEF-Med provided th",
    "full_text_length": 142579,
    "chunk_length": 1378
  },
  {
    "chunk_id": 3444,
    "paper_filename": "suheer_2024_critical_analysis_of_benchmarks_techniques_and_models_in_medical_visual_question_answering.pdf",
    "paper_title": "Suheer 2024 Critical Analysis Of Benchmarks Techniques And Models In Medical Visual Question Answering",
    "chunk_index": 22,
    "total_chunks": 146,
    "text_content": "[7]datasets deal with pathology, chest x-rays, breast cancer histology, surgical tools, and diabetic retinopathy specializations, respectively. The PATHVQA dataset has 4,998 pathology images with 32,799 open-ended questions. One problem of the PATHVQA dataset is that the lack of diversity and robustness in question-answer pairs created from captions using the linguistic rules method [8]. The RadVisDial [9]dataset contains 91,060 x-ray images with 455,300 question-answer pairs. These are split in",
    "full_text_length": 142579,
    "chunk_length": 1420
  },
  {
    "chunk_id": 3445,
    "paper_filename": "suheer_2024_critical_analysis_of_benchmarks_techniques_and_models_in_medical_visual_question_answering.pdf",
    "paper_title": "Suheer 2024 Critical Analysis Of Benchmarks Techniques And Models In Medical Visual Question Answering",
    "chunk_index": 23,
    "total_chunks": 146,
    "text_content": "pairs. The IDRiD, Tools, and BACH datasets contain complex questions. However, these three datasets use dataset annotation as the generation method for QA, which means the possibility of error if the original dataset annotation has an error. Another manual medical VQA dataset is a semantically- labeled knowledge-enhanced (SLAKE) dataset, which is created in 2021 [32]. As VQA-RAD dataset, SLAKE is based on expertise humans for form the question answers pairs, but it is larger than VQA-RAD dataset",
    "full_text_length": 142579,
    "chunk_length": 1337
  },
  {
    "chunk_id": 3446,
    "paper_filename": "suheer_2024_critical_analysis_of_benchmarks_techniques_and_models_in_medical_visual_question_answering.pdf",
    "paper_title": "Suheer 2024 Critical Analysis Of Benchmarks Techniques And Models In Medical Visual Question Answering",
    "chunk_index": 24,
    "total_chunks": 146,
    "text_content": "dataset [28], EndoVis-18- VQA [38], and Cholec80-VQA [38]. OVQA dataset [37] has been created based on hospital FQAs. Physicians verified the template of the questions and answers. OVQA has 19,020 question-answer pairs about abnormality, modality, organ, plane, condition presence, and attribute others. The dataset is split into training, validation, and testing datasets with 2,000, 1,235, and 1,234 images related to 15,216, 1,902, and 1,902 question-answer, respectively. All the questions are ab",
    "full_text_length": 142579,
    "chunk_length": 1333
  },
  {
    "chunk_id": 3447,
    "paper_filename": "suheer_2024_critical_analysis_of_benchmarks_techniques_and_models_in_medical_visual_question_answering.pdf",
    "paper_title": "Suheer 2024 Critical Analysis Of Benchmarks Techniques And Models In Medical Visual Question Answering",
    "chunk_index": 25,
    "total_chunks": 146,
    "text_content": "validation, and testing dataset. The dataset has questions about exudates\u2019 grades. The dataset has specific questions with five answers. The questions have been assigned to a region or a whole image. It is classified as a manually generated dataset. EndoVis-18-VQA dataset was generated by extracting images from the MICCAI Endoscopic Vision Challenge 2018 [40] dataset. Each image has two question types: one with a single-word answer (EndoVis-18-VQA (C)) and another with a sentence answer (EndoVis",
    "full_text_length": 142579,
    "chunk_length": 1340
  },
  {
    "chunk_id": 3448,
    "paper_filename": "suheer_2024_critical_analysis_of_benchmarks_techniques_and_models_in_medical_visual_question_answering.pdf",
    "paper_title": "Suheer 2024 Critical Analysis Of Benchmarks Techniques And Models In Medical Visual Question Answering",
    "chunk_index": 26,
    "total_chunks": 146,
    "text_content": "tool-operation provided in the original dataset [42]. The dataset has two parts; the first is for classifying 14 single words (Cholec80-VQA (C)), and the second one is for sentence answers (Cholec80- VQA (S)). Each part of the dataset has 17,000 images with 34,000 question-answer pairs and 4,500 images with 17,000 question-answer pairs for training and testing datasets. The newest medical dataset is a Patient-oriented Visual Question Answering (P-VQA) [43], which was published in 2023. The datas",
    "full_text_length": 142579,
    "chunk_length": 1400
  },
  {
    "chunk_id": 3449,
    "paper_filename": "suheer_2024_critical_analysis_of_benchmarks_techniques_and_models_in_medical_visual_question_answering.pdf",
    "paper_title": "Suheer 2024 Critical Analysis Of Benchmarks Techniques And Models In Medical Visual Question Answering",
    "chunk_index": 27,
    "total_chunks": 146,
    "text_content": "advice, medicine, treatment, examination items, prognosis, department, examination advice, prevention, pathogenesis, and review time. The dataset splitting is 1,526 images with 17,336 question-answer pairs, 218 images with 2,575 question-answer pairs, and 425 images with 4,889 question- answer pairs for taking, validation, and testing datasets, respectively. Figures 4-9show examples from above datasets. Table 3shows a summary of the described datasets. V. EVALUATION METRICS There are two types o",
    "full_text_length": 142579,
    "chunk_length": 1467
  },
  {
    "chunk_id": 3450,
    "paper_filename": "suheer_2024_critical_analysis_of_benchmarks_techniques_and_models_in_medical_visual_question_answering.pdf",
    "paper_title": "Suheer 2024 Critical Analysis Of Benchmarks Techniques And Models In Medical Visual Question Answering",
    "chunk_index": 28,
    "total_chunks": 146,
    "text_content": "Al-Hadhrami et al.: Critical Analysis of Benchmarks, Techniques, and Models in Medical VQA TABLE 3. (Continued.) Medical VQA benchmarks. Palmer Similarity (WUPS) [44], Word-based Semantic Sim- ilarity (WBSS) [45], [46], BiLingual Evaluation Understudy (BLEU) [47], Concept-based Semantic Similarity (CBSS)[46], mean-per-type (MPT), and Metric for Evaluation of Translation with Explicit ORdering (METEOR) [47].details about each metric are discussed below. 136516 VOLUME 11, 2023 S. Al-Hadhrami et al",
    "full_text_length": 142579,
    "chunk_length": 1400
  },
  {
    "chunk_id": 3451,
    "paper_filename": "suheer_2024_critical_analysis_of_benchmarks_techniques_and_models_in_medical_visual_question_answering.pdf",
    "paper_title": "Suheer 2024 Critical Analysis Of Benchmarks Techniques And Models In Medical Visual Question Answering",
    "chunk_index": 29,
    "total_chunks": 146,
    "text_content": "the ratio of the correct predicted positive answers to the number of all predicted positive answers. Precision =TP TP+FP. (4) \u2022F1-Score: F1\u2212score=2\u2217Precision \u2217Recall Precision +Recall(5) \u2022WUPS: This metric is based on the semantic meaning and how much the actual answer differs from the predicted answer. The decision of which the predicted answer is true or false is controlled using a threshold. WUPS is calculated based on the following equation: WUPS (a,t) =1 NN/summationdisplay i=1min/braceleft",
    "full_text_length": 142579,
    "chunk_length": 1399
  },
  {
    "chunk_id": 3452,
    "paper_filename": "suheer_2024_critical_analysis_of_benchmarks_techniques_and_models_in_medical_visual_question_answering.pdf",
    "paper_title": "Suheer 2024 Critical Analysis Of Benchmarks Techniques And Models In Medical Visual Question Answering",
    "chunk_index": 30,
    "total_chunks": 146,
    "text_content": "\u2192U\u2299\u2190 \u2212U;\u2212 \u2192U\u2212\u2190 \u2212U] \u2212 \u2192Ui=GRU(\u2212 \u2192Ui\u22121,xi) \u2190 \u2212Ui=GRU(\u2190 \u2212Ui+1,xi) xi=[BERT(q); BERT(c i)] (7) where qandcare the question and context. S(q,c) represent the similarity score between qand c.U,\u2212 \u2192U,\u2190 \u2212Uare the input embedded, forward hidden states, and backward hidden states, respectively. xiis theinput embeddings created by concatenating the BERT embeddings of the question qand the i-th context token ci. \u2022BLEU: BLEU metric depends on analyzing the n- grams co-occurrences between the actual answer and",
    "full_text_length": 142579,
    "chunk_length": 1393
  },
  {
    "chunk_id": 3453,
    "paper_filename": "suheer_2024_critical_analysis_of_benchmarks_techniques_and_models_in_medical_visual_question_answering.pdf",
    "paper_title": "Suheer 2024 Critical Analysis Of Benchmarks Techniques And Models In Medical Visual Question Answering",
    "chunk_index": 31,
    "total_chunks": 146,
    "text_content": "distribution of question-type or bias answer distribution for each type of question. It is based on calculating the harmonic or arithmetic mean accuracy for each question type. It is calculated suing the following equation. MPT=T/summationdisplay t=1At/slashbigg T or MPT =T/slashbigg T/summationdisplay t=1A\u22121 t (9) where T and A denoted to the number of question types and Accuracy over question type t, respectively. \u2022METEOR: This metric aims to find the similarity by when align the words in the ",
    "full_text_length": 142579,
    "chunk_length": 1342
  },
  {
    "chunk_id": 3454,
    "paper_filename": "suheer_2024_critical_analysis_of_benchmarks_techniques_and_models_in_medical_visual_question_answering.pdf",
    "paper_title": "Suheer 2024 Critical Analysis Of Benchmarks Techniques And Models In Medical Visual Question Answering",
    "chunk_index": 32,
    "total_chunks": 146,
    "text_content": "field based on our state-of-the-art and Sharma and Jalal [22]VQA survey in the general field. A. IMAGE FEATURIZATION To easily apply mathematical operations to an image, the image is represented as a numerical vector called image featurization. There are various methods to calculate image featurization, such as scale-invariant feature transform (SIFT) [48], simple RGB vector, a histogram of oriented gradients (HOG) [49], and Haar transform [50]. In deep learning systems, such as CNNs, image feat",
    "full_text_length": 142579,
    "chunk_length": 1275
  },
  {
    "chunk_id": 3455,
    "paper_filename": "suheer_2024_critical_analysis_of_benchmarks_techniques_and_models_in_medical_visual_question_answering.pdf",
    "paper_title": "Suheer 2024 Critical Analysis Of Benchmarks Techniques And Models In Medical Visual Question Answering",
    "chunk_index": 33,
    "total_chunks": 146,
    "text_content": "used in VQA. The most frequently used pre-trained model in VQA is ResNet because of the reasonable cost of its computational resources [1]. Ensemble of some deep learning method may be used too [55], [56], [57]. The visual features are extracted in this phase, and the most recent VQA image models are based on pre-trained CNNs, such as ResNet [58], [59], [60], DenseNet-121 [9], and VGGNet [18], [46], [61], [62]. Other methods used for image feature extraction that pay attention to questions in VQ",
    "full_text_length": 142579,
    "chunk_length": 1364
  },
  {
    "chunk_id": 3456,
    "paper_filename": "suheer_2024_critical_analysis_of_benchmarks_techniques_and_models_in_medical_visual_question_answering.pdf",
    "paper_title": "Suheer 2024 Critical Analysis Of Benchmarks Techniques And Models In Medical Visual Question Answering",
    "chunk_index": 34,
    "total_chunks": 146,
    "text_content": "namely count-based methods, prediction-based methods, and hybrid methods. One problem in word-embedding is choosing a suitable method for a given problem, which depends on a trial-and-error approach [1]. Count-based methods count the occurrence of words in the text using one-hot encoding, a co-occurrence matrix [63], and singular value decomposition (SVD) [64]. In prediction- based methods, word representation is learned based on a model. Neural network models [65], continuous bag-of- words (CBO",
    "full_text_length": 142579,
    "chunk_length": 1400
  },
  {
    "chunk_id": 3457,
    "paper_filename": "suheer_2024_critical_analysis_of_benchmarks_techniques_and_models_in_medical_visual_question_answering.pdf",
    "paper_title": "Suheer 2024 Critical Analysis Of Benchmarks Techniques And Models In Medical Visual Question Answering",
    "chunk_index": 35,
    "total_chunks": 146,
    "text_content": "the encoder-decoder method [7],[58], [59], [60], [61], [76]. In addition to the previous methods, pre-trained models have been used, such as Generalized Autoregressive Pretraining for Language Understanding (XLNet) [77] and the BERT model [61], [78]. Some models have ignored text featur- ization and convert the problem into image classification problem [57], [79], [80] C. FUSION METHODS Since both text and image featurization are independently processed, a fusion of the two is required for gener",
    "full_text_length": 142579,
    "chunk_length": 1381
  },
  {
    "chunk_id": 3458,
    "paper_filename": "suheer_2024_critical_analysis_of_benchmarks_techniques_and_models_in_medical_visual_question_answering.pdf",
    "paper_title": "Suheer 2024 Critical Analysis Of Benchmarks Techniques And Models In Medical Visual Question Answering",
    "chunk_index": 36,
    "total_chunks": 146,
    "text_content": "[58], dynamic parameter prediction networks (DPPNs) [86], multimodal residual network (MRNs) [87], cross- modal multistep fusion (CMF) networks [88], basic MCB model with a deep attention neural tensor network (DA- NTN) module [89], multi-layer perceptron (MLP) [90], and encoder-decoder method [91], [92]. The main reason for using the joint attention model is to address the semantic relationship between text attention and question attention [1]. There are various joint attention models, such as ",
    "full_text_length": 142579,
    "chunk_length": 1366
  },
  {
    "chunk_id": 3459,
    "paper_filename": "suheer_2024_critical_analysis_of_benchmarks_techniques_and_models_in_medical_visual_question_answering.pdf",
    "paper_title": "Suheer 2024 Critical Analysis Of Benchmarks Techniques And Models In Medical Visual Question Answering",
    "chunk_index": 37,
    "total_chunks": 146,
    "text_content": "words in the question and connecting them with specific regions or objects in the image. Attention mechanisms can be classified into single-hop and multi-hope attentions, based on the attention layers number [22]. Shih et al. [97]developed an attention-based approach for VQA that has recently emerged as a key element in almost all architectures. Current strands of research cover co-attention architectures for the generation of simultaneous attention in both textual and visual modalities, which h",
    "full_text_length": 142579,
    "chunk_length": 1358
  },
  {
    "chunk_id": 3460,
    "paper_filename": "suheer_2024_critical_analysis_of_benchmarks_techniques_and_models_in_medical_visual_question_answering.pdf",
    "paper_title": "Suheer 2024 Critical Analysis Of Benchmarks Techniques And Models In Medical Visual Question Answering",
    "chunk_index": 38,
    "total_chunks": 146,
    "text_content": "kind can produce a more refined understanding and reason regarding the image- question relationships; as such, VQA performance increases. Despite this, the absence of self-attention within every modality in dense co-attention networks such as BAN and DCN is a bottleneck, such as word-to-word relationships in the question and region-to-region relationships in the image [100]. Yu et al. [100] developed a deep Modular Co-Attention Network (MCAN) to solve this bottleneck, which is composed of variou",
    "full_text_length": 142579,
    "chunk_length": 1361
  },
  {
    "chunk_id": 3461,
    "paper_filename": "suheer_2024_critical_analysis_of_benchmarks_techniques_and_models_in_medical_visual_question_answering.pdf",
    "paper_title": "Suheer 2024 Critical Analysis Of Benchmarks Techniques And Models In Medical Visual Question Answering",
    "chunk_index": 39,
    "total_chunks": 146,
    "text_content": "flexible and expressive, there are problematic aspects to this type of attention; in particular, the result is always a weighted combination of value pairs among which the model is attending. This can create challenges when closely related context is unavailable for the model to attend over, such as when there is a word with no corresponding image region or context word. In cases such as this, attention would lead to excessive noise or, more severely, the distraction of the output vector, which ",
    "full_text_length": 142579,
    "chunk_length": 1292
  },
  {
    "chunk_id": 3462,
    "paper_filename": "suheer_2024_critical_analysis_of_benchmarks_techniques_and_models_in_medical_visual_question_answering.pdf",
    "paper_title": "Suheer 2024 Critical Analysis Of Benchmarks Techniques And Models In Medical Visual Question Answering",
    "chunk_index": 40,
    "total_chunks": 146,
    "text_content": "an attention gate. To generate an information vector, the query context is concatenated with the attention results and a linear transformation is applied. Ben-Younes et al. designed MUTAN [60], which uses multi-modal tensor-based Tucker decomposition to parametrize the interactions of bilinear between question and image features. Minh et al. utilized the operation of an inner product instead of applying low-rank bilinear pooling, designing a full-rank bilinear transformation G-MLB [103] to obtai",
    "full_text_length": 142579,
    "chunk_length": 1386
  },
  {
    "chunk_id": 3463,
    "paper_filename": "suheer_2024_critical_analysis_of_benchmarks_techniques_and_models_in_medical_visual_question_answering.pdf",
    "paper_title": "Suheer 2024 Critical Analysis Of Benchmarks Techniques And Models In Medical Visual Question Answering",
    "chunk_index": 41,
    "total_chunks": 146,
    "text_content": "models: Late Fusion Network (LF) and Recursive Visual Attention Network (RV A). D. ANSWER CLASSIFICATION AND GENERATION This phase is responsible for producing the answer. Most researchers designed classification VQA models due to easiness, whereas others designed answer generation models. Several methods were used, such as a Softmax layer for classification and LSTM or CNN models for a generation. E. VISION-AND-LANGUAGE PRE-TRAINED MODEL ResNet [54],GoogLeNet [53], and VGG [52], among other mod",
    "full_text_length": 142579,
    "chunk_length": 1412
  },
  {
    "chunk_id": 3464,
    "paper_filename": "suheer_2024_critical_analysis_of_benchmarks_techniques_and_models_in_medical_visual_question_answering.pdf",
    "paper_title": "Suheer 2024 Critical Analysis Of Benchmarks Techniques And Models In Medical Visual Question Answering",
    "chunk_index": 42,
    "total_chunks": 146,
    "text_content": "fusion and generating a prediction [33]. Nevertheless, these studies have overlooked the degree to which the pre-trained features are applicable and compatible for cross-model fusion [33]. Driven by the usefulness and value of XLNET [77], BERT [78], and other large-scale pre-trained language models, recent researchers have sought to generate image-text joint embedding from pre-training transformer-based models on V+L datasets [92]. In turn, the joint embedding is fine-tuned with a set of V +L ta",
    "full_text_length": 142579,
    "chunk_length": 1389
  },
  {
    "chunk_id": 3465,
    "paper_filename": "suheer_2024_critical_analysis_of_benchmarks_techniques_and_models_in_medical_visual_question_answering.pdf",
    "paper_title": "Suheer 2024 Critical Analysis Of Benchmarks Techniques And Models In Medical Visual Question Answering",
    "chunk_index": 43,
    "total_chunks": 146,
    "text_content": "[106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145] to learn V+L representations for specific V+L tasks. Nevertheless, almost all prior studies have not attempted to solve the problem of learning these representations through the explicit detangling of multi-modalities and",
    "full_text_length": 142579,
    "chunk_length": 1388
  },
  {
    "chunk_id": 3466,
    "paper_filename": "suheer_2024_critical_analysis_of_benchmarks_techniques_and_models_in_medical_visual_question_answering.pdf",
    "paper_title": "Suheer 2024 Critical Analysis Of Benchmarks Techniques And Models In Medical Visual Question Answering",
    "chunk_index": 44,
    "total_chunks": 146,
    "text_content": "V+L pre-trained models, namely VisualBERT [92], LXMERT [107], PixelBERT [114], UNITER [106], CTL [144], VLMixter [145], BLIP [149], OFA [150], CoCa [151], BEIT-3 [152], PaLI [153], and BLIP-2 [154]. They determined that the pre-trained model using VisualBERT achieved the highest AUC performance at 0.987, whereas the model using PixelBERT earned the lowest score. Table 4shows the performance of the existing pre-trained models that are fine-tuned on the VQA-Med 2019 dataset and which model was uti",
    "full_text_length": 142579,
    "chunk_length": 1327
  },
  {
    "chunk_id": 3467,
    "paper_filename": "suheer_2024_critical_analysis_of_benchmarks_techniques_and_models_in_medical_visual_question_answering.pdf",
    "paper_title": "Suheer 2024 Critical Analysis Of Benchmarks Techniques And Models In Medical Visual Question Answering",
    "chunk_index": 45,
    "total_chunks": 146,
    "text_content": "and models based on external knowledge. A. IMAGECLEF VQA CHALLENGES ImageCLEF calls for challenge yearly, where it started the first call in medical challenge in 2018 [3]. Although 28 groups were registered for this challenge, only five groups sent results in 17 runs [46], [73], [74], [75], [93]. Most groups built their models based on deep learning. RNN, such as BiLSTM and LSTM, were used for text featurization, whereas encoder-decoder-based frameworks, VGG, ResNet, and Inception-ResNet-v2, wer",
    "full_text_length": 142579,
    "chunk_length": 1257
  },
  {
    "chunk_id": 3468,
    "paper_filename": "suheer_2024_critical_analysis_of_benchmarks_techniques_and_models_in_medical_visual_question_answering.pdf",
    "paper_title": "Suheer 2024 Critical Analysis Of Benchmarks Techniques And Models In Medical Visual Question Answering",
    "chunk_index": 46,
    "total_chunks": 146,
    "text_content": "with ETM for vision, LSTM for text, and co-attention with MFH, followed by a convolution layer and ReLu layer for fusion. The models\u2019 performances in the first challenge were considered poor, unlike in the second challenge, where the performance levels improved [31].The best BLEU score was 0.644, whereas the best BLEU performance was 0.162 in 2018. This progress shows how this field encourages researchers to develop more robust medical VQA models. In this challenge, 17 teams[62], [103], [155], [",
    "full_text_length": 142579,
    "chunk_length": 1269
  },
  {
    "chunk_id": 3469,
    "paper_filename": "suheer_2024_critical_analysis_of_benchmarks_techniques_and_models_in_medical_visual_question_answering.pdf",
    "paper_title": "Suheer 2024 Critical Analysis Of Benchmarks Techniques And Models In Medical Visual Question Answering",
    "chunk_index": 47,
    "total_chunks": 146,
    "text_content": "2021 were based on deep learning [35], [165]. In the ImageCLEF 2020 challenge, 30, 11 [35], [61], [166], [167], [168], [169], [170], [171], and 62 teams for team registration, teams submitted, and runs, respectively. This challenge featured CNN, such as VGGNet and ResNet, trans- formers, such as RNN and BERT, multi-modal factorized bilinear (MFB) pooling, and multi-modal factorized high- order pooling (MFH) for vision, text, and fusion, respectively. Liao et al. [55] were the 2020 winners. They ",
    "full_text_length": 142579,
    "chunk_length": 1278
  },
  {
    "chunk_id": 3470,
    "paper_filename": "suheer_2024_critical_analysis_of_benchmarks_techniques_and_models_in_medical_visual_question_answering.pdf",
    "paper_title": "Suheer 2024 Critical Analysis Of Benchmarks Techniques And Models In Medical Visual Question Answering",
    "chunk_index": 48,
    "total_chunks": 146,
    "text_content": "BLEU performances, respectively. In ImageCLEF 2021, there were 48, 13 [57], [79], [80], [165], [172], [173], [174], [175], [176], and 68 registered teams, submitted teams, and runs, respectively, in the VQA task [165]. The vision part of most VQA multi-models is based on CNN models, such as ResNet, VGG, and 136520 VOLUME 11, 2023 S. Al-Hadhrami et al.: Critical Analysis of Benchmarks, Techniques, and Models in Medical VQA TABLE 5. ImageClef challenges information. TABLE 6. The models\u2019 comparison",
    "full_text_length": 142579,
    "chunk_length": 1287
  },
  {
    "chunk_id": 3471,
    "paper_filename": "suheer_2024_critical_analysis_of_benchmarks_techniques_and_models_in_medical_visual_question_answering.pdf",
    "paper_title": "Suheer 2024 Critical Analysis Of Benchmarks Techniques And Models In Medical Visual Question Answering",
    "chunk_index": 49,
    "total_chunks": 146,
    "text_content": "ResNeSt-50- HAGAP, ResNet-50-HAGAP, VGG-19-HAGAP, and VGG- 16-HAGAP. They augmented the data using a mixup strategy during the training. Table 6summarizes the ImageCLEF challenges from 2018 to 2021. Al-Hadhrami et al.2is a fused of multiple of Al-Hadhrami et al.1 models based on the greedy soup technique.B. CNN-LSTM BASED MODELS Most models fall into the CNN-LSTM-based methods. These models aim to solve problems in the field, such as data limitation, required answer types, and text and vision re",
    "full_text_length": 142579,
    "chunk_length": 1267
  },
  {
    "chunk_id": 3472,
    "paper_filename": "suheer_2024_critical_analysis_of_benchmarks_techniques_and_models_in_medical_visual_question_answering.pdf",
    "paper_title": "Suheer 2024 Critical Analysis Of Benchmarks Techniques And Models In Medical Visual Question Answering",
    "chunk_index": 50,
    "total_chunks": 146,
    "text_content": "end-user. Their model was based on using the Glove and VOLUME 11, 2023 136521 S. Al-Hadhrami et al.: Critical Analysis of Benchmarks, Techniques, and Models in Medical VQA TABLE 8. Models\u2019 Com parison on the Med-VQA 2019. TABLE 9. Models\u2019 comparison on the Med-VQA 2020. TABLE 10. Models\u2019 comparison on the Med-VQA 2021. BiLSTM technique for text featurization and the Inception- Resnet-v2 pre-trained model for vision featurization. Theyconcatenated those features using the concatenation layer foll",
    "full_text_length": 142579,
    "chunk_length": 1318
  },
  {
    "chunk_id": 3473,
    "paper_filename": "suheer_2024_critical_analysis_of_benchmarks_techniques_and_models_in_medical_visual_question_answering.pdf",
    "paper_title": "Suheer 2024 Critical Analysis Of Benchmarks Techniques And Models In Medical Visual Question Answering",
    "chunk_index": 51,
    "total_chunks": 146,
    "text_content": "model without segregation on both datasets and combined those two datasets. The model achieved 0.411, 0.132, 0.257 for BLEU on RAD, VQA-med, RAD-VQA+VQA- med datasets, respectively, and 0.437, 0.162, and 0.288 for WBSS on those three datasets, respectively. Their model did not outperform the Zhou et al. [74] model, which differed from their model thanks to the use of the fusion method. Zhou et al. [74] also used attention. Their model sufferedfrom errors that they claimed could be returned to th",
    "full_text_length": 142579,
    "chunk_length": 1243
  },
  {
    "chunk_id": 3474,
    "paper_filename": "suheer_2024_critical_analysis_of_benchmarks_techniques_and_models_in_medical_visual_question_answering.pdf",
    "paper_title": "Suheer 2024 Critical Analysis Of Benchmarks Techniques And Models In Medical Visual Question Answering",
    "chunk_index": 52,
    "total_chunks": 146,
    "text_content": "of a multi-modality dataset. Their model VOLUME 11, 2023 136523 S. Al-Hadhrami et al.: Critical Analysis of Benchmarks, Techniques, and Models in Medical VQA was based on ResNet-31 and a decoder with three MLP layers for the vision phase, LSTM for the text phase, and CMSA for the fusion. They used CMSA to focus on the image encoder for the representation learning instead of the fusion feature. The authors evaluated their proposed model on RAD-VQA and three external datasets: chest X-Ray 2, brain",
    "full_text_length": 142579,
    "chunk_length": 1301
  },
  {
    "chunk_id": 3475,
    "paper_filename": "suheer_2024_critical_analysis_of_benchmarks_techniques_and_models_in_medical_visual_question_answering.pdf",
    "paper_title": "Suheer 2024 Critical Analysis Of Benchmarks Techniques And Models In Medical Visual Question Answering",
    "chunk_index": 53,
    "total_chunks": 146,
    "text_content": "for tasks of complex medical VQA. Their model is based on the Nguyen model [12] as a backbone. They evaluated their reasoning modules on the VQA-RAD dataset and achieved 60%, 79.3%, and 71.6% accuracy for open-ended, close-ended, and overall questions, respectively. C. IMAGE-CLASSIFICATION-BASED MODELS Although the visual questions are modality questions, some authors prefer to convert the problem to an image classification problem and use the answers as labels. This method faces a generalized p",
    "full_text_length": 142579,
    "chunk_length": 1299
  },
  {
    "chunk_id": 3476,
    "paper_filename": "suheer_2024_critical_analysis_of_benchmarks_techniques_and_models_in_medical_visual_question_answering.pdf",
    "paper_title": "Suheer 2024 Critical Analysis Of Benchmarks Techniques And Models In Medical Visual Question Answering",
    "chunk_index": 54,
    "total_chunks": 146,
    "text_content": "structures, namely, VGG-16, VGG-19, MobileNet, and CNN, from scratch. Their models\u2019 accuracy achieved 0.838, 0.8344, 0.846, and 0.838 for VGG- 16, VGG-19, MobileNet, and CNN, respectively. D. ENSEMBLE-BASED MODELS Nguyen et al. utilized MAML to solve the data limitation problem and noisy medical images [12]. The model was based on LSTM, MEVF, and attention mechanisms for text, vision, and fusion phases. They also used a Convolutional Denoising Auto-Encoder (CDAE) to reduce the noise. Their model",
    "full_text_length": 142579,
    "chunk_length": 1315
  },
  {
    "chunk_id": 3477,
    "paper_filename": "suheer_2024_critical_analysis_of_benchmarks_techniques_and_models_in_medical_visual_question_answering.pdf",
    "paper_title": "Suheer 2024 Critical Analysis Of Benchmarks Techniques And Models In Medical Visual Question Answering",
    "chunk_index": 55,
    "total_chunks": 146,
    "text_content": "also noisy image labels. To solve these problems, Do et al. designed a Multiple Meta-Model Quantifying (MMQ) method for medical VQA [56].They utilized Model Agonistic Meta-Learning (MAML) to increase meta-data based on auto-annotations. MMQ has three models: meta training for image feature extraction based on MAML; data refinement based on auto-annotation to increase the data and exceed the noisy labels limitation; and meta quantifying that has the decision of meta-model selection for best perfo",
    "full_text_length": 142579,
    "chunk_length": 1349
  },
  {
    "chunk_id": 3478,
    "paper_filename": "suheer_2024_critical_analysis_of_benchmarks_techniques_and_models_in_medical_visual_question_answering.pdf",
    "paper_title": "Suheer 2024 Critical Analysis Of Benchmarks Techniques And Models In Medical Visual Question Answering",
    "chunk_index": 56,
    "total_chunks": 146,
    "text_content": "auto-creation [32]. Since VQA-RAD is a small dataset, Liu et al. created a new medical dataset, SLAKE, that was created by specialists and is more extensive than VQA-RAD [32]. Additionally, external knowledge can enhance the performance and robustness of the VQA models. Therefore, they created a medical knowledge base extracted from Wikipedia\u2019s large-scale knowledge base. They built two models based on LSTM, VGG, and SAN for the text, vision, and fusion phases. The primary difference between tho",
    "full_text_length": 142579,
    "chunk_length": 1254
  },
  {
    "chunk_id": 3479,
    "paper_filename": "suheer_2024_critical_analysis_of_benchmarks_techniques_and_models_in_medical_visual_question_answering.pdf",
    "paper_title": "Suheer 2024 Critical Analysis Of Benchmarks Techniques And Models In Medical Visual Question Answering",
    "chunk_index": 57,
    "total_chunks": 146,
    "text_content": "two ImageCLEF challenges in 2020 and 2021, utilized the ensemble methods [55], [57]. Another team in the ImageCLEF Challenge 2021 achieved third place using the ensemble technique with accuracy and BLEU figures of 0.348 and 0.391, respectively. Eslami et al. [79] model was based on converting the VQA problem to an image classification problem and ignoring the text part. It was also the winner of this challenge [57]. E. MODELS WITH V+L PRE-TRAINED MODELS According to Li et al. [147], V+L pre-trai",
    "full_text_length": 142579,
    "chunk_length": 1315
  },
  {
    "chunk_id": 3480,
    "paper_filename": "suheer_2024_critical_analysis_of_benchmarks_techniques_and_models_in_medical_visual_question_answering.pdf",
    "paper_title": "Suheer 2024 Critical Analysis Of Benchmarks Techniques And Models In Medical Visual Question Answering",
    "chunk_index": 58,
    "total_chunks": 146,
    "text_content": "multi-models with clinicalBERT leading to better multi-model than the original multi-models. However, all new multi-models behaved worse than the original multi- 136524 VOLUME 11, 2023 S. Al-Hadhrami et al.: Critical Analysis of Benchmarks, Techniques, and Models in Medical VQA models. This situation occurred because for various reasons: 1) they trained the models for only 12 epochs, which may not be enough for adjusting the weights; 2) UNITER and PixelBERT have two versions of models: basic and",
    "full_text_length": 142579,
    "chunk_length": 1313
  },
  {
    "chunk_id": 3481,
    "paper_filename": "suheer_2024_critical_analysis_of_benchmarks_techniques_and_models_in_medical_visual_question_answering.pdf",
    "paper_title": "Suheer 2024 Critical Analysis Of Benchmarks Techniques And Models In Medical Visual Question Answering",
    "chunk_index": 59,
    "total_chunks": 146,
    "text_content": "the lowest performance in the MIMIC-CXR dataset, whereas visualBERT achieved the highest performance in the same dataset. We believe that this result happened as the reason for the previous limitations in their experiments and choosing the shallower models instead of deeper ones. F. MODELS BASED ON KNOWLEDGE-BASE Medical VQA needs information about the diseases and patients\u2019 histories. This information is not included in the existing datasets. Therefore, an external knowledge base is needed to e",
    "full_text_length": 142579,
    "chunk_length": 1253
  },
  {
    "chunk_id": 3482,
    "paper_filename": "suheer_2024_critical_analysis_of_benchmarks_techniques_and_models_in_medical_visual_question_answering.pdf",
    "paper_title": "Suheer 2024 Critical Analysis Of Benchmarks Techniques And Models In Medical Visual Question Answering",
    "chunk_index": 60,
    "total_chunks": 146,
    "text_content": "2019 dataset: 91.2%m 93.8%, 95.7%, 95.9%, and 95.8% for BLEU, accuracy, precision, recall, and F-means. Although the score was high, the dataset on which the model trained had a high bias; no clarification was mentioned in relation to this problem or the use of data visualization to check whether the model learned the alignment between text and vision or if the result stemmed from the dataset bias. Kovaleva et al. [9]used the patient history trained model, which utilized LSTM and DenseNet-121 fo",
    "full_text_length": 142579,
    "chunk_length": 1271
  },
  {
    "chunk_id": 3483,
    "paper_filename": "suheer_2024_critical_analysis_of_benchmarks_techniques_and_models_in_medical_visual_question_answering.pdf",
    "paper_title": "Suheer 2024 Critical Analysis Of Benchmarks Techniques And Models In Medical Visual Question Answering",
    "chunk_index": 61,
    "total_chunks": 146,
    "text_content": "task to give summarized information about the image in the medical VQA. This information is embedded and merged with question and image features to enhance the classifi- cation task performance. They utilized ResNet-152, BERT, and Progressive Compact Bilinear Interactions (PCBI)for vision featurization, text featurization, and fusion phases, respectively. The method was validated on RAD-VQA and SLAKE datasets, where achieved 69.8 (+8.7), 79.8 ( \u22120.6),75.8 (+3.1), 80.2 (\u22121.0), 86.1 (+2.7), and 82",
    "full_text_length": 142579,
    "chunk_length": 1358
  },
  {
    "chunk_id": 3484,
    "paper_filename": "suheer_2024_critical_analysis_of_benchmarks_techniques_and_models_in_medical_visual_question_answering.pdf",
    "paper_title": "Suheer 2024 Critical Analysis Of Benchmarks Techniques And Models In Medical Visual Question Answering",
    "chunk_index": 62,
    "total_chunks": 146,
    "text_content": "whereas others have taken advantage of pre-trained models, such as ResNet, VGGNet, and BERT. However, pre-trained models focused on large image or text datasets weaken model generalizability. Li et al. [147] demonstrated that the VQA model based on the V+L pre-trained model outperforms models based on CNN-RNN models. Thus, the overall performance of the existing models needs to be enhanced. While Vu et al. [7]achieved more than 90% macro accuracy, they achieved a recall of less than 10% on BACH ",
    "full_text_length": 142579,
    "chunk_length": 1240
  },
  {
    "chunk_id": 3485,
    "paper_filename": "suheer_2024_critical_analysis_of_benchmarks_techniques_and_models_in_medical_visual_question_answering.pdf",
    "paper_title": "Suheer 2024 Critical Analysis Of Benchmarks Techniques And Models In Medical Visual Question Answering",
    "chunk_index": 63,
    "total_chunks": 146,
    "text_content": "questions whatsoever. Following a state-of-the-art example, we concluded that factors capable of improving such models involve utilizing ensemble learning in the visual aspect, GAP, and using an external knowledge-base, such as in [178]. We conducted two types of analysis: statistical analysis and SWOT analysis. The statistical analysis was designed to help researchers learn which methods are primarily utilized in a medical VQA, as well as which has a significant impact on improving per- formanc",
    "full_text_length": 142579,
    "chunk_length": 1327
  },
  {
    "chunk_id": 3486,
    "paper_filename": "suheer_2024_critical_analysis_of_benchmarks_techniques_and_models_in_medical_visual_question_answering.pdf",
    "paper_title": "Suheer 2024 Critical Analysis Of Benchmarks Techniques And Models In Medical Visual Question Answering",
    "chunk_index": 64,
    "total_chunks": 146,
    "text_content": "inspired researchers in a medical context. A statistical analysis of the medical VQA benchmarks is also presented in this discussion. 1) MEDICAL VQA BENCHMARKS According to the state-of-the-art models presented in Tables 6-14, we detected the frequencies of using each dataset, as shown in Figure 10. Table 15shows the benchmarks, as well as the best performance achieved in each instance. VOLUME 11, 2023 136525 S. Al-Hadhrami et al.: Critical Analysis of Benchmarks, Techniques, and Models in Medic",
    "full_text_length": 142579,
    "chunk_length": 1288
  },
  {
    "chunk_id": 3487,
    "paper_filename": "suheer_2024_critical_analysis_of_benchmarks_techniques_and_models_in_medical_visual_question_answering.pdf",
    "paper_title": "Suheer 2024 Critical Analysis Of Benchmarks Techniques And Models In Medical Visual Question Answering",
    "chunk_index": 65,
    "total_chunks": 146,
    "text_content": "used. Long short-term memory (LSTM) was the text encoding method most frequently used in medical VQA with a rate 136526 VOLUME 11, 2023 S. Al-Hadhrami et al.: Critical Analysis of Benchmarks, Techniques, and Models in Medical VQA TABLE 16. Text and vision techniques distribution. FIGURE 12. Medical VQA techniques - 2018 distribution. FIGURE 13. Medical VQA techniques-2019 distribution. FIGURE 14. Medical VQA techniques-2020 distribution. of 44%, followed by no text encoding and BERT. The VQA is ",
    "full_text_length": 142579,
    "chunk_length": 1285
  },
  {
    "chunk_id": 3488,
    "paper_filename": "suheer_2024_critical_analysis_of_benchmarks_techniques_and_models_in_medical_visual_question_answering.pdf",
    "paper_title": "Suheer 2024 Critical Analysis Of Benchmarks Techniques And Models In Medical Visual Question Answering",
    "chunk_index": 66,
    "total_chunks": 146,
    "text_content": "2022-2023 distribution. Gong et al. [57], who won the ImageCLEF 2021 challenge, or Al-Sadi et al., who achieved second place in the ImageCLEF 2020 challenge, this does not mean it represents a good choice for a practical VQA, especially in a medical context. Achieving a high score in VQA while ignoring one part of the multi-modal denotes the bias problem in the dataset. Putting aside those two methods, Bi-LSTM and GRU were used, achieving rates of 10% and 7%, respectively, whereas SSM and BioBER",
    "full_text_length": 142579,
    "chunk_length": 1200
  },
  {
    "chunk_id": 3489,
    "paper_filename": "suheer_2024_critical_analysis_of_benchmarks_techniques_and_models_in_medical_visual_question_answering.pdf",
    "paper_title": "Suheer 2024 Critical Analysis Of Benchmarks Techniques And Models In Medical Visual Question Answering",
    "chunk_index": 67,
    "total_chunks": 146,
    "text_content": "ResNet, Ensemble, and DenseNet were used in 20, 14, and 5 out of 78 models, respectively. While the researchers described a variety of reasons for their choices regarding the multi-modal parts, the performance of all the proposed models in this review shows that these explanations reveal a failure of understanding in terms of how the data are manipulated inside the model. Consequently, a more successful representation explaining both the model\u2019s behavior and data visualization inside the model i",
    "full_text_length": 142579,
    "chunk_length": 1246
  },
  {
    "chunk_id": 3490,
    "paper_filename": "suheer_2024_critical_analysis_of_benchmarks_techniques_and_models_in_medical_visual_question_answering.pdf",
    "paper_title": "Suheer 2024 Critical Analysis Of Benchmarks Techniques And Models In Medical Visual Question Answering",
    "chunk_index": 68,
    "total_chunks": 146,
    "text_content": "2023, we merge its analysis with the previous year, 2022. We found that VGGNet was the vision pre-trained model primarily used in 2019, 2020, and 2021, with rates of 40%, 35%, and 42%, respectively. By contrast, the ResNet was used in 50% and 67% of the 2018 and 2022-2023 models, while VGGNet was used in 40% in 2018 and not used in 2022-2023. On the other hand, LSTM was the text featurization technique most widely used, with 80%, 39%, 35%, 50%, and 50% of models utilizing the LSTM machine learni",
    "full_text_length": 142579,
    "chunk_length": 1210
  },
  {
    "chunk_id": 3491,
    "paper_filename": "suheer_2024_critical_analysis_of_benchmarks_techniques_and_models_in_medical_visual_question_answering.pdf",
    "paper_title": "Suheer 2024 Critical Analysis Of Benchmarks Techniques And Models In Medical Visual Question Answering",
    "chunk_index": 69,
    "total_chunks": 146,
    "text_content": "part is the most critical phase in the VQA multi-modal. Different techniques were utilized, as shown in Figures 12-16(c). We conducted a statistical analysis for the models used in the ImageCLEF challenges 2018-2021. in 2022, ImageCLEF did not call for VQA challenge. Figure 17shows the statistical vision, text, and fusion techniques used in these instances based on the models published by their teams. The number of published papers are six [46], [46], [73], [74], [75], [93], twelve [62], [103], ",
    "full_text_length": 142579,
    "chunk_length": 1264
  },
  {
    "chunk_id": 3492,
    "paper_filename": "suheer_2024_critical_analysis_of_benchmarks_techniques_and_models_in_medical_visual_question_answering.pdf",
    "paper_title": "Suheer 2024 Critical Analysis Of Benchmarks Techniques And Models In Medical Visual Question Answering",
    "chunk_index": 70,
    "total_chunks": 146,
    "text_content": "FIGURE 18. General VQA 2014-2020 analysis. from Figure 17 that VGG and ResNet pre-trained models were mostly used across all four challenges. Ensemble and BBN-ResNet were used in the two most recent years due totheir positive effect on general VQA and more recent medical VQA. The winning models for those two years were both ensemble models. VOLUME 11, 2023 136529 S. Al-Hadhrami et al.: Critical Analysis of Benchmarks, Techniques, and Models in Medical VQA In terms of the text phase, most partici",
    "full_text_length": 142579,
    "chunk_length": 1269
  },
  {
    "chunk_id": 3493,
    "paper_filename": "suheer_2024_critical_analysis_of_benchmarks_techniques_and_models_in_medical_visual_question_answering.pdf",
    "paper_title": "Suheer 2024 Critical Analysis Of Benchmarks Techniques And Models In Medical Visual Question Answering",
    "chunk_index": 71,
    "total_chunks": 146,
    "text_content": "using an attention mechanism, as shown in Figure 17-(c). In the 2021 challenges, no fusion method was used in most models because converting the problem into images classification or because the authors did not mention it in their papers. Besides VQA analysis in the medical field, we also conducted an analysis of VQA in the general field for vision and text featurization, based on the most comprehensive survey by Sahani et al. (2021) [23]. Sahani et al. [23]reviewed VQA in the general domain fro",
    "full_text_length": 142579,
    "chunk_length": 1278
  },
  {
    "chunk_id": 3494,
    "paper_filename": "suheer_2024_critical_analysis_of_benchmarks_techniques_and_models_in_medical_visual_question_answering.pdf",
    "paper_title": "Suheer 2024 Critical Analysis Of Benchmarks Techniques And Models In Medical Visual Question Answering",
    "chunk_index": 72,
    "total_chunks": 146,
    "text_content": "[228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250]. Figure 20 shows the distributions of the textual and visual featurization methods in the general field between 2014-2020. We also compared the text and vision featurization in general if the multi-modal structure followed Sahani et al. [23]to show whether the methods utilized in the general field influenced the researchers in medical VQA",
    "full_text_length": 142579,
    "chunk_length": 1231
  },
  {
    "chunk_id": 3495,
    "paper_filename": "suheer_2024_critical_analysis_of_benchmarks_techniques_and_models_in_medical_visual_question_answering.pdf",
    "paper_title": "Suheer 2024 Critical Analysis Of Benchmarks Techniques And Models In Medical Visual Question Answering",
    "chunk_index": 73,
    "total_chunks": 146,
    "text_content": "rarely used in medical VQA, with a rate of 6% from the proposed methods included in this review. On the other hand, general VQA models used VGGNet as the vision featurization method from 2014-2017, but from 2018-2020, the general VQA reduced using VGGNet by approximately 75%, increasing the use of ResNet by approximately 66.7% compared to previously. This analysis shows that researchers in the medical field did not follow the general development of VQA. B. SWOT ANALYSIS This section presents a S",
    "full_text_length": 142579,
    "chunk_length": 1256
  },
  {
    "chunk_id": 3496,
    "paper_filename": "suheer_2024_critical_analysis_of_benchmarks_techniques_and_models_in_medical_visual_question_answering.pdf",
    "paper_title": "Suheer 2024 Critical Analysis Of Benchmarks Techniques And Models In Medical Visual Question Answering",
    "chunk_index": 74,
    "total_chunks": 146,
    "text_content": "that cannot answer complex problems. In medical VQA, while recent data is made available annually by ImageCLEF-Med, allowing researchers to either enhance existing data or generate new data, this data remains insufficient in terms of developing a robust and practical model used in the real world [8]. The limitation comes down to the size of the detail, which needs to be sufficiently large to handle various questions. Furthermore, limited data with insufficient information regarding the images or",
    "full_text_length": 142579,
    "chunk_length": 1327
  },
  {
    "chunk_id": 3497,
    "paper_filename": "suheer_2024_critical_analysis_of_benchmarks_techniques_and_models_in_medical_visual_question_answering.pdf",
    "paper_title": "Suheer 2024 Critical Analysis Of Benchmarks Techniques And Models In Medical Visual Question Answering",
    "chunk_index": 75,
    "total_chunks": 146,
    "text_content": "borders and enhance overall performance, these efforts remain ineffective. Table 17shows a summary of the medical VQA dataset SWOT analysis. 2) MEDICAL VQA SWOT ANALYSIS Medical VQA is a new field that requires comprehensive analysis in order to achieve a practical VQA-agent that can be trusted by medical staff. Although it represents a new area of research, existing studies have made significant progress, as demonstrated by enhanced performance over the last four years. Various techniques have ",
    "full_text_length": 142579,
    "chunk_length": 1286
  },
  {
    "chunk_id": 3498,
    "paper_filename": "suheer_2024_critical_analysis_of_benchmarks_techniques_and_models_in_medical_visual_question_answering.pdf",
    "paper_title": "Suheer 2024 Critical Analysis Of Benchmarks Techniques And Models In Medical Visual Question Answering",
    "chunk_index": 76,
    "total_chunks": 146,
    "text_content": "ongoing concern. No V+L pre-trained model has been designed specifically for the medical field, opening opportunities for researchers to find solutions. The metrics used for VQA are ineffective in terms of open questions, so developing a new metric especially for those question types, is in demand. One limitation related to VQA in the medical field concerns having a large, manually validated dataset, which is a key requirement for creating a trusted medical VQA-agent. Table 18and Table 19show th",
    "full_text_length": 142579,
    "chunk_length": 1295
  },
  {
    "chunk_id": 3499,
    "paper_filename": "suheer_2024_critical_analysis_of_benchmarks_techniques_and_models_in_medical_visual_question_answering.pdf",
    "paper_title": "Suheer 2024 Critical Analysis Of Benchmarks Techniques And Models In Medical Visual Question Answering",
    "chunk_index": 77,
    "total_chunks": 146,
    "text_content": "on VQA V+L pre-trained models. in the medical field and provide recommendations to future researchers. 1) LIMITED DATASETS SIZE This limitation requires researchers to expand the dataset, either by using other existing datasets or creating a new dataset. Automatic dataset generation helps in creating a vast dataset, but this method has drawbacks. The relatedlimitations are a leak of specialists who validate the new or expanding dataset, original dataset errors, and bias. Another solution regardi",
    "full_text_length": 142579,
    "chunk_length": 1331
  },
  {
    "chunk_id": 3500,
    "paper_filename": "suheer_2024_critical_analysis_of_benchmarks_techniques_and_models_in_medical_visual_question_answering.pdf",
    "paper_title": "Suheer 2024 Critical Analysis Of Benchmarks Techniques And Models In Medical Visual Question Answering",
    "chunk_index": 78,
    "total_chunks": 146,
    "text_content": "These challenges mean researchers must spend considerable efforts on creating solutions to the medical dataset generation problem. Transfer learning is one potential solution that may help in solving limited dataset size dilemmas. According to state-of-the-art examples, only one study has utilized a V+L pre-trained model. We rec- ommend using V+L pre-trained models because they are pre-trained on vast V+L datasets and the text is aligned with images. 2) QUESTION DIVERSITY VQA-RAD [10] and SLAKE ",
    "full_text_length": 142579,
    "chunk_length": 1356
  },
  {
    "chunk_id": 3501,
    "paper_filename": "suheer_2024_critical_analysis_of_benchmarks_techniques_and_models_in_medical_visual_question_answering.pdf",
    "paper_title": "Suheer 2024 Critical Analysis Of Benchmarks Techniques And Models In Medical Visual Question Answering",
    "chunk_index": 79,
    "total_chunks": 146,
    "text_content": "than one question using logical conductive rules increases the dataset diversity. The new dataset, which has more questions diversity than those in the original dataset, needs to be validated by experts. Besides, combining more than one dataset with different question types will increase the diversity. 3) UNIMODALITY BIAS PROBLEM This limitation denotes the ability to avoid one modality of the multi-modality with significant performance, such as Lubna et al. [186], who achieved 84.6% on VQA-Med ",
    "full_text_length": 142579,
    "chunk_length": 1288
  },
  {
    "chunk_id": 3502,
    "paper_filename": "suheer_2024_critical_analysis_of_benchmarks_techniques_and_models_in_medical_visual_question_answering.pdf",
    "paper_title": "Suheer 2024 Critical Analysis Of Benchmarks Techniques And Models In Medical Visual Question Answering",
    "chunk_index": 80,
    "total_chunks": 146,
    "text_content": "shown to reduce the dataset bias [57], while a variety of methods have been proposed in the general VQA field to aid bias reduction [251], [252], [253], [254], [255]. Yuan\u2019s survey has proposed language bias in VQA [256]. 4) MULTI-MODALITY DATASET Most existing medical VQA datasets are multi-modality datasets containing different image formats, i.e., MRI, X- ray, and CT. This multi-modality increases learning difficulty. Splitting the dataset into several single modality datasets and training ea",
    "full_text_length": 142579,
    "chunk_length": 1305
  },
  {
    "chunk_id": 3503,
    "paper_filename": "suheer_2024_critical_analysis_of_benchmarks_techniques_and_models_in_medical_visual_question_answering.pdf",
    "paper_title": "Suheer 2024 Critical Analysis Of Benchmarks Techniques And Models In Medical Visual Question Answering",
    "chunk_index": 81,
    "total_chunks": 146,
    "text_content": "be better to use external knowledge to improve the learning capabilities of future models. Zhou et al. [109] and Liu et al. [32] have built models based on the external knowledge-base. We recommend that the researchers use external knowledge and multiple resources to make the model more practical. 6) MULTIPLE IMAGE One necessary procedure used in medicine is to follow up on patients\u2019 progress by periodically checking their radiology images. As no model has been developed thus far for this, we re",
    "full_text_length": 142579,
    "chunk_length": 1251
  },
  {
    "chunk_id": 3504,
    "paper_filename": "suheer_2024_critical_analysis_of_benchmarks_techniques_and_models_in_medical_visual_question_answering.pdf",
    "paper_title": "Suheer 2024 Critical Analysis Of Benchmarks Techniques And Models In Medical Visual Question Answering",
    "chunk_index": 82,
    "total_chunks": 146,
    "text_content": "the training and testing sets consist of 473 and 121 classes and 221 and 33 classes, respectively. This variance of the class number between testing and training sets can not give a trust model performance comparisons because one model may predict the classes are not in the testing set better than those in the testing set, and it is considered poor performance compared to another model that can not detect classes that are not in the testing set. On the other hand, by increasing the examples in t",
    "full_text_length": 142579,
    "chunk_length": 1255
  },
  {
    "chunk_id": 3505,
    "paper_filename": "suheer_2024_critical_analysis_of_benchmarks_techniques_and_models_in_medical_visual_question_answering.pdf",
    "paper_title": "Suheer 2024 Critical Analysis Of Benchmarks Techniques And Models In Medical Visual Question Answering",
    "chunk_index": 83,
    "total_chunks": 146,
    "text_content": "in their studies, and patients in interpreting their radiology images without the need of specialists. These aims cannot be realized until a robust model with a meager error rate and high generalization has been developed. Researchers must consider all previous comments and threats, as mentioned in Table 9-11. 9) MODEL INTERPRETATION Model interpretation represents a major challenge for researchers. One obstacle is that there is no explicit model behavior, nor any explanation regarding the reaso",
    "full_text_length": 142579,
    "chunk_length": 1258
  },
  {
    "chunk_id": 3506,
    "paper_filename": "suheer_2024_critical_analysis_of_benchmarks_techniques_and_models_in_medical_visual_question_answering.pdf",
    "paper_title": "Suheer 2024 Critical Analysis Of Benchmarks Techniques And Models In Medical Visual Question Answering",
    "chunk_index": 84,
    "total_chunks": 146,
    "text_content": "al. [7] have drawn attention to focus areas that can help to select or generate answers. Although this approach was correct in some samples, the result was not, and the researchers could not explain the reasons why. 10) EVALUATION Accuracy is mostly used in VQA. Even though it fails in terms of BLEU when used with short sentences, as is the case in VQA, it is usually used as a performance metric regarding this problem. Manmadhan and Kovoor suggest using Ngram EV Aluation (NEV A), which was propo",
    "full_text_length": 142579,
    "chunk_length": 1223
  },
  {
    "chunk_id": 3507,
    "paper_filename": "suheer_2024_critical_analysis_of_benchmarks_techniques_and_models_in_medical_visual_question_answering.pdf",
    "paper_title": "Suheer 2024 Critical Analysis Of Benchmarks Techniques And Models In Medical Visual Question Answering",
    "chunk_index": 85,
    "total_chunks": 146,
    "text_content": "remains an open, ongoing area of concern. IX. CONCLUSION VQA is a vision and language field that concerns answering natural language questions about a given image, with medical VQA used to answer questions about medical images. This paper has comprehensively reviewed numerous medical VQA models, structures, and datasets, as well as V+L pre- trained models by comparing more than 75 models with their statistical and SWOT analysis. It also statistically analyzed multi-modality parts in general fiel",
    "full_text_length": 142579,
    "chunk_length": 1282
  },
  {
    "chunk_id": 3508,
    "paper_filename": "suheer_2024_critical_analysis_of_benchmarks_techniques_and_models_in_medical_visual_question_answering.pdf",
    "paper_title": "Suheer 2024 Critical Analysis Of Benchmarks Techniques And Models In Medical Visual Question Answering",
    "chunk_index": 86,
    "total_chunks": 146,
    "text_content": "used 14% of the time, while SAN and concatenation methods were used at rates of 13%, and 10%, respectively. We found that LSTM-VGGNet and LSTM-ResNet combinations were primarily used in medical VQA, with 18% and 15% rates, respectively. Besides the statistical analysis of medical VQA 2018-2023, a statistical analysis of medical VQA in each separate year was performed, showing that LSTM and VGGNet were the main methods utilized for text and vision, respectively, in every year except 2018, where R",
    "full_text_length": 142579,
    "chunk_length": 1344
  },
  {
    "chunk_id": 3509,
    "paper_filename": "suheer_2024_critical_analysis_of_benchmarks_techniques_and_models_in_medical_visual_question_answering.pdf",
    "paper_title": "Suheer 2024 Critical Analysis Of Benchmarks Techniques And Models In Medical Visual Question Answering",
    "chunk_index": 87,
    "total_chunks": 146,
    "text_content": "diversity, unimodal bias problems, multi-modal datasets, external knowledge, multiple images,integrity in terms of practical medical applications, model interpretation, and evaluation. ACKNOWLEDGMENT The authors would like to thank King Saud University and the College of Computer and Information Sciences. Additionally, the authors would like to thank the Deanship of Scientific Research at King Saud University for funding and supporting this research through the initiative of DSR Graduate Student",
    "full_text_length": 142579,
    "chunk_length": 1349
  },
  {
    "chunk_id": 3510,
    "paper_filename": "suheer_2024_critical_analysis_of_benchmarks_techniques_and_models_in_medical_visual_question_answering.pdf",
    "paper_title": "Suheer 2024 Critical Analysis Of Benchmarks Techniques And Models In Medical Visual Question Answering",
    "chunk_index": 88,
    "total_chunks": 146,
    "text_content": "the evidence,\u2019\u2019 in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., Salt Lake City, UT, USA, Jun. 2018, pp. 8779\u20138788. [3] S. A. Hasan, Y. Ling, O. Farri, J. Liu, H. M\u00fcller, and M. Lungren, \u2018\u2018Overview of ImageCLEF 2018 medical domain visual question answering task,\u2019\u2019 in Proc. CLEF Working Notes, Sep. 2018, pp. 1\u20139. [4] Y. Xi, Y. Zhang, S. Ding, and S. Wan, \u2018\u2018Visual question answering model based on visual relationship detection,\u2019\u2019 Signal Process., Image Commun., vol. 80, Feb. 2020, Art. no. ",
    "full_text_length": 142579,
    "chunk_length": 1206
  },
  {
    "chunk_id": 3511,
    "paper_filename": "suheer_2024_critical_analysis_of_benchmarks_techniques_and_models_in_medical_visual_question_answering.pdf",
    "paper_title": "Suheer 2024 Critical Analysis Of Benchmarks Techniques And Models In Medical Visual Question Answering",
    "chunk_index": 89,
    "total_chunks": 146,
    "text_content": "on 360deg images,\u2019\u2019 in Proc. IEEE/CVF winter Conf. Appl. Comput. Vis., Jul. 2020, pp. 1607\u20131616. [7] M. H. Vu, T. L\u00f6fstedt, T. Nyholm, and R. Sznitman, \u2018\u2018A question-centric model for visual question answering in medical imaging,\u2019\u2019 IEEE Trans. Med. Imag., vol. 39, no. 9, pp. 2856\u20132868, Sep. 2020. [8] X. He, Y. Zhang, L. Mou, E. Xing, and P. Xie, \u2018\u2018PathVQA: 30000+ ques- tions for medical visual question answering,\u2019\u2019 2020, arXiv:2003.10286. [9] O. Kovaleva, C. Shivade, S. Kashyap, K. Kanjaria, J. W",
    "full_text_length": 142579,
    "chunk_length": 1234
  },
  {
    "chunk_id": 3512,
    "paper_filename": "suheer_2024_critical_analysis_of_benchmarks_techniques_and_models_in_medical_visual_question_answering.pdf",
    "paper_title": "Suheer 2024 Critical Analysis Of Benchmarks Techniques And Models In Medical Visual Question Answering",
    "chunk_index": 90,
    "total_chunks": 146,
    "text_content": "[11] W. Jifara, F. Jiang, S. Rho, M. Cheng, and S. Liu, \u2018\u2018Medical image denoising using convolutional neural network: A residual learning approach,\u2019\u2019 J. Supercomput., vol. 75, no. 2, pp. 704\u2013718, Feb. 2019. [12] B. D. Nguyen, T.-T. Do, B. X. Nguyen, T. Do, E. Tjiputra, and Q. D. Tran, \u2018\u2018Overcoming data limitation in medical visual question answering,\u2019\u2019 inProc. MICCAI 22nd Int. Conf. Med. Image Comput. Comput.- Assist. Intervent., Shenzhen, China, Cham, Switzerland: Springer, 2019, pp. 522\u2013530. [",
    "full_text_length": 142579,
    "chunk_length": 1265
  },
  {
    "chunk_id": 3513,
    "paper_filename": "suheer_2024_critical_analysis_of_benchmarks_techniques_and_models_in_medical_visual_question_answering.pdf",
    "paper_title": "Suheer 2024 Critical Analysis Of Benchmarks Techniques And Models In Medical Visual Question Answering",
    "chunk_index": 91,
    "total_chunks": 146,
    "text_content": "Hengel, \u2018\u2018Visual question answering: A survey of methods and datasets,\u2019\u2019 Comput. Vis. Image Understand., vol. 163, pp. 21\u201340, Oct. 2017. 136534 VOLUME 11, 2023 S. Al-Hadhrami et al.: Critical Analysis of Benchmarks, Techniques, and Models in Medical VQA [16] K. Kafle and C. Kanan, \u2018\u2018Visual question answering: Datasets, algo- rithms, and future challenges,\u2019\u2019 Comput. Vis. Image Understand., vol. 163, pp. 3\u201320, Oct. 2017. [17] A. K. Gupta, \u2018\u2018Survey of visual question answering: Datasets and techniq",
    "full_text_length": 142579,
    "chunk_length": 1278
  },
  {
    "chunk_id": 3514,
    "paper_filename": "suheer_2024_critical_analysis_of_benchmarks_techniques_and_models_in_medical_visual_question_answering.pdf",
    "paper_title": "Suheer 2024 Critical Analysis Of Benchmarks Techniques And Models In Medical Visual Question Answering",
    "chunk_index": 92,
    "total_chunks": 146,
    "text_content": "5th Int. Conf. Comput. Vis. Image Process. (CVIP). Prayagraj, India, Singapore: Springer, 2020, pp. 75\u201386. [21] Y. Zou and Q. Xie, \u2018\u2018A survey on VQA: Datasets and approaches,\u2019\u2019 in Proc. 2nd Int. Conf. Inf. Technol. Comput. Appl. (ITCA), Guangzhou, China, Dec. 2020, pp. 289\u2013297. [22] H. Sharma and A. S. Jalal, \u2018\u2018A survey of methods, datasets and evaluation metrics for visual question answering,\u2019\u2019 Image Vis. Comput., vol. 116, Dec. 2021, Art. no. 104327. [23] M. Sahani, P. Singh, S. Jangpangi, and",
    "full_text_length": 142579,
    "chunk_length": 1243
  },
  {
    "chunk_id": 3515,
    "paper_filename": "suheer_2024_critical_analysis_of_benchmarks_techniques_and_models_in_medical_visual_question_answering.pdf",
    "paper_title": "Suheer 2024 Critical Analysis Of Benchmarks Techniques And Models In Medical Visual Question Answering",
    "chunk_index": 93,
    "total_chunks": 146,
    "text_content": "models,\u2019\u2019 Int. J. Sci. Technol. Res., vol. 9, no. 2, pp. 3919\u20133923, Jan. 2020. [26] S. S. N. Mohamed and K. Srinivasan, \u2018\u2018A comprehensive interpretation for medical VQA: Datasets, techniques, and challenges,\u2019\u2019 J. Intell. Fuzzy Syst., vol. 44, no. 4, pp. 5803\u20135819, Apr. 2023. [27] Z. Lin, D. Zhang, Q. Tao, D. Shi, G. Haffari, Q. Wu, M. He, and Z. Ge, \u2018\u2018Medical visual question answering: A survey,\u2019\u2019 Artif. Intell. Med., vol. 143, Sep. 2023, Art. no. 102611. [28] S. Tascon-Morales, P. M\u00e1rquez-Neila",
    "full_text_length": 142579,
    "chunk_length": 1286
  },
  {
    "chunk_id": 3516,
    "paper_filename": "suheer_2024_critical_analysis_of_benchmarks_techniques_and_models_in_medical_visual_question_answering.pdf",
    "paper_title": "Suheer 2024 Critical Analysis Of Benchmarks Techniques And Models In Medical Visual Question Answering",
    "chunk_index": 94,
    "total_chunks": 146,
    "text_content": "radiology images,\u2019\u2019 in Proc. 1st Workshop Adv. Lang. Vis. Res., 2020, pp. 12\u201318. [31] A. B. Abacha, S. A. Hasan, V. V. Datla, J. Liu, D. Demner-Fushman, and H. M\u00fcller, \u2018\u2018VQA-med: Overview of the medical visual question answering task at ImageCLEF 2019,\u2019\u2019 in Proc. CLEF, Sep. 2019, pp. 1\u201311. [32] B. Liu, L.-M. Zhan, L. Xu, L. Ma, Y. Yang, and X.-M. Wu, \u2018\u2018Slake: A semantically-labeled knowledge-enhanced dataset for medical visual question answering,\u2019\u2019 in Proc. IEEE 18th Int. Symp. Biomed. Imag. (IS",
    "full_text_length": 142579,
    "chunk_length": 1258
  },
  {
    "chunk_id": 3517,
    "paper_filename": "suheer_2024_critical_analysis_of_benchmarks_techniques_and_models_in_medical_visual_question_answering.pdf",
    "paper_title": "Suheer 2024 Critical Analysis Of Benchmarks Techniques And Models In Medical Visual Question Answering",
    "chunk_index": 95,
    "total_chunks": 146,
    "text_content": "2014, pp. 740\u2013755. [35] A. B. Abacha, V. V. Datla, S. A. Hasan, D. Demner-Fushman, and H. M\u00fcller, \u2018\u2018Overview of the VQA-med task at ImageCLEF 2020: Visual question answering and generation in the medical domain,\u2019\u2019 in Proc. CLEF Conf. Labs Eval. Forum, 2020, pp. 1\u20139. [36] P. Porwal, S. Pachade, R. Kamble, M. Kokare, G. Deshmukh, V. Sahasrabuddhe, and F. Meriaudeau, \u2018\u2018Indian diabetic retinopathy image dataset (IDRiD): A database for diabetic retinopathy screening research,\u2019\u2019 Data, vol. 3, no. 3, p",
    "full_text_length": 142579,
    "chunk_length": 1296
  },
  {
    "chunk_id": 3518,
    "paper_filename": "suheer_2024_critical_analysis_of_benchmarks_techniques_and_models_in_medical_visual_question_answering.pdf",
    "paper_title": "Suheer 2024 Critical Analysis Of Benchmarks Techniques And Models In Medical Visual Question Answering",
    "chunk_index": 96,
    "total_chunks": 146,
    "text_content": "Switzerland: Springer, Sep. 2022, pp. 33\u201343. [39] E. Decenci\u00e8re, G. Cazuguel, X. Zhang, G. Thibault, J.-C. Klein, F. Meyer, B. Marcotegui, G. Quellec, M. Lamard, R. Danno, D. Elie, P. Massin, Z. Viktor, A. Erginay, B. La\u00ff, and A. Chabouis, \u2018\u2018TeleOphta: Machine learning and image processing methods for teleophthalmology,\u2019\u2019 IRBM, vol. 34, no. 2, pp. 196\u2013203, Apr. 2013. [40] M. Allan, S. Kondo, S. Bodenstedt, S. Leger, R. Kadkhodamohammadi, I. Luengo, F. Fuentes, E. Flouty, A. Mohammed, and M. Pede",
    "full_text_length": 142579,
    "chunk_length": 1285
  },
  {
    "chunk_id": 3519,
    "paper_filename": "suheer_2024_critical_analysis_of_benchmarks_techniques_and_models_in_medical_visual_question_answering.pdf",
    "paper_title": "Suheer 2024 Critical Analysis Of Benchmarks Techniques And Models In Medical Visual Question Answering",
    "chunk_index": 97,
    "total_chunks": 146,
    "text_content": "for recognition tasks on laparoscopic videos,\u2019\u2019 IEEE Trans. Med. Imag., vol. 36, no. 1, pp. 86\u201397, Jan. 2017. [43] J. Huang, Y. Chen, Y. Li, Z. Yang, X. Gong, F. L. Wang, X. Xu, and W. Liu, \u2018\u2018Medical knowledge-based network for patient-oriented visual question answering,\u2019\u2019 Inf. Process. Manage. , vol. 60, no. 2, Mar. 2023, Art. no. 103241. [44] Z. Wu and M. Palmer, \u2018\u2018Verb semantics and lexical selection,\u2019\u2019 1994, arXiv:cmp-lg/9406033. [45] G. So\u011fanc\u0131o\u011flu, H. \u00d6zt\u00fcrk, and A. \u00d6zg\u00fcr, \u2018\u2018BIOSSES: A sem",
    "full_text_length": 142579,
    "chunk_length": 1262
  },
  {
    "chunk_id": 3520,
    "paper_filename": "suheer_2024_critical_analysis_of_benchmarks_techniques_and_models_in_medical_visual_question_answering.pdf",
    "paper_title": "Suheer 2024 Critical Analysis Of Benchmarks Techniques And Models In Medical Visual Question Answering",
    "chunk_index": 98,
    "total_chunks": 146,
    "text_content": "method for automatic evaluation of machine translation,\u2019\u2019 in Proc. 40th Annu. Meeting Assoc. Comput. Linguistics, Philadelphia, PA, USA, 2002, pp. 311\u2013318. [48] D. G. Lowe, \u2018\u2018Object recognition from local scale-invariant features,\u2019\u2019 in Proc. 7th IEEE Int. Conf. Comput. Vis., vol. 2, Aug. 1999, pp. 1150\u20131157. [49] N. Dalal and B. Triggs, \u2018\u2018Histograms of oriented gradients for human detection,\u2019\u2019 in Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit. (CVPR), San Diego, CA, USA, Jul. 2005, ",
    "full_text_length": 142579,
    "chunk_length": 1294
  },
  {
    "chunk_id": 3521,
    "paper_filename": "suheer_2024_critical_analysis_of_benchmarks_techniques_and_models_in_medical_visual_question_answering.pdf",
    "paper_title": "Suheer 2024 Critical Analysis Of Benchmarks Techniques And Models In Medical Visual Question Answering",
    "chunk_index": 99,
    "total_chunks": 146,
    "text_content": "Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich, \u2018\u2018Going deeper with convolutions,\u2019\u2019 in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), Boston, MA, USA, Jun. 2015, pp. 1\u20139. [54] K. He, X. Zhang, S. Ren, and J. Sun, \u2018\u2018Deep residual learning for image recognition,\u2019\u2019 in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), Las Vegas, NV, USA, Jun. 2016, pp. 770\u2013778. [55] Z. Liao, Q. Wu, C. Shen, A. van den Hengel, and J. Verjans, \u2018\u2018AIML at VQA-Med 2020",
    "full_text_length": 142579,
    "chunk_length": 1247
  },
  {
    "chunk_id": 3522,
    "paper_filename": "suheer_2024_critical_analysis_of_benchmarks_techniques_and_models_in_medical_visual_question_answering.pdf",
    "paper_title": "Suheer 2024 Critical Analysis Of Benchmarks Techniques And Models In Medical Visual Question Answering",
    "chunk_index": 100,
    "total_chunks": 146,
    "text_content": "Li, \u2018\u2018SYSU-HCP at VQA-Med 2021: A data-centric model with efficient training methodology for medical visual question answering,\u2019\u2019 Proc. Work. Notes CLEF Conf. Labs Eval. Forum, vol. 201, Sep. 2021, pp. 1\u201311. [58] A. Fukui, D. H. Park, D. Yang, A. Rohrbach, T. Darrell, and M. Rohrbach, \u2018\u2018Multimodal compact bilinear pooling for visual question answering and visual grounding,\u2019\u2019 in Proc. Conf. Empirical Methods Natural Lang. Process., Austin, TX, USA, 2016, pp. 457\u2013468. VOLUME 11, 2023 136535 S. Al-",
    "full_text_length": 142579,
    "chunk_length": 1284
  },
  {
    "chunk_id": 3523,
    "paper_filename": "suheer_2024_critical_analysis_of_benchmarks_techniques_and_models_in_medical_visual_question_answering.pdf",
    "paper_title": "Suheer 2024 Critical Analysis Of Benchmarks Techniques And Models In Medical Visual Question Answering",
    "chunk_index": 101,
    "total_chunks": 146,
    "text_content": "Oct. 2017, pp. 2631\u20132639. [61] H. K. Verma and S. Ramachandran, \u2018\u2018Harendrakv at VQA-med 2020: Sequential VQA with attention for medical visual question answering,\u2019\u2019 in Proc. Work. Notes CLEF Conf. Labs Eval. Forum, Thessaloniki, Greece, Sep. 2020, pp. 1\u20137. [62] R. Bounaama and M. E. A. Abderrahim, \u2018\u2018Tlemcen university at ImageCLEF 2019 visual question answering task,\u2019\u2019 in Proc. Work. Notes CLEF Conf. Labs Eval. Forum, Lugano, Switzerland, Sep. 2019, pp. 1\u20136. [63] G. A. Miller and W. G. Charles, ",
    "full_text_length": 142579,
    "chunk_length": 1279
  },
  {
    "chunk_id": 3524,
    "paper_filename": "suheer_2024_critical_analysis_of_benchmarks_techniques_and_models_in_medical_visual_question_answering.pdf",
    "paper_title": "Suheer 2024 Critical Analysis Of Benchmarks Techniques And Models In Medical Visual Question Answering",
    "chunk_index": 102,
    "total_chunks": 146,
    "text_content": "Mikolov, K. Chen, G. Corrado, and J. Dean, \u2018\u2018Efficient estimation of word representations in vector space,\u2019\u2019 2013, arXiv:1301.3781. [67] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean, \u2018\u2018Dis- tributed representations of words and phrases and their compositionality,\u2019\u2019 inProc. Adv. neural Inf. Process. Syst., vol. 26, 2013, pp. 1\u20136. [68] Google. (2013). Word2Vec. Accessed: Nov. 2023. [Online]. Available: https://code.google.com/archive/p/word2vec/ [69] S. Hochreiter and J. Schmidhub",
    "full_text_length": 142579,
    "chunk_length": 1483
  },
  {
    "chunk_id": 3525,
    "paper_filename": "suheer_2024_critical_analysis_of_benchmarks_techniques_and_models_in_medical_visual_question_answering.pdf",
    "paper_title": "Suheer 2024 Critical Analysis Of Benchmarks Techniques And Models In Medical Visual Question Answering",
    "chunk_index": 103,
    "total_chunks": 146,
    "text_content": "(2020). Challenge-Pathology Visual Question Answering- Grand Challenge. Accessed Nov. 2023. [Online]. Available: https://pathvqachallenge.grand-hallenge.org/PathVQA_challenge/ [73] I. Allaouzi, B. Benamrou, M. Benamrou, and M. B. Ahmed, \u2018\u2018Deep neural networks and decision tree classifier for visual question answering in the medical domain,\u2019\u2019 in Proc. CLEF (Working Notes), Sep. 2018, pp. 1\u20137. [74] Y. Zhou, X. Kang, and F. Ren, \u2018\u2018Employing inception-ResNet-v2 and Bi- LSTM for medical domain visual",
    "full_text_length": 142579,
    "chunk_length": 1392
  },
  {
    "chunk_id": 3526,
    "paper_filename": "suheer_2024_critical_analysis_of_benchmarks_techniques_and_models_in_medical_visual_question_answering.pdf",
    "paper_title": "Suheer 2024 Critical Analysis Of Benchmarks Techniques And Models In Medical Visual Question Answering",
    "chunk_index": 104,
    "total_chunks": 146,
    "text_content": "Salakhutdinov, and Q. V. Le, \u2018\u2018XLNet: Generalized autoregressive pretraining for language understanding,\u2019\u2019 in Proc. 33rd Adv. Neural Inf. Process. Syst., Vancouver, BC, Canada, 2019, pp. 5753\u20135763. [78] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, \u2018\u2018BERT: Pre-training of deep bidirectional transformers for language understanding,\u2019\u2019 2018, arXiv:1810.04805. [79] S. Eslami, G. de Melo, and C. Meinel, \u2018\u2018Teams at VQA-Med 2021: BBN-orchestra for long-tailed medical visual question answering,\u2019\u2019 in",
    "full_text_length": 142579,
    "chunk_length": 1325
  },
  {
    "chunk_id": 3527,
    "paper_filename": "suheer_2024_critical_analysis_of_benchmarks_techniques_and_models_in_medical_visual_question_answering.pdf",
    "paper_title": "Suheer 2024 Critical Analysis Of Benchmarks Techniques And Models In Medical Visual Question Answering",
    "chunk_index": 105,
    "total_chunks": 146,
    "text_content": "Neural Netw. Learn. Syst., vol. 29, no. 12, pp. 5947\u20135959, Dec. 2018.[82] S. Antol, A. Agrawal, J. Lu, M. Mitchell, D. Batra, C. L. Zitnick, and D. Parikh, \u2018\u2018VQA: Visual question answering,\u2019\u2019 in Proc. ICCV, Araucano Park, Chile, 2015, pp. 2425\u20132433. [83] M. Malinowski, M. Rohrbach, and M. Fritz, \u2018\u2018Ask your neurons: A deep learning approach to visual question answering,\u2019\u2019 Int. J. Comput. Vis., vol. 125, nos. 1\u20133, pp. 110\u2013135, Dec. 2017. [84] K. Saito, A. Shin, Y. Ushiku, and T. Harada, \u2018\u2018DualNet:",
    "full_text_length": 142579,
    "chunk_length": 1236
  },
  {
    "chunk_id": 3528,
    "paper_filename": "suheer_2024_critical_analysis_of_benchmarks_techniques_and_models_in_medical_visual_question_answering.pdf",
    "paper_title": "Suheer 2024 Critical Analysis Of Benchmarks Techniques And Models In Medical Visual Question Answering",
    "chunk_index": 106,
    "total_chunks": 146,
    "text_content": "neural network with dynamic parameter prediction,\u2019\u2019 in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR) , Las Vegas, NV, USA: Caesars Palace, Jun. 2016, pp. 30\u201338. [87] J.-H. Kim, S.-W. Lee, D. Kwak, M.-O. Heo, J. Kim, J.-W. Ha, and B.-T. Zhang, \u2018\u2018Multimodal residual learning for visual QA,\u2019\u2019 in Proc. Adv. Neural Inf. Process. Syst., vol. 29, 2016, pp. 1\u20136. [88] M. Lao, Y. Guo, H. Wang, and X. Zhang, \u2018\u2018Cross-modal multistep fusion network with co-attention for visual question answering,\u2019\u2019 ",
    "full_text_length": 142579,
    "chunk_length": 1248
  },
  {
    "chunk_id": 3529,
    "paper_filename": "suheer_2024_critical_analysis_of_benchmarks_techniques_and_models_in_medical_visual_question_answering.pdf",
    "paper_title": "Suheer 2024 Critical Analysis Of Benchmarks Techniques And Models In Medical Visual Question Answering",
    "chunk_index": 107,
    "total_chunks": 146,
    "text_content": "pp. 451\u2013468. [91] L. Chen, X. Yan, J. Xiao, H. Zhang, S. Pu, and Y. Zhuang, \u2018\u2018Counterfac- tual samples synthesizing for robust visual question answering,\u2019\u2019 in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2020, pp. 10797\u201310806. [92] L. H. Li, M. Yatskar, D. Yin, C.-J. Hsieh, and K.-W. Chang, \u2018\u2018Visu- alBERT: A simple and performant baseline for vision and language,\u2019\u2019 Aug. 2019, arXiv:1908.03557. [93] Y. Peng, F. Liu, and M. P. Rosen, \u2018\u2018UMASS at ImageCLEF medical visual question",
    "full_text_length": 142579,
    "chunk_length": 1275
  },
  {
    "chunk_id": 3530,
    "paper_filename": "suheer_2024_critical_analysis_of_benchmarks_techniques_and_models_in_medical_visual_question_answering.pdf",
    "paper_title": "Suheer 2024 Critical Analysis Of Benchmarks Techniques And Models In Medical Visual Question Answering",
    "chunk_index": 108,
    "total_chunks": 146,
    "text_content": "visual question answering,\u2019\u2019 2015, arXiv:1511.05960. [96] Y. Shi, T. Furlanello, S. Zha, and A. Anandkumar, \u2018\u2018Question type guided attention in visual question answering,\u2019\u2019 in Proc. ECCV, Munich, Germany, Sep. 2018, pp. 151\u2013166. [97] K. J. Shih, S. Singh, and D. Hoiem, \u2018\u2018Where to look: Focus regions for visual question answering,\u2019\u2019 in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR). Las Vegas, NV, USA: Caesars Palace, Jun. 2016, pp. 4613\u20134621. [98] J.-H. Kim, J. Jun, and B.-T. Zhang, \u2018\u2018Bi",
    "full_text_length": 142579,
    "chunk_length": 1282
  },
  {
    "chunk_id": 3531,
    "paper_filename": "suheer_2024_critical_analysis_of_benchmarks_techniques_and_models_in_medical_visual_question_answering.pdf",
    "paper_title": "Suheer 2024 Critical Analysis Of Benchmarks Techniques And Models In Medical Visual Question Answering",
    "chunk_index": 109,
    "total_chunks": 146,
    "text_content": "answering,\u2019\u2019 in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR) , Long Beach, CA, USA, Jun. 2019, pp. 6274\u20136283. [101] L. Huang, W. Wang, J. Chen, and X.-Y. Wei, \u2018\u2018Attention on attention for image captioning,\u2019\u2019 in Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV), Seoul, South Korea, Oct. 2019, pp. 4633\u20134642. [102] Y. N. Dauphin, A. Fan, M. Auli, and D. Grangier, \u2018\u2018Language modeling with gated convolutional networks,\u2019\u2019 in Proc. Int. Conf. Mach. Learn., 2017, pp. 933\u2013941. [103] M. H. Vu, R",
    "full_text_length": 142579,
    "chunk_length": 1248
  },
  {
    "chunk_id": 3532,
    "paper_filename": "suheer_2024_critical_analysis_of_benchmarks_techniques_and_models_in_medical_visual_question_answering.pdf",
    "paper_title": "Suheer 2024 Critical Analysis Of Benchmarks Techniques And Models In Medical Visual Question Answering",
    "chunk_index": 110,
    "total_chunks": 146,
    "text_content": "Fei-Fei, \u2018\u2018ImageNet: A large-scale hierarchical image database,\u2019\u2019 in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., Miami, FL, USA, Jun. 2009, pp. 248\u2013255. [105] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, and V. Stoyanov, \u2018\u2018RoBERTa: A robustly optimized BERT pretraining approach,\u2019\u2019 2019, arXiv:1907.11692. [106] Y.-C. Chen, L. Li, L. Yu, A. E. Kholy, F. Ahmed, Z. Gan, Y. Cheng, and J. Liu, \u2018\u2018Uniter: Universal image-text representation learning,\u2019\u2019 in P",
    "full_text_length": 142579,
    "chunk_length": 1317
  },
  {
    "chunk_id": 3533,
    "paper_filename": "suheer_2024_critical_analysis_of_benchmarks_techniques_and_models_in_medical_visual_question_answering.pdf",
    "paper_title": "Suheer 2024 Critical Analysis Of Benchmarks Techniques And Models In Medical Visual Question Answering",
    "chunk_index": 111,
    "total_chunks": 146,
    "text_content": "Zhang, H. Hu, J. Corso, and J. Gao, \u2018\u2018Unified vision-language pre-training for image captioning and VQA,\u2019\u2019 in Proc. AAAI Conf. Artif. Intell., 2020, vol. 34, no. 7, pp. 13041\u201313049. [110] W. Su, X. Zhu, Y. Cao, B. Li, L. Lu, F. Wei, and J. Dai, \u2018\u2018VL- BERT: Pre-training of generic visual-linguistic representations,\u2019\u2019 2019, arXiv:1908.08530. [111] J. Guo, C. Zhu, Y. Zhao, H. Wang, Y. Hu, X. He, and D. Cai, \u2018\u2018LAMP: Label augmented multimodal pretraining,\u2019\u2019 2020, arXiv:2012.04446. [112] J. Cho, J. L",
    "full_text_length": 142579,
    "chunk_length": 1297
  },
  {
    "chunk_id": 3534,
    "paper_filename": "suheer_2024_critical_analysis_of_benchmarks_techniques_and_models_in_medical_visual_question_answering.pdf",
    "paper_title": "Suheer 2024 Critical Analysis Of Benchmarks Techniques And Models In Medical Visual Question Answering",
    "chunk_index": 112,
    "total_chunks": 146,
    "text_content": "F. Yu, J. Tang, W. Yin, Y. Sun, H. Tian, H. Wu, and H. Wang, \u2018\u2018ERNIE- ViL: Knowledge enhanced vision-language representations through scene graph,\u2019\u2019 2020, arXiv:2006.16934. [116] S. Zhang, T. Jiang, T. Wang, K. Kuang, Z. Zhao, J. Zhu, J. Yu, H. Yang, and F. Wu, \u2018\u2018DeVLBert: Learning deconfounded visio-linguistic representations,\u2019\u2019 in Proc. 28th ACM Int. Conf. Multimedia , Oct. 2020, pp. 4373\u20134382. [117] F. Luo, P. Yang, S. Li, X. Ren, and X. Sun, \u2018\u2018CAPT: Contrastive pre-training for learning deno",
    "full_text_length": 142579,
    "chunk_length": 1298
  },
  {
    "chunk_id": 3535,
    "paper_filename": "suheer_2024_critical_analysis_of_benchmarks_techniques_and_models_in_medical_visual_question_answering.pdf",
    "paper_title": "Suheer 2024 Critical Analysis Of Benchmarks Techniques And Models In Medical Visual Question Answering",
    "chunk_index": 113,
    "total_chunks": 146,
    "text_content": "Montreal, QC, Canada, Jun. 2021, pp. 5575\u20135584. [120] C. Li, M. Yan, H. Xu, F. Luo, W. Wang, B. Bi, and S. Huang, \u2018\u2018SemVLP: Vision-language pre-training by aligning semantics at multiple levels,\u2019\u2019 2021, arXiv:2103.07829. [121] C. Kervadec, G. Antipov, M. Baccouche, and C. Wolf, \u2018\u2018Weak supervision helps emergence of word-object alignment and improves vision-language tasks,\u2019\u2019 2019, arXiv:1912.03063. [122] J. Lin, A. Yang, Y. Zhang, J. Liu, J. Zhou, and H. Yang, \u2018\u2018InterBERT: Vision-and-language int",
    "full_text_length": 142579,
    "chunk_length": 1381
  },
  {
    "chunk_id": 3536,
    "paper_filename": "suheer_2024_critical_analysis_of_benchmarks_techniques_and_models_in_medical_visual_question_answering.pdf",
    "paper_title": "Suheer 2024 Critical Analysis Of Benchmarks Techniques And Models In Medical Visual Question Answering",
    "chunk_index": 114,
    "total_chunks": 146,
    "text_content": "vol. 9, pp. 978\u2013994, Sep. 2021. [125] W. Kim, B. Son, and I. Kim, \u2018\u2018ViLT: Vision-and-language transformer without convolution or region supervision,\u2019\u2019 2021, arXiv:2102.03334. [126] M. Zhuge, D. Gao, D.-P. Fan, L. Jin, B. Chen, H. Zhou, M. Qiu, and L. Shao, \u2018\u2018Kaleido-BERT: Vision-language pre-training on fashion domain,\u2019\u2019 in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2021, pp. 12642\u201312652.[127] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. A",
    "full_text_length": 142579,
    "chunk_length": 1261
  },
  {
    "chunk_id": 3537,
    "paper_filename": "suheer_2024_critical_analysis_of_benchmarks_techniques_and_models_in_medical_visual_question_answering.pdf",
    "paper_title": "Suheer 2024 Critical Analysis Of Benchmarks Techniques And Models In Medical Visual Question Answering",
    "chunk_index": 115,
    "total_chunks": 146,
    "text_content": "Dai, J. Gao, H. Hu, X. Huang, B. Li, and C. Li, \u2018\u2018Florence: A new foundation model for computer vision,\u2019\u2019 2021, arXiv:2111.11432. [130] C. Alberti, J. Ling, M. Collins, and D. Reitter, \u2018\u2018Fusion of detected objects in text for visual question answering,\u2019\u2019 2019, arXiv:1908.05054. [131] Y. Wang, S. Joty, M. R. Lyu, I. King, C. Xiong, and S. C. H. Hoi, \u2018\u2018VD-BERT: A unified vision and dialog transformer with BERT,\u2019\u2019 2020, arXiv:2004.13278. [132] V. Murahari, D. Batra, D. Parikh, and A. Das, \u2018\u2018Large-s",
    "full_text_length": 142579,
    "chunk_length": 1287
  },
  {
    "chunk_id": 3538,
    "paper_filename": "suheer_2024_critical_analysis_of_benchmarks_techniques_and_models_in_medical_visual_question_answering.pdf",
    "paper_title": "Suheer 2024 Critical Analysis Of Benchmarks Techniques And Models In Medical Visual Question Answering",
    "chunk_index": 116,
    "total_chunks": 146,
    "text_content": "Bharti, and A. Sacheti, \u2018\u2018ImageBERT: Cross-modal pre-training with large-scale weak-supervised image-text data,\u2019\u2019 2020, arXiv:2001.07966. [135] Q. Xia, H. Huang, N. Duan, D. Zhang, L. Ji, Z. Sui, E. Cui, T. Bharti, and M. Zhou, \u2018\u2018XGPT: Cross-modal generative pre-training for image captioning,\u2019\u2019 in Proc. CCF Int. Conf. Natural Lang. Process. Chin. Comput., Qingdao, China, Cham, Switzerland: Springer, Oct. 2021, pp. 786\u2013797. [136] T. Scialom, P. Bordes, P.-A. Dray, J. Staiano, and P. Gallinari, \u2018\u2018",
    "full_text_length": 142579,
    "chunk_length": 1414
  },
  {
    "chunk_id": 3539,
    "paper_filename": "suheer_2024_critical_analysis_of_benchmarks_techniques_and_models_in_medical_visual_question_answering.pdf",
    "paper_title": "Suheer 2024 Critical Analysis Of Benchmarks Techniques And Models In Medical Visual Question Answering",
    "chunk_index": 117,
    "total_chunks": 146,
    "text_content": "Tanaka, K. Nishida, and S. Yoshida, \u2018\u2018VisualMRC: Machine reading comprehension on document images,\u2019\u2019 2021, arXiv:2101.11272. [140] M.-J. Chiou, R. Zimmermann, and J. Feng, \u2018\u2018Visual relationship detection with visual-linguistic knowledge from multimodal representations,\u2019\u2019 IEEE Access, vol. 9, pp. 50441\u201350451, 2021. [141] J. Lu, V. Goswami, M. Rohrbach, D. Parikh, and S. Lee, \u2018\u201812-in- 1: Multi-task vision and language representation learning,\u2019\u2019 in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit",
    "full_text_length": 142579,
    "chunk_length": 1364
  },
  {
    "chunk_id": 3540,
    "paper_filename": "suheer_2024_critical_analysis_of_benchmarks_techniques_and_models_in_medical_visual_question_answering.pdf",
    "paper_title": "Suheer 2024 Critical Analysis Of Benchmarks Techniques And Models In Medical Visual Question Answering",
    "chunk_index": 118,
    "total_chunks": 146,
    "text_content": "Comput. Vis. Pattern Recognit. (CVPR), New Orleans, LA, USA, Jun. 2022, pp. 15650\u201315659. [145] T. Wang, W. Jiang, Z. Lu, F. Zheng, R. Cheng, C. Yin, and P. Luo, \u2018\u2018VLMixer: Unpaired vision-language pre-training via cross-modal cutmix,\u2019\u2019 in Proc. Int. Conf. Mach. Learn., Baltimore, MD, USA, Jul. 2022, pp. 22680\u201322690. [146] F. Liu, X. Wu, S. Ge, X. Ren, W. Fan, X. Sun, and Y. Zou, \u2018\u2018DiMBERT: Learning vision-language grounded representations with disentangled multimodal-attention,\u2019\u2019 ACM Trans. Know",
    "full_text_length": 142579,
    "chunk_length": 1331
  },
  {
    "chunk_id": 3541,
    "paper_filename": "suheer_2024_critical_analysis_of_benchmarks_techniques_and_models_in_medical_visual_question_answering.pdf",
    "paper_title": "Suheer 2024 Critical Analysis Of Benchmarks Techniques And Models In Medical Visual Question Answering",
    "chunk_index": 119,
    "total_chunks": 146,
    "text_content": "Weng, D. Jin, T. Naumann, and M. B. A. McDermott, \u2018\u2018Publicly available clinical BERT embeddings,\u2019\u2019 2019, arXiv:1904.03323. [149] J. Li, D. Li, C. Xiong, and S. Hoi, \u2018\u2018BLIP: Bootstrapping language-image pre-training for unified vision-language understanding and generation,\u2019\u2019 inProc. Int. Conf. Mach. Learn., Baltimore, MD, USA, Jul. 2022, pp. 12888\u201312900. [150] P. Wang, A. Yang, R. Men, J. Lin, S. Bai, Z. Li, J. Ma, C. Zhou, J. Zhou, and H. Yang, \u2018\u2018OFA: Unifying architectures, tasks, and modalitie",
    "full_text_length": 142579,
    "chunk_length": 1325
  },
  {
    "chunk_id": 3542,
    "paper_filename": "suheer_2024_critical_analysis_of_benchmarks_techniques_and_models_in_medical_visual_question_answering.pdf",
    "paper_title": "Suheer 2024 Critical Analysis Of Benchmarks Techniques And Models In Medical Visual Question Answering",
    "chunk_index": 120,
    "total_chunks": 146,
    "text_content": "for vision and vision-language tasks,\u2019\u2019 inProc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), Vancouver, BC, Canada, Jun. 2023, pp. 19175\u201319186. [153] X. Chen, X. Wang, S. Changpinyo, A. J. Piergiovanni, P. Padlewski, D. Salz, S. Goodman, A. Grycner, B. Mustafa, and L. Beyer, \u2018\u2018PaLI: A jointly-scaled multilingual language-image model,\u2019\u2019 2022, arXiv:2209.06794. [154] J. Li, D. Li, S. Savarese, and S. Hoi, \u2018\u2018BLIP-2: Bootstrapping language- image pre-training with frozen image encoders and ",
    "full_text_length": 142579,
    "chunk_length": 1298
  },
  {
    "chunk_id": 3543,
    "paper_filename": "suheer_2024_critical_analysis_of_benchmarks_techniques_and_models_in_medical_visual_question_answering.pdf",
    "paper_title": "Suheer 2024 Critical Analysis Of Benchmarks Techniques And Models In Medical Visual Question Answering",
    "chunk_index": 121,
    "total_chunks": 146,
    "text_content": "pp. 1\u20136. [157] A. Thanki and K. Makkithaya, \u2018\u2018Mit Manipal at ImageCLEF 2019 visual question answering in medical domain,\u2019\u2019 in Proc. Work. Notes CLEF Conf. Labs Eval. Forum, Lugano, Switzerland, Sep. 2019, pp. 1\u20139. [158] A. Turner and A. Spanier, \u2018\u2018LSTM in VQA-med, is it really needed? JCE study on the ImageCLEF 2019 dataset,\u2019\u2019 in Proc. Work. Notes CLEF Conf. Labs Eval. Forum, Lugano, Switzerland, Sep. 2019, pp. 1\u20139. [159] M. Bansal, T. Gadgil, R. Shah, and P. Verma, \u2018\u2018Medical visual question ans",
    "full_text_length": 142579,
    "chunk_length": 1219
  },
  {
    "chunk_id": 3544,
    "paper_filename": "suheer_2024_critical_analysis_of_benchmarks_techniques_and_models_in_medical_visual_question_answering.pdf",
    "paper_title": "Suheer 2024 Critical Analysis Of Benchmarks Techniques And Models In Medical Visual Question Answering",
    "chunk_index": 122,
    "total_chunks": 146,
    "text_content": "for visual question answering in the medical domain,\u2019\u2019 in Proc. Work. Notes CLEF Conf. Labs Eval. Forum, Lugano, Switzerland, Sep. 2019, pp. 1\u20139. [162] T. Kornuta, D. Rajan, C. Shivade, A. Asseman, and A. S. Ozcan, \u2018\u2018Leveraging medical visual question answering with supporting facts,\u2019\u2019 in Proc. Work. Notes CLEF Conf. Labs Eval. Forum, Lugano, Switzerland, Sep. 2019, pp. 1\u20136. [163] I. Allaouzi, M. B. Ahmed, and B. Benamrou, \u2018\u2018An encoder\u2013decoder model for visual question answering in the medical d",
    "full_text_length": 142579,
    "chunk_length": 1264
  },
  {
    "chunk_id": 3545,
    "paper_filename": "suheer_2024_critical_analysis_of_benchmarks_techniques_and_models_in_medical_visual_question_answering.pdf",
    "paper_title": "Suheer 2024 Critical Analysis Of Benchmarks Techniques And Models In Medical Visual Question Answering",
    "chunk_index": 123,
    "total_chunks": 146,
    "text_content": "task at Imageclef 2020: Visual question answering and generation in the medical domain,\u2019\u2019 in Proc. Work. Notes CLEF Conf. Labs Eval. Forum, Bucharest, Romania, 2021, pp. 1\u20139. [166] M. Sarrouti, \u2018\u2018NLM at VQA-Med 2020: Visual question answering and generation in the medical domain,\u2019\u2019 in Proc. Work. Notes CLEF Conf. Labs Eval. Forum, Thessaloniki, Greece, Sep. 2020, pp. 1\u20136. [167] H. Umada and M. Aono, \u2018\u2018Kdevqa at VQA-med 2020: Focusing on GLU-based classification,\u2019\u2019 in Proc. Work. Notes CLEF Conf.",
    "full_text_length": 142579,
    "chunk_length": 1289
  },
  {
    "chunk_id": 3546,
    "paper_filename": "suheer_2024_critical_analysis_of_benchmarks_techniques_and_models_in_medical_visual_question_answering.pdf",
    "paper_title": "Suheer 2024 Critical Analysis Of Benchmarks Techniques And Models In Medical Visual Question Answering",
    "chunk_index": 124,
    "total_chunks": 146,
    "text_content": "Conf. Labs Eval. Forum , Thessaloniki, Greece, Sep. 2020, pp. 1\u20139. [170] G. Chen, H. Gong, and G. Li, \u2018\u2018HCP-MIC at VQA-med 2020: Effective visual representation for medical visual question answering,\u2019\u2019 in Proc. Work. Notes CLEF Conf. Labs Eval. Forum, Thessaloniki, Greece, Sep. 2020, pp. 1\u20139. [171] A. Al-Sadi, Hana\u2019Al-Theiabat, and M. Al-Ayyoub, \u2018\u2018The inception team at VQA-med 2020: Pretrained VGG with data augmentation for medical VQA and VQG,\u2019\u2019 in Proc. Work. Notes CLEF Conf. Labs Eval. Forum ",
    "full_text_length": 142579,
    "chunk_length": 1256
  },
  {
    "chunk_id": 3547,
    "paper_filename": "suheer_2024_critical_analysis_of_benchmarks_techniques_and_models_in_medical_visual_question_answering.pdf",
    "paper_title": "Suheer 2024 Critical Analysis Of Benchmarks Techniques And Models In Medical Visual Question Answering",
    "chunk_index": 125,
    "total_chunks": 146,
    "text_content": "Bucharest, Romania, Sep. 2021, pp. 1\u201310. [174] Y. Li, Z. Yang, and T. Hao, \u2018\u2018TAM at VQA-med 2021: A hybrid model with feature extraction and fusion for medical visual question answering,\u2019\u2019 Work. Notes CLEF, Tech. Rep., Sep. 2021. [175] N. M. S. Sitara and S. Kavitha, \u2018\u2018SSN MLRG at VQA-med 2021: An approach for VQA to solve abnormality related queries using improved datasets,\u2019\u2019 in Proc. CEUR Workshop, Sep. 2021, 1329\u20131335. [176] I. Chebbi, \u2018\u2018VGG16: Visual generation of relevant natural language q",
    "full_text_length": 142579,
    "chunk_length": 1347
  },
  {
    "chunk_id": 3548,
    "paper_filename": "suheer_2024_critical_analysis_of_benchmarks_techniques_and_models_in_medical_visual_question_answering.pdf",
    "paper_title": "Suheer 2024 Critical Analysis Of Benchmarks Techniques And Models In Medical Visual Question Answering",
    "chunk_index": 126,
    "total_chunks": 146,
    "text_content": ", Bangkok, Thailand, Cham, Switzerland: Springer, Nov. 2020, pp. 194\u2013202. [179] B. Liu, L.-M. Zhan, and X.-M. Wu, \u2018\u2018Contrastive pre-training and representation distillation for medical visual question answering based on radiology images,\u2019\u2019 in Proc. 24th Int. Conf. Med. Image Comput. Comput. Assist. Intervent.\u2013MICCAI, Strasbourg, France, Cham, Switzerland: Springer, Sep. 2021, pp. 210\u2013220. [180] Y. Khare, V. Bagal, M. Mathew, A. Devi, U. D. Priyakumar, and C. Jawahar, \u2018\u2018MMBERT: Multimodal BERT pr",
    "full_text_length": 142579,
    "chunk_length": 1346
  },
  {
    "chunk_id": 3549,
    "paper_filename": "suheer_2024_critical_analysis_of_benchmarks_techniques_and_models_in_medical_visual_question_answering.pdf",
    "paper_title": "Suheer 2024 Critical Analysis Of Benchmarks Techniques And Models In Medical Visual Question Answering",
    "chunk_index": 127,
    "total_chunks": 146,
    "text_content": "30th ACM Int. Conf. Multimedia, New York, NY, USA, Oct. 2022, pp. 3569\u20133577. [183] M. Wang, X. He, L. Liu, L. Qing, H. Chen, Y. Liu, and C. Ren, \u2018\u2018Medical visual question answering based on question-type reasoning and semantic space constraint,\u2019\u2019 Artif. Intell. Med., vol. 131, Sep. 2022, Art. no. 102346. [184] H. Wang, H. Pan, K. Zhang, S. He, and C. Chen, \u2018\u2018M2FNet: Multi-granularity feature fusion network for medical visual question answering,\u2019\u2019 in Proc. PRICAI Trends Artif. Intell. 19th Pacifi",
    "full_text_length": 142579,
    "chunk_length": 1315
  },
  {
    "chunk_id": 3550,
    "paper_filename": "suheer_2024_critical_analysis_of_benchmarks_techniques_and_models_in_medical_visual_question_answering.pdf",
    "paper_title": "Suheer 2024 Critical Analysis Of Benchmarks Techniques And Models In Medical Visual Question Answering",
    "chunk_index": 128,
    "total_chunks": 146,
    "text_content": "VOLUME 11, 2023 S. Al-Hadhrami et al.: Critical Analysis of Benchmarks, Techniques, and Models in Medical VQA [187] D. Sharma, S. Purushotham, and C. K. Reddy, \u2018\u2018MedFuseNet: An attention-based multimodal deep learning model for visual question answering in the medical domain,\u2019\u2019 Sci. Rep., vol. 11, no. 1, pp. 1\u201318, Oct. 2021. [188] A. Al-Sadi, M. Al-Ayyoub, Y. Jararweh, and F. Costen, \u2018\u2018Visual question answering in the medical domain based on deep learning approaches: A comprehensive study,\u2019\u2019 Pat",
    "full_text_length": 142579,
    "chunk_length": 1295
  },
  {
    "chunk_id": 3551,
    "paper_filename": "suheer_2024_critical_analysis_of_benchmarks_techniques_and_models_in_medical_visual_question_answering.pdf",
    "paper_title": "Suheer 2024 Critical Analysis Of Benchmarks Techniques And Models In Medical Visual Question Answering",
    "chunk_index": 129,
    "total_chunks": 146,
    "text_content": "Forum, Thessaloniki, Greece, Sep. 2020, pp. 1\u20139. [191] H. T. Haridas, M. M. Fouda, Z. M. Fadlullah, M. Mahmoud, B. M. ElHa- lawany, and M. Guizani, \u2018\u2018MED-GPVS: A deep learning-based joint biomedical image classification and visual question answering system for precision e-health,\u2019\u2019 in Proc. IEEE Int. Conf. Commun., Seoul, South Korea, May 2022, pp. 3838\u20133843. [192] S. Al-Hadhrami, M. E. B. Menai, S. Al-Ahmadi, and A. Alnafessah, \u2018\u2018An effective med-VQA method using a transformer with weights fusi",
    "full_text_length": 142579,
    "chunk_length": 1359
  },
  {
    "chunk_id": 3552,
    "paper_filename": "suheer_2024_critical_analysis_of_benchmarks_techniques_and_models_in_medical_visual_question_answering.pdf",
    "paper_title": "Suheer 2024 Critical Analysis Of Benchmarks Techniques And Models In Medical Visual Question Answering",
    "chunk_index": 130,
    "total_chunks": 146,
    "text_content": "Recognit. (CVPR), Las Vegas, NV, USA, Jun. 2016, pp. 4995\u20135004. [196] D. Yu, J. Fu, T. Mei, and Y. Rui, \u2018\u2018Multi-level attention networks for visual question answering,\u2019\u2019 in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), Honolulu, HI, USA, Jul. 2017, pp. 4187\u20134195. [197] Z. Yu, J. Yu, J. Fan, and D. Tao, \u2018\u2018Multi-modal factorized bilinear pooling with co-attention learning for visual question answering,\u2019\u2019 in Proc. IEEE Int. Conf. Comput. Vis. (ICCV), Venice, Italy, Oct. 2017, pp. 1839\u2013184",
    "full_text_length": 142579,
    "chunk_length": 1270
  },
  {
    "chunk_id": 3553,
    "paper_filename": "suheer_2024_critical_analysis_of_benchmarks_techniques_and_models_in_medical_visual_question_answering.pdf",
    "paper_title": "Suheer 2024 Critical Analysis Of Benchmarks Techniques And Models In Medical Visual Question Answering",
    "chunk_index": 131,
    "total_chunks": 146,
    "text_content": "update (QRU),\u2019\u2019 in Proc. Adv. Neural Inf. Process. Syst., vol. 29, 2016, pp. 1\u20139. [201] B. Zhou, Y. Tian, S. Sukhbaatar, A. Szlam, and R. Fergus, \u2018\u2018Simple baseline for visual question answering,\u2019\u2019 2015, arXiv:1512.02167. [202] M. Malinowski, M. Rohrbach, and M. Fritz, \u2018\u2018Ask your neurons: A neural- based approach to answering questions about images,\u2019\u2019 in Proc. IEEE Int. Conf. Comput. Vis. (ICCV), Santiago, CL, USA, Dec. 2015, pp. 1\u20139. [203] H. Xu and K. Saenko, \u2018\u2018Ask, attend and answer: Exploring",
    "full_text_length": 142579,
    "chunk_length": 1239
  },
  {
    "chunk_id": 3554,
    "paper_filename": "suheer_2024_critical_analysis_of_benchmarks_techniques_and_models_in_medical_visual_question_answering.pdf",
    "paper_title": "Suheer 2024 Critical Analysis Of Benchmarks Techniques And Models In Medical Visual Question Answering",
    "chunk_index": 132,
    "total_chunks": 146,
    "text_content": "network,\u2019\u2019 in Proc. 13th AAAI Conf. Artif. Intell., Arizona, USA, Feb. 2016, pp. 1\u20136. [206] H. Gao, J. Mao, J. Zhou, Z. Huang, L. Wang, and W. Xu, \u2018\u2018Are you talking to a machine? Dataset and methods for multilingual image question,\u2019\u2019 in Proc. Adv. Neural Inf. Process. Syst., vol. 28, 2015, pp. 1\u20139. [207] I. Ilievski, S. Yan, and J. Feng, \u2018\u2018A focused dynamic attention model for visual question answering,\u2019\u2019 2016, arXiv:1604.01485. [208] P. Wang, Q. Wu, C. Shen, A. van den Hengel, and A. Dick, \u2018\u2018Ex",
    "full_text_length": 142579,
    "chunk_length": 1252
  },
  {
    "chunk_id": 3555,
    "paper_filename": "suheer_2024_critical_analysis_of_benchmarks_techniques_and_models_in_medical_visual_question_answering.pdf",
    "paper_title": "Suheer 2024 Critical Analysis Of Benchmarks Techniques And Models In Medical Visual Question Answering",
    "chunk_index": 133,
    "total_chunks": 146,
    "text_content": "2016, pp. 2397\u20132406. [211] L. Yu, E. Park, A. C. Berg, and T. L. Berg, \u2018\u2018Visual madlibs: Fill in the blank image generation and question answering,\u2019\u2019 2015, arXiv:1506.00278. [212] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio, \u2018\u2018Generative adversarial nets,\u2019\u2019 in Proc. Adv. Neural Inf. Process. Syst., vol. 27, 2014, pp. 1\u20139. [213] C. Zhu, Y. Zhao, S. Huang, K. Tu, and Y. Ma, \u2018\u2018Structured attentions for visual question answering,\u2019\u2019 in Proc",
    "full_text_length": 142579,
    "chunk_length": 1282
  },
  {
    "chunk_id": 3556,
    "paper_filename": "suheer_2024_critical_analysis_of_benchmarks_techniques_and_models_in_medical_visual_question_answering.pdf",
    "paper_title": "Suheer 2024 Critical Analysis Of Benchmarks Techniques And Models In Medical Visual Question Answering",
    "chunk_index": 134,
    "total_chunks": 146,
    "text_content": "Lecture Notes in Computer Science (Including Subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics), vol. 9912. Berlin, Germany: Springer Verlag, 2016, pp. 727\u2013739. [216] V. Kazemi and A. Elqursh, \u2018\u2018Show, ask, attend, and answer: A strong baseline for visual question answering,\u2019\u2019 2017, arXiv:1704.03162. [217] X. Lin and D. Parikh, \u2018\u2018Leveraging visual question answering for image- caption ranking,\u2019\u2019 in Proc. Eur. Conf. Comput. Vis. (ECCV), Amsterdam, The Netherlan",
    "full_text_length": 142579,
    "chunk_length": 1361
  },
  {
    "chunk_id": 3557,
    "paper_filename": "suheer_2024_critical_analysis_of_benchmarks_techniques_and_models_in_medical_visual_question_answering.pdf",
    "paper_title": "Suheer 2024 Critical Analysis Of Benchmarks Techniques And Models In Medical Visual Question Answering",
    "chunk_index": 135,
    "total_chunks": 146,
    "text_content": "C. Buehler, D. Teney, M. Johnson, S. Gould, and L. Zhang, \u2018\u2018Bottom-up and top-down attention for image captioning and visual question answering,\u2019\u2019 in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., Honolulu, HI, USA, Jun. 2018, pp. 6077\u20136086. [221] J. Song, P. Zeng, L. Gao, and H. T. Shen, \u2018\u2018From pixels to objects: Cubic visual attention for visual question answering,\u2019\u2019 in Proc. 27th Int. Joint Conf. Artif. Intell., Stockholm, Sweden, Jul. 2018, pp. 906\u2013912. [222] A. Osman and W. Samek, \u2018\u2018D",
    "full_text_length": 142579,
    "chunk_length": 1266
  },
  {
    "chunk_id": 3558,
    "paper_filename": "suheer_2024_critical_analysis_of_benchmarks_techniques_and_models_in_medical_visual_question_answering.pdf",
    "paper_title": "Suheer 2024 Critical Analysis Of Benchmarks Techniques And Models In Medical Visual Question Answering",
    "chunk_index": 136,
    "total_chunks": 146,
    "text_content": "111, pp. 51\u201357, Aug. 2018. [225] P. Gao, H. Li, S. Li, P. Lu, Y. Li, S. C. H. Hoi, and X. Wang, \u2018\u2018Question- guided hybrid convolution for visual question answering,\u2019\u2019 in Proc. Eur. Conf. Comput. Vis. (ECCV), Munich, Germany, Sep. 2018, pp. 469\u2013485. [226] J. Liang, L. Jiang, L. Cao, L.-J. Li, and A. Hauptmann, \u2018\u2018Focal visual- text attention for visual question answering,\u2019\u2019 in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., Salt Lake City, UT, USA, Jun. 2018, pp. 6135\u20136143. [227] D. Teney, P.",
    "full_text_length": 142579,
    "chunk_length": 1213
  },
  {
    "chunk_id": 3559,
    "paper_filename": "suheer_2024_critical_analysis_of_benchmarks_techniques_and_models_in_medical_visual_question_answering.pdf",
    "paper_title": "Suheer 2024 Critical Analysis Of Benchmarks Techniques And Models In Medical Visual Question Answering",
    "chunk_index": 137,
    "total_chunks": 146,
    "text_content": "D. Cai, Y. Chen, and J. Li, \u2018\u2018Learning visual knowledge memory networks for visual question answering,\u2019\u2019 in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., Salt Lake City, UT, USA, Jun. 2018, pp. 7736\u20137745. [230] W. Zhang, J. Yu, H. Hu, H. Hu, and Z. Qin, \u2018\u2018Multimodal feature fusion by relational reasoning and attention for visual question answering,\u2019\u2019 Inf. Fusion, vol. 55, pp. 116\u2013126, Mar. 2020. VOLUME 11, 2023 136539 S. Al-Hadhrami et al.: Critical Analysis of Benchmarks, Techniques, and",
    "full_text_length": 142579,
    "chunk_length": 1252
  },
  {
    "chunk_id": 3560,
    "paper_filename": "suheer_2024_critical_analysis_of_benchmarks_techniques_and_models_in_medical_visual_question_answering.pdf",
    "paper_title": "Suheer 2024 Critical Analysis Of Benchmarks Techniques And Models In Medical Visual Question Answering",
    "chunk_index": 138,
    "total_chunks": 146,
    "text_content": "[233] S. Hashemi Hosseinabad, M. Safayani, and A. Mirzaei, \u2018\u2018Multiple answers to a question: A new approach for visual question answering,\u2019\u2019 Vis. Comput., vol. 37, no. 1, pp. 119\u2013131, Jan. 2021. [234] Z. Bai, Y. Li, M. Wo\u017aniak, M. Zhou, and D. Li, \u2018\u2018DecomVQANet: Decomposing visual question answering deep network via tensor decomposition and regression,\u2019\u2019 Pattern Recognit., vol. 110, Feb. 2021, Art. no. 107538. [235] W. Zhang, J. Yu, Y. Wang, and W. Wang, \u2018\u2018Multimodal deep fusion for image questi",
    "full_text_length": 142579,
    "chunk_length": 1234
  },
  {
    "chunk_id": 3561,
    "paper_filename": "suheer_2024_critical_analysis_of_benchmarks_techniques_and_models_in_medical_visual_question_answering.pdf",
    "paper_title": "Suheer 2024 Critical Analysis Of Benchmarks Techniques And Models In Medical Visual Question Answering",
    "chunk_index": 139,
    "total_chunks": 146,
    "text_content": "IEEE Trans. Cybern., vol. 52, no. 6, pp. 4520\u20134533, Jun. 2022. [238] L. Gao, L. Cao, X. Xu, J. Shao, and J. Song, \u2018\u2018Question-led object attention for visual question answering,\u2019\u2019 Neurocomputing, vol. 391, pp. 227\u2013233, May 2020. [239] H. Zhong, J. Chen, C. Shen, H. Zhang, J. Huang, and X.-S. Hua, \u2018\u2018Self- adaptive neural module transformer for visual question answering,\u2019\u2019 IEEE Trans. Multimedia, vol. 23, pp. 1264\u20131273, 2021. [240] J. Hong, S. Park, and H. Byun, \u2018\u2018Selective residual learning for vi",
    "full_text_length": 142579,
    "chunk_length": 1281
  },
  {
    "chunk_id": 3562,
    "paper_filename": "suheer_2024_critical_analysis_of_benchmarks_techniques_and_models_in_medical_visual_question_answering.pdf",
    "paper_title": "Suheer 2024 Critical Analysis Of Benchmarks Techniques And Models In Medical Visual Question Answering",
    "chunk_index": 140,
    "total_chunks": 146,
    "text_content": "AAAI Conf. Artif. Intell., New Orleans, LA, USA, Feb. 2018, pp. 1\u20136. [243] K. R. Chandu, M. A. Pyreddy, M. Felix, and N. N. Joshi, \u2018\u2018Textually enriched neural module networks for visual question answering,\u2019\u2019 2018, arXiv:1809.08697. [244] J. Singh, V. Ying, and A. Nutkiewicz, \u2018\u2018Attention on attention: Architec- tures for visual question answering (VQA),\u2019\u2019 2018, arXiv:1803.07724. [245] L. Peng, Y. Yang, Y. Bin, N. Xie, F. Shen, Y. Ji, and X. Xu, \u2018\u2018Word- to-region attention network for visual quest",
    "full_text_length": 142579,
    "chunk_length": 1290
  },
  {
    "chunk_id": 3563,
    "paper_filename": "suheer_2024_critical_analysis_of_benchmarks_techniques_and_models_in_medical_visual_question_answering.pdf",
    "paper_title": "Suheer 2024 Critical Analysis Of Benchmarks Techniques And Models In Medical Visual Question Answering",
    "chunk_index": 141,
    "total_chunks": 146,
    "text_content": "D. Yu, X. Gao, and H. Xiong, \u2018\u2018Structured semantic representation for visual question answering,\u2019\u2019 in Proc. 25th IEEE Int. Conf. Image Process. (ICIP), Athens, Greece, Oct. 2018, pp. 2286\u20132290. [249] M. Malinowski, C. Doersch, A. Santoro, and P. Battaglia, \u2018\u2018Learning visual question answering by bootstrapping hard attention,\u2019\u2019 in Proc. Eur. Conf. Comput. Vis. (ECCV), Munich, Germany, Sep. 2018, pp. 3\u201320. [250] N. Ruwa, Q. Mao, L. Wang, J. Gou, and M. Dong, \u2018\u2018Mood-aware visual question answering,",
    "full_text_length": 142579,
    "chunk_length": 1261
  },
  {
    "chunk_id": 3564,
    "paper_filename": "suheer_2024_critical_analysis_of_benchmarks_techniques_and_models_in_medical_visual_question_answering.pdf",
    "paper_title": "Suheer 2024 Critical Analysis Of Benchmarks Techniques And Models In Medical Visual Question Answering",
    "chunk_index": 142,
    "total_chunks": 146,
    "text_content": "Yang, S. Feng, D. Li, H. Shen, G. Wang, and B. Jiang, \u2018\u2018Learning content and context with language bias for visual question answering,\u2019\u2019 inProc. IEEE Int. Conf. Multimedia Expo (ICME), Jul. 2021, pp. 1\u20136. [254] Z. Liang, H. Hu, and J. Zhu, \u2018\u2018LPF: A language-prior feedback objective function for de-biased visual question answering,\u2019\u2019 in Proc. 44th Int. ACM SIGIR Conf. Res. Develop. Inf. Retr., Jul. 2021, pp. 1955\u20131959.[255] R. Cadene, C. Dancette, M. Cord, and D. Parikh, \u2018\u2018RUBi: Reducing unimodal",
    "full_text_length": 142579,
    "chunk_length": 1318
  },
  {
    "chunk_id": 3565,
    "paper_filename": "suheer_2024_critical_analysis_of_benchmarks_techniques_and_models_in_medical_visual_question_answering.pdf",
    "paper_title": "Suheer 2024 Critical Analysis Of Benchmarks Techniques And Models In Medical Visual Question Answering",
    "chunk_index": 143,
    "total_chunks": 146,
    "text_content": "research interests include computer vision, NLP, machine learning, and assistive technology. MOHAMED EL BACHIR MENAI received the Ph.D. degree in computer science from the Men- touri University of Constantine, Algeria, and the University of Paris VIII, France, in 2005, and the Ph.D. degree Habilitation Universitaire in computer science from the Mentouri University of Constantine, in 2007 (it is the highest academic qualification in Algeria, France, and Germany). He is currently a Professor with ",
    "full_text_length": 142579,
    "chunk_length": 1347
  },
  {
    "chunk_id": 3566,
    "paper_filename": "suheer_2024_critical_analysis_of_benchmarks_techniques_and_models_in_medical_visual_question_answering.pdf",
    "paper_title": "Suheer 2024 Critical Analysis Of Benchmarks Techniques And Models In Medical Visual Question Answering",
    "chunk_index": 144,
    "total_chunks": 146,
    "text_content": "AI for healthcare, the IoT security, and adversarial machine learning. AHMED ALNAFESSAH is the general manager of the Smart Cities Technologies Institute at King Abdulaziz City for Science and Technology (KACST). He is also an AI Lead in the Centre for the C4IR KSA in Affiliation with the World Economic Forum WEF. He was a Senior AI and Cloud Computing Engineer at the National Centre for AI and Big Data Technologies, KACST, from 2009 to 2017. His research interests include performance engineerin",
    "full_text_length": 142579,
    "chunk_length": 1266
  },
  {
    "chunk_id": 3567,
    "paper_filename": "suheer_2024_critical_analysis_of_benchmarks_techniques_and_models_in_medical_visual_question_answering.pdf",
    "paper_title": "Suheer 2024 Critical Analysis Of Benchmarks Techniques And Models In Medical Visual Question Answering",
    "chunk_index": 145,
    "total_chunks": 146,
    "text_content": "a team member who developed the AI DevOps framework called RADON. This DevOps framework helps the European software industry to adopt serverless function as a service (FaaS) technology while avoiding lock-in within a specific FaaS provider by utilizing AI/ML and DevOps. 136540 VOLUME 11, 2023",
    "full_text_length": 142579,
    "chunk_length": 293
  },
  {
    "chunk_id": 3568,
    "paper_filename": "teo_2024_towards_generalist_biomedical_ai.pdf",
    "paper_title": "Teo 2024 Towards Generalist Biomedical Ai",
    "chunk_index": 0,
    "total_chunks": 141,
    "text_content": "Towards Generalist Biomedical AI Tao Tu\u2217,\u2021, 1, Shekoofeh Azizi\u2217,\u2021, 2, Danny Driess2, Mike Schaekermann1, Mohamed Amin1, Pi-Chuan Chang1, Andrew Carroll1, Chuck Lau1, Ryutaro Tanno2, Ira Ktena2, Basil Mustafa2, Aakanksha Chowdhery2, Yun Liu1, Simon Kornblith2, David Fleet2, Philip Mansfield1, Sushant Prakash1, Renee Wong1, Sunny Virmani1, Christopher Semturs1, S Sara Mahdavi2, Bradley Green1, Ewa Dominowska1, Blaise Aguera y Arcas1, Joelle Barral2, Dale Webster1, Greg S. Corrado1, Yossi Matias1, ",
    "full_text_length": 143742,
    "chunk_length": 1542
  },
  {
    "chunk_id": 3569,
    "paper_filename": "teo_2024_towards_generalist_biomedical_ai.pdf",
    "paper_title": "Teo 2024 Towards Generalist Biomedical Ai",
    "chunk_index": 1,
    "total_chunks": 141,
    "text_content": "radiology report generation and summarization, and genomic variant calling. We then introduce Med-PaLM Multimodal (Med-PaLM M), our proof of concept for a generalist biomedical AI system. Med-PaLM M is a large multimodal generative model that flexibly encodes and interprets biomedical data including clinical language, imaging, and genomics with the same set of model weights . Med-PaLM M reaches performance competitive with or exceeding the state of the art on all MultiMedBench tasks, often surpa",
    "full_text_length": 143742,
    "chunk_length": 1400
  },
  {
    "chunk_id": 3570,
    "paper_filename": "teo_2024_towards_generalist_biomedical_ai.pdf",
    "paper_title": "Teo 2024 Towards Generalist Biomedical Ai",
    "chunk_index": 2,
    "total_chunks": 141,
    "text_content": "over those produced by radiologists in up to 40.50% of cases, suggesting potential clinical utility. While considerable work is needed to validate these models in real-world use cases, our results represent a milestone towards the development of generalist biomedical AI systems. 1 Introduction Medicine is a multimodal discipline. Clinicians routinely interpret data from a wide range of modalities including clinical notes, laboratory tests, vital signs and observations, medical images, genomics, ",
    "full_text_length": 143742,
    "chunk_length": 1439
  },
  {
    "chunk_id": 3571,
    "paper_filename": "teo_2024_towards_generalist_biomedical_ai.pdf",
    "paper_title": "Teo 2024 Towards Generalist Biomedical Ai",
    "chunk_index": 3,
    "total_chunks": 141,
    "text_content": "a pre-specified set of possible classifications. It cannot verbally explain its prediction or engage in a collaborative dialogue to learn from a physician\u2019s feedback. This bounds performance and utility of these narrow, single-task, unimodal, specialist AI systems in real-world applications. The emergence of foundation models [5] offers an opportunity to rethink the development of medical AI systems. These models are often trained on large-scale data with self-supervised or unsupervised objectiv",
    "full_text_length": 143742,
    "chunk_length": 1602
  },
  {
    "chunk_id": 3572,
    "paper_filename": "teo_2024_towards_generalist_biomedical_ai.pdf",
    "paper_title": "Teo 2024 Towards Generalist Biomedical Ai",
    "chunk_index": 4,
    "total_chunks": 141,
    "text_content": "Medical Knowledge Pathology Mammography Dermatology Medical Question Answering Medical Visual Question Answering Medical Image Classification Radiology Report Generation Genomic Variant Calling Radiology Report Summarization MultiMedBench modalities and tasks Genomic Variant Calling Radio Report Generation Radiology Report Summarization Medical Question Answering Visual Question Answering Mammography Classification Dermatology Classification Best Prior Specialist Model Capability Med-PaLM M Capa",
    "full_text_length": 143742,
    "chunk_length": 1514
  },
  {
    "chunk_id": 3573,
    "paper_filename": "teo_2024_towards_generalist_biomedical_ai.pdf",
    "paper_title": "Teo 2024 Towards Generalist Biomedical Ai",
    "chunk_index": 5,
    "total_chunks": 141,
    "text_content": "on all tasks in MultiMedBench. Notably, Med-PaLM M achieves this using a single set of model weights, without any task-specific customization. tasks. As the pace of biomedical data generation and innovation increases, so will the potential impact of such models, with a breadth of possible downstream applications spanning fundamental biomedical discovery to care delivery. In this work, we detail our progress towards such a generalist biomedical AI system - a unified model that can interpret multi",
    "full_text_length": 143742,
    "chunk_length": 1400
  },
  {
    "chunk_id": 3574,
    "paper_filename": "teo_2024_towards_generalist_biomedical_ai.pdf",
    "paper_title": "Teo 2024 Towards Generalist Biomedical Ai",
    "chunk_index": 6,
    "total_chunks": 141,
    "text_content": "genomic variant calling. We leverage MultiMedBench to design and develop Med-PaLM Multimodal (Med-PaLM M), a large-scale generalist biomedical AI system building on the recent advances in language [8, 9] and multimodal foundation models [10, 11]. In particular, Med-PaLM M is a flexible multimodal sequence-to-sequence architecture that can easily incorporate and interleave various types of multimodal biomedical information. Further, the expressiveness of the modality-agnostic language decoder ena",
    "full_text_length": 143742,
    "chunk_length": 1387
  },
  {
    "chunk_id": 3575,
    "paper_filename": "teo_2024_towards_generalist_biomedical_ai.pdf",
    "paper_title": "Teo 2024 Towards Generalist Biomedical Ai",
    "chunk_index": 7,
    "total_chunks": 141,
    "text_content": "state-of-the-art on chest X-ray (CXR) report generation (MIMIC-CXR dataset) by over 8% on the common success metric (micro-F1) for clinical efficacy. On one of the medical visual question answering tasks (Slake-VQA [12]) in MultiMedBench, Med-PaLM M outperforms the prior SOTA results by over 10% on the BLEU-1 and F1 metrics. |2 We perform ablation studies to understand the importance of scale in our generalist multimodal biomedical models and observe significant benefits for tasks that require h",
    "full_text_length": 143742,
    "chunk_length": 1512
  },
  {
    "chunk_id": 3576,
    "paper_filename": "teo_2024_towards_generalist_biomedical_ai.pdf",
    "paper_title": "Teo 2024 Towards Generalist Biomedical Ai",
    "chunk_index": 8,
    "total_chunks": 141,
    "text_content": "data access for training such models, validating performance in real world applications, and understanding the safety implications. We outline these key limitations and directions of future research in our study. To summarize, our key contributions are as follows: \u2022CurationofMultiMedBench WeintroduceMultiMedBench, anewmultimodalbiomedicalbenchmark spanning multiple modalities including medical imaging, clinical text and genomics with 14 diverse tasks for training and evaluating generalist biomed",
    "full_text_length": 143742,
    "chunk_length": 1562
  },
  {
    "chunk_id": 3577,
    "paper_filename": "teo_2024_towards_generalist_biomedical_ai.pdf",
    "paper_title": "Teo 2024 Towards Generalist Biomedical Ai",
    "chunk_index": 9,
    "total_chunks": 141,
    "text_content": "observe evidence of zero-shot medical reasoning, generalization to novel medical concepts and tasks, and positive transfer across tasks. These experiments suggest promising potential of such systems in downstream data-scarce biomedical applications. \u2022Human evaluation of Med-PaLM M outputs Beyond automated metrics, we perform radiologist evaluation of chest X-ray reports generated by Med-PaLM M across different model scales. In a blinded side-by-side ranking on 246 retrospective chest X-rays, cli",
    "full_text_length": 143742,
    "chunk_length": 1344
  },
  {
    "chunk_id": 3578,
    "paper_filename": "teo_2024_towards_generalist_biomedical_ai.pdf",
    "paper_title": "Teo 2024 Towards Generalist Biomedical Ai",
    "chunk_index": 10,
    "total_chunks": 141,
    "text_content": "modalities [16]. While the idea of transfer learning [17, 18] using the weights of pretrained models has existed for decades [19\u201322], a shift has come about due to the scale of data and compute used for pretraining such models [23]. The notion of a foundation model further indicates that the model can be adapted to a wide range of downstream tasks [5]. Within the foundation model paradigm, multimodality [24] has also had a variety of important impacts \u2013 in the datasets [25], in the inter-modalit",
    "full_text_length": 143742,
    "chunk_length": 1324
  },
  {
    "chunk_id": 3579,
    "paper_filename": "teo_2024_towards_generalist_biomedical_ai.pdf",
    "paper_title": "Teo 2024 Towards Generalist Biomedical Ai",
    "chunk_index": 11,
    "total_chunks": 141,
    "text_content": "ImageNet [32]). The benefits of joint language- and-vision supervision has also been noteworthy in generative modeling of images [33], where text-to-image generative modeling has been notably more successful at producing high-fidelity image generation [34] than purely unconditioned generative image modeling [35]. Further, the flexibility of language also enables a wide range of task specifications all via one unified output space [36] \u2013 it is possible to phrase tasks traditionally addressed by d",
    "full_text_length": 143742,
    "chunk_length": 1309
  },
  {
    "chunk_id": 3580,
    "paper_filename": "teo_2024_towards_generalist_biomedical_ai.pdf",
    "paper_title": "Teo 2024 Towards Generalist Biomedical Ai",
    "chunk_index": 12,
    "total_chunks": 141,
    "text_content": "without finetuning, can excel at a wide variety of tasks. A single multitask [17] model which can address many tasks has been of long standing interest [38, 39], including for example in the reinforcement learning community [40]. Language-only models such as GPT-3 [6] and PaLM [8] simultaneously excel at many tasks using only prompting and in-context learning. Recent work has also explored generalist models capable not only of performing many tasks, but also of processing many modalities [41]. F",
    "full_text_length": 143742,
    "chunk_length": 1385
  },
  {
    "chunk_id": 3581,
    "paper_filename": "teo_2024_towards_generalist_biomedical_ai.pdf",
    "paper_title": "Teo 2024 Towards Generalist Biomedical Ai",
    "chunk_index": 13,
    "total_chunks": 141,
    "text_content": "significant interest in multimodal foundation models for different biomedical applications. Moor et al.[43] discuss the notion of generalist medical AI, albeit without implementation or empirical results. Theodoris et al.[44] introduce Geneformer, a transformer [45] based model pretrained on a corpus of about 30 million single-cell transcriptomes to enable context-specific predictions in low data network biology applications. BiomedGPT [46] is a multi-task biomedical foundation model pretrained ",
    "full_text_length": 143742,
    "chunk_length": 1415
  },
  {
    "chunk_id": 3582,
    "paper_filename": "teo_2024_towards_generalist_biomedical_ai.pdf",
    "paper_title": "Teo 2024 Towards Generalist Biomedical Ai",
    "chunk_index": 14,
    "total_chunks": 141,
    "text_content": "GPT-4 [48] to curate a multimodal instruction following dataset and finetune a LLaVA model with it. However, the experiments are limited to three medical visual question answering datasets and qualitative examples of conversations conditioned on a medical image. In contrast, our work is more comprehensive, spanning multiple modalities including medical imaging, clinical text, and genomics with 14 diverse tasks and expert evaluation of model outputs. 2.3 Multimodal medical AI benchmarks To the be",
    "full_text_length": 143742,
    "chunk_length": 1414
  },
  {
    "chunk_id": 3583,
    "paper_filename": "teo_2024_towards_generalist_biomedical_ai.pdf",
    "paper_title": "Teo 2024 Towards Generalist Biomedical Ai",
    "chunk_index": 15,
    "total_chunks": 141,
    "text_content": "is currently no implementation of a generalist biomedical AI system that can competently handle all these tasks simultaneously. 3 MultiMedBench: A Benchmark for Generalist Biomedical AI We next describe MultiMedBench, a benchmark we curated to enable the development and evaluation of generalist biomedical AI. MultiMedBench is a multi-task, multimodal benchmark comprising 12 de-identified open source datasets and 14 individual tasks. It measures the capability of a general-purpose biomedical AI t",
    "full_text_length": 143742,
    "chunk_length": 1487
  },
  {
    "chunk_id": 3584,
    "paper_filename": "teo_2024_towards_generalist_biomedical_ai.pdf",
    "paper_title": "Teo 2024 Towards Generalist Biomedical Ai",
    "chunk_index": 16,
    "total_chunks": 141,
    "text_content": "of MultiMedBench, the benchmark we introduce for the development and evaluation of Med-PaLM M. MultiMedBench consists of 14 individual tasks across 5 task types and 12 datasets spanning 7 biomedical data modalities. In total, the benchmark contains over 1 million samples. Task Type Modality Dataset Description Question Answering TextMedQA US medical licensing exam-style, multiple-choice MedMCQA Indian medical entrance exams, multiple-choice PubMedQA Biomedical literature questions, multiple-choi",
    "full_text_length": 143742,
    "chunk_length": 1694
  },
  {
    "chunk_id": 3585,
    "paper_filename": "teo_2024_towards_generalist_biomedical_ai.pdf",
    "paper_title": "Teo 2024 Towards Generalist Biomedical Ai",
    "chunk_index": 17,
    "total_chunks": 141,
    "text_content": "consist of medical question answering, including three of the MultiMedQA tasks used in Singhal et al.[9], and radiology report summarization. They were selected to assess a model\u2019s ability to comprehend, recall, and manipulate medical knowledge. Multimodal tasks include medical visual question answering (VQA), medical image classification, chest X-ray report generation, and genomic variant calling, which are well-suited to evaluate both the visual understanding and multimodal reasoning capabilit",
    "full_text_length": 143742,
    "chunk_length": 1356
  },
  {
    "chunk_id": 3586,
    "paper_filename": "teo_2024_towards_generalist_biomedical_ai.pdf",
    "paper_title": "Teo 2024 Towards Generalist Biomedical Ai",
    "chunk_index": 18,
    "total_chunks": 141,
    "text_content": "training details involved in the finetuning and specialization of the model to the biomedical domain Section 4.2. 4.1 Model preliminaries Note that Med-PaLM M inherits not only the architectures of these pretrained models, but also the general domain knowledge encoded in their model parameters. Pathways Language Model (PaLM) introduced by Chowdhery et al.[8] is a densely-connected decoder- only Transformer [45] based large language model (LLM) trained using Pathways [50], a large-scale ML accele",
    "full_text_length": 143742,
    "chunk_length": 1363
  },
  {
    "chunk_id": 3587,
    "paper_filename": "teo_2024_towards_generalist_biomedical_ai.pdf",
    "paper_title": "Teo 2024 Towards Generalist Biomedical Ai",
    "chunk_index": 19,
    "total_chunks": 141,
    "text_content": "models on a suite of multi-step reasoning tasks and exceeding average human performance on BIG-bench [51]. Vision Transformer (ViT) introduced by Dosovitskiy et al.[52] extends the Transformer [45] architecture to visual data such as images and videos. In this work, we consider two ViT pre-trained models as vision |5 encoders, the 4 billion (4B) parameters model from Chen et al.[11] and the 22 billion (22B) parameters model from Dehghani et al.[15]. Both of these models were pretrained via super",
    "full_text_length": 143742,
    "chunk_length": 1347
  },
  {
    "chunk_id": 3588,
    "paper_filename": "teo_2024_towards_generalist_biomedical_ai.pdf",
    "paper_title": "Teo 2024 Towards Generalist Biomedical Ai",
    "chunk_index": 20,
    "total_chunks": 141,
    "text_content": "Furthermore, PaLM-E offers the flexibility to interleave images, text and sensor signals in a single prompt, enabling the model to make predictions with a fully multimodal context. PaLM-E also exhibits a wide array of capabilities including zero-shot multimodal chain-of-thought (CoT) reasoning, and few-shot in-context learning. We therefore leverage the PaLM-E model as the base architecture for Med-PaLM M. We consider three different combinations of LLM and vision encoders in our study - PaLM 8B",
    "full_text_length": 143742,
    "chunk_length": 1339
  },
  {
    "chunk_id": 3589,
    "paper_filename": "teo_2024_towards_generalist_biomedical_ai.pdf",
    "paper_title": "Teo 2024 Towards Generalist Biomedical Ai",
    "chunk_index": 21,
    "total_chunks": 141,
    "text_content": "the model. Dataset and preprocessing We resized all the images in MultiMedBench to 224\u00d7224\u00d73, while preserving the original aspect ratio with padding if needed. The gray-scale images were converted to 3-channel images by stacking up the same image along the channel dimension. Task-specific prepossessing methods such as class balancing and image data augmentation are described in detail for each task in Section A.1. Instruction task prompting and one-shot exemplar Our goal is to train a generalis",
    "full_text_length": 143742,
    "chunk_length": 1312
  },
  {
    "chunk_id": 3590,
    "paper_filename": "teo_2024_towards_generalist_biomedical_ai.pdf",
    "paper_title": "Teo 2024 Towards Generalist Biomedical Ai",
    "chunk_index": 22,
    "total_chunks": 141,
    "text_content": "a question. For example, as shown in Figure 2, in the chest X-ray report generation task, we included the reason for the study and the image orientation information as additional context information for the model to condition its prediction on. Similarly, for the dermatology classification task, we provided the patient clinical history associated with the skin lesion image. We formulated all classification tasks as multiple choice questions where all possible class labels are provided as individ",
    "full_text_length": 143742,
    "chunk_length": 1323
  },
  {
    "chunk_id": 3591,
    "paper_filename": "teo_2024_towards_generalist_biomedical_ai.pdf",
    "paper_title": "Teo 2024 Towards Generalist Biomedical Ai",
    "chunk_index": 23,
    "total_chunks": 141,
    "text_content": "one-shot exemplar helps prompt the model with a partial input-output pair. Importantly, for multimodal tasks, we replaced the actual image in the exemplar with a dummy text placeholder (with the text string \u201c<img>\u201d): this (i) preserves training compute efficiency for single-image training, and also (ii) bypasses potential interference from cross-attention between a given text token and image tokens from multiple images [28]. Our results show that this scheme is effective in prompting the model t",
    "full_text_length": 143742,
    "chunk_length": 1304
  },
  {
    "chunk_id": 3592,
    "paper_filename": "teo_2024_towards_generalist_biomedical_ai.pdf",
    "paper_title": "Teo 2024 Towards Generalist Biomedical Ai",
    "chunk_index": 24,
    "total_chunks": 141,
    "text_content": "Instructions: You are a helpful radiology assistant. Describe what lines, tubes, and devices are present and each of their locations. Describe if pneumothorax is present; if present, describe size on each side. Describe if pleural e\u0166usion is present; if present, describe amount on each side. Describe if lung opacity (atelectasis, \u016fbrosis, consolidation, in\u016fltrate, lung mass, pneumonia, pulmonary edema) is present; if present, describe kinds and locations. Describe the cardiac silhoue\u0175e size. Des",
    "full_text_length": 143742,
    "chunk_length": 1311
  },
  {
    "chunk_id": 3593,
    "paper_filename": "teo_2024_towards_generalist_biomedical_ai.pdf",
    "paper_title": "Teo 2024 Towards Generalist Biomedical Ai",
    "chunk_index": 25,
    "total_chunks": 141,
    "text_content": "of the thoracic aorta. No pathologic \u016fndings in the lung parenchyma notably no evidence of \u016fbrotic lung parenchymal changes. A faint 2 mm rounded opacity projecting over the lower aspect of the fourth right rib and internally to the upper border of the second right rib is seen on the frontal radiograph only and likely re\u0172ects structure on the skin. Given the PA view X-ray image <img> . Reason for the study: History m with malaise pneumonia. Q: Describe the \u016fndings in the image following the inst",
    "full_text_length": 143742,
    "chunk_length": 1255
  },
  {
    "chunk_id": 3594,
    "paper_filename": "teo_2024_towards_generalist_biomedical_ai.pdf",
    "paper_title": "Teo 2024 Towards Generalist Biomedical Ai",
    "chunk_index": 26,
    "total_chunks": 141,
    "text_content": "Lesion elevation: false, Fitzpatrick scale: 1.0, Diameters (mm): [12.0, 8.0]. Q: Which of the following is the most likely diagnosis of the patient's skin lesion? (A) Nevus (B) Basal Cell Carcinoma (C) Squamous Cell Carcinoma (D) Actinic Keratosis (E) Seborrheic Keratosis (F) Melanoma A: Basal Cell Carcinoma. Given <img> . Patient History: Age: 39, Gender: unknown, Smoke: unknown, Drink: unknown, Family skin cancer history: unknown, Family any cancer history: unknown, Lesion region: neck, Lesion",
    "full_text_length": 143742,
    "chunk_length": 1365
  },
  {
    "chunk_id": 3595,
    "paper_filename": "teo_2024_towards_generalist_biomedical_ai.pdf",
    "paper_title": "Teo 2024 Towards Generalist Biomedical Ai",
    "chunk_index": 27,
    "total_chunks": 141,
    "text_content": "task-specific instructions, a text-only \u201cone-shot exemplar\u201d (omitting the corresponding image but preserving the target answer), and the actual question. The X-ray image is embedded and interleaved with textual context including view orientation and reason for the study in addition to the question. (bottom) shows the task prompt for the dermatology classification task. We formulate the skin lesion classification task as a multiple choice question answering task with all the class labels provided",
    "full_text_length": 143742,
    "chunk_length": 1302
  },
  {
    "chunk_id": 3596,
    "paper_filename": "teo_2024_towards_generalist_biomedical_ai.pdf",
    "paper_title": "Teo 2024 Towards Generalist Biomedical Ai",
    "chunk_index": 28,
    "total_chunks": 141,
    "text_content": "The multimodal context input contains at most 1 image for all finetuning tasks. However, we note that Med-PaLM M is able to process inputs with multiple images during inference. We used the Adafactor optimizer [58] with momentum of \u03b21= 0.9, dropout rate of 0.1, and a constant learning rate schedule. We used different sets of hyperparameters in our finetuning experiments for different model sizes, which are further detailed in Table A.2. The resulting model, Med-PaLM M (12B, 84B, and 562B), is ad",
    "full_text_length": 143742,
    "chunk_length": 1378
  },
  {
    "chunk_id": 3597,
    "paper_filename": "teo_2024_towards_generalist_biomedical_ai.pdf",
    "paper_title": "Teo 2024 Towards Generalist Biomedical Ai",
    "chunk_index": 29,
    "total_chunks": 141,
    "text_content": "We evaluated Med-PaLM M on all tasks in MultiMedBench across model scales. We provide initial insights on the effect of scaling ViT and LLM components across different tasks. We compared performance to previous SOTA (including specialist single-task or single-modality methods) and a state-of-art generalist model (PaLM-E) without biomedical finetuning. \u2022Explore novel emergent capabilities One hypothesized benefit of training a single flexible multimodal generalist AI system across diverse tasks i",
    "full_text_length": 143742,
    "chunk_length": 1506
  },
  {
    "chunk_id": 3598,
    "paper_filename": "teo_2024_towards_generalist_biomedical_ai.pdf",
    "paper_title": "Teo 2024 Towards Generalist Biomedical Ai",
    "chunk_index": 30,
    "total_chunks": 141,
    "text_content": "on a mixture of language-only and multimodal biomedical tasks in MultiMedBench. We assessed the model\u2019s in-distribution performance on these tasks by comparing to the corresponding SOTA results obtained from separate specialist models. Specifically, we used the same few-shot setup as in training for each task during evaluation. Task-specific metrics were computed on the test split of each task and compared to prior SOTA specialist AI systems. Note that for a small number of tasks described in Ta",
    "full_text_length": 143742,
    "chunk_length": 1263
  },
  {
    "chunk_id": 3599,
    "paper_filename": "teo_2024_towards_generalist_biomedical_ai.pdf",
    "paper_title": "Teo 2024 Towards Generalist Biomedical Ai",
    "chunk_index": 31,
    "total_chunks": 141,
    "text_content": "are normal cases and 58 cases have manifestations of TB [59]. Each case also contains annotations on the abnormality seen in the lung. We note that Med-PaLM M has been trained on MIMIC-CXR dataset; however, it is not trained to explicitly predict the TB disease label. We evaluated the accuracy across model scales by formulating this problem as a two-choice question answering task where the model was prompted (with a text-only one-shot exemplar) to generate a yes/no answer about the presence of T",
    "full_text_length": 143742,
    "chunk_length": 1268
  },
  {
    "chunk_id": 3600,
    "paper_filename": "teo_2024_towards_generalist_biomedical_ai.pdf",
    "paper_title": "Teo 2024 Towards Generalist Biomedical Ai",
    "chunk_index": 32,
    "total_chunks": 141,
    "text_content": "placeholder instead) and the text exemplar was hand-crafted rather than drawn from the training set. Hence, this approach can be considered zero-shot rather than one-shot. In order to assess Med-PaLM M\u2019s ability to generalize to novel task scenarios, we evaluated the model performance on two-view chest X-ray report generation - this is a novel task given the model was trained to generate reports only from a single-view chest X-ray. Finally, we also probed for evidence of positive task transfer a",
    "full_text_length": 143742,
    "chunk_length": 1276
  },
  {
    "chunk_id": 3601,
    "paper_filename": "teo_2024_towards_generalist_biomedical_ai.pdf",
    "paper_title": "Teo 2024 Towards Generalist Biomedical Ai",
    "chunk_index": 33,
    "total_chunks": 141,
    "text_content": "performance in the latter. 5.3 Clinician evaluation of radiology report generation To further assess the quality and clinical applicability of chest X-ray reports generated by Med-PaLM M and understand the effect of model scaling, we conducted a human evaluation using the MIMIC-CXR dataset. The evaluation was performed by four qualified thoracic radiologists based in India. Dataset The evaluation set consisted of 246 cases selected from the MIMIC-CXR test split. To match the expected input forma",
    "full_text_length": 143742,
    "chunk_length": 1317
  },
  {
    "chunk_id": 3602,
    "paper_filename": "teo_2024_towards_generalist_biomedical_ai.pdf",
    "paper_title": "Teo 2024 Towards Generalist Biomedical Ai",
    "chunk_index": 34,
    "total_chunks": 141,
    "text_content": "we iterated upon the instructions for the raters and calibrated their grades using a pilot set of 25 cases that were distinct from the evaluation set. Side-by-side evaluation was performed for all 246 cases, where each case was rated by a single radiologist randomly selected from a pool of four. For independent evaluation, each of the four radiologists independently annotated findings generated by three Med-PaLM M model variants (12B, 84B, and 562B) for every case in the evaluation set. Radiolog",
    "full_text_length": 143742,
    "chunk_length": 1303
  },
  {
    "chunk_id": 3603,
    "paper_filename": "teo_2024_towards_generalist_biomedical_ai.pdf",
    "paper_title": "Teo 2024 Towards Generalist Biomedical Ai",
    "chunk_index": 35,
    "total_chunks": 141,
    "text_content": "the dataset reference report\u2019s findings, and findings generated by three Med-PaLM M model variants (12B, 84B, 562B). Raters were asked to rank the four alternative findings based on their overall quality using their best clinical judgement. Independent evaluation For independent evaluation, raters were also presented with a single chest X-ray, along with the indication and reference report\u2019s findings from the MIMIC-CXR study (marked explicitly as such), but this time only a single findings parag",
    "full_text_length": 143742,
    "chunk_length": 1320
  },
  {
    "chunk_id": 3604,
    "paper_filename": "teo_2024_towards_generalist_biomedical_ai.pdf",
    "paper_title": "Teo 2024 Towards Generalist Biomedical Ai",
    "chunk_index": 36,
    "total_chunks": 141,
    "text_content": "Next, they annotated all passages in the model-generated findings that they disagreed with (errors), and all missing parts (omissions). Raters categorized each error passage by its type (no finding, incorrect finding location, incorrect severity, reference to non-existent view or prior study), assessed its clinical significance, and suggested alternative text to replace the selected passage. Likewise, for each omission, raters specified a passage that should have been included and determined if ",
    "full_text_length": 143742,
    "chunk_length": 1323
  },
  {
    "chunk_id": 3605,
    "paper_filename": "teo_2024_towards_generalist_biomedical_ai.pdf",
    "paper_title": "Teo 2024 Towards Generalist Biomedical Ai",
    "chunk_index": 37,
    "total_chunks": 141,
    "text_content": "constraints. Results are summarized in Table 2. Across MultiMedBench tasks, Med-PaLM M\u2019s best result (across three model sizes) exceeded prior SOTA results on 5 out of 12 tasks (for two tasks, we were unable to find a prior SOTA comparable to our setup) while being competitive on the rest. Notably, these results were achieved with a generalist model using the same set of model weights without any task-specific architecture customization or optimization. On medical question answering tasks, we co",
    "full_text_length": 143742,
    "chunk_length": 1278
  },
  {
    "chunk_id": 3606,
    "paper_filename": "teo_2024_towards_generalist_biomedical_ai.pdf",
    "paper_title": "Teo 2024 Towards Generalist Biomedical Ai",
    "chunk_index": 38,
    "total_chunks": 141,
    "text_content": "improvements on all 14 tasks often by a significant margin, demonstrating the importance of domain adaptation. Taken together, these results illustrate the strong capabilities of Med-PaLM M as a generalist biomedical AI model. We further describe the results in detail for each of the individual tasks in Section A.3. |9 Table 2 |Performance comparison on MultiMedBench. We compare Med-PaLM M with specialist SOTA models and a generalist model (PaLM-E 84B) without biomedical domain finetuning. Acros",
    "full_text_length": 143742,
    "chunk_length": 1429
  },
  {
    "chunk_id": 3607,
    "paper_filename": "teo_2024_towards_generalist_biomedical_ai.pdf",
    "paper_title": "Teo 2024 Towards Generalist Biomedical Ai",
    "chunk_index": 39,
    "total_chunks": 141,
    "text_content": "16.20% [62] 0.34% 15.36% F1-RadGraph 40.80% [62] 8.00% 34.71% Visual Question AnsweringRadiologyVQA-RADBLEU-1 71.03% [63] 59.19% 71.27% F1 N/A 38.67% 62.06% Slake-VQABLEU-1 78.60% [64] 52.65% 92.7% F1 78.10% [64] 24.53% 89.28% Pathology Path-VQABLEU-1 70.30% [64] 54.92% 72.27% F1 58.40% [64] 29.68% 62.69% Report Generation Chest X-ray MIMIC-CXRMicro-F1-14 44.20% [65] 15.40% 53.56% Macro-F1-14 30.70% [65] 10.11% 39.83% Micro-F1-5 56.70% [66] 5.51% 57.88% Macro-F1-5 N/A 4.85% 51.60% F1-RadGraph 24",
    "full_text_length": 143742,
    "chunk_length": 1533
  },
  {
    "chunk_id": 3608,
    "paper_filename": "teo_2024_towards_generalist_biomedical_ai.pdf",
    "paper_title": "Teo 2024 Towards Generalist Biomedical Ai",
    "chunk_index": 40,
    "total_chunks": 141,
    "text_content": "99.70% [71] 52.84% 99.35% Med-PaLM M performance across model scales We summarize Med-PaLM M performance across model scales (12B, 84B, and 562B) in Table 3. The key observations are: \u2022Language reasoning tasks benefit from scale For tasks that require language understanding and reasoning such as medical question answering, medical visual question answering and radiology report summarization, we see significant improvements as we scale up the model from 12B to 562B. \u2022Multimodal tasks bottlenecked",
    "full_text_length": 143742,
    "chunk_length": 1311
  },
  {
    "chunk_id": 3609,
    "paper_filename": "teo_2024_towards_generalist_biomedical_ai.pdf",
    "paper_title": "Teo 2024 Towards Generalist Biomedical Ai",
    "chunk_index": 41,
    "total_chunks": 141,
    "text_content": "acting as a bottleneck to observing a scaling benefit. We note the possibility of additional confounders here such as the input image resolution. The scaling results on the chest X-ray report generation task are interesting (Table 3). While on the surface, the task seems to require complex language understanding and reasoning capabilities and would thus benefit |10 Table 3 |Performance of Med-PaLM M on MultiMedBench across model scales. We summarize the performance of Med-PaLM M across three mod",
    "full_text_length": 143742,
    "chunk_length": 1415
  },
  {
    "chunk_id": 3610,
    "paper_filename": "teo_2024_towards_generalist_biomedical_ai.pdf",
    "paper_title": "Teo 2024 Towards Generalist Biomedical Ai",
    "chunk_index": 42,
    "total_chunks": 141,
    "text_content": "69.68% MedMCQA Accuracy 32.20% 47.60% 62.59% PubMedQA Accuracy 48.60% 71.40% 80.00% Report Summarization Radiology MIMIC-IIIROUGE-L 29.45% 31.47% 32.03% BLEU 12.14% 15.36% 15.21% F1-RadGraph 31.43% 33.96% 34.71% Visual Question AnsweringRadiologyVQA-RADBLEU-1 64.02% 69.38% 71.27% F1 50.66% 59.90% 62.06% Slake-VQABLEU-1 90.77% 92.70% 91.64% F1 86.22% 89.28% 87.50% Pathology Path-VQABLEU-1 68.97% 70.16% 72.27% F1 57.24% 59.51% 62.69% Report Generation Chest X-ray MIMIC-CXRMicro-F1-14 51.41% 53.56%",
    "full_text_length": 143742,
    "chunk_length": 1569
  },
  {
    "chunk_id": 3611,
    "paper_filename": "teo_2024_towards_generalist_biomedical_ai.pdf",
    "paper_title": "Teo 2024 Towards Generalist Biomedical Ai",
    "chunk_index": 43,
    "total_chunks": 141,
    "text_content": "from scaling the language model, we find the Med-PaLM M 84B model to be roughly on-par or slightly exceeding the 562B model on a majority of metrics, which may simply be due to fewer training steps used for the larger model. Another possibility for the diminishing return of increasing the size of language model is likely that the output space for chest X-ray report generation in the MIMIC-CXR dataset is fairly confined to a set of template sentences and limited number of conditions. This insight",
    "full_text_length": 143742,
    "chunk_length": 1271
  },
  {
    "chunk_id": 3612,
    "paper_filename": "teo_2024_towards_generalist_biomedical_ai.pdf",
    "paper_title": "Teo 2024 Towards Generalist Biomedical Ai",
    "chunk_index": 44,
    "total_chunks": 141,
    "text_content": "medical tasks and concepts Training a generalist biomedical AI system with language as a common grounding across different tasks allows the system to tackle new tasks by combining the knowledge it has learned for other tasks (i.e. combinatorial |11 generalization). We highlight preliminary evidence which suggests Med-PaLM M can generalize to novel medical concepts and unseen tasks in a zero-shot fashion. We further observe zero-shot multimodal reasoning as an emergent capability [13] of Med-PaLM",
    "full_text_length": 143742,
    "chunk_length": 1355
  },
  {
    "chunk_id": 3613,
    "paper_filename": "teo_2024_towards_generalist_biomedical_ai.pdf",
    "paper_title": "Teo 2024 Towards Generalist Biomedical Ai",
    "chunk_index": 45,
    "total_chunks": 141,
    "text_content": "ensemble model optimized for this dataset [74]. We observed similar performance across three model variants, consistent with findings on other medical image classification tasks in MultiMedBench. Given the classification task was set up as an open-ended question answering task, we did not report the AUC metric which requires the normalized predicted probability of each possible class. Table 4 |Zero-shot classification performance of Med-PaLM M on the tuberculosis (TB) detection task. Med-PaLM M ",
    "full_text_length": 143742,
    "chunk_length": 1321
  },
  {
    "chunk_id": 3614,
    "paper_filename": "teo_2024_towards_generalist_biomedical_ai.pdf",
    "paper_title": "Teo 2024 Towards Generalist Biomedical Ai",
    "chunk_index": 46,
    "total_chunks": 141,
    "text_content": "0 87.68% 6.2.2 Evidence of emergent zero-shot multimodal medical reasoning We also qualitatively explored the zero-shot chain-of-thought (CoT) capability of Med-PaLM M on the MC TB dataset. In contrast to the classification setup, we prompted the model with a text-only exemplar to generate a report describing the findings in a given image in addition to a yes/no classification prediction. In Figure 3, we present qualitative examples of zero-shot CoT reasoning from the Med-PaLM M 84B and 562B var",
    "full_text_length": 143742,
    "chunk_length": 1292
  },
  {
    "chunk_id": 3615,
    "paper_filename": "teo_2024_towards_generalist_biomedical_ai.pdf",
    "paper_title": "Teo 2024 Towards Generalist Biomedical Ai",
    "chunk_index": 47,
    "total_chunks": 141,
    "text_content": "zero-shot CoT multimodal reasoning capability (i.e. this might be an emergent capability [13]). 6.2.3 Evidence of generalization to novel tasks Although Med-PaLM M was only trained with single-view chest X-ray image inputs, we observed the capability of the model to generalize to a novel task setup with multi-view visual inputs. Specifically, on a subset of studies from MIMIC-CXR where each report is accompanied with both a frontal and a lateral view X-ray image. we observe that Med-PaLM M is ab",
    "full_text_length": 143742,
    "chunk_length": 1280
  },
  {
    "chunk_id": 3616,
    "paper_filename": "teo_2024_towards_generalist_biomedical_ai.pdf",
    "paper_title": "Teo 2024 Towards Generalist Biomedical Ai",
    "chunk_index": 48,
    "total_chunks": 141,
    "text_content": "a Med-PaLM M 84B variant by excluding the MIMIC-CXR classification task from the task mixture and compared this model variant against Med-PaLM M 84B trained on the full MultiMedBench mixture. As seen in Table 6, we observed that the model trained jointly on both report |12 Instructions: You are a helpful radiology assistant. The following are questions about tuberculosis vs normal chest X-rays. Solve it step by step, output a Yes/No answer and explanation. Given <img>. Q: Is it a normal chest x-",
    "full_text_length": 143742,
    "chunk_length": 1183
  },
  {
    "chunk_id": 3617,
    "paper_filename": "teo_2024_towards_generalist_biomedical_ai.pdf",
    "paper_title": "Teo 2024 Towards Generalist Biomedical Ai",
    "chunk_index": 49,
    "total_chunks": 141,
    "text_content": "and mediastinal adenopathy. There is scarring in the right middle lobe. There is no pleural effusion. Med-PaLM M 562B Incorrectness: The left lung is not clear, with a small cavitary lesion in the left upper lobe related to TB. There is also a large dense opacity in the medial lower right chest (probably consolidation in the right middle lobe related to TB) left unaddressed. This opacity partially obscures the right cardiac silhouette. Correctness: There is a cavitary lesion in the right upper l",
    "full_text_length": 143742,
    "chunk_length": 1265
  },
  {
    "chunk_id": 3618,
    "paper_filename": "teo_2024_towards_generalist_biomedical_ai.pdf",
    "paper_title": "Teo 2024 Towards Generalist Biomedical Ai",
    "chunk_index": 50,
    "total_chunks": 141,
    "text_content": "middle lobe, and right hilar and inferior mediastinal adenopathy may both allude to the large dense opacity in the medial lower right chest (probably consolidation in the right middle lobe related to TB). Figure 3 |Evidence of emergent zero-shot multimodal medical reasoning with Med-PaLM M. Large Med-PaLM M models exhibit zero-shot CoT reasoning capability in identifying and describing tuberculosis related findings in chest X-ray images. The model is prompted with task-specific instructions and ",
    "full_text_length": 143742,
    "chunk_length": 1295
  },
  {
    "chunk_id": 3619,
    "paper_filename": "teo_2024_towards_generalist_biomedical_ai.pdf",
    "paper_title": "Teo 2024 Towards Generalist Biomedical Ai",
    "chunk_index": 51,
    "total_chunks": 141,
    "text_content": "example as it also alluded to the opacity in the right middle lobe and did not make the incorrect statement of left lung being clear). Notably, Med-PaLM M 12B failed to generate a coherent report, indicating the importance of scaling for zero-shot COT reasoning. generation and classification has higher performance across the board on all report generation metrics. We also observe that the model trained only on chest X-ray report generation can generalize to abnormality detection in a zero-shot f",
    "full_text_length": 143742,
    "chunk_length": 1387
  },
  {
    "chunk_id": 3620,
    "paper_filename": "teo_2024_towards_generalist_biomedical_ai.pdf",
    "paper_title": "Teo 2024 Towards Generalist Biomedical Ai",
    "chunk_index": 52,
    "total_chunks": 141,
    "text_content": "reference human baselines). Under this evaluation framework, we observe encouraging quality of Med-PaLM M generated reports across model scales as detailed below. 6.3.1 Side-by-side evaluation In a side-by-side evaluation, four clinician raters ranked the quality of four radiology reports, comparing the radiologist-provided reference report from the MIMIC-CXR dataset with reports generated by different Med-PaLM M model scales (12B, 84B, and 562B). Figure 4a summarizes how often each rater ranked",
    "full_text_length": 143742,
    "chunk_length": 1375
  },
  {
    "chunk_id": 3621,
    "paper_filename": "teo_2024_towards_generalist_biomedical_ai.pdf",
    "paper_title": "Teo 2024 Towards Generalist Biomedical Ai",
    "chunk_index": 53,
    "total_chunks": 141,
    "text_content": "results on clinical efficacy metrics for the two view report generation task. Metric SOTA Med-PaLM M (12B) Med-PaLM M (84B) Med-PaLM M (562B) Micro-F1-14 44.20% 49.80% 50.54% 48.85% Macro-F1-14 30.70% 37.69% 37.78% 37.29% Micro-F1-5 56.70% 54.49% 56.37% 54.36% Macro-F1-5 N/A 48.33% 51.23% 48.49% F1-RadGraph 24.40% 26.73% 28.30% 27.28% BLEU-1 39.48% 33.31% 34.58% 33.83% BLEU-4 13.30% 11.51% 12.44% 12.47% ROUGE-L 29.60% 27.84% 28.71% 28.49% CIDEr-D 49.50% 27.58% 29.80% 29.80% Table 6 |Positive tas",
    "full_text_length": 143742,
    "chunk_length": 1429
  },
  {
    "chunk_id": 3622,
    "paper_filename": "teo_2024_towards_generalist_biomedical_ai.pdf",
    "paper_title": "Teo 2024 Towards Generalist Biomedical Ai",
    "chunk_index": 54,
    "total_chunks": 141,
    "text_content": "Dataset Metric Med-PaLM M (84B)Med-PaLM M (84B) No CXR classification MIMIC-CXRMicro-F1-14 53.56% 52.94% Macro-F1-14 39.83% 38.92% Micro-F1-5 57.88% 57.58% Macro-F1-5 51.60% 51.32% F1-RadGraph 26.71% 26.08% BLEU-1 32.31% 31.72% BLEU-4 11.31% 10.87% ROUGE-L 27.29% 26.67% CIDEr-D 26.17% 25.17% MIMIC-CXR (5 conditions)Macro-AUC 78.35% 73.88% Macro-F1 36.83% 43.97% which was ranked best in 25.78% of cases, and the other two model scales, 12B and 562B, which were ranked best in 19.49% and 17.59% of c",
    "full_text_length": 143742,
    "chunk_length": 1322
  },
  {
    "chunk_id": 3623,
    "paper_filename": "teo_2024_towards_generalist_biomedical_ai.pdf",
    "paper_title": "Teo 2024 Towards Generalist Biomedical Ai",
    "chunk_index": 55,
    "total_chunks": 141,
    "text_content": "34.05% and 32.00% of cases, respectively. 6.3.2 Independent evaluation We report the rates of omissions and errors radiologists identified in findings paragraphs generated by Med- PaLM M. Figure 5 provides breakdowns by model scales (12B, 84B, 562B). We observed different trends for omissions and errors. For omissions, we observed the lowest rate of 0.12 (95% CI, 0.10 - 0.15) omissions per report on average for both the Med-PaLM M 12B and 84B models, followed by 0.13 (95% CI, 0.11 - 0.16) for th",
    "full_text_length": 143742,
    "chunk_length": 1388
  },
  {
    "chunk_id": 3624,
    "paper_filename": "teo_2024_towards_generalist_biomedical_ai.pdf",
    "paper_title": "Teo 2024 Towards Generalist Biomedical Ai",
    "chunk_index": 56,
    "total_chunks": 141,
    "text_content": "84B 12B Reference Report(a)Best-ranked report in four-way comparison R1R2R3R4 31.0%43.9%27.3%25.8% 69.0%56.1%72.7%74.2% 48.7%32.4%41.5%39.5% 51.3%67.6%58.5%60.5% 37.5%43.9%40.0%14.8% 62.5%56.1%60.0%85.2%562B 84B 12B Reference Report (b)Pairwise preference of each model scale compared to reference report Figure 4 |Side-by-side human evaluation. Four clinician raters ranked the quality of four radiology reports in a side-by-side evaluation, comparing the radiologist-provided reference report from ",
    "full_text_length": 143742,
    "chunk_length": 1454
  },
  {
    "chunk_id": 3625,
    "paper_filename": "teo_2024_towards_generalist_biomedical_ai.pdf",
    "paper_title": "Teo 2024 Towards Generalist Biomedical Ai",
    "chunk_index": 57,
    "total_chunks": 141,
    "text_content": "limited to errors of clinical relevance, ensuring a specific focus on clinical interpretation. This includes those errors related to the presence, location or severity of a clinical finding. Example of non-clinical errors are passages referring to views or prior studies not present, which stem from training artifacts. These trends across model scales were identical for the subset of omissions and errors that were marked as significant by radiologist raters. We refer the reader to Table A.8 for a",
    "full_text_length": 143742,
    "chunk_length": 1279
  },
  {
    "chunk_id": 3626,
    "paper_filename": "teo_2024_towards_generalist_biomedical_ai.pdf",
    "paper_title": "Teo 2024 Towards Generalist Biomedical Ai",
    "chunk_index": 58,
    "total_chunks": 141,
    "text_content": "M 562B report to have one clinically insignificant errors and no omissions. |15 Endotracheal tube terminates 6 cm from the carina. Nasogastric tube loops within the stomach with side port within the gastric body. There are no significant pleural e\u0166usions or pneumothorax. Mild retrocardiac atelectasis is present. Heart size is normal. The aorta is tortuous. Reference Report An endotracheal tube terminates 5 cm above the carina. There is no pneumothorax or large pleural e\u0166usion. Opacities in the m",
    "full_text_length": 143742,
    "chunk_length": 1314
  },
  {
    "chunk_id": 3627,
    "paper_filename": "teo_2024_towards_generalist_biomedical_ai.pdf",
    "paper_title": "Teo 2024 Towards Generalist Biomedical Ai",
    "chunk_index": 59,
    "total_chunks": 141,
    "text_content": "and unfolded. No pleural e\u0166usions or pneumothorax. Med-PaLM M 84B Endotracheal tube terminates 5 cm above the carina. Nasogastric tube courses below the diaphragm and inferiorly beyond the film. Lung volumes are low with crowding of bronchovascular markings. There is a le\u0173 retrocardiac opacity. There is no pneumothorax or pleural e\u0166usions. Med-PaLM M 562B -year-old male with intubation. No prior examinations for comparison. Significant error Insignificant error [Omission] Indication Figure 6 |Qu",
    "full_text_length": 143742,
    "chunk_length": 1301
  },
  {
    "chunk_id": 3628,
    "paper_filename": "teo_2024_towards_generalist_biomedical_ai.pdf",
    "paper_title": "Teo 2024 Towards Generalist Biomedical Ai",
    "chunk_index": 60,
    "total_chunks": 141,
    "text_content": "and no omissions. 7 Discussion To the best of our knowledge, Med-PaLM M is the first demonstration of a generalist biomedical AI system that can interpret a wide range of medical modalities, perform competently (including near or exceeding prior SOTA) on a diverse array of tasks, and generalize to unseen biomedical concepts and tasks. This potentially opens up new possibilities in applications spanning scientific discovery to care delivery. We elaborate on the implications of this development as",
    "full_text_length": 143742,
    "chunk_length": 1326
  },
  {
    "chunk_id": 3629,
    "paper_filename": "teo_2024_towards_generalist_biomedical_ai.pdf",
    "paper_title": "Teo 2024 Towards Generalist Biomedical Ai",
    "chunk_index": 61,
    "total_chunks": 141,
    "text_content": "need. However, the benchmark has several important limitations including limited size of the individual datasets (a cumulative size of \u02dc1 million samples) and limited modality and task diversity (e.g., lacking life sciences such as transcriptomics and proteomics). Another key barrier to developing models for use across an even wider variety of biomedical data types is the lack of large scale multimodal datasets, which would permit joint learning and alignment of the modality-specific encoders wi",
    "full_text_length": 143742,
    "chunk_length": 1378
  },
  {
    "chunk_id": 3630,
    "paper_filename": "teo_2024_towards_generalist_biomedical_ai.pdf",
    "paper_title": "Teo 2024 Towards Generalist Biomedical Ai",
    "chunk_index": 62,
    "total_chunks": 141,
    "text_content": "presented by the domain overall compared to the plethora of non-medical tasks and modalities. Scaling multimodal AI models is challenging In the language domain, scaling the model has led to leapfrog improvements in performance and emergent capabilities. However, our preliminary experiments suggest this is likely more challenging for multimodal generalist models in the biomedical task domain due to the medical data scarcity. Given the wide array of modalities and tasks such generalist models are",
    "full_text_length": 143742,
    "chunk_length": 1294
  },
  {
    "chunk_id": 3631,
    "paper_filename": "teo_2024_towards_generalist_biomedical_ai.pdf",
    "paper_title": "Teo 2024 Towards Generalist Biomedical Ai",
    "chunk_index": 63,
    "total_chunks": 141,
    "text_content": "the potential key bottleneck is the vision encoder. It is possible that the small volume of medical data in MultiMedBench is not be sufficient to effectively adapt a ViT pretrained on natural images to the medical domain, thereby limiting the benefits of model scaling. As such, our study only provides some initial insights on the effect of model scaling on biomedical task performance. Future research is needed to fully understand the effect of model scaling by teasing apart the scaling effect of",
    "full_text_length": 143742,
    "chunk_length": 1304
  },
  {
    "chunk_id": 3632,
    "paper_filename": "teo_2024_towards_generalist_biomedical_ai.pdf",
    "paper_title": "Teo 2024 Towards Generalist Biomedical Ai",
    "chunk_index": 64,
    "total_chunks": 141,
    "text_content": "such as the use of one-shot training with dummy image tokens make an important difference in the quality and compute efficiency of the final model. With increasing generality of the AI system, the number of details requiring careful consideration tends to increase as well. We also note that Med-PaLM M architecture as setup currently is not optimal for few-shot in-context learning. Progress in AI for radiology report generation Our evaluation by radiologists of Med-PaLM M generated radiology repo",
    "full_text_length": 143742,
    "chunk_length": 1350
  },
  {
    "chunk_id": 3633,
    "paper_filename": "teo_2024_towards_generalist_biomedical_ai.pdf",
    "paper_title": "Teo 2024 Towards Generalist Biomedical Ai",
    "chunk_index": 65,
    "total_chunks": 141,
    "text_content": "and suggest the potential for clinical utility in the future. Generalist agents are not the only approach to multimodal biomedical AI While generalist biomedical AI systems offer exciting possibilities [43], there are other approaches to developing multimodal biomedical AI systems that might be more applicable depending on data availability, pretrained models, compute and application scenarios. These include leveraging frozen encoders with adapter layers [75] to glue together a multimodal biomed",
    "full_text_length": 143742,
    "chunk_length": 1370
  },
  {
    "chunk_id": 3634,
    "paper_filename": "teo_2024_towards_generalist_biomedical_ai.pdf",
    "paper_title": "Teo 2024 Towards Generalist Biomedical Ai",
    "chunk_index": 66,
    "total_chunks": 141,
    "text_content": "necessitates careful considerations of safety and equity in the development and validation of such systems. 8 Perspective on Generalist Biomedical AI Reaching near or above SOTA on a diverse range of biomedical tasks with a single set of model weights is a noteworthy milestone for the development of generalist biomedical AI systems. While human clinicians can train for \u201cgeneral practice\u201d [77], helpful subspecialty-specific expertise is often found in different experts [78], to whom non-specialis",
    "full_text_length": 143742,
    "chunk_length": 1340
  },
  {
    "chunk_id": 3635,
    "paper_filename": "teo_2024_towards_generalist_biomedical_ai.pdf",
    "paper_title": "Teo 2024 Towards Generalist Biomedical Ai",
    "chunk_index": 67,
    "total_chunks": 141,
    "text_content": "hints at new frontiers for impact in applications. This includes the potential for near zero-shot insight in new domains, as a tool for discovery integrating insights from distinct areas of biomedicine, and as a common point of assistance providing access to expertise from many different fields. |17 9 Conclusion Medicine is a multidisciplinary endeavour. Generalist biomedical AI systems that effectively assimilate and encode multimodal medical data at scale and rapidly adapt to new clinical cont",
    "full_text_length": 143742,
    "chunk_length": 1364
  },
  {
    "chunk_id": 3636,
    "paper_filename": "teo_2024_towards_generalist_biomedical_ai.pdf",
    "paper_title": "Teo 2024 Towards Generalist Biomedical Ai",
    "chunk_index": 68,
    "total_chunks": 141,
    "text_content": "Roy Lee, Naama Hammel, Jay Hartford, Preeti Singh, Kavita Kulkarni, Gavriel Goidel, Anil Palepu, Si Wai Man, Amy Wang, Sami Lachgar, Lauren Winer, Maggie Shiels, Annisah Um\u2019rani, John Guilyard, Shravya Shetty and Evan Rapoport for their valuable insights and feedback during our research. We are also grateful to Karen DeSalvo, Zoubin Ghahramani, James Manyika, and Jeff Dean for their support during the course of this project. Data Availability The benchmark used for training and evaluation in thi",
    "full_text_length": 143742,
    "chunk_length": 1299
  },
  {
    "chunk_id": 3637,
    "paper_filename": "teo_2024_towards_generalist_biomedical_ai.pdf",
    "paper_title": "Teo 2024 Towards Generalist Biomedical Ai",
    "chunk_index": 69,
    "total_chunks": 141,
    "text_content": "Novoa, R. A., Ko, J., Swetter, S. M., Blau, H. M. & Thrun, S. Dermatologist-level classification of skin cancer with deep neural networks. nature542,115\u2013118 (2017). 2.Gulshan, V., Peng, L., Coram, M., Stumpe, M. C., Wu, D., Narayanaswamy, A., Venugopalan, S., Widner, K., Madams, T., Cuadros, J., et al.Development and validation of a deep learning algorithm for detection of diabetic retinopathy in retinal fundus photographs. Jama316,2402\u20132410 (2016). 3.Toma\u0161ev, N., Glorot, X., Rae, J. W., Zielins",
    "full_text_length": 143742,
    "chunk_length": 1355
  },
  {
    "chunk_id": 3638,
    "paper_filename": "teo_2024_towards_generalist_biomedical_ai.pdf",
    "paper_title": "Teo 2024 Towards Generalist Biomedical Ai",
    "chunk_index": 70,
    "total_chunks": 141,
    "text_content": "Bernstein, M. S., Bohg, J., Bosselut, A., Brunskill, E., et al.On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258 (2021). 6.Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al.Language models are few-shot learners. Advances in neural information processing systems 33,1877\u20131901 (2020). 7.Azizi, S., Culp, L., Freyberg, J., Mustafa, B., Baur, S., Kornblith, S., Chen, T., Tomasev, N., Mi",
    "full_text_length": 143742,
    "chunk_length": 1368
  },
  {
    "chunk_id": 3639,
    "paper_filename": "teo_2024_towards_generalist_biomedical_ai.pdf",
    "paper_title": "Teo 2024 Towards Generalist Biomedical Ai",
    "chunk_index": 71,
    "total_chunks": 141,
    "text_content": "Language Models Encode Clinical Knowledge. arXiv preprint arXiv:2212.13138 (2022). 10.Driess, D., Xia, F., Sajjadi, M. S. M., Lynch, C., Chowdhery, A., Ichter, B., Wahid, A., Tompson, J., Vuong, Q., Yu, T., Huang, W., Chebotar, Y., Sermanet, P., Duckworth, D., Levine, S., Vanhoucke, V., Hausman, K., Toussaint, M., Greff, K., Zeng, A., Mordatch, I. & Florence, P. PaLM-E: An Embodied Multimodal Language Model inarXiv preprint arXiv:2303.03378 (2023). 11.Chen, X., Wang, X., Changpinyo, S., Piergiov",
    "full_text_length": 143742,
    "chunk_length": 1423
  },
  {
    "chunk_id": 3640,
    "paper_filename": "teo_2024_towards_generalist_biomedical_ai.pdf",
    "paper_title": "Teo 2024 Towards Generalist Biomedical Ai",
    "chunk_index": 72,
    "total_chunks": 141,
    "text_content": "Metzler, D., et al.Emergent abilities of large language models. arXiv preprint arXiv:2206.07682 (2022). 14.Jeong, J., Tian, K., Li, A., Hartung, S., Behzadi, F., Calle, J., Osayande, D., Pohlen, M., Adithan, S. & Rajpurkar, P. Multimodal image-text matching improves retrieval-based chest X-ray report generation. arXiv preprint arXiv:2303.17579 (2023). 15.Dehghani, M., Djolonga, J., Mustafa, B., Padlewski, P., Heek, J., Gilmer, J., Steiner, A., Caron, M., Geirhos, R., Alabdulmohsin, I., et al.Sca",
    "full_text_length": 143742,
    "chunk_length": 1451
  },
  {
    "chunk_id": 3641,
    "paper_filename": "teo_2024_towards_generalist_biomedical_ai.pdf",
    "paper_title": "Teo 2024 Towards Generalist Biomedical Ai",
    "chunk_index": 73,
    "total_chunks": 141,
    "text_content": "deep belief nets. Neural computation 18,1527\u20131554 (2006). 20.Bengio, Y., Lamblin, P., Popovici, D. & Larochelle, H. Greedy layer-wise training of deep networks. Advances in neural information processing systems 19(2006). 21.Vincent, P., Larochelle, H., Bengio, Y. & Manzagol, P. -A.Extracting and composing robust features with denoising autoencoders inProceedings of the 25th international conference on Machine learning (2008), 1096\u20131103. 22.Bengio, Y. Deep learning of representations for unsuperv",
    "full_text_length": 143742,
    "chunk_length": 1448
  },
  {
    "chunk_id": 3642,
    "paper_filename": "teo_2024_towards_generalist_biomedical_ai.pdf",
    "paper_title": "Teo 2024 Towards Generalist Biomedical Ai",
    "chunk_index": 74,
    "total_chunks": 141,
    "text_content": "et al.Laion-5b: An open large-scale dataset for training next generation image-text models. Advances in Neural Information Processing Systems 35,25278\u201325294 (2022). 26.Jaegle, A., Gimeno, F., Brock, A., Vinyals, O., Zisserman, A. & Carreira, J. Perceiver: General perception with iterative attention inInternational conference on machine learning (2021), 4651\u20134664. 27.Tsimpoukelli, M., Menick, J. L., Cabi, S., Eslami, S., Vinyals, O. & Hill, F. Multimodal few-shot learning with frozen language mod",
    "full_text_length": 143742,
    "chunk_length": 1419
  },
  {
    "chunk_id": 3643,
    "paper_filename": "teo_2024_towards_generalist_biomedical_ai.pdf",
    "paper_title": "Teo 2024 Towards Generalist Biomedical Ai",
    "chunk_index": 75,
    "total_chunks": 141,
    "text_content": "C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al. Learning transferable visual models from natural language supervision inInternational conference on machine learning (2021), 8748\u20138763. |19 31.Thomee, B., Shamma, D. A., Friedland, G., Elizalde, B., Ni, K., Poland, D., Borth, D. & Li, L. -J. YFCC100M: The new data in multimedia research. Communications of the ACM 59,64\u201373 (2016). 32.Deng, J., Dong, W., Socher, R., Li, L. -J., Li, K. & Fei-Fei, L. Imagene",
    "full_text_length": 143742,
    "chunk_length": 1394
  },
  {
    "chunk_id": 3644,
    "paper_filename": "teo_2024_towards_generalist_biomedical_ai.pdf",
    "paper_title": "Teo 2024 Towards Generalist Biomedical Ai",
    "chunk_index": 76,
    "total_chunks": 141,
    "text_content": "in Neural Information Processing Systems 35,36479\u201336494 (2022). 35.Dhariwal, P. & Nichol, A. Diffusion models beat gans on image synthesis. Advances in neural information processing systems34,8780\u20138794 (2021). 36.Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et al.Language models are unsupervised multitask learners. OpenAI blog 1,9 (2019). 37.Chen, T., Saxena, S., Li, L., Fleet, D. J. & Hinton, G. Pix2seq: A language modeling framework for object detection. arXiv preprint ",
    "full_text_length": 143742,
    "chunk_length": 1434
  },
  {
    "chunk_id": 3645,
    "paper_filename": "teo_2024_towards_generalist_biomedical_ai.pdf",
    "paper_title": "Teo 2024 Towards Generalist Biomedical Ai",
    "chunk_index": 77,
    "total_chunks": 141,
    "text_content": "(2022). 41.Lu, J., Clark, C., Zellers, R., Mottaghi, R. & Kembhavi, A. Unified-io: A unified model for vision, language, and multi-modal tasks.arXiv preprint arXiv:2206.08916 (2022). 42.Reed, S., Zolna, K., Parisotto, E., Colmenarejo, S. G., Novikov, A., Barth-Maron, G., Gimenez, M., Sulsky, Y., Kay, J., Springenberg, J. T., et al.A generalist agent. arXiv preprint arXiv:2205.06175 (2022). 43.Moor, M., Banerjee, O., Abad, Z. S. H., Krumholz, H. M., Leskovec, J., Topol, E. J. & Rajpurkar, P. Foun",
    "full_text_length": 143742,
    "chunk_length": 1333
  },
  {
    "chunk_id": 3646,
    "paper_filename": "teo_2024_towards_generalist_biomedical_ai.pdf",
    "paper_title": "Teo 2024 Towards Generalist Biomedical Ai",
    "chunk_index": 78,
    "total_chunks": 141,
    "text_content": "46.Zhang, K., Yu, J., Yan, Z., Liu, Y., Adhikarla, E., Fu, S., Chen, X., Chen, C., Zhou, Y., Li, X., et al.BiomedGPT: A Unified and Generalist Biomedical Generative Pre-trained Transformer for Vision, Language, and Multimodal Tasks. arXiv preprint arXiv:2305.17100 (2023). 47.Li, C., Wong, C., Zhang, S., Usuyama, N., Liu, H., Yang, J., Naumann, T., Poon, H. & Gao, J. LLaVA-Med: Training a Large Language-and-Vision Assistant for Biomedicine in One Day. arXiv preprint arXiv:2306.00890 (2023). 48.Bu",
    "full_text_length": 143742,
    "chunk_length": 1316
  },
  {
    "chunk_id": 3647,
    "paper_filename": "teo_2024_towards_generalist_biomedical_ai.pdf",
    "paper_title": "Teo 2024 Towards Generalist Biomedical Ai",
    "chunk_index": 79,
    "total_chunks": 141,
    "text_content": "P., Chowdhery, A., Dean, J., Ghemawat, S., Hand, S., Hurt, D., Isard, M., Lim, H., Pang, R., Roy, S., et al. Pathways: Asynchronous distributed dataflow for ML. Proceedings of Machine Learning and Systems 4,430\u2013449 (2022). 51.Srivastava, A., Rastogi, A., Rao, A., Shoeb, A. A. M., Abid, A., Fisch, A., Brown, A. R., Santoro, A., Gupta, A., Garriga-Alonso, A., et al.Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. arXiv preprint arXiv:2206.04615 (2022). ",
    "full_text_length": 143742,
    "chunk_length": 1405
  },
  {
    "chunk_id": 3648,
    "paper_filename": "teo_2024_towards_generalist_biomedical_ai.pdf",
    "paper_title": "Teo 2024 Towards Generalist Biomedical Ai",
    "chunk_index": 80,
    "total_chunks": 141,
    "text_content": "L. Scaling vision transformers inProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (2022), 12104\u201312113. 55.Marino, K., Rastegari, M., Farhadi, A. & Mottaghi, R. Ok-vqa: A visual question answering benchmark requiring external knowledge inProceedings of the IEEE/cvf conference on computer vision and pattern recognition (2019), 3195\u20133204. 56.Goyal, Y., Khot, T., Summers-Stay, D., Batra, D. & Parikh, D. Making the v in vqa matter: Elevating the role of image understa",
    "full_text_length": 143742,
    "chunk_length": 1314
  },
  {
    "chunk_id": 3649,
    "paper_filename": "teo_2024_towards_generalist_biomedical_ai.pdf",
    "paper_title": "Teo 2024 Towards Generalist Biomedical Ai",
    "chunk_index": 81,
    "total_chunks": 141,
    "text_content": "J., Lu, P. -X. & Thoma, G. Two public chest X-ray datasets for computer-aided screening of pulmonary diseases. Quantitative imaging in medicine and surgery 4,475 (2014). 60.Yu, F., Endo, M., Krishnan, R., Pan, I., Tsai, A., Reis, E. P., Fonseca, E. K. U. N., Lee, H. M. H., Abad, Z. S. H., Ng, A. Y., et al.Evaluating progress in automatic chest x-ray radiology report generation. medRxiv, 2022\u201308 (2022). 61.Singhal, K., Tu, T., Gottweis, J., Sayres, R., Wulczyn, E., Hou, L., Clark, K., Pfohl, S., ",
    "full_text_length": 143742,
    "chunk_length": 1328
  },
  {
    "chunk_id": 3650,
    "paper_filename": "teo_2024_towards_generalist_biomedical_ai.pdf",
    "paper_title": "Teo 2024 Towards Generalist Biomedical Ai",
    "chunk_index": 82,
    "total_chunks": 141,
    "text_content": "M. A., Bashmal, L. & Zuair, M. Vision\u2013Language Model for Visual Question Answering in Medical Imagery. Bioengineering 10,380 (2023). |20 64.Van Sonsbeek, T., Derakhshani, M. M., Najdenkoska, I., Snoek, C. G. & Worring, M. Open-Ended Medical Visual Question Answering Through Prefix Tuning of Language Models. arXiv preprint arXiv:2303.05977 (2023). 65.Nicolson, A., Dowling, J. & Koopman, B. Improving chest X-Ray report generation by leveraging warm-starting. arXiv preprint arXiv:2201.09405 (2022).",
    "full_text_length": 143742,
    "chunk_length": 1462
  },
  {
    "chunk_id": 3651,
    "paper_filename": "teo_2024_towards_generalist_biomedical_ai.pdf",
    "paper_title": "Teo 2024 Towards Generalist Biomedical Ai",
    "chunk_index": 83,
    "total_chunks": 141,
    "text_content": "and Explainable Region-guided Radiology Report Generation inProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (2023), 7433\u20137442. 69.Rammuni Silva, R. S. & Fernando, P. Effective utilization of multiple convolutional neural networks for chest X-ray classification. SN Computer Science 3,492 (2022). 70.Panambur, A. B., Madhu, P. & Maier, A. Effect of Random Histogram Equalization on Breast Calcification Analysis Using Deep Learning inBildverarbeitung f\u00fcr die Medizin ",
    "full_text_length": 143742,
    "chunk_length": 1442
  },
  {
    "chunk_id": 3652,
    "paper_filename": "teo_2024_towards_generalist_biomedical_ai.pdf",
    "paper_title": "Teo 2024 Towards Generalist Biomedical Ai",
    "chunk_index": 84,
    "total_chunks": 141,
    "text_content": "preprint arXiv:2210.03029 (2022). 73.Endo, M., Krishnan, R., Krishna, V., Ng, A. Y. & Rajpurkar, P. Retrieval-based chest x-ray report generation using a pre-trained contrastive language-image model inMachine Learning for Health (2021), 209\u2013219. 74.Oloko-Oba, M., Viriri, S., et al.Ensemble of EfficientNets for the Diagnosis of Tuberculosis. Computational Intelligence and Neuroscience 2021(2021). 75.Zhang, R., Han, J., Zhou, A., Hu, X., Yan, S., Lu, P., Li, H., Gao, P. & Qiao, Y. Llama-adapter: E",
    "full_text_length": 143742,
    "chunk_length": 1390
  },
  {
    "chunk_id": 3653,
    "paper_filename": "teo_2024_towards_generalist_biomedical_ai.pdf",
    "paper_title": "Teo 2024 Towards Generalist Biomedical Ai",
    "chunk_index": 85,
    "total_chunks": 141,
    "text_content": "international evidence. British Journal of General Practice 64,e765\u2013e774 (2014). 79.Jin, D., Pan, E., Oufattole, N., Weng, W. -H., Fang, H. & Szolovits, P. What disease does this patient have? a large-scale open domain question answering dataset from medical exams. Applied Sciences 11,6421 (2021). 80.Pal, A., Umapathi, L. K. & Sankarasubbu, M. MedMCQA: A Large-scale Multi-Subject Multi-Choice Dataset for Medical domain Question Answering inConference on Health, Inference, and Learning (2022), 24",
    "full_text_length": 143742,
    "chunk_length": 1353
  },
  {
    "chunk_id": 3654,
    "paper_filename": "teo_2024_towards_generalist_biomedical_ai.pdf",
    "paper_title": "Teo 2024 Towards Generalist Biomedical Ai",
    "chunk_index": 86,
    "total_chunks": 141,
    "text_content": "Langlotz, C. ViLMedic: a framework for research at the intersection of vision and language in medical AI inProceedings of the 60th Annual Meeting of the Association for Computational Linguistics: System Demonstrations (2022), 23\u201334. 84.Delbrouck, J. -B., Varma, M. & Langlotz, C. P. Toward expanding the scope of radiology report summarization to multiple anatomies and modalities. arXiv preprint arXiv:2211.08584 (2022). 85.Pacheco, A. G., Lima, G. R., Salomao, A. S., Krohling, B., Biral, I. P., de",
    "full_text_length": 143742,
    "chunk_length": 1290
  },
  {
    "chunk_id": 3655,
    "paper_filename": "teo_2024_towards_generalist_biomedical_ai.pdf",
    "paper_title": "Teo 2024 Towards Generalist Biomedical Ai",
    "chunk_index": 87,
    "total_chunks": 141,
    "text_content": "H. Q., Pham, H. H., Lam, K., Le, L. T., Dao, M. & Vu, V. VinDr-Mammo: A large-scale benchmark dataset for computer-aided diagnosis in full-field digital mammography. Scientific Data 10,277 (2023). 88.Lee, R. S., Gimenez, F., Hoogi, A., Miyake, K. K., Gorovoy, M. & Rubin, D. L. A curated mammography data set for use in computer-aided detection and diagnosis research. Scientific data 4,1\u20139 (2017). 89.Olson, N. D., Wagner, J., McDaniel, J., Stephens, S. H., Westreich, S. T. & et al. PrecisionFDA Tr",
    "full_text_length": 143742,
    "chunk_length": 1280
  },
  {
    "chunk_id": 3656,
    "paper_filename": "teo_2024_towards_generalist_biomedical_ai.pdf",
    "paper_title": "Teo 2024 Towards Generalist Biomedical Ai",
    "chunk_index": 88,
    "total_chunks": 141,
    "text_content": "S., Moore, N., Elmarakeby, H., Salari, K., Choudhry, H., Al-Rubaish, A. M., Al-Sulaiman, A. A., Al-Ali, A. K., Taylor-Weiner, A. & Allen, E. M. V. Detection of Pathogenic Variants With Germline Genetic Testing Using Deep Learning vs Standard Methods in Patients With Prostate Cancer and Melanoma. JAMA324,1957 (Nov. 2020). 92.Liao, W.-W., Asri, M., Ebler, J., Doerr, D., Haukness, M., Hickey, G. & et al. A draft human pangenome reference. Nature 617,312\u2013324 (May 2023). 93.Thorvaldsdottir, H., Robin",
    "full_text_length": 143742,
    "chunk_length": 1321
  },
  {
    "chunk_id": 3657,
    "paper_filename": "teo_2024_towards_generalist_biomedical_ai.pdf",
    "paper_title": "Teo 2024 Towards Generalist Biomedical Ai",
    "chunk_index": 89,
    "total_chunks": 141,
    "text_content": "Scientific data 5,1\u201310 (2018). |21 96.He, X., Zhang, Y., Mou, L., Xing, E. & Xie, P. Pathvqa: 30000+ questions for medical visual question answering. arXiv preprint arXiv:2003.10286 (2020). 97.Johnson, A. E., Pollard, T. J., Berkowitz, S. J., Greenbaum, N. R., Lungren, M. P., Deng, C. -y., Mark, R. G. & Horng, S. MIMIC-CXR, a de-identified publicly available database of chest radiographs with free-text reports. Scientific data 6,317 (2019). 98.Irvin, J., Rajpurkar, P., Ko, M., Yu, Y., Ciurea-Ilc",
    "full_text_length": 143742,
    "chunk_length": 1352
  },
  {
    "chunk_id": 3658,
    "paper_filename": "teo_2024_towards_generalist_biomedical_ai.pdf",
    "paper_title": "Teo 2024 Towards Generalist Biomedical Ai",
    "chunk_index": 90,
    "total_chunks": 141,
    "text_content": "S., Ward, T. & Zhu, W. -J.Bleu: a method for automatic evaluation of machine translation in Proceedings of the 40th annual meeting of the Association for Computational Linguistics (2002), 311\u2013318. 102.Jain, S., Agrawal, A., Saporta, A., Truong, S. Q., Duong, D. N., Bui, T., Chambon, P., Zhang, Y., Lungren, M. P., Ng, A. Y.,et al.Radgraph: Extracting clinical entities and relations from radiology reports. arXiv preprint arXiv:2106.14463 (2021). 103.Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., ",
    "full_text_length": 143742,
    "chunk_length": 1315
  },
  {
    "chunk_id": 3659,
    "paper_filename": "teo_2024_towards_generalist_biomedical_ai.pdf",
    "paper_title": "Teo 2024 Towards Generalist Biomedical Ai",
    "chunk_index": 91,
    "total_chunks": 141,
    "text_content": "Liu, R., Wu, T., Wang, M., Yin, J. & Liu, J. Deeply Supervised Skin Lesions Diagnosis with Stage and Branch Attention. arXiv preprint arXiv:2205.04326 (2022). 107.De Lima, L. M. & Krohling, R. A. Exploring Advances in Transformers and CNN for Skin Lesion Diagnosis on Small Datasets inIntelligent Systems: 11th Brazilian Conference, BRACIS 2022, Campinas, Brazil, November 28\u2013December 1, 2022, Proceedings, Part II (2022), 282\u2013296. 108.Liu, Y., Wang, Z., Xu, D. & Zhou, L. Q2atransformer: Improving m",
    "full_text_length": 143742,
    "chunk_length": 1348
  },
  {
    "chunk_id": 3660,
    "paper_filename": "teo_2024_towards_generalist_biomedical_ai.pdf",
    "paper_title": "Teo 2024 Towards Generalist Biomedical Ai",
    "chunk_index": 92,
    "total_chunks": 141,
    "text_content": "111.Smit, A., Jain, S., Rajpurkar, P., Pareek, A., Ng, A. Y. & Lungren, M. P. CheXbert: combining automatic labelers and expert annotations for accurate radiology report labeling using BERT. arXiv preprint arXiv:2004.09167 (2020). 112.Liu, G., Hsu, T. -M. H., McDermott, M., Boag, W., Weng, W. -H., Szolovits, P. & Ghassemi, M. Clinically accurate chest x-ray report generation inMachine Learning for Healthcare Conference (2019), 249\u2013269. 113.Xu, Y., Zhang, Q., Zhang, J. & Tao, D. Vitae: Vision tra",
    "full_text_length": 143742,
    "chunk_length": 1484
  },
  {
    "chunk_id": 3661,
    "paper_filename": "teo_2024_towards_generalist_biomedical_ai.pdf",
    "paper_title": "Teo 2024 Towards Generalist Biomedical Ai",
    "chunk_index": 93,
    "total_chunks": 141,
    "text_content": "\u2022Med-PaLM M training procedure \u2022Interpretations of Med-PaLM M performance by task type: \u2013Performance analysis on language-only medical question answering \u2013Performance analysis on radiology report summarization \u2013Performance analysis on medical image classification tasks \u2013Performance analysis on medical visual question answering \u2013Performance analysis on chest X-ray report generation \u2022Human evaluation of model-generated chest X-ray reports \u2022Examples from MultiMedBench tasks A.1 MultiMedBench In thi",
    "full_text_length": 143742,
    "chunk_length": 1480
  },
  {
    "chunk_id": 3662,
    "paper_filename": "teo_2024_towards_generalist_biomedical_ai.pdf",
    "paper_title": "Teo 2024 Towards Generalist Biomedical Ai",
    "chunk_index": 94,
    "total_chunks": 141,
    "text_content": "questions from MedMCQA. The test set comprises 1,273 questions from MedQA, 4,183 questions from MedMCQA, and 500 questions from PubMedQA. Note that PubMedQA was not included in the training data mixture and only used for evaluation. MIMIC-III is a large publicly-available medical database that contains medical records of patients admitted to intensive care units [82]. It contains 79,790 radiology reports across two imaging modalities (CT and MRI) and seven anatomic regions (head, abdomen, chest,",
    "full_text_length": 143742,
    "chunk_length": 1290
  },
  {
    "chunk_id": 3663,
    "paper_filename": "teo_2024_towards_generalist_biomedical_ai.pdf",
    "paper_title": "Teo 2024 Towards Generalist Biomedical Ai",
    "chunk_index": 95,
    "total_chunks": 141,
    "text_content": "MRI neck. This resulted in a total of 58,405 reports for training, 7,413 reports for validation, and 13,057 reports for testing. Note that chest X-ray reports are excluded from this dataset to avoid data contamination with the MIMIC-CXR dataset for the report generation task. For each report, we used the same preprocessing functions as in [83, 84] to extract the findings and impression sections. Specifically, we filtered out the reports whose findings section are longer than 600 tokens. We perfo",
    "full_text_length": 143742,
    "chunk_length": 1636
  },
  {
    "chunk_id": 3664,
    "paper_filename": "teo_2024_towards_generalist_biomedical_ai.pdf",
    "paper_title": "Teo 2024 Towards Generalist Biomedical Ai",
    "chunk_index": 96,
    "total_chunks": 141,
    "text_content": "a nonprofit |23 MedQAMedMCQAPubMedQAMIMIC-IIIVQA-RADSlake-VQAPath-VQAMIMIC-CXRReportPAD-UFES-20VinDr-MammoCBIS-DDSMGenomicsPrecisionFDA MIMIC-CXR114511870055007887535151402832799788752298200002620210068 361242 Medical QA Report SummarizationMedical VQAReportGeneration Medical Image ClassificationMultiMedBenchFigure A.1 |MultiMedBench overview. MultiMedBench is a benchmark that covers 14 different biomedical tasks, including question answering, visual question answering, image classification, rad",
    "full_text_length": 143742,
    "chunk_length": 1715
  },
  {
    "chunk_id": 3665,
    "paper_filename": "teo_2024_towards_generalist_biomedical_ai.pdf",
    "paper_title": "Teo 2024 Towards Generalist Biomedical Ai",
    "chunk_index": 97,
    "total_chunks": 141,
    "text_content": "associated clinical textual features as the multimodal input. Specifically, we selected 14 clinical attributes in the metadata for each lesion including: age,gender,smoke,drink,skin cancer history,cancer history ,region,fitspatrick ,horizontal and vertical diameters ,itch,grew,bleed, andelevation . The class ratio is approximately 16:1:4:14:5:4 over three skin cancers (BCC, MEL, and SCC) and three skin disease (ACK, NEV, and SEK). Since there are no published official train/test splits, we rando",
    "full_text_length": 143742,
    "chunk_length": 1464
  },
  {
    "chunk_id": 3666,
    "paper_filename": "teo_2024_towards_generalist_biomedical_ai.pdf",
    "paper_title": "Teo 2024 Towards Generalist Biomedical Ai",
    "chunk_index": 98,
    "total_chunks": 141,
    "text_content": "images where the left and right breasts are imaged with mediolateral-oblique (MLO) and cranio-caudal (CC) views. Each image has breast-level assessment following the Breast Imaging Reporting and Data System (BI-RADS). BI-RADS assessment ranges from 1 (negative) to 5 (highly suggestive of malignancy). In addition to the BI-RADS score, the breast density level is also provided as well as regional abnormality finding annotations. We performed a breast-level 5-class BI-RADS classification task simil",
    "full_text_length": 143742,
    "chunk_length": 1377
  },
  {
    "chunk_id": 3667,
    "paper_filename": "teo_2024_towards_generalist_biomedical_ai.pdf",
    "paper_title": "Teo 2024 Towards Generalist Biomedical Ai",
    "chunk_index": 99,
    "total_chunks": 141,
    "text_content": "in the training data, we upsampled for each minority class (BI-RADS 2-5) by a factor of 3. CBIS-DDSM is the Curated Breast Imaging Subset of Digital Database for Screening Mammography [88]. This dataset contains 2,620 scanned film mammography studies. Unlike VinDr-Mammo, CBIS-DDSM does not have breast-level BI-RADS assessment. Annotations are provided at the lesion level including BI-RADS, subtlety level, and pathology type. There are two types of lesions: mass and calcification. Both of them ar",
    "full_text_length": 143742,
    "chunk_length": 1329
  },
  {
    "chunk_id": 3668,
    "paper_filename": "teo_2024_towards_generalist_biomedical_ai.pdf",
    "paper_title": "Teo 2024 Towards Generalist Biomedical Ai",
    "chunk_index": 100,
    "total_chunks": 141,
    "text_content": "the training and test sets contain 1,318 and 378 images (class ratio: 6:1:6), respectively. For calcification cases, the total number of images in the training and test sets are 1,544 and 326 (class ratio: 1:1:1), respectively. For both cases, we applied the same image augmentation as in VinDr-Mammo to the training set. PrecisionFDA Truth Challenge V2 was developed for benchmarking the state-of-the-art of variant calling in challenging genomics regions [89]. Genomic variant calling is a task aim",
    "full_text_length": 143742,
    "chunk_length": 1305
  },
  {
    "chunk_id": 3669,
    "paper_filename": "teo_2024_towards_generalist_biomedical_ai.pdf",
    "paper_title": "Teo 2024 Towards Generalist Biomedical Ai",
    "chunk_index": 101,
    "total_chunks": 141,
    "text_content": "extensively characterized groundtruth set from the National Institute of Standards and Technology (NIST) [94] for the HG002 sample. We generated examples from sequencing from the PrecisionFDA Truth Challenge V2. For training, we use 4% of the examples from the whole genome (except for chromosome 20, 21, and 22). For evaluation, we used chromosome20, bases 3000001-9444417. This generated 197,038 candidate variants for training and 13,030 candidate variants for evaluation. For each example, the mo",
    "full_text_length": 143742,
    "chunk_length": 1319
  },
  {
    "chunk_id": 3670,
    "paper_filename": "teo_2024_towards_generalist_biomedical_ai.pdf",
    "paper_title": "Teo 2024 Towards Generalist Biomedical Ai",
    "chunk_index": 102,
    "total_chunks": 141,
    "text_content": "221, 6) corresponding to (height, width, channels). Channels are shown in grey-scale below in the following order: 1. Read base: different intensities represent A, C, G, and T. 2. Base quality: set by the sequencing machine. White is higher quality. 3. Mapping quality: set by the aligner. White is higher quality. 4. Strand of alignment: Black is forward; white is reverse. 5.Read supports variant: White means the read supports the given alternate allele, grey means it does not. 6.Base differs fro",
    "full_text_length": 143742,
    "chunk_length": 1172
  },
  {
    "chunk_id": 3671,
    "paper_filename": "teo_2024_towards_generalist_biomedical_ai.pdf",
    "paper_title": "Teo 2024 Towards Generalist Biomedical Ai",
    "chunk_index": 103,
    "total_chunks": 141,
    "text_content": "padded the image on the width and height dimensions to give it a final shape of (224, 224, 3). VQA-RAD is a radiology visual question answering (VQA) dataset which consists of 315 radiology images and 3,515 question\u2013answer pairs created and validated by clinicians [95]. The radiology images are selected from three imaging modalities (CT, MRI, and X-rays) and three anatomical regions (head, abdominal, chest). |25 The types of question fall into 11 categories including modality, plane, organ syste",
    "full_text_length": 143742,
    "chunk_length": 1343
  },
  {
    "chunk_id": 3672,
    "paper_filename": "teo_2024_towards_generalist_biomedical_ai.pdf",
    "paper_title": "Teo 2024 Towards Generalist Biomedical Ai",
    "chunk_index": 104,
    "total_chunks": 141,
    "text_content": "pathology images with 32,799 question- answer pairs [96]. Pathology images are extracted from medical textbooks and online digital libraries. Each image is associated with multiple QA pairs pertaining to different aspects of the pathology including color, location, appearance, shape, etc. Open-ended questions account for 50.2% of all questions, which are categorized into 7 categories: what, where, when, whose, how, and how much/how many, accounting for 50.2% of all questions. The rest are close-",
    "full_text_length": 143742,
    "chunk_length": 1414
  },
  {
    "chunk_id": 3673,
    "paper_filename": "teo_2024_towards_generalist_biomedical_ai.pdf",
    "paper_title": "Teo 2024 Towards Generalist Biomedical Ai",
    "chunk_index": 105,
    "total_chunks": 141,
    "text_content": "to various aspects of the image content including plane, quality, position, organ, abnormality, size, color, shape, knowledge graph, etc. The training, validation, and test sets contain 9,849, 2,109, and 2,070 samples, respectively. MIMIC-CXR is a large dataset of chest radiographs with free-text radiology reports [97]. A total of 377,110 images are available in the dataset from 227,835 image studies collected for 65,379 patients. Each patient may have multiple studies and each study may contain",
    "full_text_length": 143742,
    "chunk_length": 1412
  },
  {
    "chunk_id": 3674,
    "paper_filename": "teo_2024_towards_generalist_biomedical_ai.pdf",
    "paper_title": "Teo 2024 Towards Generalist Biomedical Ai",
    "chunk_index": 106,
    "total_chunks": 141,
    "text_content": "report is annotated with structured labels of 14 common radiological observations using CheXpert labeler [98]. We performed two tasks using this dataset: chest X-ray report generation and binary classification of clinically-relevant pathology observations. We preprocessed the radiology reports by extracting the indication, findings, and impression sections, removing redundant white-spaces in the reports, following previous work [99]. We used the official train/validation/test splits. We discarde",
    "full_text_length": 143742,
    "chunk_length": 1380
  },
  {
    "chunk_id": 3675,
    "paper_filename": "teo_2024_towards_generalist_biomedical_ai.pdf",
    "paper_title": "Teo 2024 Towards Generalist Biomedical Ai",
    "chunk_index": 107,
    "total_chunks": 141,
    "text_content": "images of different view positions. In a separate evaluation, we also studied a subset of samples where reports are accompanied by both a front and lateral view (two-view report generation). For the report generation task, we combined the chest X-ray image with the contextual information from the indication section (reason for the study) to predict the findings section of the target report. The total number of samples in the training, validation, and test sets are: 353,542, 2,866, and 4,834, res",
    "full_text_length": 143742,
    "chunk_length": 1404
  },
  {
    "chunk_id": 3676,
    "paper_filename": "teo_2024_towards_generalist_biomedical_ai.pdf",
    "paper_title": "Teo 2024 Towards Generalist Biomedical Ai",
    "chunk_index": 108,
    "total_chunks": 141,
    "text_content": "the model to distinguish normal cases from cases with any type of abnormality. Due to class imbalance, during training we upsampled the positive class by a factor of 2 for the following conditions: consolidation, enlarged cardiomediastinum, fracture, and pneumonia. These binary classification tasks are auxiliary to the report generation task when they are trained simultaneously since they help the model to distinguish among different types of clinical observations in the chest X-ray images. |26 ",
    "full_text_length": 143742,
    "chunk_length": 1508
  },
  {
    "chunk_id": 3677,
    "paper_filename": "teo_2024_towards_generalist_biomedical_ai.pdf",
    "paper_title": "Teo 2024 Towards Generalist Biomedical Ai",
    "chunk_index": 109,
    "total_chunks": 141,
    "text_content": "medical vision-language tasks, with less than 15% consisting of language-only tasks. While the majority of vision-language tasks were trained with a text-only 1-shot setup (without the corresponding image), the CBIS-DDSM classification and genomic variant calling tasks were trained with a 0-shot setup. A.2.2 Training hyperparameters PaLM-E projects the multimodal inputs into the same language embedding space as latent vectors such that continuous observations (e.g., images, time series) can be p",
    "full_text_length": 143742,
    "chunk_length": 1351
  },
  {
    "chunk_id": 3678,
    "paper_filename": "teo_2024_towards_generalist_biomedical_ai.pdf",
    "paper_title": "Teo 2024 Towards Generalist Biomedical Ai",
    "chunk_index": 110,
    "total_chunks": 141,
    "text_content": "on text-only medical question answering We report the few-shot performance of Med- PaLM M on MedQA, MedMCQA, and PubMedQA in Table A.3. SOTA results were chosen from Med-PaLM 2 with ensemble refinement prompting and PaLM 540B few-shot results reported in [9, 61]. Med-PaLM M outperformed the baseline PaLM model (from which it inherits) by a large margin on all three datasets, despite falling behind the Med-PaLM 2 best results obtained with ensemble refinement. Scaling up the language model from 8",
    "full_text_length": 143742,
    "chunk_length": 1473
  },
  {
    "chunk_id": 3679,
    "paper_filename": "teo_2024_towards_generalist_biomedical_ai.pdf",
    "paper_title": "Teo 2024 Towards Generalist Biomedical Ai",
    "chunk_index": 111,
    "total_chunks": 141,
    "text_content": "text-only 1-shot Report Generation Chest X-ray MIMIC-CXR 59.90% text-only 1-shot Medical Image ClassificationDermatology PAD-UFES-20 6.25% text-only 1-shot MammographyVinDr-Mammo 1.56% text-only 1-shot CBIS-DDSM 1.56% 0-shot Chest X-ray MIMIC-CXR 11.98% text-only 1-shot GenomicsPrecisionFDA Truth Challenge V2 [89]1.56% 0-shot Table A.2 |Med-PaLM M finetuning hyperparameters. Summary of the finetuning hyperparameters for Med-PaLM M 12B, 84B, and 562B. Hyperparameter Med-PaLM M (12B) Med-PaLM M (8",
    "full_text_length": 143742,
    "chunk_length": 1445
  },
  {
    "chunk_id": 3680,
    "paper_filename": "teo_2024_towards_generalist_biomedical_ai.pdf",
    "paper_title": "Teo 2024 Towards Generalist Biomedical Ai",
    "chunk_index": 112,
    "total_chunks": 141,
    "text_content": "a large margin, despite falling short of the state-of-the-art Med-PaLM 2. Dataset Med-PaLM 2 PaLM Med-PaLM M (12B) Med-PaLM M (84B) Med-PaLM M (562B) MedQA (USMLE) 86.50% 58.90% 29.22% 46.11% 69.68% MedMCQA 72.30% 54.50% 32.20% 47.60% 62.59% PubMedQA 81.80% 55.00% 48.60% 71.40% 80.00% Performance on radiology report summarization We report commonly used metrics such as ROUGE- L [100], BLEU [101], and F1-RadGraph [102] scores on the radiology report summarization task as in Van Veen et al.[62] in",
    "full_text_length": 143742,
    "chunk_length": 1277
  },
  {
    "chunk_id": 3681,
    "paper_filename": "teo_2024_towards_generalist_biomedical_ai.pdf",
    "paper_title": "Teo 2024 Towards Generalist Biomedical Ai",
    "chunk_index": 113,
    "total_chunks": 141,
    "text_content": "on the test set of MIMIC-III which led to potential data leakage. Notably, Med-PaLM M compared favorably to the results in Van Veen et al.[62] based on the T5 model which was not pretrained on clinical text, similar to the PaLM model. |28 Table A.4 |Med-PaLM M performance on MIMIC-III radiology report summarization. Dataset Metric SOTA Med-PaLM M (12B) Med-PaLM M (84B) Med-PaLM M (562B) MIMIC-IIIROUGE-L 38.70% 29.45% 31.47% 32.03% BLEU 16.20% 12.14% 15.36% 15.21% F1-RadGraph 40.80% 31.43% 33.96%",
    "full_text_length": 143742,
    "chunk_length": 1307
  },
  {
    "chunk_id": 3682,
    "paper_filename": "teo_2024_towards_generalist_biomedical_ai.pdf",
    "paper_title": "Teo 2024 Towards Generalist Biomedical Ai",
    "chunk_index": 114,
    "total_chunks": 141,
    "text_content": "the F1 scores for single nucleotide polymorphisms (SNPs) and short insertions and deletions (indels) in the context of variant discovery were used instead. On VinDr-Mammo, all size variants of Med-PaLM M exceeded the prior SOTA using a smaller ViT (9.7M) on macro-AUC [49]. On CBIS-DDSM, our model achieved the best macro-F1 of 51.12% and 67.86% on the mass and calcification classification, respectively, behind the SOTA F1 of 70.71% reported on the calcification classification [70]. Note that most",
    "full_text_length": 143742,
    "chunk_length": 1328
  },
  {
    "chunk_id": 3683,
    "paper_filename": "teo_2024_towards_generalist_biomedical_ai.pdf",
    "paper_title": "Teo 2024 Towards Generalist Biomedical Ai",
    "chunk_index": 115,
    "total_chunks": 141,
    "text_content": "macro-average of F1 scores across the binary classification of 5 major conditions: atelectasis, cardiomegaly, consolidation, edema, and pleural effusion. Med-PaLM M (562B) achieved a macro-AUC of 79.09%, slightly lower than the SOTA result of 81.27% obtained from ParallelXNet [69], which used a parallelization of various CNN Architectures. On the variant calling task, DeepVariant model [71] outperformed Med-PaLM M on both Indel-F1 and SNP-F1 scores. The SOTA DeepVariant model was trained with 2,",
    "full_text_length": 143742,
    "chunk_length": 1374
  },
  {
    "chunk_id": 3684,
    "paper_filename": "teo_2024_towards_generalist_biomedical_ai.pdf",
    "paper_title": "Teo 2024 Towards Generalist Biomedical Ai",
    "chunk_index": 116,
    "total_chunks": 141,
    "text_content": "single model compared to highly specialized SOTA models. It is worth noting that we did not perform any fine-grained task-specific customization and hyperparameter tuning beyond data augmentation and class balancing. It is expected that scaling up the language model does not significantly benefit the classification tasks where the vision encoder is likely the bottleneck for the model performance. There is no overall evidence to suggest that larger vision model outperforms the small one across al",
    "full_text_length": 143742,
    "chunk_length": 1319
  },
  {
    "chunk_id": 3685,
    "paper_filename": "teo_2024_towards_generalist_biomedical_ai.pdf",
    "paper_title": "Teo 2024 Towards Generalist Biomedical Ai",
    "chunk_index": 117,
    "total_chunks": 141,
    "text_content": "an open-ended language decoding task conditioned on visual input, we used BLEU-1 and token-level F1 scores to assess the performance of Med-PaLM M. This is in contrast with many prior works which used a string-level accuracy evaluation metric as they often considered VQA as a classification task on a set of pre-defined fixed-number answer candidates [108, 109]. This accuracy metric has the weakness of failing to capture \"near misses\" of groundtruth answers, particularly in our open-ended generat",
    "full_text_length": 143742,
    "chunk_length": 1362
  },
  {
    "chunk_id": 3686,
    "paper_filename": "teo_2024_towards_generalist_biomedical_ai.pdf",
    "paper_title": "Teo 2024 Towards Generalist Biomedical Ai",
    "chunk_index": 118,
    "total_chunks": 141,
    "text_content": "report macro-averaged AUC and F1 for all tasks. For MIMIC-CXR, metrics are averaged over 5 major pathological conditions. Dataset # Classes Metric SOTAMed-PaLM M (12B)Med-PaLM M (84B)Med-PaLM M (562B) MIMIC-CXR (5 conditions)2-classMacro-AUC 81.27% 76.67% 78.35% 79.09% Macro-F1 N/A 38.33% 36.83% 41.57% PAD-UFES-20 6-classMacro-AUC N/A 95.57% 97.27% 96.08% Macro-F1 N/A 78.42% 84.32% 77.03% Variant Calling 3-classIndel-F1 99.40% 96.42% 97.04% 95.46% SNP-F1 99.70% 99.35% 99.32% 99.16% VinDr-Mammo 5",
    "full_text_length": 143742,
    "chunk_length": 1488
  },
  {
    "chunk_id": 3687,
    "paper_filename": "teo_2024_towards_generalist_biomedical_ai.pdf",
    "paper_title": "Teo 2024 Towards Generalist Biomedical Ai",
    "chunk_index": 119,
    "total_chunks": 141,
    "text_content": "VQA tasks. Dataset Metric SOTA Med-PaLM M (12B) Med-PaLM M (84B) Med-PaLM M (562B) VQA-RADBLEU-1 71.03% 64.02% 69.38% 71.27% F1 N/A 50.66% 59.90% 62.06% Path-VQABLEU-1 70.30% 68.97% 70.16% 72.27% F1 58.40% 57.24% 59.51% 62.69% Slake-VQABLEU-1 78.60% 90.77% 92.7% 91.64% F1 78.10% 86.22% 89.28% 87.50% Performance on chest X-ray report generation To measure the quality of generated chest X-ray reports using automatic metrics, we computed common natural language generation metrics such as BLEU-1, BL",
    "full_text_length": 143742,
    "chunk_length": 1374
  },
  {
    "chunk_id": 3688,
    "paper_filename": "teo_2024_towards_generalist_biomedical_ai.pdf",
    "paper_title": "Teo 2024 Towards Generalist Biomedical Ai",
    "chunk_index": 120,
    "total_chunks": 141,
    "text_content": "labeller to more observation categories by measuring the overlapping clinical entities and relations between a generated report and the reference report [60]. In line with previous studies [14, 65\u201368, 99, 112], we reported the macro-F1 and micro-F1 scores averaged over 5 major observations and all 14 observations for CE metrics, respectively. As shown in Table A.7, Med-PaLM M achieved a new SOTA on all CE metrics and F1-RadGraph, with a substantial increase of about 9 points on macro-F1-14 and m",
    "full_text_length": 143742,
    "chunk_length": 1281
  },
  {
    "chunk_id": 3689,
    "paper_filename": "teo_2024_towards_generalist_biomedical_ai.pdf",
    "paper_title": "Teo 2024 Towards Generalist Biomedical Ai",
    "chunk_index": 121,
    "total_chunks": 141,
    "text_content": "likely due to the benefit of jointly training with the classification tasks on those |30 minority conditions. We consider such positive task transfer as one of the main advantages of a generalist multi-task model over a specialized single-task model. On text overlap based natural language generation metrics, Med-PaLM M did not outperform existing SOTA results. However, the pitfalls of these automatic metrics have been raised by many studies, particularly in that they fail to capture the factual ",
    "full_text_length": 143742,
    "chunk_length": 1256
  },
  {
    "chunk_id": 3690,
    "paper_filename": "teo_2024_towards_generalist_biomedical_ai.pdf",
    "paper_title": "Teo 2024 Towards Generalist Biomedical Ai",
    "chunk_index": 122,
    "total_chunks": 141,
    "text_content": "fairly confined to a set of template sentences and limited number of conditions. It is also possible that the task performance is primarily limited by the vision encoder, particularly in how well it is adapted for this domain. As noted by Xu et al.[113], ViT lacks inductive bias for modeling local visual features which are often crucial for interpreting medical images. To overcome this limitation, large-scale medical training data may be required to enable benefit from size scaling. Additionally",
    "full_text_length": 143742,
    "chunk_length": 1310
  },
  {
    "chunk_id": 3691,
    "paper_filename": "teo_2024_towards_generalist_biomedical_ai.pdf",
    "paper_title": "Teo 2024 Towards Generalist Biomedical Ai",
    "chunk_index": 123,
    "total_chunks": 141,
    "text_content": "to capture clinical efficacy and correctness. Across three Med-PaLM M variants, the medium-sized model achieves the best performance. Metric SOTA Med-PaLM M (12B) Med-PaLM M (84B) Med-PaLM M (562B) Micro-F1-14 44.20% 51.41% 53.56% 51.60% Macro-F1-14 30.70% 37.31% 39.83% 37.81% Micro-F1-5 56.70% 56.54% 57.88% 56.28% Macro-F1-5 N/A 50.57% 51.60% 49.86% F1-RadGraph 24.40% 25.20% 26.71% 26.06% BLEU-1 39.48% 30.90% 32.31% 31.73% BLEU-4 13.30% 10.43% 11.31% 11.50% ROUGE-L 29.60% 26.16% 27.29% 27.49% C",
    "full_text_length": 143742,
    "chunk_length": 1348
  },
  {
    "chunk_id": 3692,
    "paper_filename": "teo_2024_towards_generalist_biomedical_ai.pdf",
    "paper_title": "Teo 2024 Towards Generalist Biomedical Ai",
    "chunk_index": 124,
    "total_chunks": 141,
    "text_content": "dataset contain references to prior studies (e.g., \u201cCompared to the prior radiograph [...]\u201d) or references to multiple views (e.g., \u201cAp and lateral views of the chest are compared.\u201d). By contrast, the input to our model is a single image and indication from a single study. As a result, these artifacts in the training corpus render the model prone to hallucination of references to non-existent prior imaging studies or non-existent X-ray views. Our human evaluation task interface accounted for the",
    "full_text_length": 143742,
    "chunk_length": 1266
  },
  {
    "chunk_id": 3693,
    "paper_filename": "teo_2024_towards_generalist_biomedical_ai.pdf",
    "paper_title": "Teo 2024 Towards Generalist Biomedical Ai",
    "chunk_index": 125,
    "total_chunks": 141,
    "text_content": "\u201cIncorrect location of finding\u201d) and non-clinical errors (i.e., \u201cRefers to view that is not present\u201d or \u201cRefers to study that is not present\u201d). Table A.8 summarizes the rates of omissions and errors identified by clinician raters in radiology reports generated by different Med-PaLM M models. Here, we report the rate of total errors, including all clinical and non-clinical error types. On average, the best performing Med-PaLM M |31 Figure A.3 |Side-by-side human evaluation task interface. Radiolo",
    "full_text_length": 143742,
    "chunk_length": 1349
  },
  {
    "chunk_id": 3694,
    "paper_filename": "teo_2024_towards_generalist_biomedical_ai.pdf",
    "paper_title": "Teo 2024 Towards Generalist Biomedical Ai",
    "chunk_index": 126,
    "total_chunks": 141,
    "text_content": "One important limitation of our human evaluation approach is the inter-rater variability. Similar to [60], which used a comparable evaluation scheme, we also observed that the same radiology report often was annotated with varying error and omission passages by different radiologist raters. While this is a common phenomenon in studies that use subjective ratings from clinicians, future work may aim to further refine rater instructions and improve rater calibration to reduce variability. Table A.",
    "full_text_length": 143742,
    "chunk_length": 1255
  },
  {
    "chunk_id": 3695,
    "paper_filename": "teo_2024_towards_generalist_biomedical_ai.pdf",
    "paper_title": "Teo 2024 Towards Generalist Biomedical Ai",
    "chunk_index": 127,
    "total_chunks": 141,
    "text_content": "(95% CI, 0.07 - 0.10) 0.08 (95% CI, 0.06 - 0.10) Total Omissions 0.13 (95% CI, 0.11 - 0.16) 0.12 (95% CI, 0.10 - 0.15) 0.12 (95% CI, 0.10 - 0.15) Significant Clinical Errors 0.26 (95% CI, 0.23 - 0.29) 0.23 (95% CI, 0.20 - 0.27) 0.26 (95% CI, 0.22 - 0.29) Total Clinical Errors 0.29 (95% CI, 0.25 - 0.32) 0.25 (95% CI, 0.22 - 0.28) 0.28 (95% CI, 0.24 - 0.31) Total Errors 0.63 (95% CI, 0.58 - 0.68) 0.59 (95% CI, 0.54 - 0.64) 0.58 (95% CI, 0.53 - 0.63) A.5 MultiMedBench Examples In Tables A.9 to A.13",
    "full_text_length": 143742,
    "chunk_length": 1193
  },
  {
    "chunk_id": 3696,
    "paper_filename": "teo_2024_towards_generalist_biomedical_ai.pdf",
    "paper_title": "Teo 2024 Towards Generalist Biomedical Ai",
    "chunk_index": 128,
    "total_chunks": 141,
    "text_content": "the patient history. Given <img>. Patient History: Age: 51, Gender: female, Smoke: false, Drink: false, Family skin cancer history: true, Family any cancer history: false, Lesion region: back, Lesion itch: false, Lesion grew: false, Lesion bled: false, Lesion elevation: false, Fitzpatrick scale: 1.0, Diameters (mm): [12.0, 8.0]. Q: Which of the following is the most likely diagnosis of the patient\u2019s skin lesion? (A) Nevus (B) Basal Cell Carcinoma (C) Squamous Cell Carcinoma (D) Actinic Keratosis",
    "full_text_length": 143742,
    "chunk_length": 1350
  },
  {
    "chunk_id": 3697,
    "paper_filename": "teo_2024_towards_generalist_biomedical_ai.pdf",
    "paper_title": "Teo 2024 Towards Generalist Biomedical Ai",
    "chunk_index": 129,
    "total_chunks": 141,
    "text_content": "(C) Squamous Cell Carcinoma (D) Actinic Keratosis (E) Seborrheic Keratosis (F) Melanoma A:Nevus. Classification (MIMIC-CXR) Instructions: You are a helpful radiology assistant. The following are questions about findings in chest X-ray in different views. Identify if a specific type of abnormality is shown in the X-ray. Given the AP view X-ray image <img>. Q: Is cardiomegaly indicated by the image? (A) No (B) Yes A: Yes. Given the AP view X-ray image <img>. Q: Is cardiomegaly indicated by the ima",
    "full_text_length": 143742,
    "chunk_length": 1357
  },
  {
    "chunk_id": 3698,
    "paper_filename": "teo_2024_towards_generalist_biomedical_ai.pdf",
    "paper_title": "Teo 2024 Towards Generalist Biomedical Ai",
    "chunk_index": 130,
    "total_chunks": 141,
    "text_content": "(B) 2 (C) 3 (D) 4 (E) 5 A:3. Classification (CBIS-DDSM Calcification) Givenmammogramimage <img>. Imageview: CCQ:Whichofthefollowing is the most likely type of the patient\u2019s breast calcification? (A) BENIGN (B) BENIGN_WITHOUT_CALLBACK (C) MALIGNANT A:MALIGNANT. Classification (CBIS-DDSM Mass) Given mammogram image <img>. Image view: CC Q: Which of the following is the most likely type of the patient\u2019s breast mass? (A) BENIGN (B) BENIGN_WITHOUT_CALLBACK (C) MALIGNANT A:BENIGN. Genomic variant call",
    "full_text_length": 143742,
    "chunk_length": 1327
  },
  {
    "chunk_id": 3699,
    "paper_filename": "teo_2024_towards_generalist_biomedical_ai.pdf",
    "paper_title": "Teo 2024 Towards Generalist Biomedical Ai",
    "chunk_index": 131,
    "total_chunks": 141,
    "text_content": "about medical knowledge. Solve them in a step-by-step fashion, referring to authoritative sources as needed. Given <img>. Q: Can you diagnose a pericardial effusion from this image? (closed domain) A: No. Given<img>. Q: What cut of the body is this image? (open domain) A:Axial. Slake-VQA Instructions: You are a helpful medical assistant. The following are questions about medical knowledge. Solve them in a step-by-step fashion, referring to authoritative sources as needed. Given <img>. Q: Is the ",
    "full_text_length": 143742,
    "chunk_length": 1310
  },
  {
    "chunk_id": 3700,
    "paper_filename": "teo_2024_towards_generalist_biomedical_ai.pdf",
    "paper_title": "Teo 2024 Towards Generalist Biomedical Ai",
    "chunk_index": 132,
    "total_chunks": 141,
    "text_content": "the alveolar walls? (other) A:accumulation of large numbers of macrophage. Chest X-ray report generation Instructions: You are a helpful radiology assistant. Describe what lines, tubes, and devices are present and each of their locations. De- scribe if pneumothorax is present; if present, describe size on each side. Describe if pleural effusion is present; if present, describe amount on each side. Describe if lung opacity (atelectasis, fibrosis, consolida- tion, infiltrate, lung mass, pneumonia,",
    "full_text_length": 143742,
    "chunk_length": 1353
  },
  {
    "chunk_id": 3701,
    "paper_filename": "teo_2024_towards_generalist_biomedical_ai.pdf",
    "paper_title": "Teo 2024 Towards Generalist Biomedical Ai",
    "chunk_index": 133,
    "total_chunks": 141,
    "text_content": "Normal lung volumes. Mild bilateral apical scarring. Normal size of the cardiac silhouette and tortuosity of the thoracic aorta. No pathologic findings in the lung parenchyma notably no evidence of fibrotic lung parenchymal changes. A faint 2 mm rounded opacity projecting over the lower aspect of the fourth right rib and internally to the upper border of the second right rib is seen on the frontal radiograph only and likely reflects structure on the skin. Given the PA view X-ray image <img>. Rea",
    "full_text_length": 143742,
    "chunk_length": 1282
  },
  {
    "chunk_id": 3702,
    "paper_filename": "teo_2024_towards_generalist_biomedical_ai.pdf",
    "paper_title": "Teo 2024 Towards Generalist Biomedical Ai",
    "chunk_index": 134,
    "total_chunks": 141,
    "text_content": "MultiMedBench. Input Instructions: You are a helpful radiology assistant. The following are questions about radiology reports. Summarize the findings in the report into diagnostic statements. Given the findings: there is an intraparenchymal hemorrhage in the right cerebellar hemisphere measuring 1.7 cm with vasogenic edema and mass effect to the fourth ventricle. there is high density within the fissure of the right cerebellum suggestive of subarachnoid component. there is high density along the",
    "full_text_length": 143742,
    "chunk_length": 1418
  },
  {
    "chunk_id": 3703,
    "paper_filename": "teo_2024_towards_generalist_biomedical_ai.pdf",
    "paper_title": "Teo 2024 Towards Generalist Biomedical Ai",
    "chunk_index": 135,
    "total_chunks": 141,
    "text_content": "right cerebellar parenchymal hemorrhage with surrounding vasogenic edema and mass effect to the fourth ventricle, with adjacent subarachnoid hemorrhage. possible right subdural hemorrhage along the right tentorium, however, the evaluation is limited. differential diagnosis of the etiology of the bleeding included tumor, avm, and hypertension. the finding was discussed with dr. by telephone immediately after interpretation. Table A.12 |Example from MedMCQA in MultiMedBench. Input Instructions: Th",
    "full_text_length": 143742,
    "chunk_length": 1411
  },
  {
    "chunk_id": 3704,
    "paper_filename": "teo_2024_towards_generalist_biomedical_ai.pdf",
    "paper_title": "Teo 2024 Towards Generalist Biomedical Ai",
    "chunk_index": 136,
    "total_chunks": 141,
    "text_content": "most likely embryologic mechanism responsible for Hirschsprung disease? (A) Failure of neural crest cells to migrate into the walls of the colon (B) Incomplete separation of the cloaca (C) Failure of recanalization of the colon (D) Defective rotation of the hindgut Answer: Failure of neural crest cells to migrate into the walls of the colon. Question: Chronic urethral obstruction due to benign prismatic hyperplasia can lead to the following change in kidney parenchyma (A) Hyperplasia (B) Hyperop",
    "full_text_length": 143742,
    "chunk_length": 1291
  },
  {
    "chunk_id": 3705,
    "paper_filename": "teo_2024_towards_generalist_biomedical_ai.pdf",
    "paper_title": "Teo 2024 Towards Generalist Biomedical Ai",
    "chunk_index": 137,
    "total_chunks": 141,
    "text_content": "which he has been taking metformin and vildagliptin. He has smoked 10\u201315 cigarettes daily for 29 years. Family history is irrelevant. Vital signs include: temperature 36.6 \u00b0C (97.8 \u00b0F), blood pressure 152/87 mm Hg and pulse 88/min. Examination reveals moderate abdominal obesity with a body mass index of 32 kg/m \u00b2. The remainder of the examination is unremarkable. His fasting lipid profile is shown: Total cholesterol (TC) 280 mg/dL Low-density lipoprotein (LDL)-cholesterol 210 mg/dL High-density ",
    "full_text_length": 143742,
    "chunk_length": 1322
  },
  {
    "chunk_id": 3706,
    "paper_filename": "teo_2024_towards_generalist_biomedical_ai.pdf",
    "paper_title": "Teo 2024 Towards Generalist Biomedical Ai",
    "chunk_index": 138,
    "total_chunks": 141,
    "text_content": "of blood clots in her past, but she says that her mother has also had to be treated for pulmonary embolism in the recent past, and her brother has had to deal with anemia his entire life. The patient\u2019s past medical history is noncontributory other than frequent middle ear infections. The vital signs upon arrival include: temperature, 36.7 \u00b0C (98.0 \u00b0F); blood pressure, 106/74 mm Hg; heart rate, 111/min and regular; and respiratory rate, 17/min. On physical examination, her pulses are bounding and",
    "full_text_length": 143742,
    "chunk_length": 1336
  },
  {
    "chunk_id": 3707,
    "paper_filename": "teo_2024_towards_generalist_biomedical_ai.pdf",
    "paper_title": "Teo 2024 Towards Generalist Biomedical Ai",
    "chunk_index": 139,
    "total_chunks": 141,
    "text_content": "to thrombotic events (B) A recessive beta-globin mutation causing morphological changes to the RBC (C) An X-linked recessive disease in which red blood cells are increasingly sensitive to oxidative stress (D) Secondarily caused by EBV, mycoplasma, CLL, or rheumatoid disease Answer: A recessive beta-globin mutation causing morphological changes to the RBC. Question: A pulmonary autopsy specimen from a 58-year-old woman who died of acute hypoxic respiratory failure was examined. She had recently u",
    "full_text_length": 143742,
    "chunk_length": 1136
  },
  {
    "chunk_id": 3708,
    "paper_filename": "teo_2024_towards_generalist_biomedical_ai.pdf",
    "paper_title": "Teo 2024 Towards Generalist Biomedical Ai",
    "chunk_index": 140,
    "total_chunks": 141,
    "text_content": "Pulmonary ischemia (C) Pulmonary hypertension (D) Pulmonary passive congestion Answer: Target Thromboembolism. |37",
    "full_text_length": 143742,
    "chunk_length": 114
  },
  {
    "chunk_id": 3709,
    "paper_filename": "tianyu_2023_contrast_enhanced_mammography_better_with_Artifital_intellegence.pdf",
    "paper_title": "Tianyu 2023 Contrast Enhanced Mammography Better With Artifital Intellegence",
    "chunk_index": 0,
    "total_chunks": 13,
    "text_content": "See discussions, st ats, and author pr ofiles f or this public ation at : https://www .researchgate.ne t/public ation/373664179 Contrast-enhanced mammography: better with AI? Article in Europe an R adiolog y \u00b7 Sept ember 2023 DOI: 10.1007/s00330-023-10190-5 CITATIONS 4READS 98 2 author s: Tianyu Zhang Radboud Univ ersity Medic al Centr e (R adboudumc) 43 PUBLICA TIONS 496 CITATIONS SEE PROFILE Ritse M Mann Radboud Univ ersity Medic al Centr e (R adboudumc) 316 PUBLICA TIONS 14,511 CITATIONS SEE ",
    "full_text_length": 12694,
    "chunk_length": 1282
  },
  {
    "chunk_id": 3710,
    "paper_filename": "tianyu_2023_contrast_enhanced_mammography_better_with_Artifital_intellegence.pdf",
    "paper_title": "Tianyu 2023 Contrast Enhanced Mammography Better With Artifital Intellegence",
    "chunk_index": 1,
    "total_chunks": 13,
    "text_content": "licence to European Society of Radiology 2023 Breast cancer is the most common cancer and also the most frequent cause of cancer-related mortality in women [1 ]. Population-based breast cancer screening provides the pos- sibility for early detection of breast cancer. Early detec- tion and subsequent precision treatment are important to improve outcomes. Currently, full-field digital mammogra- phy (FFDM) and digital breast tomosynthesis (DBT) fulfill a crucial role in breast cancer screening and ",
    "full_text_length": 12694,
    "chunk_length": 1333
  },
  {
    "chunk_id": 3711,
    "paper_filename": "tianyu_2023_contrast_enhanced_mammography_better_with_Artifital_intellegence.pdf",
    "paper_title": "Tianyu 2023 Contrast Enhanced Mammography Better With Artifital Intellegence",
    "chunk_index": 2,
    "total_chunks": 13,
    "text_content": "in 2011, contrast-enhanced mam- mography (CEM) has received increasing attention. A CEM examination is performed with iodinated contrast agent and uses dual-energy mammography. Enhancement of tumors is based on the same physiological principle as in breast MRI. Compared with FFDM and DBT, CEM has great feasibility and potential in primary breast cancer screening, improv - ing the detection ability of tumors shielded by dense breast tissue through contrast enhancement, and at the same time, CEM a",
    "full_text_length": 12694,
    "chunk_length": 1327
  },
  {
    "chunk_id": 3712,
    "paper_filename": "tianyu_2023_contrast_enhanced_mammography_better_with_Artifital_intellegence.pdf",
    "paper_title": "Tianyu 2023 Contrast Enhanced Mammography Better With Artifital Intellegence",
    "chunk_index": 3,
    "total_chunks": 13,
    "text_content": "informa- tion [3 ]. The increasing recognition of the potential value of artificial intelligence-based analysis in breast imaging has logically also resulted in artificial intelligence mod- els developed for CEM to perform breast imaging-related tasks, such as segmentation and classification [ 1, 7\u201310]. Early radiomics-based machine learning-based algorithms were trained to classify breast lesions, as well as to distin- guish invasive and non-invasive breast cancers and predict molecular subtype",
    "full_text_length": 12694,
    "chunk_length": 1322
  },
  {
    "chunk_id": 3713,
    "paper_filename": "tianyu_2023_contrast_enhanced_mammography_better_with_Artifital_intellegence.pdf",
    "paper_title": "Tianyu 2023 Contrast Enhanced Mammography Better With Artifital Intellegence",
    "chunk_index": 4,
    "total_chunks": 13,
    "text_content": "model that segmented single-mass breast lesions and clas- sified them on CEM to assist the diagnostic workflow. The segmentation task based on fully automated pipeline sys- tem achieved a Dice coefficient of 0.837 \u00b1 0.132 in the prospective test set, and the classification task (benign vs malignant) achieved an AUC of 0.891 (95% CI: 0.816, 0.945). Beuque et al likewise [ 10] developed a compre - hensive machine learning tool able to fully automatically identify, segment, and classify breast lesi",
    "full_text_length": 12694,
    "chunk_length": 1362
  },
  {
    "chunk_id": 3714,
    "paper_filename": "tianyu_2023_contrast_enhanced_mammography_better_with_Artifital_intellegence.pdf",
    "paper_title": "Tianyu 2023 Contrast Enhanced Mammography Better With Artifital Intellegence",
    "chunk_index": 5,
    "total_chunks": 13,
    "text_content": "Mann Ritse.Mann@radboudumc.nl 1 Department of Diagnostic Imaging, Radboud University Medical Center, Geert Grooteplein 10, 6525 GA Nijmegen, The Netherlands 2 Department of Radiology, Netherlands Cancer Institute (NKI), Amsterdam, The Netherlands 3 GROW School for Oncology and Development Biology, Maastricht University, Maastricht, The Netherlands European Radiology 1 3 radiomics model achieved good diagnostic performance, with an AUC of 0.95 (95% CI: 0.94\u20130.96). In this issue of European Radiol",
    "full_text_length": 12694,
    "chunk_length": 1293
  },
  {
    "chunk_id": 3715,
    "paper_filename": "tianyu_2023_contrast_enhanced_mammography_better_with_Artifital_intellegence.pdf",
    "paper_title": "Tianyu 2023 Contrast Enhanced Mammography Better With Artifital Intellegence",
    "chunk_index": 6,
    "total_chunks": 13,
    "text_content": "accuracy of 0.90 (95% CI: 0.88, 0.92) and an AUC of 0.96 (95% CI: 0.95, 0.97) on an internal test set, and an accuracy of 0.85 (95% CI: 0.82, 0.89) with an AUC of 0.92 (95% CI: 0.89, 0.95) was reported on an external test set. One unique aspect of this study is that the authors compared non-fusion models with feature fusion models [11]. Usually, the deep learning models extract the fea - tures through the base layer, and then analyze the features through the fully connected layer, and finally ma",
    "full_text_length": 12694,
    "chunk_length": 1256
  },
  {
    "chunk_id": 3716,
    "paper_filename": "tianyu_2023_contrast_enhanced_mammography_better_with_Artifital_intellegence.pdf",
    "paper_title": "Tianyu 2023 Contrast Enhanced Mammography Better With Artifital Intellegence",
    "chunk_index": 7,
    "total_chunks": 13,
    "text_content": "the craniocaudal (CC) and mediolateral oblique (MLO) view images of both breasts using both the low-energy and the high-energy input. In particular, the authors designed a multi-feature fusion strategy, including a left\u2013right fusion module and a CC-MLO fusion module. Compared to the no-fusion model, the left\u2013right fusion model performed better (AUC of 0.92 vs 0.95), and the multi-fusion model combining left\u2013right fusion with CC-MLO fusion per - formed best, with an AUC of 0.96. The improvement b",
    "full_text_length": 12694,
    "chunk_length": 1286
  },
  {
    "chunk_id": 3717,
    "paper_filename": "tianyu_2023_contrast_enhanced_mammography_better_with_Artifital_intellegence.pdf",
    "paper_title": "Tianyu 2023 Contrast Enhanced Mammography Better With Artifital Intellegence",
    "chunk_index": 8,
    "total_chunks": 13,
    "text_content": "robust. This study also has some limitations [11]. First, the proposed model was only trained using data from a single center. Collecting multi-center training data may potentially improve the generalization ability of the model. Second, the accuracy of the model for Breast Imaging Reporting and Data System category 4 lesions needs to be improved, and further work is needed to strengthen the prediction ability of the model for lesions presenting with microcalcification. In summary, artificial in",
    "full_text_length": 12694,
    "chunk_length": 1371
  },
  {
    "chunk_id": 3718,
    "paper_filename": "tianyu_2023_contrast_enhanced_mammography_better_with_Artifital_intellegence.pdf",
    "paper_title": "Tianyu 2023 Contrast Enhanced Mammography Better With Artifital Intellegence",
    "chunk_index": 9,
    "total_chunks": 13,
    "text_content": "benefit the healthcare of patients with breast disease. Acknowledgements The authors are thankful for the support from the Guangzhou Elite Project (TZ\u2013JY201948). Funding The authors state that this work has not received any funding. Declarations Guarantor The scientifc guarantor of this publication is R.M. Mann. Conflict of interest R.M. Mann is advisory Editorial Board member of European Radiology, associate Editor for breast imaging of Radiol- ogy and member of the executive board of the Europ",
    "full_text_length": 12694,
    "chunk_length": 1383
  },
  {
    "chunk_id": 3719,
    "paper_filename": "tianyu_2023_contrast_enhanced_mammography_better_with_Artifital_intellegence.pdf",
    "paper_title": "Tianyu 2023 Contrast Enhanced Mammography Better With Artifital Intellegence",
    "chunk_index": 10,
    "total_chunks": 13,
    "text_content": "C et al (2022) Intratumoral and peritumoral radiomics for preoperative prediction of neoadjuvant chemother - apy effect in breast cancer based on contrast-enhanced spectral mammography. Eur Radiol 32(5):3207\u20133219. https:// doi. org/ 10. 1007/ s00330- 021- 08414-7 2. Mann RM, Athanasiou A, Baltzer PAT et al (2022) Breast cancer screening in women with extremely dense breasts rec- ommendations of the European Society of Breast Imaging (EUSOBI). Eur Radiol 32(6):4036\u20134045. https:// doi. org/ 10. 10",
    "full_text_length": 12694,
    "chunk_length": 1349
  },
  {
    "chunk_id": 3720,
    "paper_filename": "tianyu_2023_contrast_enhanced_mammography_better_with_Artifital_intellegence.pdf",
    "paper_title": "Tianyu 2023 Contrast Enhanced Mammography Better With Artifital Intellegence",
    "chunk_index": 11,
    "total_chunks": 13,
    "text_content": "https:// doi. org/ 10. 1148/ radiol. 212530 6. Cozzi A, Magni V, Zanardo M et al (2022) Contrast-enhanced mammography: a systematic review and meta-analysis of diag- nostic performance. Radiology 302(3):568\u2013581. https:// doi. org/ 10. 1148/ radiol. 211412 7. Zhang S, Shao H, Li W et al (2023) Intra-and peritumoral radiomics for predicting malignant BiRADS category 4 breast lesions on contrast-enhanced spectral mammography: a multi- center study. Eur Radiol 33:5411\u20135422. https:// doi. org/ 10. 10",
    "full_text_length": 12694,
    "chunk_length": 1334
  },
  {
    "chunk_id": 3721,
    "paper_filename": "tianyu_2023_contrast_enhanced_mammography_better_with_Artifital_intellegence.pdf",
    "paper_title": "Tianyu 2023 Contrast Enhanced Mammography Better With Artifital Intellegence",
    "chunk_index": 12,
    "total_chunks": 13,
    "text_content": "10. Beuque MPL, Lobbes MBI, van Wijk Y et al (2023) Com- bining deep learning and handcrafted radiomics for classifi- cation of suspicious lesions on contrast-enhanced mammo- grams. Radiology 307(5):e221843. https:// doi. org/ 10. 1148/ radiol. 221843 11. Qian N, Jiang W, Guo Y et al (2023) Breast cancer diagnosis from contrast-enhanced mammography using multi-feature fusion neural network. Eur Radiol. https:// doi. org/ 10. 1007/ s00330- 023- 10170-9 Publisher's note Springer Nature remains neu",
    "full_text_length": 12694,
    "chunk_length": 614
  },
  {
    "chunk_id": 3722,
    "paper_filename": "vera_2024_utilizing_largelanguage_models_in_breast_cancer_managmetn_systematic_revew.pdf",
    "paper_title": "Vera 2024 Utilizing Largelanguage Models In Breast Cancer Managmetn Systematic Revew",
    "chunk_index": 0,
    "total_chunks": 30,
    "text_content": "Vol.:(0123456789)Journal of Cancer Research and Clinical Oncology (2024) 150:140 https://doi.org/10.1007/s00432-024-05678-6 REVIEW Utilizing large language models in breast cancer management: systematic review Vera Sorin1,2 \u00b7 Benjamin S. Glicksberg3 \u00b7 Yaara Artsi4 \u00b7 Yiftach Barash1,2 \u00b7 Eli Konen1 \u00b7 Girish N. Nadkarni3,5 \u00b7 Eyal Klang3,5 Received: 30 December 2023 / Accepted: 1 March 2024 / Published online: 19 March 2024 \u00a9 The Author(s) 2024 Abstract Purpose Despite advanced technologies in breas",
    "full_text_length": 28983,
    "chunk_length": 1481
  },
  {
    "chunk_id": 3723,
    "paper_filename": "vera_2024_utilizing_largelanguage_models_in_breast_cancer_managmetn_systematic_revew.pdf",
    "paper_title": "Vera 2024 Utilizing Largelanguage Models In Breast Cancer Managmetn Systematic Revew",
    "chunk_index": 1,
    "total_chunks": 30,
    "text_content": "analysis, guideline-based question-answering, and patient management recommendations. Accuracy varied between studies, ranging from 50 to 98%. Higher accuracy was seen in structured tasks like information retrieval. Half of the studies used real patient data, adding practical clinical value. Challenges included inconsistent accuracy, dependency on the way questions are posed (prompt-dependency), and in some cases, missing critical clinical information. Conclusion LLMs hold potential in breast ca",
    "full_text_length": 28983,
    "chunk_length": 1484
  },
  {
    "chunk_id": 3724,
    "paper_filename": "vera_2024_utilizing_largelanguage_models_in_breast_cancer_managmetn_systematic_revew.pdf",
    "paper_title": "Vera 2024 Utilizing Largelanguage Models In Breast Cancer Managmetn Systematic Revew",
    "chunk_index": 2,
    "total_chunks": 30,
    "text_content": "- eraging billions of parameters for highly accurate text pro- cessing (Sorin et al. 2020a, b; Bubeck et al. 2023). Despite this, the integration of such sophisticated NLP algorithms into practical healthcare settings, particularly in managing complex diseases like breast cancer, remains a technological, operational, and ethical challenge. Breast cancer, the most common cancer among women, continues to pose significant challenges in terms of morbid- ity, mortality, and information overload (Kuhl",
    "full_text_length": 28983,
    "chunk_length": 1306
  },
  {
    "chunk_id": 3725,
    "paper_filename": "vera_2024_utilizing_largelanguage_models_in_breast_cancer_managmetn_systematic_revew.pdf",
    "paper_title": "Vera 2024 Utilizing Largelanguage Models In Breast Cancer Managmetn Systematic Revew",
    "chunk_index": 3,
    "total_chunks": 30,
    "text_content": "Sorin verasrn@gmail.com 1 Department of Diagnostic Imaging, Chaim Sheba Medical Center, Affiliated to the Sackler School of Medicine, Tel- Aviv University, Emek Haela St. 1, 52621 Ramat Gan, Israel 2 DeepVision Lab, Chaim Sheba Medical Center, Tel Hashomer, Israel 3 Division of Data-Driven and Digital Medicine (D3M), Icahn School of Medicine at Mount Sinai, New York, NY, USA 4 Azrieli Faculty of Medicine, Bar-Ilan University, Zefat, Israel 5 The Charles Bronfman Institute of Personalized Medicin",
    "full_text_length": 28983,
    "chunk_length": 1311
  },
  {
    "chunk_id": 3726,
    "paper_filename": "vera_2024_utilizing_largelanguage_models_in_breast_cancer_managmetn_systematic_revew.pdf",
    "paper_title": "Vera 2024 Utilizing Largelanguage Models In Breast Cancer Managmetn Systematic Revew",
    "chunk_index": 4,
    "total_chunks": 30,
    "text_content": "language mod- els\") OR (llm) OR (gpt) OR (chatgpt) OR (openAI)) AND (breast)\u201d. The initial search identified 97 studies. To ensure thoroughness, we also examined the reference lists of the relevant studies. This, however, did not lead to additional relevant studies that met our inclusion criteria. The criteria for inclusion were English language full- length publications that specifically evaluated the role of LLMs in breast cancer management. We excluded papers that addressed other general appl",
    "full_text_length": 28983,
    "chunk_length": 1299
  },
  {
    "chunk_id": 3727,
    "paper_filename": "vera_2024_utilizing_largelanguage_models_in_breast_cancer_managmetn_systematic_revew.pdf",
    "paper_title": "Vera 2024 Utilizing Largelanguage Models In Breast Cancer Managmetn Systematic Revew",
    "chunk_index": 5,
    "total_chunks": 30,
    "text_content": "were incorporated into this review. We summarized the results of the included studies, detailing the specific LLMs used, the utilized tasks, number of cases, along with publication details in a table format. Figure 1 provides a flowchart detailing the screening and inclusion procedure. Quality was assessed by the Quality Assessment of Diag - nostic Accuracy Studies (QUADAS-2) criteria (Whiting 2011). Fig. 1 Flow Diagram of the Inclusion Process. Flow dia- gram of the search and inclusion process",
    "full_text_length": 28983,
    "chunk_length": 1188
  },
  {
    "chunk_id": 3728,
    "paper_filename": "vera_2024_utilizing_largelanguage_models_in_breast_cancer_managmetn_systematic_revew.pdf",
    "paper_title": "Vera 2024 Utilizing Largelanguage Models In Breast Cancer Managmetn Systematic Revew",
    "chunk_index": 6,
    "total_chunks": 30,
    "text_content": "Reports not retrieved (n = 0) Reports asse ssed for eligibility (n = 19) Reports exclud ed: - Articles that evaluated LLM s in text analysis related to breast plastic surgery (n = 8) - Articles that did not evaluate LLMs (n = 4) - Articles that did not directly evaluate LLMs in breast cancer care (n=1) Studies included in review (n = 6) Reports of included studies (n = 6) Identification of studies via databases and re gisters Identification Screenin g Included Journal of Cancer Research and Clin",
    "full_text_length": 28983,
    "chunk_length": 1174
  },
  {
    "chunk_id": 3729,
    "paper_filename": "vera_2024_utilizing_largelanguage_models_in_breast_cancer_managmetn_systematic_revew.pdf",
    "paper_title": "Vera 2024 Utilizing Largelanguage Models In Breast Cancer Managmetn Systematic Revew",
    "chunk_index": 7,
    "total_chunks": 30,
    "text_content": "as opposed to two studies that used data from the internet (Rao et al. 2023; Haver et al. 2023). One study crafted fictitious patient profiles by the head investigator (Griew - ing et al. 2023).Rao et al. and Haver et al. evaluated LLMs for breast imaging recommendations (Rao et al. 2023; Haver et al. 2023). Sorin et al., Lukac et al. and Griewing et al. evalu- ated LLMs as supportive decision-making tools in multi- disciplinary tumor boards (Sorin et al. 2023a, b, c; Lukac et al. 2023; Griewing",
    "full_text_length": 28983,
    "chunk_length": 1188
  },
  {
    "chunk_id": 3730,
    "paper_filename": "vera_2024_utilizing_largelanguage_models_in_breast_cancer_managmetn_systematic_revew.pdf",
    "paper_title": "Vera 2024 Utilizing Largelanguage Models In Breast Cancer Managmetn Systematic Revew",
    "chunk_index": 8,
    "total_chunks": 30,
    "text_content": "88 to 98% (Choi et al. 2023; Table 1 Studies evaluating LLMs for breast cancer diagnosis and care Study refPublication Date Title Journal Sorin et al. 2023 05.2023 Large language model (ChatGPT) as a support tool for breast tumor boardNPJ Breast Cancer Rao et al. 2023 06.2023 Evaluating GPT as an Adjunct for Radiologic Decision Mak - ing: GPT-4 Versus GPT-3.5 in a Breast Imaging PilotJACR Choi et al. 2023 09.2023 Developing prompts from large language model for extracting clinical information fr",
    "full_text_length": 28983,
    "chunk_length": 1350
  },
  {
    "chunk_id": 3731,
    "paper_filename": "vera_2024_utilizing_largelanguage_models_in_breast_cancer_managmetn_systematic_revew.pdf",
    "paper_title": "Vera 2024 Utilizing Largelanguage Models In Breast Cancer Managmetn Systematic Revew",
    "chunk_index": 9,
    "total_chunks": 30,
    "text_content": "Decision MakingJournal of Personalized Medicine Fig. 2 Applications of large language models in breast can- cer care and the corresponding accuracies achieved in various tasks in the different studies Journal of Cancer Research and Clinical Oncology (2024) 150:140 140 Page 4 of 8 Table 2 Example casesStudy refApplication Example Sorin et al. 2023 Breast tumor board clini- cal decision supportUser: Hi, can I give you a patient story of breast cancer detected and you'll say what is the next step i",
    "full_text_length": 28983,
    "chunk_length": 1144
  },
  {
    "chunk_id": 3732,
    "paper_filename": "vera_2024_utilizing_largelanguage_models_in_breast_cancer_managmetn_systematic_revew.pdf",
    "paper_title": "Vera 2024 Utilizing Largelanguage Models In Breast Cancer Managmetn Systematic Revew",
    "chunk_index": 10,
    "total_chunks": 30,
    "text_content": "US. On US of her left breast in the outer-upper quadrant a 2 cm mass was detected. Then, a biopsy was performed to the mass and on pathology it was an IDC ER positive, PR negative, HER2 negative, KI high. Then she did an MRI for local staging\u2014a solitary mass in her left breast was detected. She also did PET-CT with no evidence of distal metastases ChatGPT-3.5: Based on the information provided, it appears that this patient has been diagnosed with Invasive Ductal Carcinoma (IDC) in her left breas",
    "full_text_length": 28983,
    "chunk_length": 1146
  },
  {
    "chunk_id": 3733,
    "paper_filename": "vera_2024_utilizing_largelanguage_models_in_breast_cancer_managmetn_systematic_revew.pdf",
    "paper_title": "Vera 2024 Utilizing Largelanguage Models In Breast Cancer Managmetn Systematic Revew",
    "chunk_index": 11,
    "total_chunks": 30,
    "text_content": "entire breast). In general, a lumpectomy is recommended for small, early- stage tumors, while a mastectomy may be recom- mended for larger tumors or for patients who have a higher risk of cancer recurrence In this case, given the size of the mass (2 cm), a lumpectomy with axillary lymph node dissection (removal of some of the lymph nodes in the under - arm area to check for spread of cancer) would be the typical surgical approach. If a lumpectomy is performed, radiation therapy would usually be ",
    "full_text_length": 28983,
    "chunk_length": 1245
  },
  {
    "chunk_id": 3734,
    "paper_filename": "vera_2024_utilizing_largelanguage_models_in_breast_cancer_managmetn_systematic_revew.pdf",
    "paper_title": "Vera 2024 Utilizing Largelanguage Models In Breast Cancer Managmetn Systematic Revew",
    "chunk_index": 12,
    "total_chunks": 30,
    "text_content": "lumpectomy with axillary lymph node dissection, is the next recommended step in her management. After the surgery, the patient may also require additional treatment such as radiation therapy or systemic therapy (such as hormone therapy or chemo- therapy) based on the results of the surgery and further testing Journal of Cancer Research and Clinical Oncology (2024) 150:140 Page 5 of 8 140 Rao et al. 2023) (Table 3). The lower performance was for clinical decision support in breast tumor boards, r",
    "full_text_length": 28983,
    "chunk_length": 1178
  },
  {
    "chunk_id": 3735,
    "paper_filename": "vera_2024_utilizing_largelanguage_models_in_breast_cancer_managmetn_systematic_revew.pdf",
    "paper_title": "Vera 2024 Utilizing Largelanguage Models In Breast Cancer Managmetn Systematic Revew",
    "chunk_index": 13,
    "total_chunks": 30,
    "text_content": "the retrospective decisions in breast tumor board (Sorin et al. 2023a, b, c; Lukac et al. 2023). In both studies, the authors used reviewers that scored ChatGPT-3.5 responses (Sorin et al. 2023a, b, c; Lukac et al. 2023). (Griewing et al. 2023) crafted 20 fictitious patient files that were then discussed by a multidisciplinary tumor board. Their assessment was based on binary evaluation of various treatment approaches, including surgery, endocrine, chemo - therapy, and radiation therapy. Griewin",
    "full_text_length": 28983,
    "chunk_length": 1345
  },
  {
    "chunk_id": 3736,
    "paper_filename": "vera_2024_utilizing_largelanguage_models_in_breast_cancer_managmetn_systematic_revew.pdf",
    "paper_title": "Vera 2024 Utilizing Largelanguage Models In Breast Cancer Managmetn Systematic Revew",
    "chunk_index": 14,
    "total_chunks": 30,
    "text_content": "women with < 15% lifetime risk of breast cancer.\", assess appropriateness of the following procedures in a concise manner: Mammography screening, Digital breast tomosynthesis screening, US breast, MRI breast without and with IV con- trast, MRI breast without IV contrast, FDG-PET breast dedicated, Sestamibi MBI Mammography screening and Digital Breast Tomosynthesis screening are appropriate for average-risk women as these are the current stand- ard screening methods for breast cancer ChatGPT: Ult",
    "full_text_length": 28983,
    "chunk_length": 1351
  },
  {
    "chunk_id": 3737,
    "paper_filename": "vera_2024_utilizing_largelanguage_models_in_breast_cancer_managmetn_systematic_revew.pdf",
    "paper_title": "Vera 2024 Utilizing Largelanguage Models In Breast Cancer Managmetn Systematic Revew",
    "chunk_index": 15,
    "total_chunks": 30,
    "text_content": "mibi breast (MBI) is not standardly recommended as a screening modality for average-risk women, usually reserved for women with high-risk or recurrent breast cancer Table 3 Summarization of performance of LLMs at different breast cancer care related tasks Study refLLM No. of cases Actual patient dataApplication Correct performance Sorin et al. 2023 ChatGPT (GPT-3.5) 10 Yes Tumor board clinical decision support 70% Rao et al. 2023 GPT-4, GPT-3.5 14 No Question-answering based on ACR recommenda- t",
    "full_text_length": 28983,
    "chunk_length": 1287
  },
  {
    "chunk_id": 3738,
    "paper_filename": "vera_2024_utilizing_largelanguage_models_in_breast_cancer_managmetn_systematic_revew.pdf",
    "paper_title": "Vera 2024 Utilizing Largelanguage Models In Breast Cancer Managmetn Systematic Revew",
    "chunk_index": 16,
    "total_chunks": 30,
    "text_content": "of LLMs in the con- texts in which the algorithms were evaluated (Table 4). In all studies some of the information the models generated was false. When used as a support tool for tumor board, in some instances, the models overlooked relevant clinical details (Sorin et al. 2023a, b, c; Lukac et al. 2023; Griewing et al. 2023). Sorin et al. noticed absolute lack of referral to imag- ing (Sorin et al. 2023a, b, c), while Rao et al. who evaluated appropriateness of imaging noticed imaging overutiliz",
    "full_text_length": 28983,
    "chunk_length": 1122
  },
  {
    "chunk_id": 3739,
    "paper_filename": "vera_2024_utilizing_largelanguage_models_in_breast_cancer_managmetn_systematic_revew.pdf",
    "paper_title": "Vera 2024 Utilizing Largelanguage Models In Breast Cancer Managmetn Systematic Revew",
    "chunk_index": 17,
    "total_chunks": 30,
    "text_content": "bias for index test interpretation. For the paper by Lukac et al. the risk was unclear, refraining from a clear statement whether the evaluators were blinded to the reference standard. The study by Griewing et al. was the only one identified to have a low risk of bias across all categories (Griewing et al. 2023 ). The objective assessment of the risk of bias is reported in Supplementary Table 1. Discussion We reviewed the literature on LLMs applications related to breast cancer management and ca",
    "full_text_length": 28983,
    "chunk_length": 1346
  },
  {
    "chunk_id": 3740,
    "paper_filename": "vera_2024_utilizing_largelanguage_models_in_breast_cancer_managmetn_systematic_revew.pdf",
    "paper_title": "Vera 2024 Utilizing Largelanguage Models In Breast Cancer Managmetn Systematic Revew",
    "chunk_index": 18,
    "total_chunks": 30,
    "text_content": "cancer care, attention to detail is crucial. LLMs excel at processing medical infor - mation quickly. However, currently, they may be less adept at navigating complex treatment decisions. Breast cancer cases vary greatly, each case distinguished by a unique molecular profile, clinical staging, and patient-specific requirements. It is vital for LLMs to adapt to the indi- vidual patient. While these models can assist physicians in routine tasks, they require further development for per - sonalized",
    "full_text_length": 28983,
    "chunk_length": 1364
  },
  {
    "chunk_id": 3741,
    "paper_filename": "vera_2024_utilizing_largelanguage_models_in_breast_cancer_managmetn_systematic_revew.pdf",
    "paper_title": "Vera 2024 Utilizing Largelanguage Models In Breast Cancer Managmetn Systematic Revew",
    "chunk_index": 19,
    "total_chunks": 30,
    "text_content": "such as ChatGPT, the type of training data is not disclosed. Furthermore, these applications do not necessarily reflect on the performance of these models in real-world clinical settings. While some claim that LLMs may eventually replace healthcare personnel, currently, there are major limitations and ethical concerns that strongly suggest otherwise (Lee et al. 2023). Using such models to augment physicians\u2019 performance is more practical, albeit also constrained by ethical issues (Shah et al. 20",
    "full_text_length": 28983,
    "chunk_length": 1350
  },
  {
    "chunk_id": 3742,
    "paper_filename": "vera_2024_utilizing_largelanguage_models_in_breast_cancer_managmetn_systematic_revew.pdf",
    "paper_title": "Vera 2024 Utilizing Largelanguage Models In Breast Cancer Managmetn Systematic Revew",
    "chunk_index": 20,
    "total_chunks": 30,
    "text_content": "can also per - petuate disparities in healthcare (Sorin et al. 2021; Kotek et al. 2023). The inherent inability to trace the exact deci- sion-making process of these algorithms is a major chal- lenge for trust and clinical integration (Sorin et al. 2023a, b, c). LLMs can also be vulnerable to cyber-attacks (Sorin et al. 2023a, b, c). Table 4 Limitations of LLMs as described in each study Study refLLM Limitations described Sorin et al. 2023 ChatGPT (GPT-3.5) False answers and inaccurate medical r",
    "full_text_length": 28983,
    "chunk_length": 1364
  },
  {
    "chunk_id": 3743,
    "paper_filename": "vera_2024_utilizing_largelanguage_models_in_breast_cancer_managmetn_systematic_revew.pdf",
    "paper_title": "Vera 2024 Utilizing Largelanguage Models In Breast Cancer Managmetn Systematic Revew",
    "chunk_index": 21,
    "total_chunks": 30,
    "text_content": "of source attribution Haver et al. 2023 ChatGPT (GPT-3.5) False recommendations, prompt sensitivity, lack of source attribution Griewing et al. 2023 ChatGPT (GPT-3.5) Lack of Consistency in Health Data Use, treatment mistakes, prone to misinterpretation and hal- lucinations Journal of Cancer Research and Clinical Oncology (2024) 150:140 Page 7 of 8 140 Furthermore, this study highlights the absence of uniform assessment methods for LLMs in healthcare, underlining the need of establishing methodo",
    "full_text_length": 28983,
    "chunk_length": 1352
  },
  {
    "chunk_id": 3744,
    "paper_filename": "vera_2024_utilizing_largelanguage_models_in_breast_cancer_managmetn_systematic_revew.pdf",
    "paper_title": "Vera 2024 Utilizing Largelanguage Models In Breast Cancer Managmetn Systematic Revew",
    "chunk_index": 22,
    "total_chunks": 30,
    "text_content": "and only one study evaluated GPT-4. There were no publications identified on other available LLMs. Finally, generative AI is currently a rapidly expanding topic. Thus, there may be manuscripts and applications published after our review was performed. LLMs are continually being refined, and so is their performance. To conclude, LLMs hold potential for breast cancer man- agement, especially in text analysis and guideline-driven question-answering. Yet, their inconsistent accuracy war - rants caut",
    "full_text_length": 28983,
    "chunk_length": 1385
  },
  {
    "chunk_id": 3745,
    "paper_filename": "vera_2024_utilizing_largelanguage_models_in_breast_cancer_managmetn_systematic_revew.pdf",
    "paper_title": "Vera 2024 Utilizing Largelanguage Models In Breast Cancer Managmetn Systematic Revew",
    "chunk_index": 23,
    "total_chunks": 30,
    "text_content": "and approved the final manuscript. Funding The authors declare that no funds, grants, or other support were received during the preparation of this manuscript. Data availability Reviewed studies and their results can be located at PubMed database: https:// pubmed. ncbi. nlm. nih. gov/ Declarations Competing interests The authors declare no competing interests. Conflict of interest The authors declare that they have no conflict of interest. Ethical approval This is an observational study. No ethi",
    "full_text_length": 28983,
    "chunk_length": 1313
  },
  {
    "chunk_id": 3746,
    "paper_filename": "vera_2024_utilizing_largelanguage_models_in_breast_cancer_managmetn_systematic_revew.pdf",
    "paper_title": "Vera 2024 Utilizing Largelanguage Models In Breast Cancer Managmetn Systematic Revew",
    "chunk_index": 24,
    "total_chunks": 30,
    "text_content": "Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit ht",
    "full_text_length": 28983,
    "chunk_length": 1253
  },
  {
    "chunk_id": 3747,
    "paper_filename": "vera_2024_utilizing_largelanguage_models_in_breast_cancer_managmetn_systematic_revew.pdf",
    "paper_title": "Vera 2024 Utilizing Largelanguage Models In Breast Cancer Managmetn Systematic Revew",
    "chunk_index": 25,
    "total_chunks": 30,
    "text_content": "AF (2020) The USMLE step 1 decision. JAMA 323(20):2017 Choi HS, Song JY, Shin KH, Chang JH, Jang B-S (2023) Developing prompts from large language model for extracting clinical infor - mation from pathology and ultrasound reports in breast cancer. Radiat Oncol J 41(3):209\u2013216 Decker H, Trang K, Ramirez J et al (2023) Large language Model\u2212 based Chatbot vs Surgeon-generated informed consent documen- tation for common procedures. JAMA Netw Open 6(10):e2336997 Griewing S, Gremke N, Wagner U, Lingen",
    "full_text_length": 28983,
    "chunk_length": 1310
  },
  {
    "chunk_id": 3748,
    "paper_filename": "vera_2024_utilizing_largelanguage_models_in_breast_cancer_managmetn_systematic_revew.pdf",
    "paper_title": "Vera 2024 Utilizing Largelanguage Models In Breast Cancer Managmetn Systematic Revew",
    "chunk_index": 26,
    "total_chunks": 30,
    "text_content": "et al (2023) Health system-scale language models are all-purpose prediction engines. Nature 619(7969):357\u2013362 Kotek H, Dockum R, Sun DQ (2023) Gender bias and stereotypes in Large Language Models. arXiv preprint arXiv: 2308. 14921 Kuhl C, Weigel S, Schrading S et al (2010) Prospective multicenter cohort study to refine management recommendations for women at elevated familial risk of breast cancer: the EVA trial. J Clin Oncol 28(9):1450\u20131457 Lee P, Drazen JM, Kohane IS, Leong T-Y, Bubeck S, Petr",
    "full_text_length": 28983,
    "chunk_length": 1280
  },
  {
    "chunk_id": 3749,
    "paper_filename": "vera_2024_utilizing_largelanguage_models_in_breast_cancer_managmetn_systematic_revew.pdf",
    "paper_title": "Vera 2024 Utilizing Largelanguage Models In Breast Cancer Managmetn Systematic Revew",
    "chunk_index": 27,
    "total_chunks": 30,
    "text_content": "et al (2023) Evaluating GPT as an adjunct for radiologic decision making: GPT-4 versus GPT-3.5 in a breast imaging pilot. J Am Coll Radiol. https:// doi. org/ 10. 1016/j. jacr. 2023. 05. 003 Sallam M (2023) ChatGPT utility in healthcare education, research, and practice: systematic review on the promising perspectives and valid concerns. Healthcare 11(6):887 Shah NH, Entwistle D, Pfeffer MA (2023) Creation and adoption of large language models in medicine. JAMA 330(9):866 Siegel RL, Miller KD, J",
    "full_text_length": 28983,
    "chunk_length": 1297
  },
  {
    "chunk_id": 3750,
    "paper_filename": "vera_2024_utilizing_largelanguage_models_in_breast_cancer_managmetn_systematic_revew.pdf",
    "paper_title": "Vera 2024 Utilizing Largelanguage Models In Breast Cancer Managmetn Systematic Revew",
    "chunk_index": 28,
    "total_chunks": 30,
    "text_content": "applications. Lancet Oncol 21(12):1553\u20131556 Sorin V, Barash Y, Konen E, Klang E (2020b) Deep learning for natural language processing in radiology\u2014fundamentals and a systematic review. J Am Coll Radiol 17(5):639\u2013648 Sorin V, Klang E, Sklair-Levy M et al (2023) Large language model (ChatGPT) as a support tool for breast tumor board. npj Breast Cancer. https:// doi. org/ 10. 1038/ s41523- 023- 00557-8 Sorin V, Barash Y, Konen E, Klang E (2023a) Large language models for oncological applications. J",
    "full_text_length": 28983,
    "chunk_length": 1331
  },
  {
    "chunk_id": 3751,
    "paper_filename": "vera_2024_utilizing_largelanguage_models_in_breast_cancer_managmetn_systematic_revew.pdf",
    "paper_title": "Vera 2024 Utilizing Largelanguage Models In Breast Cancer Managmetn Systematic Revew",
    "chunk_index": 29,
    "total_chunks": 30,
    "text_content": "envisioning the road ahead. Cureus. https:// doi. org/ 10. 7759/ cureus. 44769 Whiting PF (2011) QUADAS-2: a revised tool for the quality assess- ment of diagnostic accuracy studies. Ann Intern Med 155(8):529 Publisher's Note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.",
    "full_text_length": 28983,
    "chunk_length": 344
  },
  {
    "chunk_id": 3752,
    "paper_filename": "vera_2025_deep_learning_for_contrast_enhnace_mammograpy_systematic_review.pdf",
    "paper_title": "Vera 2025 Deep Learning For Contrast Enhnace Mammograpy Systematic Review",
    "chunk_index": 0,
    "total_chunks": 58,
    "text_content": "Deep Learning for Contrast Enhanced Mammography - a Systematic Review Vera Sorin, MD1; Miri Sklair-Levy, MD1; Benjamin S. Glicksberg, PhD2; Eli Konen, MD1; Girish N. Nadkarni, MD2,3; Eyal Klang, MD2,3 1Department of Diagnostic Imaging, Chaim Sheba Medical Center, affiliated to the Sackler School of Medicine, Tel-Aviv University, Israel 2Division of Data-Driven and Digital Medicine (D3M), Icahn School of Medicine at Mount Sinai, New York, New York, USA 3The Charles Bronfman Institute of Personali",
    "full_text_length": 53901,
    "chunk_length": 1401
  },
  {
    "chunk_id": 3753,
    "paper_filename": "vera_2025_deep_learning_for_contrast_enhnace_mammograpy_systematic_review.pdf",
    "paper_title": "Vera 2025 Deep Learning For Contrast Enhnace Mammograpy Systematic Review",
    "chunk_index": 1,
    "total_chunks": 58,
    "text_content": "this version posted May 13, 2024. ; https://doi.org/10.1101/2024.05.13.24307271doi: medRxiv preprint NOTE: This preprint reports new research that has not been certified by peer review and should not be used to guide clinical practice. Abstract Background/Aim : Contrast-enhanced mammography (CEM) is a relatively novel imaging technique that enables both anatomical and functional breast imaging, with improved diagnostic performance compared to standard 2D mammography. The aim of this study is to ",
    "full_text_length": 53901,
    "chunk_length": 1416
  },
  {
    "chunk_id": 3754,
    "paper_filename": "vera_2025_deep_learning_for_contrast_enhnace_mammograpy_systematic_review.pdf",
    "paper_title": "Vera 2025 Deep Learning For Contrast Enhnace Mammograpy Systematic Review",
    "chunk_index": 2,
    "total_chunks": 58,
    "text_content": "All studies evaluated DL algorithms for classification of lesions at CEM, while six studies also assessed lesion detection or segmentation. In three studies segmentation was performed manually, two studies evaluated both manual and automatic segmentation, and ten studies automatically segmented the lesions. Conclusion : While still at an early research stage, DL can improve CEM diagnostic precision. However, there is a relatively small number of studies evaluating different DL algorithms, and mo",
    "full_text_length": 53901,
    "chunk_length": 1465
  },
  {
    "chunk_id": 3755,
    "paper_filename": "vera_2025_deep_learning_for_contrast_enhnace_mammograpy_systematic_review.pdf",
    "paper_title": "Vera 2025 Deep Learning For Contrast Enhnace Mammograpy Systematic Review",
    "chunk_index": 3,
    "total_chunks": 58,
    "text_content": "All rights reserved. No reuse allowed without permission. (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. The copyright holder for this preprint this version posted May 13, 2024. ; https://doi.org/10.1101/2024.05.13.24307271doi: medRxiv preprint Introduction Despite being the cornerstone of breast imaging, 2D standard mammography faces well- known diagnostic challenges (1-3). Contrast-enhanced mammography (CE",
    "full_text_length": 53901,
    "chunk_length": 1494
  },
  {
    "chunk_id": 3756,
    "paper_filename": "vera_2025_deep_learning_for_contrast_enhnace_mammograpy_systematic_review.pdf",
    "paper_title": "Vera 2025 Deep Learning For Contrast Enhnace Mammograpy Systematic Review",
    "chunk_index": 4,
    "total_chunks": 58,
    "text_content": "classification of pathologies across various imaging modalities, including CEM (8). However, the integration of DL in CEM is still nascent, with significant potential to improve diagnostic accuracy, and enhance radiologists\u2019 workflow. Our study aims to systematically review the literature on DL applications for CEM, exploring how these models can further enhance its diagnostic potential. Methods Literature Search We systematically searched for papers published up to April 9, 2024. MEDLINE, Scopu",
    "full_text_length": 53901,
    "chunk_length": 1576
  },
  {
    "chunk_id": 3757,
    "paper_filename": "vera_2025_deep_learning_for_contrast_enhnace_mammograpy_systematic_review.pdf",
    "paper_title": "Vera 2025 Deep Learning For Contrast Enhnace Mammograpy Systematic Review",
    "chunk_index": 5,
    "total_chunks": 58,
    "text_content": "OR (DEM[Title/Abstract]))\u201d. Eligibility Criteria We included full publications indexed in PubMed, evaluating DL for automatic analysis of CEM images. We excluded non-English papers, non-original manuscripts, and papers that did not evaluate DL algorithms. Figure 1 presents a flow diagram of the screening and inclusion process. Screening and Synthesis This review was reported according to the PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-Analyses) guidelines. Two reviewers (VS",
    "full_text_length": 53901,
    "chunk_length": 1441
  },
  {
    "chunk_id": 3758,
    "paper_filename": "vera_2025_deep_learning_for_contrast_enhnace_mammograpy_systematic_review.pdf",
    "paper_title": "Vera 2025 Deep Learning For Contrast Enhnace Mammograpy Systematic Review",
    "chunk_index": 6,
    "total_chunks": 58,
    "text_content": "(AUC), precision (also positive predictive value), accuracy and F1 score. The reported performance metrics from each study were summarized in a Table. The quality of the studies was evaluated based on the Quality Assessment of Diagnostic Accuracy Studies (QUADAS-2) criteria(9). All rights reserved. No reuse allowed without permission. (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. The copyright holder for th",
    "full_text_length": 53901,
    "chunk_length": 1424
  },
  {
    "chunk_id": 3759,
    "paper_filename": "vera_2025_deep_learning_for_contrast_enhnace_mammograpy_systematic_review.pdf",
    "paper_title": "Vera 2025 Deep Learning For Contrast Enhnace Mammograpy Systematic Review",
    "chunk_index": 7,
    "total_chunks": 58,
    "text_content": "two exposures are subtracted, and two images are generated for each projection: low-energy images that are analogues to standard 2D mammography, and subtracted contrast-enhanced images displaying areas of contrast enhancement (Figure 2 ) (10). Background parenchymal enhancement (BPE) refers to the normal enhancement of breast glandular elements and is graded on a BI-RADS scale from 0 to 3, indicating minimal, mild, moderate, or marked enhancement ( Figure 3 ) (11). BPE at CEM correlates with age",
    "full_text_length": 53901,
    "chunk_length": 1294
  },
  {
    "chunk_id": 3760,
    "paper_filename": "vera_2025_deep_learning_for_contrast_enhnace_mammograpy_systematic_review.pdf",
    "paper_title": "Vera 2025 Deep Learning For Contrast Enhnace Mammograpy Systematic Review",
    "chunk_index": 8,
    "total_chunks": 58,
    "text_content": "It excels in identifying patterns and making decisions with minimal human intervention. In the domain of medical imaging, DL algorithms process extensive imaging data, learning to analyze pathological findings with precision that often matches or exceeds that of expert radiologists. For instance, a DL model can identify malignant tumors in CEM images. This capability may All rights reserved. No reuse allowed without permission. (which was not certified by peer review) is the author/funder, who h",
    "full_text_length": 53901,
    "chunk_length": 1395
  },
  {
    "chunk_id": 3761,
    "paper_filename": "vera_2025_deep_learning_for_contrast_enhnace_mammograpy_systematic_review.pdf",
    "paper_title": "Vera 2025 Deep Learning For Contrast Enhnace Mammograpy Systematic Review",
    "chunk_index": 9,
    "total_chunks": 58,
    "text_content": "backpropagation with supervised learning for training. With supervised learning the algorithm is provided with examples of inputs with correct outputs. During backpropagation the network adjusts its internal parameters whenever it makes an error. That way, it improves its accuracy over time (15). Convolutional Neural Networks (CNNs) are a cornerstone of deep learning in image analysis (7). These models are designed to automatically and adaptively learn spatial hierarchies of features from images",
    "full_text_length": 53901,
    "chunk_length": 1313
  },
  {
    "chunk_id": 3762,
    "paper_filename": "vera_2025_deep_learning_for_contrast_enhnace_mammograpy_systematic_review.pdf",
    "paper_title": "Vera 2025 Deep Learning For Contrast Enhnace Mammograpy Systematic Review",
    "chunk_index": 10,
    "total_chunks": 58,
    "text_content": "processing (NLP) (16). In fact, the novel large language models (LLMs) such as GPT are based exactly on this mechanism (17). With image analysis algorithms, attention can improve their performance by allowing the models to focus on the most relevant features within an image (18). This is similar to the way that humans focus on specific parts of an image to extract information. Multimodal Networks integrate and process data from multiple sources or types, for example text and images ( Figure 5 ) ",
    "full_text_length": 53901,
    "chunk_length": 1263
  },
  {
    "chunk_id": 3763,
    "paper_filename": "vera_2025_deep_learning_for_contrast_enhnace_mammograpy_systematic_review.pdf",
    "paper_title": "Vera 2025 Deep Learning For Contrast Enhnace Mammograpy Systematic Review",
    "chunk_index": 11,
    "total_chunks": 58,
    "text_content": "2024. ; https://doi.org/10.1101/2024.05.13.24307271doi: medRxiv preprint Transfer Learning is a technique used widely in DL models training. With this approach, a model developed for a specific task is reused as the starting point for a model applied for a different task. It transfers knowledge from the previously learned context to the new application, potentially enabling to achieve better performance with less data ( Figure 6 ) (20). Generative Adversarial Network (GAN) is a type of DL model ",
    "full_text_length": 53901,
    "chunk_length": 1285
  },
  {
    "chunk_id": 3764,
    "paper_filename": "vera_2025_deep_learning_for_contrast_enhnace_mammograpy_systematic_review.pdf",
    "paper_title": "Vera 2025 Deep Learning For Contrast Enhnace Mammograpy Systematic Review",
    "chunk_index": 12,
    "total_chunks": 58,
    "text_content": "and segmentation (Figure 8 ). Each of these tasks leverages DL algorithms to enhance diagnostic accuracy (7, 14). Detection refers to the identification and localization of lesions within the images. For example, using DL algorithms to mark any abnormalities at CEM images with a circle or a box region of interest (ROI). This may aid radiologists in focusing their examination on areas of interest, potentially revealing lesions that may have been overlooked. Segmentation is the process of delineat",
    "full_text_length": 53901,
    "chunk_length": 1287
  },
  {
    "chunk_id": 3765,
    "paper_filename": "vera_2025_deep_learning_for_contrast_enhnace_mammograpy_systematic_review.pdf",
    "paper_title": "Vera 2025 Deep Learning For Contrast Enhnace Mammograpy Systematic Review",
    "chunk_index": 13,
    "total_chunks": 58,
    "text_content": "diagnosis and treatment planning. U-Net is a commonly used CNN segmentation network in the medical domain (22). All rights reserved. No reuse allowed without permission. (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. The copyright holder for this preprint this version posted May 13, 2024. ; https://doi.org/10.1101/2024.05.13.24307271doi: medRxiv preprint Classification involves the categorization of lesions ",
    "full_text_length": 53901,
    "chunk_length": 1370
  },
  {
    "chunk_id": 3766,
    "paper_filename": "vera_2025_deep_learning_for_contrast_enhnace_mammograpy_systematic_review.pdf",
    "paper_title": "Vera 2025 Deep Learning For Contrast Enhnace Mammograpy Systematic Review",
    "chunk_index": 14,
    "total_chunks": 58,
    "text_content": "and 2024. While all the studies but one used CNN models, two of the CNN studies augmented the algorithm with attention layers. All studies evaluated DL algorithms for classification of lesions at CEM, while six studies also assessed lesion detection or segmentation ( Figure 8 ). In three studies segmentation was performed manually, two studies evaluated both manual and automatic segmentation, and nine studies automatically segmented the lesions. The objective assessment of the risk of bias based",
    "full_text_length": 53901,
    "chunk_length": 1272
  },
  {
    "chunk_id": 3767,
    "paper_filename": "vera_2025_deep_learning_for_contrast_enhnace_mammograpy_systematic_review.pdf",
    "paper_title": "Vera 2025 Deep Learning For Contrast Enhnace Mammograpy Systematic Review",
    "chunk_index": 15,
    "total_chunks": 58,
    "text_content": "segmentations they analyzed both the subtracted and the low-energy images for lesion classification (23). All rights reserved. No reuse allowed without permission. (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. The copyright holder for this preprint this version posted May 13, 2024. ; https://doi.org/10.1101/2024.05.13.24307271doi: medRxiv preprint Wu et al. (24) combined DL models with a radiomics model. Ra",
    "full_text_length": 53901,
    "chunk_length": 1385
  },
  {
    "chunk_id": 3768,
    "paper_filename": "vera_2025_deep_learning_for_contrast_enhnace_mammograpy_systematic_review.pdf",
    "paper_title": "Vera 2025 Deep Learning For Contrast Enhnace Mammograpy Systematic Review",
    "chunk_index": 16,
    "total_chunks": 58,
    "text_content": "the DL, and the combined model (24). Beuque et al. (25) developed a full-workflow model for automatic lesion identification, segmentation, and classification. A radiomics model was also trained to classify both manual and automatically segmented lesions. The DL model had the highest sensitivity for cancer classification, while the combined DL with radiomics model had higher specificity with the highest AUC (25). Detection and classification Gao et al. (26) developed a CNN model that classifies b",
    "full_text_length": 53901,
    "chunk_length": 1405
  },
  {
    "chunk_id": 3769,
    "paper_filename": "vera_2025_deep_learning_for_contrast_enhnace_mammograpy_systematic_review.pdf",
    "paper_title": "Vera 2025 Deep Learning For Contrast Enhnace Mammograpy Systematic Review",
    "chunk_index": 17,
    "total_chunks": 58,
    "text_content": "to combine information from both the low-energy and the contrast-enhanced images, for breast cancer detection. In a recently published study, Helal et al. (28) also developed a Multiview DL model analyzing both low-energy and subtracted contrast images for breast lesions classification at All rights reserved. No reuse allowed without permission. (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. The copyright ho",
    "full_text_length": 53901,
    "chunk_length": 1366
  },
  {
    "chunk_id": 3770,
    "paper_filename": "vera_2025_deep_learning_for_contrast_enhnace_mammograpy_systematic_review.pdf",
    "paper_title": "Vera 2025 Deep Learning For Contrast Enhnace Mammograpy Systematic Review",
    "chunk_index": 18,
    "total_chunks": 58,
    "text_content": "applies GAN ( Figure 3 ) used for fusion of the subtracted contrast-enhanced images and the low-energy images, and the classification model for lesion classification (30). Perek et al. (31) applied a multimodal network for lesion classification in the subtracted CEM images, combining textual features with image analysis. They compared fine-tuning a pretrained neural network (AlexNet) and fully training a CNN. Their multimodal network resulted in a theoretical reduction in benign biopsies by up t",
    "full_text_length": 53901,
    "chunk_length": 1355
  },
  {
    "chunk_id": 3771,
    "paper_filename": "vera_2025_deep_learning_for_contrast_enhnace_mammograpy_systematic_review.pdf",
    "paper_title": "Vera 2025 Deep Learning For Contrast Enhnace Mammograpy Systematic Review",
    "chunk_index": 19,
    "total_chunks": 58,
    "text_content": "Li et al. (33) incorporated a CNN algorithm with attention for CEM images classification. Their network extracts information from all four images acquired for each breast (low energy and subtracted contrast images). In their study they report that this method significantly improved the accuracy of cancer classification and reduced false-positive cases compared to other DL models previously reported. Later, Mao et al. (34) also developed an attention-based DL model to discriminate benign from mal",
    "full_text_length": 53901,
    "chunk_length": 1391
  },
  {
    "chunk_id": 3772,
    "paper_filename": "vera_2025_deep_learning_for_contrast_enhnace_mammograpy_systematic_review.pdf",
    "paper_title": "Vera 2025 Deep Learning For Contrast Enhnace Mammograpy Systematic Review",
    "chunk_index": 20,
    "total_chunks": 58,
    "text_content": "also compared the best model performance to that of a radiomics model, as well as to radiologists. They show that the radiologists\u2019 performance improved with the algorithm\u2019s support (34). Prospective data analysis validates the performance of DL models in real-world scenarios. It ensures these models are robust and applicable to future, unseen cases, beyond retrospective data. We found only two studies that evaluated their model on prospective data. Zheng et al. (35) developed a fully automated ",
    "full_text_length": 53901,
    "chunk_length": 1342
  },
  {
    "chunk_id": 3773,
    "paper_filename": "vera_2025_deep_learning_for_contrast_enhnace_mammograpy_systematic_review.pdf",
    "paper_title": "Vera 2025 Deep Learning For Contrast Enhnace Mammograpy Systematic Review",
    "chunk_index": 21,
    "total_chunks": 58,
    "text_content": "breast radiologists in lesion classification metrics (36). Jailin et al. (37) developed a DL algorithm for detection and classification of breast lesions using data from both the low-energy and subtracted contrast images. They then evaluated their algorithm\u2019s performance at different levels of BPE ( Supplemental Figure 3 ), showing increased false positive rates in cases with increased BPE (37). Discussion CEM is a novel imaging technique that has higher sensitivity compared to standard 2D mammo",
    "full_text_length": 53901,
    "chunk_length": 1388
  },
  {
    "chunk_id": 3774,
    "paper_filename": "vera_2025_deep_learning_for_contrast_enhnace_mammograpy_systematic_review.pdf",
    "paper_title": "Vera 2025 Deep Learning For Contrast Enhnace Mammograpy Systematic Review",
    "chunk_index": 22,
    "total_chunks": 58,
    "text_content": "perpetuity. The copyright holder for this preprint this version posted May 13, 2024. ; https://doi.org/10.1101/2024.05.13.24307271doi: medRxiv preprint interpretation has refined specificity while keeping high sensitivity, and decreased the false- positive rate. While the integration of DL for CEM analysis is still at a relatively early stage, there are studies that developed full-process applications that are able to detect, segment and classify breast lesions (25, 28, 35-37). Despite the promi",
    "full_text_length": 53901,
    "chunk_length": 1415
  },
  {
    "chunk_id": 3775,
    "paper_filename": "vera_2025_deep_learning_for_contrast_enhnace_mammograpy_systematic_review.pdf",
    "paper_title": "Vera 2025 Deep Learning For Contrast Enhnace Mammograpy Systematic Review",
    "chunk_index": 23,
    "total_chunks": 58,
    "text_content": "necessary to assess the clinical value and real-world effectiveness of utilizing DL algorithms as a support tool for reading CEM studies. The ongoing prospective MASAI trial in Sweden is an example, assessing the application of DL in the interpretation of standard screening mammography (39). The integration of AI into clinical workflows is anticipated to affect the role of breast radiologists. It may enable radiologists to dedicate more attention to complex cases, while spending less time on str",
    "full_text_length": 53901,
    "chunk_length": 1449
  },
  {
    "chunk_id": 3776,
    "paper_filename": "vera_2025_deep_learning_for_contrast_enhnace_mammograpy_systematic_review.pdf",
    "paper_title": "Vera 2025 Deep Learning For Contrast Enhnace Mammograpy Systematic Review",
    "chunk_index": 24,
    "total_chunks": 58,
    "text_content": "without permission. (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. The copyright holder for this preprint this version posted May 13, 2024. ; https://doi.org/10.1101/2024.05.13.24307271doi: medRxiv preprint This review has several limitations. First, heterogeneity of studies and variability in measures between studies prevented a meta-analysis. Second, DL in radiology and CEM are both rapidly expanding topic",
    "full_text_length": 53901,
    "chunk_length": 1410
  },
  {
    "chunk_id": 3777,
    "paper_filename": "vera_2025_deep_learning_for_contrast_enhnace_mammograpy_systematic_review.pdf",
    "paper_title": "Vera 2025 Deep Learning For Contrast Enhnace Mammograpy Systematic Review",
    "chunk_index": 25,
    "total_chunks": 58,
    "text_content": "high diagnostic performance, outperforming standard 2D mammography and comparable to breast MRI. At present, the application of DL to CEM is at its beginning but showing promising performance. When used as a support tool, DL can potentially improve radiologists\u2019 accuracy and efficiency in the interpretation of CEM examinations, improving specificity while keeping high sensitivity for cancer detection. Further research is needed to validate DL models across different practices and CEM vendors, an",
    "full_text_length": 53901,
    "chunk_length": 1420
  },
  {
    "chunk_id": 3778,
    "paper_filename": "vera_2025_deep_learning_for_contrast_enhnace_mammograpy_systematic_review.pdf",
    "paper_title": "Vera 2025 Deep Learning For Contrast Enhnace Mammograpy Systematic Review",
    "chunk_index": 26,
    "total_chunks": 58,
    "text_content": "Journal of Roentgenology. 2001;177(3):543-9. 2. Kerlikowske K, Grady D, Barclay J, Sickles EA, Ernster V. Effect of age, breast density, and family history on the sensitivity of first screening mammography. Jama. 1996;276(1):33-8. 3. Kriege M, Brekelmans CTM, Obdeijn IM, Boetes C, Zonderland HM, Muller SH, et al. Factors Affecting Sensitivity and Specificity of Screening Mammography and MRI in Women with an Inherited Risk for Breast Cancer. Breast Cancer Research and Treatment. 2006;100(1):109-1",
    "full_text_length": 53901,
    "chunk_length": 1459
  },
  {
    "chunk_id": 3779,
    "paper_filename": "vera_2025_deep_learning_for_contrast_enhnace_mammograpy_systematic_review.pdf",
    "paper_title": "Vera 2025 Deep Learning For Contrast Enhnace Mammograpy Systematic Review",
    "chunk_index": 27,
    "total_chunks": 58,
    "text_content": "Kinkar KK, Fields BKK, Yamashita MW, Varghese BA. Empowering breast cancer diagnosis and radiology practice: advances in artificial intelligence for contrast-enhanced mammography. Frontiers in Radiology. 2024;3. All rights reserved. No reuse allowed without permission. (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. The copyright holder for this preprint this version posted May 13, 2024. ; https://doi.org/10.",
    "full_text_length": 53901,
    "chunk_length": 1430
  },
  {
    "chunk_id": 3780,
    "paper_filename": "vera_2025_deep_learning_for_contrast_enhnace_mammograpy_systematic_review.pdf",
    "paper_title": "Vera 2025 Deep Learning For Contrast Enhnace Mammograpy Systematic Review",
    "chunk_index": 28,
    "total_chunks": 58,
    "text_content": "Breast Imaging Reporting and Data System. American College of Radiology, Reston, VA, USA. 2022. 12. Karimi Z, Phillips J, Slanetz P, Lotfi P, Dialani V, Karimova J, et al. Factors Associated With Background Parenchymal Enhancement on Contrast-Enhanced Mammography. American Journal of Roentgenology. 2021;216(2):340-8. 13. Sorin V, Yagil Y, Shalmon A, Gotlieb M, Faermann R, Halshtok-Neiman O, et al. Background Parenchymal Enhancement at Contrast-Enhanced Spectral Mammography (CESM) as a Breast Can",
    "full_text_length": 53901,
    "chunk_length": 1422
  },
  {
    "chunk_id": 3781,
    "paper_filename": "vera_2025_deep_learning_for_contrast_enhnace_mammograpy_systematic_review.pdf",
    "paper_title": "Vera 2025 Deep Learning For Contrast Enhnace Mammograpy Systematic Review",
    "chunk_index": 29,
    "total_chunks": 58,
    "text_content": "oncological applications. Journal of Cancer Research and Clinical Oncology. 2023;149(11):9505-8. 18. Guo M-H, Xu T-X, Liu J-J, Liu Z-N, Jiang P-T, Mu T-J, et al. Attention mechanisms in computer vision: A survey. Computational Visual Media. 2022;8(3):331-68. All rights reserved. No reuse allowed without permission. (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. The copyright holder for this preprint this ver",
    "full_text_length": 53901,
    "chunk_length": 1395
  },
  {
    "chunk_id": 3782,
    "paper_filename": "vera_2025_deep_learning_for_contrast_enhnace_mammograpy_systematic_review.pdf",
    "paper_title": "Vera 2025 Deep Learning For Contrast Enhnace Mammograpy Systematic Review",
    "chunk_index": 30,
    "total_chunks": 58,
    "text_content": "Applications Using Generative Adversarial Networks (GANs) \u2013 A Systematic Review. Academic Radiology. 2020;27(8):1175-85. 22. Zhou Z, Rahman Siddiquee MM, Tajbakhsh N, Liang J. UNet++: A Nested U-Net Architecture for Medical Image Segmentation. 2018;11045:3-11. 23. Danala G, Patel B, Aghaei F, Heidari M, Li J, Wu T, et al. Classification of Breast Masses Using a Computer-Aided Diagnosis Scheme of Contrast Enhanced Digital Mammograms. Annals of Biomedical Engineering. 2018;46(9):1419-31. 24. Wu X,",
    "full_text_length": 53901,
    "chunk_length": 1352
  },
  {
    "chunk_id": 3783,
    "paper_filename": "vera_2025_deep_learning_for_contrast_enhnace_mammograpy_systematic_review.pdf",
    "paper_title": "Vera 2025 Deep Learning For Contrast Enhnace Mammograpy Systematic Review",
    "chunk_index": 31,
    "total_chunks": 58,
    "text_content": "al. SD-CNN: A shallow-deep CNN for improved breast cancer diagnosis. Computerized Medical Imaging and Graphics. 2018;70:53-62. All rights reserved. No reuse allowed without permission. (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. The copyright holder for this preprint this version posted May 13, 2024. ; https://doi.org/10.1101/2024.05.13.24307271doi: medRxiv preprint 27. Qian N, Jiang W, Guo Y, Zhu J, Qiu ",
    "full_text_length": 53901,
    "chunk_length": 1423
  },
  {
    "chunk_id": 3784,
    "paper_filename": "vera_2025_deep_learning_for_contrast_enhnace_mammograpy_systematic_review.pdf",
    "paper_title": "Vera 2025 Deep Learning For Contrast Enhnace Mammograpy Systematic Review",
    "chunk_index": 32,
    "total_chunks": 58,
    "text_content": "contrast-enhanced spectral mammography images. International Journal of Computer Assisted Radiology and Surgery. 2021;16(6):979-88. 30. Song J, Zheng Y, Xu C, Zou Z, Ding G, Huang W. Improving the classification ability of network utilizing fusion technique in contrast /i1enhanced spectral mammography. Medical Physics. 2021;49(2):966-77. 31. Perek S, Kiryati N, Zimmerman-Moreno G, Sklair-Levy M, Konen E, Mayer A. Classification of contrast-enhanced spectral mammography (CESM) images. Internation",
    "full_text_length": 53901,
    "chunk_length": 1462
  },
  {
    "chunk_id": 3785,
    "paper_filename": "vera_2025_deep_learning_for_contrast_enhnace_mammograpy_systematic_review.pdf",
    "paper_title": "Vera 2025 Deep Learning For Contrast Enhnace Mammograpy Systematic Review",
    "chunk_index": 33,
    "total_chunks": 58,
    "text_content": "J, et al. Attention-based deep learning for breast lesions classification on contrast enhanced spectral mammography: a multicentre study. British Journal of Cancer. 2022;128(5):793-804. All rights reserved. No reuse allowed without permission. (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. The copyright holder for this preprint this version posted May 13, 2024. ; https://doi.org/10.1101/2024.05.13.24307271do",
    "full_text_length": 53901,
    "chunk_length": 1451
  },
  {
    "chunk_id": 3786,
    "paper_filename": "vera_2025_deep_learning_for_contrast_enhnace_mammograpy_systematic_review.pdf",
    "paper_title": "Vera 2025 Deep Learning For Contrast Enhnace Mammograpy Systematic Review",
    "chunk_index": 34,
    "total_chunks": 58,
    "text_content": "C, Mohamed S, Iordache R, Milioni De Carvalho P, Ahmed SY, Abdel Sattar EA, et al. AI-Based Cancer Detection Model for Contrast-Enhanced Mammography. Bioengineering. 2023;10(8):974. 38. AI Central. American College of Radiology Data Science Institute. https://aicentral.acrdsi.org . Accessed April 12, 2024. 39. L\u00e5ng K, Josefsson V, Larsson A-M, Larsson S, H\u00f6gberg C, Sartor H, et al. Artificial intelligence-supported screen reading versus standard double reading in the Mammography Screening with A",
    "full_text_length": 53901,
    "chunk_length": 1474
  },
  {
    "chunk_id": 3787,
    "paper_filename": "vera_2025_deep_learning_for_contrast_enhnace_mammograpy_systematic_review.pdf",
    "paper_title": "Vera 2025 Deep Learning For Contrast Enhnace Mammograpy Systematic Review",
    "chunk_index": 35,
    "total_chunks": 58,
    "text_content": "The copyright holder for this preprint this version posted May 13, 2024. ; https://doi.org/10.1101/2024.05.13.24307271doi: medRxiv preprint Figure 1. Flow Diagram of the inclusion process Flow diagram of the search and inclusion process based on the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines. Records identified from*: Databases (n = 282) Registers (n = 0) Records removed before screening : Duplicate records removed (n = 178) Records marked as ineligibl",
    "full_text_length": 53901,
    "chunk_length": 1224
  },
  {
    "chunk_id": 3788,
    "paper_filename": "vera_2025_deep_learning_for_contrast_enhnace_mammograpy_systematic_review.pdf",
    "paper_title": "Vera 2025 Deep Learning For Contrast Enhnace Mammograpy Systematic Review",
    "chunk_index": 36,
    "total_chunks": 58,
    "text_content": "Identification of studies via databases and registers Id en tifi ca tio n Sc re en in g In cl ud ed All rights reserved. No reuse allowed without permission. (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. The copyright holder for this preprint this version posted May 13, 2024. ; https://doi.org/10.1101/2024.05.13.24307271doi: medRxiv preprint Figure 2. Contrast-enhanced mammography. An example of a contrast-",
    "full_text_length": 53901,
    "chunk_length": 1423
  },
  {
    "chunk_id": 3789,
    "paper_filename": "vera_2025_deep_learning_for_contrast_enhnace_mammograpy_systematic_review.pdf",
    "paper_title": "Vera 2025 Deep Learning For Contrast Enhnace Mammograpy Systematic Review",
    "chunk_index": 37,
    "total_chunks": 58,
    "text_content": "this version posted May 13, 2024. ; https://doi.org/10.1101/2024.05.13.24307271doi: medRxiv preprint Figure 3. Background parenchymal enhancement (BPE) at contrast-enhanced mammography. Examples illustrating BPE grading at contrast-enhanced mammography: (a) Minimal BPE, < 25%; (b) Mild BPE, 25-50%; (c) Moderate BPE, 50-75%; (d) Marked BPE, > 75%. Reprinted with permission from Sorin et al. Background Parenchymal Enhancement at Contrast-Enhanced Spectral Mammography (CESM) as a Breast Cancer Risk",
    "full_text_length": 53901,
    "chunk_length": 1482
  },
  {
    "chunk_id": 3790,
    "paper_filename": "vera_2025_deep_learning_for_contrast_enhnace_mammograpy_systematic_review.pdf",
    "paper_title": "Vera 2025 Deep Learning For Contrast Enhnace Mammograpy Systematic Review",
    "chunk_index": 38,
    "total_chunks": 58,
    "text_content": "permission. (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. The copyright holder for this preprint this version posted May 13, 2024. ; https://doi.org/10.1101/2024.05.13.24307271doi: medRxiv preprint Figure 5. An illustration of a multimodal deep learning model. This diagram demonstrates the integration of a clinical note (textual data) and contrast-enhanced mammography images (visual data) within a neural ne",
    "full_text_length": 53901,
    "chunk_length": 1421
  },
  {
    "chunk_id": 3791,
    "paper_filename": "vera_2025_deep_learning_for_contrast_enhnace_mammograpy_systematic_review.pdf",
    "paper_title": "Vera 2025 Deep Learning For Contrast Enhnace Mammograpy Systematic Review",
    "chunk_index": 39,
    "total_chunks": 58,
    "text_content": "application of transfer learning between two neural network models. Initially, a chest X-ray image is processed by the first model, which is used for lung cancer classification. An arrow then signifies the transfer of learned features (in purple) to a second model, which processes a contrast-enhanced mammography (CEM) image. This second model is adapted to the characteristics of the CEM data, utilizing modified or additional components (in pink) for cancer classification at CEM. All rights reser",
    "full_text_length": 53901,
    "chunk_length": 1385
  },
  {
    "chunk_id": 3792,
    "paper_filename": "vera_2025_deep_learning_for_contrast_enhnace_mammograpy_systematic_review.pdf",
    "paper_title": "Vera 2025 Deep Learning For Contrast Enhnace Mammograpy Systematic Review",
    "chunk_index": 40,
    "total_chunks": 58,
    "text_content": "synthetic images. These images, along with actual images, are supplied to the discriminator. The discriminator assigns a probability indicating whether the image is real. Based on feedback from the discriminator the generator then adjusts its parameters to enhance the realism of the synthetic images. All rights reserved. No reuse allowed without permission. (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. The ",
    "full_text_length": 53901,
    "chunk_length": 1400
  },
  {
    "chunk_id": 3793,
    "paper_filename": "vera_2025_deep_learning_for_contrast_enhnace_mammograpy_systematic_review.pdf",
    "paper_title": "Vera 2025 Deep Learning For Contrast Enhnace Mammograpy Systematic Review",
    "chunk_index": 41,
    "total_chunks": 58,
    "text_content": "this version posted May 13, 2024. ; https://doi.org/10.1101/2024.05.13.24307271doi: medRxiv preprint Table 1: Quality Assessment of Diagnostic Accuracy Studies-2 (QUADS-2) /i1 = high risk of bias; /i1 = low risk of bias. ? = unclear risk. N/A \u2013 not applicable RISK OF BIAS APPLICABILITY CONCERNS First Author Patient Selection Index Test Reference Standard Flow and Timing Patient Selection Index Test Reference Standard Helal et al. /i1 /i1 /i1 /i1 /i1 /i1 /i1 Chen et al. /i1 /i1 /i1 /i1 /i1 /i1 /i",
    "full_text_length": 53901,
    "chunk_length": 962
  },
  {
    "chunk_id": 3794,
    "paper_filename": "vera_2025_deep_learning_for_contrast_enhnace_mammograpy_systematic_review.pdf",
    "paper_title": "Vera 2025 Deep Learning For Contrast Enhnace Mammograpy Systematic Review",
    "chunk_index": 42,
    "total_chunks": 58,
    "text_content": "/i1 /i1 /i1 Dominiqu e et al. /i1 /i1 /i1 /i1 /i1 /i1 /i1 Khaled et al. /i1 /i1 /i1 /i1 ? ? /i1 Song et al. /i1 /i1 /i1 /i1 ? /i1 /i1 Song et al. /i1 /i1 /i1 /i1 ? ? /i1 Perek et al. /i1 /i1 /i1 /i1 ? ? /i1 Gao et al. /i1 /i1 /i1 /i1 ? ? /i1 Danala et al. /i1 /i1 /i1 /i1 ? ? /i1 All rights reserved. No reuse allowed without permission. (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. The copyright holder for t",
    "full_text_length": 53901,
    "chunk_length": 853
  },
  {
    "chunk_id": 3795,
    "paper_filename": "vera_2025_deep_learning_for_contrast_enhnace_mammograpy_systematic_review.pdf",
    "paper_title": "Vera 2025 Deep Learning For Contrast Enhnace Mammograpy Systematic Review",
    "chunk_index": 43,
    "total_chunks": 58,
    "text_content": "n = pa tien ts) Te s t i n g D a t a s e t (n=pa tien ts ) Seg men t ation P erf o rm ance Metr ic s 1 Helal e t al . (28) 202 4 R e t r o sp ec t ive CNN L es i on det ec tion and cl a ss if i ca t i o n 326 ( tr ain ing ) 37 Automatic A UC - RO C 0. 936; (95% CI: 0.89 8 , 0.973 ). S en sitivity 75% , s p eci fic ity 96.3% , t o t al ac cur ac y of 90 .1%, PPV 87.1 %, and NPV 92% 2 Chen e t a l. (36) 202 3 R e t r o sp ec t ive a nd pr o spec t i ve CNN L es i on s det ec tion and cl a ss if i ",
    "full_text_length": 53901,
    "chunk_length": 619
  },
  {
    "chunk_id": 3796,
    "paper_filename": "vera_2025_deep_learning_for_contrast_enhnace_mammograpy_systematic_review.pdf",
    "paper_title": "Vera 2025 Deep Learning For Contrast Enhnace Mammograpy Systematic Review",
    "chunk_index": 44,
    "total_chunks": 58,
    "text_content": "es i on s det ec tion and cl a ss if i ca t i o n 1355 (t r ai ning ) and 352 (v a lidation ) 95 (r e t r o sp ective ) and 10 1 (pr o sp ec tiv e ) Automatic Lesion de t ection : AFR O C 0.95 3 (e xtern al ) and 0.963 (pr o s p e ctiv e ). Lesion cla s sificat ion : A UC 0.909 (e xtern a l, 95% CI: 0.822 \u22120.996 ), and 0.912 (pr o s p e ctiv e , 95% CI: 0 .8 40\u22120.98 5 ). 3 Jailin et al . (37) 202 3 R e t r o sp ec t ive Y O LO v 5 (C NN ) L e si o n s det ec tion and cl a ss if i ca t i o n 787 ",
    "full_text_length": 53901,
    "chunk_length": 659
  },
  {
    "chunk_id": 3797,
    "paper_filename": "vera_2025_deep_learning_for_contrast_enhnace_mammograpy_systematic_review.pdf",
    "paper_title": "Vera 2025 Deep Learning For Contrast Enhnace Mammograpy Systematic Review",
    "chunk_index": 45,
    "total_chunks": 58,
    "text_content": "t i o n 787 ( tr ain ing ) and 150 (v a lidation ) 150 Automatic Lesion de t ection: A UFRO C 0.891 and 0 .712 f or lo w and h igh BPE , r e s p ec tiv e ly . Lesion cla s sificat ion : A UC 0.986 ( 9 5% C I : 0. 976 \u2013 0 . 9 95) All rights reserved. No reuse allowed without permission. (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. The copyright holder for this preprint this version posted May 13, 2024. ; ht",
    "full_text_length": 53901,
    "chunk_length": 828
  },
  {
    "chunk_id": 3798,
    "paper_filename": "vera_2025_deep_learning_for_contrast_enhnace_mammograpy_systematic_review.pdf",
    "paper_title": "Vera 2025 Deep Learning For Contrast Enhnace Mammograpy Systematic Review",
    "chunk_index": 46,
    "total_chunks": 58,
    "text_content": "e t r o sp ec t ive CNN Cla ssifi c a tion 1718 (t r ai ning ) and 255 (v a lidation ) 523 No segmen ta tion w as p er f or m e d A UC o f 0.92 ( 9 5% C I : 0. 89 , 0 . 9 5) 5 Beuque e t al. (25 ) 202 3 R e t r o sp ec t ive CNN L es i on det ec tion, segme n t a t i on, and cl a ss if i ca t i o n 850 (t r ai ning ) and 212 (v a lidation ) 279 Manual and a u to m a ti c Lesion de t ection: Acc ur acy 88%, sen s i tiv ity 99% . Seg men t ation: Dice c o effic ie n t 0.80 Class i fic a tion: Manu",
    "full_text_length": 53901,
    "chunk_length": 656
  },
  {
    "chunk_id": 3799,
    "paper_filename": "vera_2025_deep_learning_for_contrast_enhnace_mammograpy_systematic_review.pdf",
    "paper_title": "Vera 2025 Deep Learning For Contrast Enhnace Mammograpy Systematic Review",
    "chunk_index": 47,
    "total_chunks": 58,
    "text_content": "0.80 Class i fic a tion: Manual s e gmen ta tion: A UC 0.88 (95 % CI: 0.84-0. 93 ). Sen s i tiv it y 89% (95%C I 85 -93% ), speci fic ity 73 % (95% CI 64-81% ). Aut om a tic s e gmen ta tion: A UC 0.87 (95 % CI: 0.82-0. 92 ). Sen s i tiv it y 100% 6(95% CI 100 - 100% ), s p ecific ity 45% (95% C I 34- 57% ) . 6 Zheng et al. (35) 202 3 R e t r o sp ec t ive a nd CNN Se gment ation and 1538 172 (int ern al te st i n g ) , 3 7 Automatic Seg men t ation : Dice c o effic ie n t All rights reserved. N",
    "full_text_length": 53901,
    "chunk_length": 856
  },
  {
    "chunk_id": 3800,
    "paper_filename": "vera_2025_deep_learning_for_contrast_enhnace_mammograpy_systematic_review.pdf",
    "paper_title": "Vera 2025 Deep Learning For Contrast Enhnace Mammograpy Systematic Review",
    "chunk_index": 48,
    "total_chunks": 58,
    "text_content": "is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. The copyright holder for this preprint this version posted May 13, 2024. ; https://doi.org/10.1101/2024.05.13.24307271doi: medRxiv preprint p r os p e ct iv e cl a ss if i ca t i o n ( e x t e r na l t e s ting ), 105 (pr o sp ec tiv e ) 0.820 \u00b1 0.14 8 (e xtern al r etr o spec tive) a n d 0.837 \u00b1 0.13 2 (pr o s p e ctiv e ). Class i fic a tion: A UC : 0.940 (e xtern a l r etr o spec tive, 95% CI: 0 .8 ",
    "full_text_length": 53901,
    "chunk_length": 781
  },
  {
    "chunk_id": 3801,
    "paper_filename": "vera_2025_deep_learning_for_contrast_enhnace_mammograpy_systematic_review.pdf",
    "paper_title": "Vera 2025 Deep Learning For Contrast Enhnace Mammograpy Systematic Review",
    "chunk_index": 49,
    "total_chunks": 58,
    "text_content": "r o sp ec t ive CNN + A ttention Cla ssifi c a tion 98 (tr aini ng ) and 12 (v a lidation ) 12 Automatic Acc ur acy 88.06% , Pr eci s io n 88.03% , Sen s i tiv it y 88.1% , speci fic ity 88 .01%, F1- sc o r e 0.88 8 Mao et a l . (34) 202 3 R e t r o sp ec t ive CNN + A ttention Cla ssifi c a tion 1093 146 Manual and a u to m a ti c A UC o f 0.970 , an A UP R C o f 0.98 8, F1 s co r e 0.918 , ac cur ac y 0.89 1, sen s i tiv ity 0.8 48, speci fic ity 1. 000, PPV 1 .0 00, an d NPV 0 .722 9 W u et a",
    "full_text_length": 53901,
    "chunk_length": 648
  },
  {
    "chunk_id": 3802,
    "paper_filename": "vera_2025_deep_learning_for_contrast_enhnace_mammograpy_systematic_review.pdf",
    "paper_title": "Vera 2025 Deep Learning For Contrast Enhnace Mammograpy Systematic Review",
    "chunk_index": 50,
    "total_chunks": 58,
    "text_content": "al . (24) 202 2 R e t r o sp ec t ive CNN L es i on segme n t a t i on and lymph node s cl a ss if i ca t i o n Fo r segme nt ation 177, f or cl assi fic a tion 120 Fo r s eg me nt ation 20, f or cla ssi ficat i on 31 Automatic Seg men t ation : Dice c o effic ie n t 0.84 \u00b1 0.10 Class i fic a tion: Deep l e arni ng model: A UC 0 .53 (95% CI 0.31 -All rights reserved. No reuse allowed without permission. (which was not certified by peer review) is the author/funder, who has granted medRxiv a lice",
    "full_text_length": 53901,
    "chunk_length": 839
  },
  {
    "chunk_id": 3803,
    "paper_filename": "vera_2025_deep_learning_for_contrast_enhnace_mammograpy_systematic_review.pdf",
    "paper_title": "Vera 2025 Deep Learning For Contrast Enhnace Mammograpy Systematic Review",
    "chunk_index": 51,
    "total_chunks": 58,
    "text_content": "0.75), de ep lea r ni ng model with r a diomic s fe a t u r e s s c o r e : A UC 0.76 (95 % CI 0.59-0. 93 ). 10 Dominique et al. (32 ) 202 2 R e t r o sp ec t ive CNN T umor c har acteri stics pr edic ti on (c la s si fic ation ) 1574 imag e s f or tr a ining and 390 image s f or v a lida ti on ( n o d e ta i l s o n th e number o f pa ti en ts in eac h gr oup) 496 ima g es (no det ail s on the number o f pa ti ent s in the t e s ting gr oup ) Manual T umo r g r ade A U C 0.611 , se n s itiv ity",
    "full_text_length": 53901,
    "chunk_length": 654
  },
  {
    "chunk_id": 3804,
    "paper_filename": "vera_2025_deep_learning_for_contrast_enhnace_mammograpy_systematic_review.pdf",
    "paper_title": "Vera 2025 Deep Learning For Contrast Enhnace Mammograpy Systematic Review",
    "chunk_index": 52,
    "total_chunks": 58,
    "text_content": "C 0.611 , se n s itiv ity 60.34 %, spec i ficity 60.26 %, ac cur ac y 60.28 %; ER A UC 0.858 , se n s itiv ity 80.23 %, spec i ficity 80.3% , ac cur ac y 80.24 %; PR A U C 0.615 , se n s itiv ity 80.23 %, spec i ficity 80.3% , ac cur ac y 80.24 %; HER2 A UC 0.62 , sen s i tiv ity 60% , speci fic ity 54 .82%, ac cur ac y 55.4 4%; Ki67 A U C 0 .593, sen s i tiv ity 55. 97%, speci fic ity 56 .14%, ac cur ac y 56.0 5%; TN A UC 0.876 , sen s i tiv ity 72. 22%, speci fic ity 73 .26%, ac cur ac y 73. 1",
    "full_text_length": 53901,
    "chunk_length": 757
  },
  {
    "chunk_id": 3805,
    "paper_filename": "vera_2025_deep_learning_for_contrast_enhnace_mammograpy_systematic_review.pdf",
    "paper_title": "Vera 2025 Deep Learning For Contrast Enhnace Mammograpy Systematic Review",
    "chunk_index": 53,
    "total_chunks": 58,
    "text_content": "r o sp ec t ive CNN Se gment ation Ov er all 2006 i mag e s fr om 326 pa ti en ts Automatic IOU 0 .65, F 1 0 .71 All rights reserved. No reuse allowed without permission. (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. The copyright holder for this preprint this version posted May 13, 2024. ; https://doi.org/10.1101/2024.05.13.24307271doi: medRxiv preprint 12 Song et al. (30) 202 2 R e t r o sp ec t ive CNN C",
    "full_text_length": 53901,
    "chunk_length": 833
  },
  {
    "chunk_id": 3806,
    "paper_filename": "vera_2025_deep_learning_for_contrast_enhnace_mammograpy_systematic_review.pdf",
    "paper_title": "Vera 2025 Deep Learning For Contrast Enhnace Mammograpy Systematic Review",
    "chunk_index": 54,
    "total_chunks": 58,
    "text_content": "benig n c a s e s we r e e xclude d. In the prepr oc e s si ng of image s, image s w e r e fli pped, a n d noi se w e r e added, r e s ul ting ultimately in 1 7,360 im ag es th a t we r e div ided r ando mly t o 80 % tr aini ng , 15% v al ida tion a nd 5% t e s ting . Automatic Acc ur acy 96.6% , sen s i tiv ity 96. 4%, speci fic ity 96 .4%, pr eci s ion 96.8% , F1 s co r e 0.966 , A UC 0.966 13 Song et al. (29) 202 1 R e t r o sp ec t ive CNN Cla ssifi c a tion 95 patie n t s Automatic Acc ur a",
    "full_text_length": 53901,
    "chunk_length": 669
  },
  {
    "chunk_id": 3807,
    "paper_filename": "vera_2025_deep_learning_for_contrast_enhnace_mammograpy_systematic_review.pdf",
    "paper_title": "Vera 2025 Deep Learning For Contrast Enhnace Mammograpy Systematic Review",
    "chunk_index": 55,
    "total_chunks": 58,
    "text_content": "acy 94.78% , pr eci s ion 95.02 @, r ec a ll 95.9 1%, speci fic ity 94 .5%, F1 s co r e 0.955 , A UC 0.947 14 P erek et al. (31) 201 9 R e t r o sp ec t ive CNN Cla ssifi c a tion 54 patie n t s with 129 image s us ed fo r va l i d at i o n . Manual F or 10 0% sen s i tiv ity speci fic ity wa s up t o 66% , depen ding on t he model 15 Gao et al . (26) 201 8 R e t r o sp ec t ive CNN Cla ssifi c a tion 49 patie n t s ( tr ai ni ng and te s ti ng unc lear ). Manual Acc ur acy 89%, sen s i tiv ity ",
    "full_text_length": 53901,
    "chunk_length": 652
  },
  {
    "chunk_id": 3808,
    "paper_filename": "vera_2025_deep_learning_for_contrast_enhnace_mammograpy_systematic_review.pdf",
    "paper_title": "Vera 2025 Deep Learning For Contrast Enhnace Mammograpy Systematic Review",
    "chunk_index": 56,
    "total_chunks": 58,
    "text_content": "tiv ity 93% , speci fic ity 86 %, and A UC 0 . 91 16 Danal a et al. (23) 201 8 R e t r o sp ec t ive ML P Cla ssifi c a t i on 111 pa tie n t s Automatic Acc ur acy 68.5% , PPV 82 .1%, NP V 47.7% , A UC 0.737 \u00b10.048 Table 2. Studies Evaluating Deep Learning Applications for Contrast Enhanced Mammography All rights reserved. No reuse allowed without permission. (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. T",
    "full_text_length": 53901,
    "chunk_length": 972
  },
  {
    "chunk_id": 3809,
    "paper_filename": "vera_2025_deep_learning_for_contrast_enhnace_mammograpy_systematic_review.pdf",
    "paper_title": "Vera 2025 Deep Learning For Contrast Enhnace Mammograpy Systematic Review",
    "chunk_index": 57,
    "total_chunks": 58,
    "text_content": "perpetuity. The copyright holder for this preprint this version posted May 13, 2024. ; https://doi.org/10.1101/2024.05.13.24307271doi: medRxiv preprint",
    "full_text_length": 53901,
    "chunk_length": 151
  },
  {
    "chunk_id": 3810,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 0,
    "total_chunks": 159,
    "text_content": "CLIMB: Data Foundations for Large Scale Multimodal Clinical Foundation Models Wei Dai1Peilin Chen1Malinda Lu1Daniel Li1Haowen Wei2Hejie Cui3Paul Pu Liang1 Abstract Recent advances in clinical AI have enabled re- markable progress across many clinical domains. However, existing benchmarks and models are primarily limited to a small set of modalities and tasks, which hinders the development of large-scale multimodal methods that can make holistic assessments of patient health and well- being. To b",
    "full_text_length": 166013,
    "chunk_length": 1479
  },
  {
    "chunk_id": 3811,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 1,
    "total_chunks": 159,
    "text_content": "Pretraining on CLIMB also effec- tively improves models\u2019 generalization capability to new tasks, and strong unimodal encoder perfor- mance translates well to multimodal performance when paired with task-appropriate fusion strate- gies. Our findings provide a foundation for new architecture designs and pretraining strategies to advance clinical AI research. Code is released at this link. 1. Introduction Advances in AI for clinical data have significantly helped doctors process and analyze complex",
    "full_text_length": 166013,
    "chunk_length": 1389
  },
  {
    "chunk_id": 3812,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 2,
    "total_chunks": 159,
    "text_content": "modalities primarily in the image and text domain (Thakoor et al., 2019; Jing et al., 2023b; Sharma et al., 2022), failing to capture the interactions between many medical indicators that clinicians routinely combine to make holistic assess- ments on patient health and well-being (Liang et al., 2024b; Rajendran et al., 2023; Shaik et al., 2024). To develop the next generation of holistic multimodal clin- ical foundation models, we introduce Clinical Large-scale Integrative Multi-modal Benchmark ",
    "full_text_length": 166013,
    "chunk_length": 1377
  },
  {
    "chunk_id": 3813,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 3,
    "total_chunks": 159,
    "text_content": "graph data (brain networks, molecules) and 1.03M multi- modal data combining multiple of the above modalities. We accomplish this through a novel data collection and pre- processing pipeline that standardizes diverse data formats from 33 different medical institutions while preserving the natural patterns of missing data. The dataset encompasses 96 different clinical conditions across 13 clinical domains, making it one of the largest and most diverse public clinical benchmarks to date. Through e",
    "full_text_length": 166013,
    "chunk_length": 1425
  },
  {
    "chunk_id": 3814,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 4,
    "total_chunks": 159,
    "text_content": "performance in novel and understudied tasks for 1arXiv:2503.07667v2 [cs.LG] 20 Mar 2025 CLIMB: Data Foundations for Large Scale Multimodal Clinical Foundation Models OOD Dataset Process Process CLIMB 3D Graph 2D 1D ECG/EEG Genomic EHR X-ray Dermo. Fundus Mammo. Patho. CT Scan MRI Endoscopic Ultrasound Molecular BrainNet Encoder Graph Fusion Multimodal Datasets Transfer Process Vision Time Series Training Testing Downstream Tasks Disease Diagnosis Patient Risk Prediction . . . Fusion LayerPretrai",
    "full_text_length": 166013,
    "chunk_length": 1527
  },
  {
    "chunk_id": 3815,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 5,
    "total_chunks": 159,
    "text_content": "appropriate fusion strategies, ultimately advancing performance on critical clinical applications like disease diagnosis and patient risk prediction. general-domain encoders, specialized clinical encoders, and clinical large vision language models (LVLMs). 2.Few-shot transfer: We test how models pretrained on CLIMB generalize to new clinical tasks with limited labeled data. Models pretrained on CLIMB demonstrate significant improvements in few-shot learning scenarios, achieving up to 29% improve",
    "full_text_length": 166013,
    "chunk_length": 1493
  },
  {
    "chunk_id": 3816,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 6,
    "total_chunks": 159,
    "text_content": "multimodal models trained on CLIMB, which achieve state-of-the-art performance on multiple clin- ical tasks. We also provide detailed recommendations for model architecture selection and pretraining strategies across clinical modalities, establishing a practical frame- work for future clinical AI development. All code for data collection, training, evaluation, and pretrained weights is available at this link.2. Related Work We cover related work in unimodal and multimodal clinical benchmarks and",
    "full_text_length": 166013,
    "chunk_length": 1373
  },
  {
    "chunk_id": 3817,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 7,
    "total_chunks": 159,
    "text_content": "benchmarks typically focus on limited modalities like X-rays, pathology, or their combi- nations (Moses, 2021; Schneider et al., 2022; Nasir et al., 2023). Large benchmarks include BenchMD (Wantlin et al., 2023) and CARES (Xia et al., 2024), covering 7 clinical modalities (1D, 2D, and 3D) and 16 different 2D and 3D im- age modalities, respectively. As shown in Table 1, however, our dataset uniquely incorporates time series and graph data alongside traditional clinical imaging while maintaining t",
    "full_text_length": 166013,
    "chunk_length": 1411
  },
  {
    "chunk_id": 3818,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 8,
    "total_chunks": 159,
    "text_content": "Regions South America Ultrasound Other Viral Pneumonia...Graphs...Vietnam...Data Samples Label: COVID, Severity 2 Comment: broken pleura line, very small consolidated areas, B-lines Source: COVID-BLUE Source: PPMI Label: Parkinson's Disease Source: IIIC Label: Seizure (b) (e) (d) (c) Figure 2. Overview of CLIMB benchmark and code. (a) Visualization of CLIMB dataset composition. The inner ring displays the primary data modalities (2D, 1D, Graph, Multimodal). The middle ring represents major clini",
    "full_text_length": 166013,
    "chunk_length": 1372
  },
  {
    "chunk_id": 3819,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 9,
    "total_chunks": 159,
    "text_content": "sites in CLIMB. Red regions indicate areas where clinical datasets are commonly collected, whereas blue regions indicate places where clinical dataset collections are rare. (d) Example code usage on CLIMB framework. This code example loads a custom mixed subset of CLIMB spanning across three modalities, then trains a ConvNextv2 classifier on the dataset mixture with unified vocabulary. (e) Sample data from CLIMB. CLIMB preserves detailed labels, metadata and comments explaining the diagnosis. ta",
    "full_text_length": 166013,
    "chunk_length": 1335
  },
  {
    "chunk_id": 3820,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 10,
    "total_chunks": 159,
    "text_content": "EV A-2 (Liu et al., 2022; Fang et al., 2024) have demonstrated strong performance on natural images, efforts to develop clinical-specific en- coders have largely focused on adapting older architectures, as seen in CLIP-based PMC-CLIP (Lin et al., 2023) and Vision Transformer-based MedViT (Manzari et al., 2023), leaving the potential of modern architectures for clinical tasks largely unexplored. 3. Dataset In this section, we provide an overview of the CLIMB dataset, sourced from 44 public datase",
    "full_text_length": 166013,
    "chunk_length": 1328
  },
  {
    "chunk_id": 3821,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 11,
    "total_chunks": 159,
    "text_content": "and evaluating multimodal medical AI systems. To maximize the diversity of the data, we established three key criteria to guide our dataset selection process. As illustrated in Figure 2(b), we prioritize datasets that address one or more of the following objectives: 1.Novel tasks: Recent emerging clinical challenges, such as COVID-19 diagnosis from chest imaging. 2.Understudied modalities: Data types traditionally un- derrepresented in clinical AI, including electroencephalo- grams (EEG), endosc",
    "full_text_length": 166013,
    "chunk_length": 1402
  },
  {
    "chunk_id": 3822,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 12,
    "total_chunks": 159,
    "text_content": "(Wie, 2021; Cui et al., 2022; Jing et al., 2023a). Notably, additional metadata and explanation labels for the datasets are also preserved, as shown in the 3 CLIMB: Data Foundations for Large Scale Multimodal Clinical Foundation Models Table 1. Comparison of clinical benchmarks. Abbreviations: BN = Brain Networks, Mol = Molecules, ECG = Electrocardiogram, EEG = Electroencephalogram, Genom = Genomics, Mammo = Mammography, Derm = Dermoscopy, Fund = Fundus, Path = Pathology, CT = Computed Tomograph",
    "full_text_length": 166013,
    "chunk_length": 1171
  },
  {
    "chunk_id": 3823,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 13,
    "total_chunks": 159,
    "text_content": "EEG Genom Gait Text X-ray Mammo Derm Fund Path CT MRI US Endo BenchMD (Wantlin et al., 2023) 5.2M* \u2717 \u2717 \u2713 \u2713 \u2717 \u2717 \u2717 \u2713 \u2713 \u2713 \u2713 \u2717\u2713 \u2717 \u2717 \u2717 PMC-VQA (Zhang et al., 2023) 149K \u2717 \u2717 \u2717 \u2717 \u2717 \u2717 \u2713\u2713 \u2717 \u2717 \u2717 \u2717 \u2713 \u2713 \u2713 \u2717 GMAI-MMBench (Chen et al., 2024) 26K \u2717 \u2717 \u2717 \u2717 \u2717 \u2717 \u2713\u2713 \u2717 \u2713 \u2713 \u2717\u2713 \u2713 \u2713 \u2713 CARES (Xia et al., 2024) 18K \u2717 \u2717 \u2717 \u2717 \u2717 \u2717 \u2713\u2713 \u2717 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 CLIMB (ours) 4.51M \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 COVID-19 ultrasound example. To enable holistic training and benchmarking, we need to unify the input data loading and predicti",
    "full_text_length": 166013,
    "chunk_length": 973
  },
  {
    "chunk_id": 3824,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 14,
    "total_chunks": 159,
    "text_content": "tasks as multi-label classification given clinical data samples from different modalities. We combine the vocab- ularies in each dataset while merging semantically equiv- alent labels. Specifically, given a heterogeneous dataset collection D={D1, ..., D K}with mixed annotation types (multi-label/multi-class), we define a unified label vocab- ularyV=SK k=1Vkwhere Vkrepresents the label set of dataset Dk. To ensure consistency, we standardize terminol- ogy variations and combine similar concepts s",
    "full_text_length": 166013,
    "chunk_length": 1392
  },
  {
    "chunk_id": 3825,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 15,
    "total_chunks": 159,
    "text_content": "App. B. In addition, we preserve the metadata, demographic infor- mation, segmentation masks, and associated clinical reports from the original dataset and link them to every sample where applicable. To ensure comparability across model architectures, this information is not exposed to the model in the experiments, although we hope future works could utilize it to develop more robust and fair methods. 3.3. Dataset Statistics CLIMB contains 4.51 million samples totaling 19.01 ter- abytes, with th",
    "full_text_length": 166013,
    "chunk_length": 1357
  },
  {
    "chunk_id": 3826,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 16,
    "total_chunks": 159,
    "text_content": "widest range of modal- ities while incorporating time series and graph data, which distinguishes it from existing multimodal benchmarks in the field that typically only include images and text. Figure 2(a) shows the distribution across primary modalities and the size of individual datasets. We carefully balance the dataset such that each modality contains 3-5 datasets, pro- viding multiple data sources per modality while maintaining diversity within each category. The geographic distribution of ",
    "full_text_length": 166013,
    "chunk_length": 1343
  },
  {
    "chunk_id": 3827,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 17,
    "total_chunks": 159,
    "text_content": "for a standard workflow of loading multiple medical imag- ing datasets and training a classification model with our CLIMB framework. The entire training and evaluation script can be completed in under ten lines of code while maintaining the flexibility for any custom models or train- ing loops. We also provide a standardized training pipeline that is easily reproducible and parallelizable across multiple machines and instances. 4. Experiments We run extensive experiments to investigate the core ",
    "full_text_length": 166013,
    "chunk_length": 1420
  },
  {
    "chunk_id": 3828,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 18,
    "total_chunks": 159,
    "text_content": "models be fused effectively to tackle multimodal clinical tasks? (a) (b) (c)...Transfer Time Series Encoder Eval Eval Eval ... ... Vision EncoderFusion... ... Figure 3. Experimental setup for evaluating (a) multitask, (b) transfer, and (c) fusion learning strategies, addressing RQ1, 2, 3 respectively. (a) investigates how multitask pretrained clinical models work across multiple tasks consistently, especially understudied tasks. (b) explores how well multitask pretrained clinical models transfer",
    "full_text_length": 166013,
    "chunk_length": 1419
  },
  {
    "chunk_id": 3829,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 19,
    "total_chunks": 159,
    "text_content": "To answer the above research questions, we design our experiments as follows: RQ1: Multitask pretraining. We investigate whether mul- titask learning can enable robust universal encoders for clin- ical tasks. For each input modality (vision 2D/3D, graph, EEG, ECG), we train a single encoder jointly on all re- lated tasks in CLIMB. Each encoder is combined with a classification head that predicts task-specific labels from an aggregated vocabulary V, which encompasses diagnos- tic terms across all",
    "full_text_length": 166013,
    "chunk_length": 1314
  },
  {
    "chunk_id": 3830,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 20,
    "total_chunks": 159,
    "text_content": "data, we evaluate ECG-specific model, ECG JEPA (Kim, 2024), against gen- eral time-series architectures, UniTS (Gao et al., 2024). In the EEG domain, we test specialized architectures includ- ing SPARCNet (Jing et al., 2023b), CNNTransformer (Peh et al., 2022), FFCL (Li et al., 2022), ContraWR (Yang et al., 2021), STTransformer (Song et al., 2021), and BIOT (Yang et al., 2024). We also evaluate SoTA clinical VLM, LLaVa- Med (Li et al., 2023), on CLIMB-QA, a question-answeringversion of CLIMB des",
    "full_text_length": 166013,
    "chunk_length": 1276
  },
  {
    "chunk_id": 3831,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 21,
    "total_chunks": 159,
    "text_content": "same modality, simulating real-world scenarios where models must adapt to novel diagnostic tasks with limited labeled examples (1, 8, and 32 samples). We curated a diverse set of 10 datasets spanning 9 modalities, as detailed in App. C.7.1. For each modality, we use the best-performing model from our RQ1 experiments: ConvNextv2 for vision tasks, ECG JEPA for ECG analysis, and BIOT for EEG processing. We compare two scenarios: (1) models initialized with publicly released pretrained weights, and ",
    "full_text_length": 166013,
    "chunk_length": 1392
  },
  {
    "chunk_id": 3832,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 22,
    "total_chunks": 159,
    "text_content": "MIMIC-IV (Johnson et al., 2023), a large-scale multimodal clinical dataset. We evaluate three fusion strategies with increasing levels of cross-modal interaction: Late fusion, MLP fusion, and cross-attention fusion. Detailed architec- tural specifications are provided in App. C.2.3. We fix the encoder architectures across all experiments: ConvNextv2 for visual inputs, ClinicalBERT for text, and ECG-JEPA for time series data. In addition, to evaluate how large scale pretraining helps with multimo",
    "full_text_length": 166013,
    "chunk_length": 1367
  },
  {
    "chunk_id": 3833,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 23,
    "total_chunks": 159,
    "text_content": "best performance-compute tradeoff. ModelChest X-Ray Mammography Dermoscopy CT Scan Fundus Ultrasound Overall AUC Sen Spe AUC Sen Spe AUC Sen Spe AUC Sen Spe AUC Sen Spe AUC Sen Spe AUC Sen Spe Clinical Encoders MedViT .670 .253 .833 .627 .417 .583 .522 .361 .639 .604 .382 .616 .320 .317 .688 .452 .500 .583 .579 .364 .690 PMC-CLIP .725 .251 .883 .614 .312 .710 .674 .325 .706 .619 .407 .593 .508 .220 .785 .521 .384 .609 .635 .341 .724 RAD-DINO .818 .406 .928 .566 .314 .701 .717 .348 .715 .653 .408",
    "full_text_length": 166013,
    "chunk_length": 1069
  },
  {
    "chunk_id": 3834,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 24,
    "total_chunks": 159,
    "text_content": "A-2 .863 .382 .929 .516 .320 .699 .716 .353 .724 .531 .496 .496 .780 .295 .822 .462 .340 .659 .685 .372 .737 InternViT .815 .413 .930 .532 .340 .713 .868 .543 .770 .706 .469 .652 .839 .431 .851 .735 .549 .718 .772 .492 .789 ConvNeXTv2 .817 .436 .939 .558 .330 .706 .901 .568 .777 .671 .466 .641 .873 .563 .888 .774 .641 .770 .787 .537 .806 Table 3. Performance comparison of graph neural networks across brain networks and protein structures. The best perfor- mance of each model is bolded. Graph tra",
    "full_text_length": 166013,
    "chunk_length": 1234
  },
  {
    "chunk_id": 3835,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 25,
    "total_chunks": 159,
    "text_content": "initialized with our CLIMB pretrained weights against those using publicly available pretrained weights. We evaluate on two common clinical prediction tasks: length of stay (LOS) prediction and 48-hour in-hospital mortality prediction (48 IHM). These tasks are clinically significant and require integrating infor- mation across modalities. Evaluation metrics. For consistency, we evaluate all classi- fication tasks with balanced AUC, sensitivity, and specificity. Regression tasks (e.g., length of ",
    "full_text_length": 166013,
    "chunk_length": 1416
  },
  {
    "chunk_id": 3836,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 26,
    "total_chunks": 159,
    "text_content": "show the highest performance improvements, as exemplified by COVID-US with an AUC gain of 0.3254. In temporal modalities, particularly ECG analysis, the Ga dataset demonstrates this trend with an absolute AUC improvement of 23 percentage points (from 0 . 3 2 5 40 . 2 3 0 00 . 2 0 3 60 . 1 8 5 10 . 1 6 1 00 . 1 2 8 30 . 1 0 0 20 . 0 9 5 60 . 0 8 7 30 . 0 6 9 50 . 0 6 0 00 . 0 4 7 10 . 0 3 4 30 . 0 2 9 00 . 0 2 1 80 . 0 1 1 2 - 0 . 0 0 0 1- 0 . 0 0 3 9 - 0 . 0 0 5 6- 0 . 0 0 8 0- 0 . 0 1 0 8- 0 . ",
    "full_text_length": 166013,
    "chunk_length": 744
  },
  {
    "chunk_id": 3837,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 27,
    "total_chunks": 159,
    "text_content": "0 . 0 1 3 2 - 0 . 0 1 8 1- 0 . 0 2 8 0- 0 . 0 3 3 3- 0 . 0 4 0 7- 0 . 0 4 2 6 COVID-USGa COVID-BLUESBrain Tumor 2CPSCBUSI Brain TumorPAD-UFES-20KiTS23 CBIS-DDSMChapman COVID-19 CXRCT INSPECTPTB-XL Fundus APTOSCoronaHackLC25000 MIMIC-CXRMessidor-2CT LNDBHAM10000 VinDr MammoCT HemorrhageCT RSPECTBCSS ISIC 2020 Fundus JICHI\u22120.0500.050.10.150.20.250.3 DatasetAUC DifferenceNovel Task Understudied Modality Underrepresented Regions Multiple Categories Other DatasetsFigure 4. Difference in AUC achieved ",
    "full_text_length": 166013,
    "chunk_length": 1412
  },
  {
    "chunk_id": 3838,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 28,
    "total_chunks": 159,
    "text_content": "regions, and/or modalities. 0.474 to 0.704) when comparing single-task to multitask pretraining approaches, as shown in Table D.1. These re- sults suggest that multitask learning is particularly effective for scenarios where data or research attention has been his- torically limited. Comparison of encoder models. Our analysis reveals coun- terintuitive patterns in encoder effectiveness across differ- ent domains. In the visual domain, image-based models consistently outperform video models acros",
    "full_text_length": 166013,
    "chunk_length": 1373
  },
  {
    "chunk_id": 3839,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 29,
    "total_chunks": 159,
    "text_content": "behind SoTA multitask encoders, namely CLIMB-ConvNextv2, the SoTA multitask encoder trained on CLIMB. DatasetZero-Shot Fine-Tuned Acc Sens Spe Acc Sens Spe Chest X-ray .088 .192 .808 .309 .207 .795 MRI .363 .473 .650 .480 .375 .625 Ultrasound .448 .427 .640 .579 .389 .611 Mammography .049 .203 .800 .741 .300 .700 Dermoscopy .466 .296 .700 .673 .245 .756 Fundus .434 .202 .794 .578 .217 .783 CT .448 .424 .575 .788 .435 .585 Endoscopic .000 1.00 .000 .296 .143 .857 Average .287 .402 .621 .555 .289 ",
    "full_text_length": 166013,
    "chunk_length": 1340
  },
  {
    "chunk_id": 3840,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 30,
    "total_chunks": 159,
    "text_content": "Standard shows the performance when pretrained on their datasets from the original paper, while PT on CLIMB shows the perfor- mance when pretrained on our CLIMB dataset. Models pretrained on CLIMB demonstrate consistent improvements over the original ECG domain-specific models. addition, general-purpose architectures like ConvNextv2 significantly outperform clinical-specific encoders such as MedViT, achieving a 35.9% performance improvement. We hypothesize this superiority stems from the more di",
    "full_text_length": 166013,
    "chunk_length": 1464
  },
  {
    "chunk_id": 3841,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 31,
    "total_chunks": 159,
    "text_content": "on CLIMB-QA to assess current clinical VLMs\u2019 capabilities in multimodal understanding. While fine-tuning on CLIMB-QA improves performance over zero- 1 Shot 8 Shots Full0.80.850.90.95 1 Shot 8 Shots Full0.70.750.80.85 1 Shot 8 Shots Full0.450.50.550.60.650.7 1 Shot 8 Shots Full0.650.70.750.8 1 Shot 8 Shots Full0.50.60.70.8 1 Shot 8 Shots Full0.650.70.750.8InternVL ConvNextAUC AUC AUCAUC AUC AUCChest X-Ray Dermoscopy CT Scan Fundus Pathology OverallFigure 6. Few-shot performance on out-of-distribu",
    "full_text_length": 166013,
    "chunk_length": 1474
  },
  {
    "chunk_id": 3842,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 32,
    "total_chunks": 159,
    "text_content": "chest X-rays (0.309 accuracy) and endoscopic images (0.296 accuracy), where the model struggles to maintain balanced sensitivity and specificity. These results suggest that CLIMB is a rich data source of clinical AI that can substantially improve existing models and that current vision-language models still require fundamental architectural innovations and novel training paradigms. 4.3. On RQ2: Few-shot Transfer Performance Strength of few-shot transfer. As illustrated in Figure 7, our large-sca",
    "full_text_length": 166013,
    "chunk_length": 1391
  },
  {
    "chunk_id": 3843,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 33,
    "total_chunks": 159,
    "text_content": "in Figure 6, in the vision domain, we again found both ConvNextv2 and ViT- based models perform similarly under both full and few shot settings, as shown in Figure 6. ConvNext exhibit better performance in fundus and dermoscopy images, while ViT performs better for CT scans. We also found specialized ECG models like ECG-JEPA transfer better than universal time series models like UniTS, as demonstrated in Figure 5. 7 CLIMB: Data Foundations for Large Scale Multimodal Clinical Foundation Models CX",
    "full_text_length": 166013,
    "chunk_length": 1248
  },
  {
    "chunk_id": 3844,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 34,
    "total_chunks": 159,
    "text_content": "of training one shot example on top of our CLIMB dataset. For EEGs, 8 shot results are used instead due to the inherent complexity of the task. 4.4. RQ3: Unimodal Pretraining to Multimodal Fusion Pretraining results. Experimental results in Table 5 demon- strate that encoders pretrained on CLIMB consistently out- perform those pretrained on other datasets across all eval- uation settings. This performance advantage is maintained across diverse tasks, including length of stay prediction and in-ho",
    "full_text_length": 166013,
    "chunk_length": 1404
  },
  {
    "chunk_id": 3845,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 35,
    "total_chunks": 159,
    "text_content": "supe- rior performance, achieving the lowest MAE of 2.61. In contrast, for binary classification tasks such as 48-hour in- hospital-mortality (48 IHM) prediction, MLP-based concate- nation proves more effective, achieving the highest AUC while maintaining balanced sensitivity (0.824) and speci- ficity (0.975). While late fusion appears to achieve higher specificity in some cases, its near-zero sensitivity indicates that it effectively defaults to predicting the majority class without any meaning",
    "full_text_length": 166013,
    "chunk_length": 1393
  },
  {
    "chunk_id": 3846,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 36,
    "total_chunks": 159,
    "text_content": "48 IHM work well under simple MLP fusion. Enc. FusionLOS 48 IHM (Full) 48 IHM (8-Shots) MAE AUC Sens Spec AUC Sens Spec SoTALate 4.78 0.689 0.495 0.760 0.524 0.001 0.994 MLP 2.98 0.957 0.806 0.979 0.556 0.536 0.538 CrossAtt 2.77 0.786 0.628 0.814 0.580 0.286 0.766 OursLate 4.71 0.859 0.017 0.983 0.628 0.022 0.993 MLP 2.84 0.961 0.824 0.975 0.672 0.295 0.858 CrossAtt 2.61 0.796 0.822 0.590 0.570 0.294 0.753 4.5. Comparison with Dataset-Specific SoTAs To contextualize the performance of our pretra",
    "full_text_length": 166013,
    "chunk_length": 1327
  },
  {
    "chunk_id": 3847,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 37,
    "total_chunks": 159,
    "text_content": "tuning utilized in CFT (Chong et al., 2023), makes them difficult to adapt or retrain for different clinical tasks. There- fore, there is much value in training generalizable unimodal and multimodal models that can effectively adapt across diverse clinical modalities, tasks, and scenarios. 5. Conclusion We present CLIMB, a comprehensive multimodal clinical benchmark unifying 4.51M samples across 44 datasets, 15 modalities, and 13 clinical domains. Our extensive em- pirical evaluation revealed se",
    "full_text_length": 166013,
    "chunk_length": 1539
  },
  {
    "chunk_id": 3848,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 38,
    "total_chunks": 159,
    "text_content": "for understudied domains. For multimodal applications, we suggest matching fusion complexity to task requirements and utilizing large-scale unimodal pretraining before multimodal integration. Looking ahead, our findings point to several emerging re- 8 CLIMB: Data Foundations for Large Scale Multimodal Clinical Foundation Models search directions: developing novel architectures that better balance general and domain-specific features, finding new ways to combine unexplored modality combinations, ",
    "full_text_length": 166013,
    "chunk_length": 1493
  },
  {
    "chunk_id": 3849,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 39,
    "total_chunks": 159,
    "text_content": "data, make predictions, and investigate interventions. Furthermore, increasingly many indicators are no longer taken in the doctor\u2019s office, but daily, such as physiolog- ical sensors that track sleep, mood, stress, diet, exercise, and social interactions. Our findings can have a broad im- pact on developing holistic AI models of human health and wellness. At the same time, data privacy and model fairness are crit- ical qualities. There may be privacy risks associated with collecting and making ",
    "full_text_length": 166013,
    "chunk_length": 1310
  },
  {
    "chunk_id": 3850,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 40,
    "total_chunks": 159,
    "text_content": "provided. To deploy these algorithms at scale in the real world, it is also important to keep data and features secure without public sharing. Overall, CLIMB offers opportunities to study the promises of multimodal AI while mitigating potential risks at scale across clinical modalities, tasks, and domains. We will con- tinue expanding CLIMB to rigorously test for these social impacts and improve the safety and reliability of multimodal clinical models. Our holistic evaluation metrics will also e",
    "full_text_length": 166013,
    "chunk_length": 1320
  },
  {
    "chunk_id": 3851,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 41,
    "total_chunks": 159,
    "text_content": "10.1001/jamaophthalmol.2013.1743. URL https://doi.org/10.1001/jamaophthalmol.2013.1743 . Al-Dhabyani, W., Gomaa, M., Khaled, H., and Fahmy, A. Dataset of breast ultrasound images. Data in brief , 28:104863, 2020. Alday, E. A. P., Gu, A., Shah, A. J., Robichaux, C., Wong, A.-K. I., Liu, C., Liu, F., Rad, A. B., Elola, A., Seyedi, S., et al. Classifi- cation of 12-lead ecgs: the physionet/computing in cardiology challenge 2020. Physiological measurement , 41(12):124003, 2020. Asia Pacific Tele-Oph",
    "full_text_length": 166013,
    "chunk_length": 1401
  },
  {
    "chunk_id": 3852,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 42,
    "total_chunks": 159,
    "text_content": "can- cer histopathological image dataset (lc25000). arXiv preprint arXiv:1912.12142 , 2019. Cao, H., Wang, Y ., Chen, J., Jiang, D., Zhang, X., Tian, Q., and Wang, M. Swin-unet: Unet-like pure transformer for medical im- age segmentation. In Karlinsky, L., Michaeli, T., and Nishino, K. (eds.), Computer Vision - ECCV 2022 Workshops - Tel Aviv, Is- rael, October 23-27, 2022, Proceedings, Part III , volume 13803 ofLecture Notes in Computer Science , pp. 205\u2013218. Springer, 2022. doi: 10.1007/978-3-0",
    "full_text_length": 166013,
    "chunk_length": 1293
  },
  {
    "chunk_id": 3853,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 43,
    "total_chunks": 159,
    "text_content": "Xing, S., Zhong, M., Zhang, Q., Zhu, X., Lu, L., Li, B., Luo, P., Lu, T., Qiao, Y ., and Dai, J. Internvl: Scaling up vision foundation models and align- ing for generic visual-linguistic tasks. CoRR , abs/2312.14238, 2023. doi: 10.48550/ARXIV .2312.14238. URL https: //doi.org/10.48550/arXiv.2312.14238 . Chereshnev, R. and Kert \u00b4esz-Farkas, A. Hugadb: Human gait database for activity recognition from wearable inertial sensor networks. In Analysis of Images, Social Networks and Texts: 6th Interna",
    "full_text_length": 166013,
    "chunk_length": 1349
  },
  {
    "chunk_id": 3854,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 44,
    "total_chunks": 159,
    "text_content": "C., Hobbs, S. B., Wu, C. C., Lungren, M. P., Prevedello, L. M., Kalpathy-Cramer, J., Ball, R. L., Shih, 9 CLIMB: Data Foundations for Large Scale Multimodal Clinical Foundation Models G., Stein, A., et al. The rsna pulmonary embolism ct dataset. Radiology: Artificial Intelligence , 3(2):e200254, 2021. Cui, C., Li, L., Cai, H., Fan, Z., Zhang, L., Dan, T., Li, J., and Wang, J. The chinese mammography database (cmmd): An online mammography database with biopsy confirmed types for machine diagnosis",
    "full_text_length": 166013,
    "chunk_length": 1261
  },
  {
    "chunk_id": 3855,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 45,
    "total_chunks": 159,
    "text_content": "Li, X., and Yang, C. Biomedical visual instruction tuning with clinician preference alignment. arXiv preprint arXiv:2406.13173 , 2024. Decenci `ere, E., LaGraize, C., P \u00b4el\u00b4egrin, P., Benassi, F., R \u00b4eg\u00b4er, C., and Vautrin, T. Feedback on a publicly distributed database: the messidor database. Image Analysis & Stereology , 33(3): 231\u2013234, 2014. ISSN 1854-5165. doi: 10.5566/ias.1155. URL http://dx.doi.org/10.5566/ias.1155 . Diederik, P. K. Adam: A method for stochastic optimization. (No Title) , ",
    "full_text_length": 166013,
    "chunk_length": 1366
  },
  {
    "chunk_id": 3856,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 46,
    "total_chunks": 159,
    "text_content": ". EV A-02: A visual representation for neon genesis. Image Vis. Comput. , 149:105171, 2024. doi: 10.1016/J.IMA VIS.2024. 105171. URL https://doi.org/10.1016/j.imavis. 2024.105171 . Gao, S., Koker, T., Queen, O., Hartvigsen, T., Tsiligkaridis, T., and Zitnik, M. Units: A unified multi-task time series model, 2024. URLhttps://arxiv.org/abs/2403.00131 . Goldberger, A. L., Amaral, L. A., Glass, L., Hausdorff, J. M., Ivanov, P. C., Mark, R. G., Mietus, J. E., Moody, G. B., Peng, C.- K., and Stanley, ",
    "full_text_length": 166013,
    "chunk_length": 1411
  },
  {
    "chunk_id": 3857,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 47,
    "total_chunks": 159,
    "text_content": "tomography images for intracranial hemorrhage detection and segmentation. Intracranial hemor- rhage segmentation using a deep convolutional model. Data , 5 (1):14, 2020. Huang, K., Altosaar, J., and Ranganath, R. Clinicalbert: Model- ing clinical notes and predicting hospital readmission. arXiv preprint arXiv:1904.05342 , 2019.Huang, S.-C., Huo, Z., Steinberg, E., Chiang, C.-C., Lungren, M. P., Langlotz, C. P., Yeung, S., Shah, N. H., and Fries, J. A. Inspect: a multimodal dataset for pulmonary ",
    "full_text_length": 166013,
    "chunk_length": 1273
  },
  {
    "chunk_id": 3858,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 48,
    "total_chunks": 159,
    "text_content": "L., Shpanskaya, K. S., Seekins, J., Mong, D. A., Halabi, S. S., Sandberg, J. K., Jones, R., Larson, D. B., Langlotz, C. P., Patel, B. N., Lungren, M. P., and Ng, A. Y . Chexpert: A large chest ra- diograph dataset with uncertainty labels and expert compari- son. In The Thirty-Third AAAI Conference on Artificial Intel- ligence, AAAI 2019, The Thirty-First Innovative Applications of Artificial Intelligence Conference, IAAI 2019, The Ninth AAAI Symposium on Educational Advances in Artificial In- te",
    "full_text_length": 166013,
    "chunk_length": 1329
  },
  {
    "chunk_id": 3859,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 49,
    "total_chunks": 159,
    "text_content": "Intervention - MICCAI 2024 - 27th International Conference, Marrakesh, Morocco, October 6-10, 2024, Proceedings, Part XII , volume 15012 of Lecture Notes in Computer Science , pp. 621\u2013631. Springer, 2024. doi: 10.1007/978-3-031-72390-2 \\58. URL https: //doi.org/10.1007/978-3-031-72390-2_58 . Jing, J., Ge, W., Hong, S., Fernandes, M. B., Lin, Z., Yang, C., An, S., Struck, A. F., Herlopian, A., Karakis, I., et al. Development of expert-level classification of seizures and rhythmic and pe- riodic p",
    "full_text_length": 166013,
    "chunk_length": 1319
  },
  {
    "chunk_id": 3860,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 50,
    "total_chunks": 159,
    "text_content": "G. Mimic-iii, a freely accessible critical care database. Scientific data , 3(1):1\u20139, 2016. Johnson, A. E., Bulgarelli, L., Shen, L., Gayles, A., Shammout, A., Horng, S., Pollard, T. J., Hao, S., Moody, B., Gow, B., et al. Mimic-iv, a freely accessible electronic health record dataset. Scientific data , 10(1):1, 2023. Johnson, A. E. W., Pollard, T. J., Berkowitz, S. J., Greenbaum, N. R., Lungren, M. P., Deng, C., Mark, R. G., and Horng, S. MIMIC-CXR: A large publicly available database of labele",
    "full_text_length": 166013,
    "chunk_length": 1299
  },
  {
    "chunk_id": 3861,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 51,
    "total_chunks": 159,
    "text_content": "in one day. In Oh, A., Naumann, T., Globerson, A., Saenko, K., Hardt, M., and Levine, S. (eds.), Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Pro- cessing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023 , 2023. Li, H., Ding, M., Zhang, R., and Xiu, C. Motor imagery eeg clas- sification algorithm based on cnn-lstm feature fusion network. Biomedical signal processing and control , 72:103342, 2022. Liang, P. P., Goindani, A.,",
    "full_text_length": 166013,
    "chunk_length": 1260
  },
  {
    "chunk_id": 3862,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 52,
    "total_chunks": 159,
    "text_content": "Y ., Wang, Y ., and Xie, W. Pmc-clip: Contrastive language-image pre-training using biomedical documents. arXiv preprint arXiv:2303.07240 , 2023. Liu, F., Liu, C., Zhao, L., Zhang, X., Wu, X., Xu, X., Liu, Y ., Ma, C., Wei, S., He, Z., et al. An open access database for evaluating the algorithms of electrocardiogram rhythm and morphology abnormality detection. Journal of Medical Imaging and Health Informatics , 8(7):1368\u20131373, 2018. Liu, Z., Lin, Y ., Cao, Y ., Hu, H., Wei, Y ., Zhang, Z., Lin, ",
    "full_text_length": 166013,
    "chunk_length": 1333
  },
  {
    "chunk_id": 3863,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 53,
    "total_chunks": 159,
    "text_content": "Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022 , pp. 11966\u201311976. IEEE, 2022. doi: 10.1109/CVPR52688.2022.01167. URL https: //doi.org/10.1109/CVPR52688.2022.01167 . Lopez, S., Suarez, G., Jungreis, D., Obeid, I., and Picone, J. Au- tomated identification of abnormal adult eegs. In 2015 IEEE signal processing in medicine and biology symposium (SPMB) , pp. 1\u20135. IEEE, 2015. Lopez, S., Gross, A., Yang, S., Golmohammadi, M., Obeid, I., and Picone, J. An analysis of two common ",
    "full_text_length": 166013,
    "chunk_length": 1309
  },
  {
    "chunk_id": 3864,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 54,
    "total_chunks": 159,
    "text_content": "learner. In Hegselmann, S., Parziale, A., Shanmugam, D., Tang, S., Asiedu, M. N., Chang, S., Hartvigsen, T., and Singh, H. (eds.), Machine Learning for Health, ML4H@NeurIPS 2023, 10 December 2023, New Or- leans, Louisiana, USA , volume 225 of Proceedings of Machine Learning Research , pp. 353\u2013367. PMLR, 2023. URL https: //proceedings.mlr.press/v225/moor23a.html . Moses, D. A. Deep learning applied to automatic disease detection using chest x-rays. Journal of Medical Imaging and Radiation Oncolog",
    "full_text_length": 166013,
    "chunk_length": 1319
  },
  {
    "chunk_id": 3865,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 55,
    "total_chunks": 159,
    "text_content": "Yuan, N., Ebinger, J., Langlotz, C. P., Heidenreich, P. A., Harrington, R. A., Liang, D. H., Ash- ley, E. A., et al. Video-based ai for beat-to-beat assessment of cardiac function. Nature , 580(7802):252\u2013256, 2020. Pacheco, A. G., Lima, G. R., Salom \u02dcao, A. S., Krohling, B., Biral, I. P., de Angelo, G. G., Alves Jr, F. C., Esgario, J. G., Simora, A. C., Castro, P. B., Rodrigues, F. B., Frasson, P. H., Krohling, R. A., Knidel, H., Santos, M. C., do Esp \u00b4\u0131rito Santo, R. B., Macedo, T. L., Canuto, ",
    "full_text_length": 166013,
    "chunk_length": 1228
  },
  {
    "chunk_id": 3866,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 56,
    "total_chunks": 159,
    "text_content": "et al. Expression atlas: gene and protein expression across multiple studies and organisms. Nucleic acids research , 46(D1):D246\u2013D251, 2018. Pedrosa, J., Guilherme, C., M \u00b4arcio, P., Andr \u00b4e, J., Eduardo, I., and Ant\u00b4onio, A. Lndb dataset (version 4). In 17th International Conference on Image Analysis and Recognition (ICIAR 2020) . Zenodo, 2023. doi: 10.5281/zenodo.8348419. URL https: //doi.org/10.5281/zenodo.8348419 . Peh, W. Y ., Yao, Y ., and Dauwels, J. Transformer convolutional neural netwo",
    "full_text_length": 166013,
    "chunk_length": 1388
  },
  {
    "chunk_id": 3867,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 57,
    "total_chunks": 159,
    "text_content": "H. Q. Vindr-mammo: A large-scale benchmark dataset for computer-aided detection and diagnosis in full-field digital mammography. PhysioNet , 2022. URLhttps://doi.org/10.13026/br2v-7517 . Philiastides, M. G., Tu, T., and Sajda, P. Inferring macroscale brain 11 CLIMB: Data Foundations for Large Scale Multimodal Clinical Foundation Models dynamics via fusion of simultaneous eeg-fmri. Annual Review of Neuroscience , 44(1):315\u2013334, 2021. R., S.-L., F., G., A., H., and D., R. Curated breast imaging su",
    "full_text_length": 166013,
    "chunk_length": 1353
  },
  {
    "chunk_id": 3868,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 58,
    "total_chunks": 159,
    "text_content": "arXiv preprint arXiv:2305.06217 , 2023. Rajkomar, A., Dean, J., and Kohane, I. Machine learning in medicine. New England Journal of Medicine , 380(14):1347\u2013 1358, 2019. Rajpurkar, P., Chen, E., Banerjee, O., and Topol, E. J. Ai in health and medicine. Nature medicine , 28(1):31\u201338, 2022. Rotemberg, V ., Kurtansky, N., Betz-Stablein, B., and et al. A patient-centric dataset of images and metadata for identifying melanomas using clinical context. Scientific Data , 8(1):34, 2021. doi: 10.1038/s4159",
    "full_text_length": 166013,
    "chunk_length": 1357
  },
  {
    "chunk_id": 3869,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 59,
    "total_chunks": 159,
    "text_content": "wisdom. Information Fusion , 102: 102040, 2024. Sharma, S., Gupta, S., Gupta, D., Juneja, S., Gupta, P., Dhiman, G., and Kautish, S. [retracted] deep learning model for the automatic classification of white blood cells. Computational Intelligence and Neuroscience , 2022(1):7384131, 2022. Sone, K., Toyohara, Y ., Taguchi, A., Miyamoto, Y ., Tanikawa, M., Uchino-Mori, M., Iriyama, T., Tsuruga, T., and Osuga, Y . Ap- plication of artificial intelligence in gynecologic malignancies: A review. Journa",
    "full_text_length": 166013,
    "chunk_length": 1360
  },
  {
    "chunk_id": 3870,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 60,
    "total_chunks": 159,
    "text_content": "Biogrid: a general repository for interac- tion datasets. Nucleic acids research , 34(suppl 1):D535\u2013D539, 2006. Takahashi, H., Tampo, H., Arai, Y ., Inoue, Y ., and Kawashima, H. Applying artificial intelligence to disease staging: Deeplearning for improved staging of diabetic retinopathy. PloS one , 12(6):e0179790, 2017. Thakoor, K. A., Li, X., Tsamis, E., Sajda, P., and Hood, D. C. Enhancing the accuracy of glaucoma detection from oct proba- bility maps using convolutional neural networks. In ",
    "full_text_length": 166013,
    "chunk_length": 1354
  },
  {
    "chunk_id": 3871,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 61,
    "total_chunks": 159,
    "text_content": "diabetic retinopathy detection and classification based on fundus images: A review. Computers in biology and medicine , 135:104599, 2021. Twinanda, A. P., Shehata, S., Mutter, D., Marescaux, J., De Mathe- lin, M., and Padoy, N. Endonet: a deep architecture for recogni- tion tasks on laparoscopic videos. IEEE transactions on medical imaging , 36(1):86\u201397, 2016. Vyas, N., Morwani, D., Zhao, R., Shapira, I., Brandfonbrener, D., Janson, L., and Kakade, S. Soap: Improving and stabilizing shampoo usin",
    "full_text_length": 166013,
    "chunk_length": 1291
  },
  {
    "chunk_id": 3872,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 62,
    "total_chunks": 159,
    "text_content": "arXiv:2304.08486 , 2023. Wie, N. Covid-blues: A large-scale lung ultrasound dataset for covid-19 diagnosis. https://github.com/NinaWie/ COVID-BLUES , 2021. Maastricht University Medical Center. Xia, P., Chen, Z., Tian, J., Gong, Y ., Hou, R., Xu, Y ., Wu, Z., Fan, Z., Zhou, Y ., Zhu, K., Zheng, W., Wang, Z., Wang, X., Zhang, X., Bansal, C., Niethammer, M., Huang, J., Zhu, H., Li, Y ., Sun, J., Ge, Z., Li, G., Zou, J., and Yao, H. CARES: A comprehensive benchmark of trustworthiness in medical vi-",
    "full_text_length": 166013,
    "chunk_length": 1263
  },
  {
    "chunk_id": 3873,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 63,
    "total_chunks": 159,
    "text_content": "automatic sleep staging. arXiv preprint arXiv:2110.15278 , 2021. Yang, C., Westover, M., and Sun, J. Biot: Biosignal transformer for cross-data learning in the wild. Advances in Neural Information Processing Systems , 36, 2024. Zhang, X., Wu, C., Zhao, Z., Lin, W., Zhang, Y ., Wang, Y ., and Xie, W. Pmc-vqa: Visual instruction tuning for medical visual 12 CLIMB: Data Foundations for Large Scale Multimodal Clinical Foundation Models question answering. arXiv preprint:2305.10415 , 2023. URL https:",
    "full_text_length": 166013,
    "chunk_length": 1438
  },
  {
    "chunk_id": 3874,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 64,
    "total_chunks": 159,
    "text_content": "Chapman-Shaoxing 40K Cardiology ECG Diagnostics / Georgia 20K Cardiology ECG Diagnostics / CPSC 6K Sleep Cardiology ECG Abnormality Detection / IIIC 134.5k Neurological Disorders EEG Diagnostics / TUAB 409.5k Neurological Disorders EEG Abnormality Detection / TUEV 111.9k Neurological Disorders EEG Diagnostics / MIMIC-CXR 356K Radiology Chest X-ray Diagnostics, Classification No CheXpert 212K Radiology Chest X-ray Diagnostics, Classification No VinDr-CXR 18K Radiology Chest X-ray Diagnostics, Abn",
    "full_text_length": 166013,
    "chunk_length": 1689
  },
  {
    "chunk_id": 3875,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 65,
    "total_chunks": 159,
    "text_content": "Fundus Diagnostics, Prognostics, Severity Grading No LNDb 5.6K Radiology, Oncology CT Diagnostics, Abnormality Detection Yes INSPECT 23K Radiology CT Diagnostics, Prognostics No KiTS23 478 Radiology, Oncology CT Segmentation Yes Hemorrhage 2.5K Radiology CT Diagnostics, Segmentation Both RSPECT 1.79M Radiology CT Diagnostics, Classification Yes EchoNet-Dynamic 10K Radiology Ultrasound Segmentation Yes BUSI 780 Radiology, Oncology Ultrasound Diagnostics, Segmentation Both COVID-BLUES 362 Radiolog",
    "full_text_length": 166013,
    "chunk_length": 1608
  },
  {
    "chunk_id": 3876,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 66,
    "total_chunks": 159,
    "text_content": "Atlas 4.5K Molecular Biology, Genetics Gene Expression Expression Analysis, Classification Yes Geo 126K Molecular Biology, Genetics Gene Expression Expression Analysis, Classification Yes Vital 210K Multiple Multimodal Diagnostics Both MIMIC-IV 800K Cardiology,Radiology EHR, ECG, X-ray Diagnostics, Prognostics, Severity Grading Both Table 6 shows a breakdown of the data sources in CLIMB. In this section, we provide details for each dataset. We describe the source and structure of the datasets, s",
    "full_text_length": 166013,
    "chunk_length": 1368
  },
  {
    "chunk_id": 3877,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 67,
    "total_chunks": 159,
    "text_content": "as many clinical domains as possible. The final dataset spans 13 domains, including radiology, cardiology, pathology, dermatology, oncology, ophthalmology, molecular biology, sleep cardiology and neurological disorders. Within radiology, our dataset spans diseases such as breast cancer, kidney cancer, and pneumonia. Modality involves the type of data at play. Our data ranges from time series (1D), vision (2D & 3D), text records and graphs, making it one of the most diverse datasets up to date. T",
    "full_text_length": 166013,
    "chunk_length": 1434
  },
  {
    "chunk_id": 3878,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 68,
    "total_chunks": 159,
    "text_content": "localized reasoning and reference to a specific object in an image are classified as fine-grained. In contrast, tasks that make a prediction based on the entire image (e.g. diagnostics) are considered coarse-grained. Here, we first introduce the list of modalities in CLIMB, followed by a detailed description of each individual dataset. A.1. List of Modalities ECG (Electrocardiogram) Electrocardiogram is a cardiac diagnostic tool that records the electrical activity of the heart over time. The da",
    "full_text_length": 166013,
    "chunk_length": 1556
  },
  {
    "chunk_id": 3879,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 69,
    "total_chunks": 159,
    "text_content": "diagnosing neurological disorders and detecting abnormalities. The classes across these datasets include Seizure (SZ), Lateralized Periodic Discharges (LPD), Generalized Periodic Discharges (GPD), Lateralized Rhythmic Delta Activity (LRDA), Generalized Rhythmic Delta Activity (GRDA), Spike and Slow Wave (SPSW), Generalized Periodic Epileptiform Discharge (GPED), Periodic Lateralized Epileptiform Discharge (PLED), Eye Movement (EYEM), Artifact (ARTF), Background (BCKG), and simple Normal/Abnormal",
    "full_text_length": 166013,
    "chunk_length": 1580
  },
  {
    "chunk_id": 3880,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 70,
    "total_chunks": 159,
    "text_content": "of breast cancer. The datasets in this category include {VinDr-Mammo, CBIS-DDSM, CMMD }. The classes are primarily based on the BI-RADS scoring system (ranging from 0-5) and binary classification of Benign versus Malignant lesions. Dermoscopy Dermoscopy is a non-invasive skin imaging technique used for examining skin lesions and early detection of skin cancer. The datasets in this modality include {ISIC-2020, HAM10000, PAD-UFES-20 }. The classes across these datasets include Melanoma (MEL), Nevu",
    "full_text_length": 166013,
    "chunk_length": 1455
  },
  {
    "chunk_id": 3881,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 71,
    "total_chunks": 159,
    "text_content": "diabetic retinopathy) and PPDR (pre-proliferative diabetic retinopathy). CT (Computed Tomography) CT is an advanced imaging technique that produces detailed cross-sectional images of the body. The datasets in this modality include {LNDb, INSPECT, KiTS23, Hemorrhage, RSPECT }. The classes across these datasets cover various conditions including nodule classification ( \u22653mm, <3mm, non-nodule), Pulmonary Embolism (PE) categories (No PE, Acute PE, Chronic PE, Subsegmental PE), Hemorrhage detection, ",
    "full_text_length": 166013,
    "chunk_length": 1486
  },
  {
    "chunk_id": 3882,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 72,
    "total_chunks": 159,
    "text_content": "on tumor detection and 15 CLIMB: Data Foundations for Large Scale Multimodal Clinical Foundation Models classification, including No Tumor, Pituitary Tumor, Glioma Tumor, Meningioma Tumor, and simple presence/absence of tumors. BrainNet Brain Network represents brain connectivity networks derived from neuroimaging data. The datasets include {ABCD, ABIDE, PPMI }. The classes focus on binary classifications including Normal/Abnormal, ASD/Typical controls, and Control/PD patients. Molecule Molecula",
    "full_text_length": 166013,
    "chunk_length": 1566
  },
  {
    "chunk_id": 3883,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 73,
    "total_chunks": 159,
    "text_content": "focuses on surgery phase annotations and tool labels rather than traditional classification tasks. IMU (Inertial Measurement Unit) IMU data captures motion and orientation information. The dataset {HuGaDB } includes classes for basic physical activities: Sitting, Standing, Sitting down, and Standing up. Gene Expression Gene expression data measures the activity levels of genes. The datasets {Expression Atlas, Geo }are not primarily used for classification tasks but rather for expression analysis",
    "full_text_length": 166013,
    "chunk_length": 1390
  },
  {
    "chunk_id": 3884,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 74,
    "total_chunks": 159,
    "text_content": "7 classes: Normal, CD, HYP, MI, STTC, A. Fib/ Aflutter and Other, following conventions from (Wantlin et al., 2023). For out-of-domain transfer learning, we utilize the subclass diagnostic labels from the PTB-XL dataset, which provides a more challenging 24-label classification task. Split : For multitask training, we use the BenchMD split, which includes label remapping to 7 diagnostic categories. This split consists of 17,476 records in the training set and 4,361 records in the test set, total",
    "full_text_length": 166013,
    "chunk_length": 1401
  },
  {
    "chunk_id": 3885,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 75,
    "total_chunks": 159,
    "text_content": "grouped label of 7 classes: Normal, CD, HYP, MI, STTC, A. Fib/ Aflutter and Other, following conventions from (Wantlin et al., 2023). Split : For multitask training, we use the BenchMD split, which includes label remapping to 7 diagnostic categories. The split consists of 38,207 records in the training set and 2,051 records in the test set, totaling 40,258 records. Access restrictions : The dataset is available to download from https://www.kaggle.com/datasets/ erarayamorenzomuten/chapmanshaoxing",
    "full_text_length": 166013,
    "chunk_length": 1410
  },
  {
    "chunk_id": 3886,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 76,
    "total_chunks": 159,
    "text_content": "We provide a grouped label of 7 classes: Normal, 16 CLIMB: Data Foundations for Large Scale Multimodal Clinical Foundation Models CD, HYP, MI, STTC, A. Fib/ Aflutter and Other, following conventions from (Wantlin et al., 2023). Split : For multitask training, we use the BenchMD split, which includes label remapping to 7 diagnostic categories. The split consists of 18,622 records in the training set and 2,067 records in the test set, totaling 20,689 records. Access restrictions : The dataset is a",
    "full_text_length": 166013,
    "chunk_length": 1384
  },
  {
    "chunk_id": 3887,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 77,
    "total_chunks": 159,
    "text_content": "200 Hz. We provide a grouped label of 7 classes: Normal, CD, HYP, MI, STTC, A. Fib/ Aflutter and Other, following conventions from (Wantlin et al., 2023). Split : For multitask training, we use the BenchMD split, which includes label remapping to 7 diagnostic categories. The split consists of 4,815 records in the training set and 1,377 records in the test set, totaling 6,192 records. Access restrictions : The dataset is available to download from https://physionet.org/files/cpsc2021/1.0.0/. Lice",
    "full_text_length": 166013,
    "chunk_length": 1437
  },
  {
    "chunk_id": 3888,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 78,
    "total_chunks": 159,
    "text_content": "set is non-public. Our evaluation focuses on the 6 diagnostic categories: seizure (SZ), lateralized periodic discharges (LPD), generalized periodic discharges (GPD), lateralized rhythmic delta activity (LRDA), generalized rhythmic delta activity (GRDA), and \u201cOther\u201d if none of those patterns was present. Split : Since the test dataset is not publicly available, we divide patient groups into training/validation/test sets by 60%:20%:20%. Access restrictions : The dataset is available to download fr",
    "full_text_length": 166013,
    "chunk_length": 1530
  },
  {
    "chunk_id": 3889,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 79,
    "total_chunks": 159,
    "text_content": ": The training and test separation is provided by the dataset. Access restrictions : The dataset is available to download from https://isip.piconepress.com/projects/nedc/html/tuh eeg/. Licenses : Users must apply using the form described on https://isip.piconepress.com/projects/nedc/html/tuh eeg/. Ethical considerations : No personally identifiable information or offensive content is present in the dataset. 7.TUEV (Lopez et al., 2016) is a dataset from the Temple University EEG Corpus. The datas",
    "full_text_length": 166013,
    "chunk_length": 1541
  },
  {
    "chunk_id": 3890,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 80,
    "total_chunks": 159,
    "text_content": "described on https://isip.piconepress.com/projects/nedc/html/tuh eeg/. Ethical considerations : No personally identifiable information or offensive content is present in the dataset. 8.MIMIC-CXR (Johnson et al., 2019) is a dataset of chest X-rays in JPG format. Our evaluation utilizes the 14 disease labels: \u201dAtelectasis\u201d, \u201dCardiomegaly\u201d, \u201dConsolidation\u201d, \u201dEdema\u201d, \u201dEnlarged Cardiomediastinum\u201d, \u201dFracture\u201d, \u201dLung Lesion\u201d, \u201dLung Opacity\u201d, \u201dPleural Effusion\u201d, \u201dPneumonia\u201d, \u201dPneumothorax\u201d, \u201dPleural Oth",
    "full_text_length": 166013,
    "chunk_length": 1629
  },
  {
    "chunk_id": 3891,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 81,
    "total_chunks": 159,
    "text_content": "(Irvin et al., 2019b) is a chest radiology dataset collected from Stanford Hospital, covering 65,240 patients and 224,316 radiographs. The original dataset labels each record with a uncertainty level for 14 diagnostic observations including Atelectasis, Cardiomegaly, Consolidation, Edema, Enlarged Cardiomediastinum, Fracture, Lung Lesion, Lung Opacity, Pleural Effusion, Pneumonia, Pneumothorax, Pleural Other, Support Device and No Finding. Our evaluation focuses on predicting the labels that are",
    "full_text_length": 166013,
    "chunk_length": 1537
  },
  {
    "chunk_id": 3892,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 82,
    "total_chunks": 159,
    "text_content": "Hanoi Medical University Hospital in Vietnam. The dataset contains local labels for bounding boxes, however we evaluate our models based on the 6 global labels: \u201dLung tumor\u201d, \u201dPneumonia\u201d, \u201dTuberculosis\u201d, \u201dCOPD\u201d, \u201dOther diseases\u201d, and \u201dNo finding\u201d, all annotated by 17 radiologists with at least 8 years of experience. Split : We use a training set of 15,000 records, a test set of 3,000 records, and a total size of 18,000 records. Access restrictions : The dataset is available to download from http",
    "full_text_length": 166013,
    "chunk_length": 1446
  },
  {
    "chunk_id": 3893,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 83,
    "total_chunks": 159,
    "text_content": "Pneumonia\u201d. Split : We use a training set of 2,002 records, a test set of 988 records, and a total size of 2,990 records. Access restrictions : The dataset is available to download from https://www.kaggle.com/datasets/darshan1504/covid19- detection-xray-dataset. Licenses : This dataset is available in the Creative Commons Attribution 4.0 International License https://creativecommons.org/licenses/by/4.0/ Ethical considerations : No personally identifiable information or offensive content is prese",
    "full_text_length": 166013,
    "chunk_length": 1492
  },
  {
    "chunk_id": 3894,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 84,
    "total_chunks": 159,
    "text_content": "information or offensive content is present in the dataset. 13.VinDr-Mammo (Pham et al., 2022) consists of mammography collected from Hospital 108 and Hanoi Medical University Hospital in Vietnam. The dataset contains local labels for bounding boxes, however we evaluate our models based on the 5 global labels for BI-RAD 1-5. Split : We use a training set of 16,000 records, a test set of 4,000 records, and a total size of 20,000 records. Access restrictions : The dataset is available to download ",
    "full_text_length": 166013,
    "chunk_length": 1394
  },
  {
    "chunk_id": 3895,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 85,
    "total_chunks": 159,
    "text_content": "Large Scale Multimodal Clinical Foundation Models Split : We use a training set of 2230 records, a test set of 595 records, and a total of 2,825 records. Access restrictions : The dataset is available to download from https://www.cancerimagingarchive.net/collection/cbis- ddsm/ Licenses : Images under this dataset are available in Creative Commons Attribution 3.0 Unported License https://creativecommons.org/licenses/by/3.0/ Ethical considerations : No personally identifiable information or offens",
    "full_text_length": 166013,
    "chunk_length": 1435
  },
  {
    "chunk_id": 3896,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 86,
    "total_chunks": 159,
    "text_content": "1,872 records. Access restrictions : The dataset is available to download from https://www.cancerimagingarchive.net/collection/cmmd/. Licenses : This dataset is available in the Creative Commons Attribution 4.0 International License https://creativecommons.org/licenses/by/4.0/ Ethical considerations : No personally identifiable information or offensive content is present in the dataset. 16.ISIC-2020 (Rotemberg et al., 2021) consists of dermoscopy of skin lesions from over 2000 patients, generate",
    "full_text_length": 166013,
    "chunk_length": 1555
  },
  {
    "chunk_id": 3897,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 87,
    "total_chunks": 159,
    "text_content": "International License https://creativecommons.org/licenses/by-nc/4.0/ Ethical considerations : No personally identifiable information or offensive content is present in the dataset. 17.HAM10000 (Tschandl et al., 2018) is a dataset from the ISIC 2018 classification challenge, comprising dermoscopy images of pigmented lesions from from the ISIC archive. Our evaluation focuses on the 5 diagnostic categories: Melanoma (MEL), Nevus (NV), Basal Cell Carcinoma (BCC), Actinic Keratosis/Intraepithelial C",
    "full_text_length": 166013,
    "chunk_length": 1523
  },
  {
    "chunk_id": 3898,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 88,
    "total_chunks": 159,
    "text_content": "models on the 5 skin diagnostics, three of which are skin disease and three of which are skin cancers: Melanoma (MEL), Nevus (NV), Basal Cell Carcinoma (BCC), Actinic Keratosis/Intraepithelial Carcinoma (AKIEC), Other (OTHER). All of the skin cancers are biopsy-proven, and more than half of the skin disease are biopsy-proven as well. Split : We use a training set of 1,839 records, a test set of 459 records, and a total size of 2,298 records. Access restrictions : The dataset is available to down",
    "full_text_length": 166013,
    "chunk_length": 1367
  },
  {
    "chunk_id": 3899,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 89,
    "total_chunks": 159,
    "text_content": "kindly provided by the Messidor program partners (see https://www.adcis.net/en/third-party/messidor/). We utilize the 5 point ICDR grades: \u201dNone\u201d, \u201dMild DR\u201d, \u201dModerate DR\u201d, \u201dSevere DR\u201d, and \u201dPDR\u201d. Split : We use a training set of 1,394 records, a test set of 350 records, and a total size of 1,744 records. Access restrictions : The dataset is available to download from https://www.kaggle.com/datasets/google- brain/messidor2-dr-grades. 19 CLIMB: Data Foundations for Large Scale Multimodal Clinical",
    "full_text_length": 166013,
    "chunk_length": 1471
  },
  {
    "chunk_id": 3900,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 90,
    "total_chunks": 159,
    "text_content": "and \u201dProliferative DR\u201d. Split : We use a training set of 2,929 records, a test set of 733 records, and a total size of 3,662 records. Access restrictions : The dataset is available to download from https://www.kaggle.com/competitions/aptos2019- blindness-detection/data. Licenses : Images under this dataset are available under the Kaggle Competition Rules https://www.kaggle.com/competitions/aptos2019-blindness-detection/rules#7-competition-data Ethical considerations : No personally identifiable ",
    "full_text_length": 166013,
    "chunk_length": 1526
  },
  {
    "chunk_id": 3901,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 91,
    "total_chunks": 159,
    "text_content": "from https://pmc.ncbi.nlm.nih.gov/articles/PMC5480986/#notes1. Licenses : Images under this dataset are available under the Creative Commons Attribution 4.0 International License. https://creativecommons.org/licenses/by/4.0/ Ethical considerations : No personally identifiable information or offensive content is present in the dataset. 22.LNDb (Pedrosa et al., 2023) is lung cancer CT scan dataset collected at the Centro Hispitalar e Universitario de Sao Joao in Portugal between 2016 and 2018. Our",
    "full_text_length": 166013,
    "chunk_length": 1497
  },
  {
    "chunk_id": 3902,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 92,
    "total_chunks": 159,
    "text_content": "is multi-modal dataset containing CT images, radiology report impression sections, and structured electronic health records (EHR) from 19,438 patients. We focus on the pulmonary embolism (PE) labels which include \u201dNo PE\u201d, \u201dAcute Subsegmental-only PE\u201d, \u201dAcute PE\u201d, \u201dSubsegmental-only PE\u201d, and \u201dChronic PE\u201d. Split : We use a training set of 17,434 records, a test set of 5,806 records, and a total size of 23,240 records. Access restrictions : The dataset is available to download from https://stanford",
    "full_text_length": 166013,
    "chunk_length": 1455
  },
  {
    "chunk_id": 3903,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 93,
    "total_chunks": 159,
    "text_content": "and \u201dMalignant\u201d key of each patient. Split : We use a training set of 361 records, a test set of 117 records, and a total size of 478 records. Access restrictions : The dataset is available to download from https://github.com/neheller/kits23. Licenses : This dataset is available in the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License https://creativecommons.org/licenses/by-nc-sa/4.0/ Ethical considerations : No personally identifiable information or offensive conte",
    "full_text_length": 166013,
    "chunk_length": 1474
  },
  {
    "chunk_id": 3904,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 94,
    "total_chunks": 159,
    "text_content": "Foundation Models Access restrictions : The dataset is available to download from https://www.kaggle.com/datasets/vbookshelf/computed- tomography-ct-imagesc . Licenses : This dataset is available in the Creative Commons Attribution 4.0 International Public License https://physionet.org/content/ct-ich/view-license/1.0.0/ Ethical considerations : No personally identifiable information or offensive content is present in the dataset. 26.RSPECT (Colak et al., 2021) consists of CT scans for patients f",
    "full_text_length": 166013,
    "chunk_length": 1569
  },
  {
    "chunk_id": 3905,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 95,
    "total_chunks": 159,
    "text_content": "rules https://www.kaggle.com/competitions/rsna- str-pulmonary-embolism-detection/data Ethical considerations : No personally identifiable information or offensive content is present in the dataset. 27.EchoNet-Dynamic (Ouyang et al., 2020) consists of 10,030 apical-4-chamber echocardiography videos from patients who underwent imaging between 2016 and 2018 as part of routine clinical care at Stanford University Hospital. Each video comes with two pairs of human tracings used to estimate ventricula",
    "full_text_length": 166013,
    "chunk_length": 1524
  },
  {
    "chunk_id": 3906,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 96,
    "total_chunks": 159,
    "text_content": "present in the dataset. 28.BUSI (Al-Dhabyani et al., 2020) is a breast cancer ultrasound image dataset from 600 femaile patients between 25 and 75 years old in 2018. We utilize the labels \u201dNormal\u201d, \u201dMalignant\u201d, and \u201dBenign\u201d. Split : We use a training set of 583 records, a test set of 197 records, and a total size of 780 records. Access restrictions : The dataset is available to download from https://www.kaggle.com/datasets/aryashah2k/breast- ultrasound-images-dataset. Licenses : This dataset is ",
    "full_text_length": 166013,
    "chunk_length": 1477
  },
  {
    "chunk_id": 3907,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 97,
    "total_chunks": 159,
    "text_content": "of 266 records, a test set of 96 records, and a total size of 362 records. Access restrictions : The dataset is available to download from https://github.com/NinaWie/COVID- BLUES?tab=readme-ov-file. Licenses : This dataset is available in the Creative Commons Attribution-Noncommercial-NoDerivatives 4.0 Interna- tional License https://creativecommons.org/licenses/by-nc-nd/4.0/ Ethical considerations : No personally identifiable information or offensive content is present in the dataset. 30.COVID-",
    "full_text_length": 166013,
    "chunk_length": 1427
  },
  {
    "chunk_id": 3908,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 98,
    "total_chunks": 159,
    "text_content": "Tumor (Bhuvaji et al., 2020) consists of brain MRI images. Each image is labeled as either \u201dNo Tumor\u201d, 21 CLIMB: Data Foundations for Large Scale Multimodal Clinical Foundation Models \u201dPituitary Tumor\u201d, \u201dGlioma Tumor\u201d, or \u201dMeningioma Tumor\u201d. Split : We use a training set of 2,870 records, a test set of 394 records, and a total size of 3,264 records. Access restrictions : The dataset is available to download from https://www.kaggle.com/datasets/sartajbhuvaji/brain- tumor-classification-mri?select",
    "full_text_length": 166013,
    "chunk_length": 1374
  },
  {
    "chunk_id": 3909,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 99,
    "total_chunks": 159,
    "text_content": "available to download from https://www.kaggle.com/datasets/jjprotube/brain-mri- images-for-brain-tumor-detection. Licenses : This dataset does not have a license. Ethical considerations : No personally identifiable information or offensive content is present in the dataset. 33.ABCD (Cui et al., 2022) is a study supported by the NIH on adolescent brain cognitive development on nearly 12,000 youths of ages 9-10, who were studied for 10 years. The dataset contains MRI images, behavioral and cogniti",
    "full_text_length": 166013,
    "chunk_length": 1293
  },
  {
    "chunk_id": 3910,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 100,
    "total_chunks": 159,
    "text_content": "No personally identifiable information or offensive content is present in the dataset. 34.ABIDE (Cui et al., 2022) is a autism brain MRI diagnosis dataset with 1112 samples, including 539 from individuals with ASD and 573 from typical controls. Split : We use a random split to build a training set of 807 records, a test set of 202 records, and a total size of 1009 records. Access restrictions : The dataset is available to download from https://fcon 1000.projects.nitrc.org/indi/abide/ with accoun",
    "full_text_length": 166013,
    "chunk_length": 1313
  },
  {
    "chunk_id": 3911,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 101,
    "total_chunks": 159,
    "text_content": "curation, we use random split to build a training set of 572 records, a test set of 143 records, and a total size of 718 records. Access restrictions : The dataset is available to download from Link. Licenses : The license is available in the Link. Ethical considerations : No personally identifiable information or offensive content is present in the dataset. 36.PROTEINS (Borgwardt et al., 2005) consists of 1,113 graphs where the nodes represent amino acids, and two nodes are connected by an edge",
    "full_text_length": 166013,
    "chunk_length": 1207
  },
  {
    "chunk_id": 3912,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 102,
    "total_chunks": 159,
    "text_content": "The dataset does not have a license. Ethical considerations : No personally identifiable information or offensive content is present in the dataset. 37.PPI(Stark et al., 2006) is a protein dataset from BioGRID covering physical and genetic interaction of proteins. Split : We use a training set of 45555 records, a test set of 11389 records, and a total size of 56944 records. Access restrictions : The dataset is available to download from https://snap.stanford.edu/graphsage/#datasets. Licenses : T",
    "full_text_length": 166013,
    "chunk_length": 1398
  },
  {
    "chunk_id": 3913,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 103,
    "total_chunks": 159,
    "text_content": "a test set of 3,750 records, and a total size of 18,750 records. Access restrictions : The dataset is available to download from https://github.com/tampapath/lung colon image set. Licenses : The dataset does not have a license. Ethical considerations : No personally identifiable information or offensive content is present in the dataset. 39.BCSS contains 151 breast cancer slides from 25 participants. We curate 5264 non-overlapping samples, with labels tumor, stroma, lymphocytic infiltrate, and n",
    "full_text_length": 166013,
    "chunk_length": 1251
  },
  {
    "chunk_id": 3914,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 104,
    "total_chunks": 159,
    "text_content": "13 surgeons. We evaluate the models based on the surgery phase annotations (at 25 fps) and the surgery tool labels (at 1 fps). Split : We use a training set of 5,760 records, a test set of 1,440, and a total size of 7,200 records. Access restrictions : The dataset is available to download from http://camma.u-strasbg.fr/datasets/ through a request form. Licenses : This dataset is available in the Creative Commons Attribution-Noncomercial-Sharealike 4.0 International License https://creativecommon",
    "full_text_length": 166013,
    "chunk_length": 1366
  },
  {
    "chunk_id": 3915,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 105,
    "total_chunks": 159,
    "text_content": "use a training set of 291 records, a test set of 73 records, and a total size of 364 records. Access restrictions : The dataset is available to download from https://github.com/romanchereshnev/HuGaDB. Licenses : The dataset does not have a license. Ethical considerations : No personally identifiable information or offensive content is present in the dataset. 42.Expression Atlas (Papatheodorou et al., 2018) consists of RNA gene expression data across species and biological conditions. Split : We ",
    "full_text_length": 166013,
    "chunk_length": 1422
  },
  {
    "chunk_id": 3916,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 106,
    "total_chunks": 159,
    "text_content": "use a training set of 101162 records, a test set of 25290 records, and a total size of 126,452 records. Access restrictions : The dataset is available to download from https://www.ncbi.nlm.nih.gov/geo/. Licenses : The license is available at Link. Ethical considerations : No personally identifiable information or offensive content is present in the dataset. 44.Vital (Cui et al., 2024) is a medical image-language dataset based on PMC-15, where instructional data is generated using the gpt-4-visio",
    "full_text_length": 166013,
    "chunk_length": 1384
  },
  {
    "chunk_id": 3917,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 107,
    "total_chunks": 159,
    "text_content": "is a multimodal medical dataset on patients admitted to the emergency department or intesnive care unit at the Beth Israel Deaconess Medical Center in Boston, MA. We train and evaluate our models using the EHR, vital sign, and chest-xray modalities. 23 CLIMB: Data Foundations for Large Scale Multimodal Clinical Foundation Models Split : CLIMB provide a training set of 640,000 records, a test set of 160,000 records, and a total size of 800,000 records. In the fusion experiment, however, we focus ",
    "full_text_length": 166013,
    "chunk_length": 1458
  },
  {
    "chunk_id": 3918,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 108,
    "total_chunks": 159,
    "text_content": "dataset collection and curation process follows a systematic two-stage approach to ensure both comprehensive coverage and accessibility while maintaining data quality and diversity. Stage 1: Initial Dataset Identification We first conducted a comprehensive literature review of publicly available clinical datasets across different modalities. Our inclusion criteria at this stage focused on accessibility: \u2022 Datasets with direct download access through public repositories \u2022 Datasets requiring appli",
    "full_text_length": 166013,
    "chunk_length": 1522
  },
  {
    "chunk_id": 3919,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 109,
    "total_chunks": 159,
    "text_content": "Examples include CheXpert and MIMIC-IV . \u2022Tier 2: We then systematically identified and included datasets that addressed our three key criteria: \u2013Novel tasks: Datasets covering emerging clinical challenges (e.g., COVID-19 diagnosis) \u2013Understudied modalities: Datasets from underrepresented data types (e.g., EEG, endoscopic videos) \u2013Underrepresented regions: Datasets from developing regions with limited representation Selection Criteria Here, we elaborate on our methodology for identifying underst",
    "full_text_length": 166013,
    "chunk_length": 1610
  },
  {
    "chunk_id": 3920,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 110,
    "total_chunks": 159,
    "text_content": "We analyzed the total number of publicly available samples per modality: \u2022 High availability ( >500K): X-ray (595,264), CT Scan (1,810,256), EEG (655,786) 24 CLIMB: Data Foundations for Large Scale Multimodal Clinical Foundation Models \u2022 Medium availability (50K-500K): ECG (84,172), Genomic (130,958) \u2022Low availability (5K-50K): Mammography (24,697), Dermoscopy (45,439), Fundus (18,067), Endoscopic (14,400), Pathology (24,014), MRI (14,807) \u2022 Very low availability ( <5k): Ultrasound (1,633) Based",
    "full_text_length": 166013,
    "chunk_length": 1484
  },
  {
    "chunk_id": 3921,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 111,
    "total_chunks": 159,
    "text_content": "South and Southeast Asia 2.Economic Development: We mapped datasets to their countries of origin, specifically identifying datasets from developing nations. This analysis highlighted the importance of including datasets from: \u2022 Asia: India, Vietnam \u2022 Middle East: Iraq \u2022 South America: Brazil This analysis informed our targeted efforts to include datasets from these underrepresented regions, aiming to improve the geographic and demographic diversity of our benchmark. The complete list of included",
    "full_text_length": 166013,
    "chunk_length": 1526
  },
  {
    "chunk_id": 3922,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 112,
    "total_chunks": 159,
    "text_content": "to use. All additional metadata and multi-view images are preserved and made available, though our benchmark experiments utilize only the primary labels to ensure fair comparison across models. The complete preprocessing scripts and documentation are available in our code repository. B. CLIMB-QA Construction To enable standardized evaluation of large vision-language models (VLMs), we construct CLIMB-QA, a question-answering version of our dataset. For each sample xi\u2208Dkwith label set yi\u2286Vk, we ge",
    "full_text_length": 166013,
    "chunk_length": 1460
  },
  {
    "chunk_id": 3923,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 113,
    "total_chunks": 159,
    "text_content": "while preserving the original classification task structure. For example, in the APTOS dataset for diabetic retinopathy grading: qi=\u201dAbove is a retinal image of a patient. Grade the diabetic retinopathy on the Davis Scale, choosing from: No DR, Mild DR, Moderate DR, Severe DR, Proliferative DR.\u201d ai=\u201dModerate DR\u201d For multi-label classification tasks, we evaluate predictions using order-agnostic matching: given a predicted answer set \u02c6ai and ground truth ai, we consider the prediction correct if \u02c6",
    "full_text_length": 166013,
    "chunk_length": 1336
  },
  {
    "chunk_id": 3924,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 114,
    "total_chunks": 159,
    "text_content": "series, we extract nsequential elements. The input processing varies by modality: 1. Images: \u03d5img:RH\u00d7W\u00d7C\u2192Rn\u00d7\u03c3\u00d7\u03c3\u00d7C 2. Videos: \u03d5vid:RT\u00d7H\u00d7W\u00d7C\u2192Rn\u00d7\u03c3\u00d7\u03c3\u00d7C 3. Time Series: \u03d5ts:RT\u00d7C\u2192Rn\u00d7C where \u03c3denotes the model-specific input size, and H, W, C, T represent spatial dimensions, channels, and temporal length, respectively. The objective is to learn a function f:X \u2192 { 0,1}|V|that maps each input to a binary vector over V. RQ3: Given multimodal inputs M=mvis, mlang, mts, where each modality-specific input xm",
    "full_text_length": 166013,
    "chunk_length": 1414
  },
  {
    "chunk_id": 3925,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 115,
    "total_chunks": 159,
    "text_content": "encoder for each input type across all clinical tasks. Specifically, we train vision, graph and time series encoders on the complete Dto assess general diagnostic capabilities. For each sample xi\u2208Dk, we evaluate performance using a dataset-specific vocabulary mask 1Vk\u2208 {0,1}|V|, where only predictions corresponding to labels in Vkare considered in the evaluation metrics. Performance is measured using Area Under the ROC Curve (AUC) and binary classification metrics (specificity and sensitivity) w",
    "full_text_length": 166013,
    "chunk_length": 1390
  },
  {
    "chunk_id": 3926,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 116,
    "total_chunks": 159,
    "text_content": "the practical scenario where models must adapt to novel diagnostic tasks with limited labeled examples while leveraging pre-trained representations from related but distinct tasks. A detailed description of the dataset composition, as well as the experimental procedure, are included in App. C.7.1. C.2.3. RQ3: S INGLE MODALITY TO MULTIMODALITY TRANSFER VIA ROBUST FUSION STRATEGY . Finally, we investigate optimal fusion mechanisms for integrating heterogeneous modalities as defined in App. C.1. We",
    "full_text_length": 166013,
    "chunk_length": 1455
  },
  {
    "chunk_id": 3927,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 117,
    "total_chunks": 159,
    "text_content": "2022) for visual inputs, ClinicalBERT (Huang et al., 2019) for text, and ECG-JEPA (Kim, 2024) for time series data. C.3. Evaluation Metrics For binary classification tasks f:X \u2192 { 0,1}, we employ three complementary metrics to assess model performance: 27 CLIMB: Data Foundations for Large Scale Multimodal Clinical Foundation Models 1.Area Under the ROC Curve (AUC): Given predicted probabilities \u02c6yi\u2208[0,1]and true labels yi\u2208 {0,1}, AUC measures the model\u2019s ability to discriminate between classes a",
    "full_text_length": 166013,
    "chunk_length": 1371
  },
  {
    "chunk_id": 3928,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 118,
    "total_chunks": 159,
    "text_content": "The choice of these three metrics is particularly motivated by clinical considerations. In medical diagnosis, there is often an inherent trade-off between sensitivity and specificity, where improving one typically comes at the cost of the other. Sensitivity is crucial in cases where missing a positive diagnosis (false negative) could have severe consequences for patient outcomes, such as failing to detect a life-threatening condition. Conversely, specificity is vital when false positives could l",
    "full_text_length": 166013,
    "chunk_length": 1442
  },
  {
    "chunk_id": 3929,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 119,
    "total_chunks": 159,
    "text_content": "are computed using the dataset-specific vocabulary masks 1Vkas defined in Sec. C.1. Below, we describe in detail about the setup and procedures of vision, time series and graph experiments. C.4. Vision Model Experiments C.4.1. V ISION MODEL DETAILS MedViT (Manzari et al., 2023) is a Vision Transformer variant specifically designed for medical imaging tasks. It incorporates a hierarchical structure with varying token sizes across different stages and employs medical-specific attention mechanisms ",
    "full_text_length": 166013,
    "chunk_length": 1514
  },
  {
    "chunk_id": 3930,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 120,
    "total_chunks": 159,
    "text_content": "SBB2 (Radford et al., 2021) is an enhanced vision backbone that builds upon the clip architecture. It introduces improved spatial mixing operations and hierarchical feature representations while maintaining computational efficiency. 28 CLIMB: Data Foundations for Large Scale Multimodal Clinical Foundation Models Swin Transformer (Liu et al., 2021) is a hierarchical vision transformer that computes self-attention within shifted windows. It introduces a hierarchical architecture with varying windo",
    "full_text_length": 166013,
    "chunk_length": 1505
  },
  {
    "chunk_id": 3931,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 121,
    "total_chunks": 159,
    "text_content": "2022) is a pure convolutional architecture that modernizes traditional CNN design principles. It incorporates fully convolutional design, global response normalization, and gradient checkpointing, achieving strong performance across various vision tasks. C.4.2. V ISION HYPERPARAMETERS AND EXPERIMENTAL PROCEDURES All experiments are ran on a GPU server with 8xH200 141GB GPUs. We used the SOAP optimizer (Vyas et al., 2024) as it offers the best performance. Depending on the model sizes, we use a p",
    "full_text_length": 166013,
    "chunk_length": 1320
  },
  {
    "chunk_id": 3932,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 122,
    "total_chunks": 159,
    "text_content": "C.5. EEG Model Experiments We investigate the performance of five baseline models and four variants of foundational time series models for EEG classifications. Our EEG experiment is built upon the repository sheared by (Yang et al., 2024), and the pre-trained weights were downloaded from https://github.com/ycq091044/BIOT . C.5.1. EEG D ATASETS AND PREPROCESSING We evaluated the models on IIIC (Jing et al., 2023b), TUAB (Lopez et al., 2015), and TUEV (Lopez et al., 2016). All EEG channels, S[i], ",
    "full_text_length": 166013,
    "chunk_length": 1263
  },
  {
    "chunk_id": 3933,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 123,
    "total_chunks": 159,
    "text_content": "batch size was set to 512, while for few-shot experiments, it was set to 4. All experiments were conducted using the PyTorch framework. We saved the model with the lowest CrossEntropy loss over 20 epochs for evaluation on the test split. We report the AUROC, Sensitivity, Specificity, and F1 Score of our experiments in App. D. C.5.3. EEG M ODEL DETAILS SPaRCNet (Jing et al., 2023b) is a 1D-CNN designed for EEG classification. It employs a hierarchical feature extraction process, beginning with an",
    "full_text_length": 166013,
    "chunk_length": 1351
  },
  {
    "chunk_id": 3934,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 124,
    "total_chunks": 159,
    "text_content": "deep residual CNN with four stacked ResBlocks for hierarchical feature extraction. The CNN embeddings are segmented and passed through a Transformer encoder with positional encoding, allowing the model to capture long-range temporal dependencies. ContraWR (Yang et al., 2021) is a EEG classification model that integrates STFT with a 2D-CNN for sleep staging. The model first converts raw EEG signals into spectrograms using STFT, which are then processed through a deep residual CNN 29 CLIMB: Data F",
    "full_text_length": 166013,
    "chunk_length": 1384
  },
  {
    "chunk_id": 3935,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 125,
    "total_chunks": 159,
    "text_content": "temporal compression using a downsampling operation before being fed into a bidirectional LSTM for sequential feature extraction. The final representation is obtained by concatenating CNN and LSTM embeddings, which are passed through a fully connected classification layer. STTransformer (Song et al., 2021) is a spatiotemporal Transformer model for EEG classification that integrates channel- wise attention with Transformer-based sequence modeling. The model first applies a ChannelAttention mechan",
    "full_text_length": 166013,
    "chunk_length": 1496
  },
  {
    "chunk_id": 3936,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 126,
    "total_chunks": 159,
    "text_content": "transformed into spectrogram representations via STFT. Channel-specific positional embedding and temporal positional embedding are added to the tokens to enhance both temporal and spatial representations. These spectral embeddings are then processed using a Linear Attention Transformer. C.6. ECG Model Experiments We compare the performance of ECG-JEPA, a time series specific model, and UniTS, a generalized time series model for ECG classifciations. Our ECG experiment and pretrained encoder weigh",
    "full_text_length": 166013,
    "chunk_length": 1364
  },
  {
    "chunk_id": 3937,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 127,
    "total_chunks": 159,
    "text_content": "12 to 8, as the remaining 4 channels can be derived using linear combinations of the selected leads. For details on dataset access and data splitting, please refer to App. A. C.6.2. H YPERPARAMETERS AND EXPERIMENTAL PROCEDURES For the ECG-JEPA model, we use the Adam optimizer (Diederik, 2014) with a learning rate of 1\u00d710\u22123and a weight decay of1\u00d710\u22122. For the UniTS model, we use the Adam optimizer with a learning rate of 1\u00d710\u22124and a weight decay of 5\u00d710\u22126. For all experiments, the batch size was ",
    "full_text_length": 166013,
    "chunk_length": 1313
  },
  {
    "chunk_id": 3938,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 128,
    "total_chunks": 159,
    "text_content": "across multiple leads, enhancing performance on downstream tasks. UniTS (Gao et al., 2024) is a unified multitask time series model that integrates predictive and generative tasks using task tokenization within a single framework. It employs a modified transformer block to learn transferable time series representations across diverse domains, handling variations in sampling rates and temporal patterns. C.7. Out-of-distribution Experiment Details In this section, we describe the details on how we",
    "full_text_length": 166013,
    "chunk_length": 1332
  },
  {
    "chunk_id": 3939,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 129,
    "total_chunks": 159,
    "text_content": "CLIMB: Data Foundations for Large Scale Multimodal Clinical Foundation Models (like COVID-19), a new task (cancer vs pulmonary embolism), or a different granularity of the same task (6-way BI-RADS classification instead of 5-way). C.7.2. E XPERIMENTAL PROCEDURES Vision Encoders. For vision encoders, we run a mixed training on the full dataset, with the OOD dataset filtered out. D. Full Experimental Results D.1. Full Dataset Multitask Training Results In this section, we report the detailed model",
    "full_text_length": 166013,
    "chunk_length": 1472
  },
  {
    "chunk_id": 3940,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 130,
    "total_chunks": 159,
    "text_content": "Generalized Periodic Discharges (GPD), Lateralized Rhythmic Delta Activity (LRDA), Generalized Rhythmic Delta Activity (GRDA), Other TUAB 2 Normal, Abnormal TUEV 6 Spike and Slow Wave (SPSW), Generalized Periodic Epileptiform Discharge (GPED), Periodic Lateralized Epileptiform Discharge (PLED), Eye Movement (EYEM), Artifact (ARTF), Back- ground (BCKG) MIMIC-CXR 14 Atelectasis, Cardiomegaly, Consolidation, Edema, Enlarged Cardiomediastinum, Fracture, Lung Lesion, Lung Opacity, Pleural Effusion, P",
    "full_text_length": 166013,
    "chunk_length": 1554
  },
  {
    "chunk_id": 3941,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 131,
    "total_chunks": 159,
    "text_content": "DR, Moderate DR, Severe DR, PDR APTOS 2019 5 No DR, Mild, Moderate, Severe, Proliferative DR Jichi 3 SDR (simple diabetic retinopathy), PPDR (pre-proliferative diabetic retinopathy), PDR (prolifer- ative diabetic retinopathy) LNDb 3 nodule \u22653mm, nodule \u00a13mm, non-nodule INSPECT 5 No PE, Acute Subsegmental-only PE, Acute PE, Subsegmental-only PE, Chronic PE KiTS23 2 Benign, Malignant Hemorrhage 2 No Hemorrhage, Has Hemorrhage RSPECT 3 No PE, Chronic PE, Acute PE EchoNet-Dynamic - Not classificatio",
    "full_text_length": 166013,
    "chunk_length": 1367
  },
  {
    "chunk_id": 3942,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 132,
    "total_chunks": 159,
    "text_content": "Surgery phase annotations and surgery tool labels HuGaDB 4 Sitting, Standing, Sitting down, Standing up Expression Atlas - Not classification Geo - Not classification Vital - Not classification MIMIC-IV 2 48 Hour In-Hospital-Mortality (48 IHM) (Yes/No) 32 CLIMB: Data Foundations for Large Scale Multimodal Clinical Foundation Models Table 8. Dataset Demographics and Location Information. Dataset Locations Demographic Information PTB-XL Multiple sex: 52% male, 48% female; age range: 0-95 (median: ",
    "full_text_length": 166013,
    "chunk_length": 1431
  },
  {
    "chunk_id": 3943,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 133,
    "total_chunks": 159,
    "text_content": "44.10% female VinDr-Mammo The Institutional Review Board of Hanoi Medical Uni- versity Hospital (HMUH) and Hospital 108 (H108)age; imaging device\u2019s model CBIS-DDSM Stanford, California, US Unknown ISIC-2020 Hospital Cl \u00b4\u0131nic de Barcelona, Medical University of Vienna, Memorial Sloan Kettering Cancer Center, Melanoma Institute Australia, University of Queens- land, and the University of Athens Medical Schoolsex: female: 15981 (48%), male: 17080 (52%); age range: 0-90 (median: 48.87) HAM10000 unkn",
    "full_text_length": 166013,
    "chunk_length": 1441
  },
  {
    "chunk_id": 3944,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 134,
    "total_chunks": 159,
    "text_content": "Multiple sex: 54.5% male, 45.5% female; race; age HuGaDB unknown sex: 4 females, 14 males; age: average: 23.67; height: average: 179.06 cm; weight: average: 73.44 kg INSPECT Stanford Medicine (2000-2021) gender: female: 10,733, male: 8,666, unknown: 3; age: 18-39: 2,912, 39-69: 9,974, 69-89: 5,859, \u00bf89: 657; race: white: 10,704, asian: 2,976, black: 1,103, native: 415, unknown: 2,404; ethnicity: hispanic: 3,018, not hispanic: 15,628, unknown: 756 EchoNet-Dynamic Stanford University unknown BUSI ",
    "full_text_length": 166013,
    "chunk_length": 1429
  },
  {
    "chunk_id": 3945,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 135,
    "total_chunks": 159,
    "text_content": "raters are physician experts TUAB The Temple University Hospital, Philadelphia, Penn- sylvania, USAEvaluation Dataset: Total: 276 files, 253 subjects; Ab- normal Female: 63 files, 51 subjects; Abnormal Male: 63 files, 54 subjects; Normal Female: 85 files, 84 sub- jects; Normal Male: 65 files, 64 subjects; Train Dataset: Total: 2,717 files, 2,130 subjects TUEV The Temple University Hospital, Philadelphia, Penn- sylvania, USA290 patients in train; 78 patients in eval MIMIC-IV Beth Israel Deaconess",
    "full_text_length": 166013,
    "chunk_length": 1599
  },
  {
    "chunk_id": 3946,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 136,
    "total_chunks": 159,
    "text_content": "Jichi SDR, PPDR, PDR Different classification scheme for diabetic retinopathy pro- gression compared to Messidor-2 and APTOS 2019\u2019s five- stage classification BCSS Tumor, Stroma, Lymphocytic infiltrate, Necro- sis/debrisDifferent task type (tissue component classification) compared to LC25000\u2019s focus on cancer type classification BUSI Normal, Malignant, Benign Different task focus (breast lesion classification) compared to other ultrasound datasets (COVID-BLUES, COVID-US) which focus on lung pat",
    "full_text_length": 166013,
    "chunk_length": 1584
  },
  {
    "chunk_id": 3947,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 137,
    "total_chunks": 159,
    "text_content": "Left anterior/left posterior fascicular block (LAFB/LPFB), In- complete right bundle branch block (IRBBB), A V block ( A VB), Non-specific intraventricu- lar conduction disturbance (IVCD), Complete right bundle branch block (CRBBB), Com- plete left bundle branch block (CLBBB), Wolff- Parkinson-White syndrome (WPW), Incomplete left bundle branch block (ILBBB)More fine-grained classification (24 categories) compared to PTB-XL superclass (7 categories in BenchMD). TUEV Spike and slow wave (SPSW), G",
    "full_text_length": 166013,
    "chunk_length": 1481
  },
  {
    "chunk_id": 3948,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 138,
    "total_chunks": 159,
    "text_content": "0.7625 0.1951 0.8077 Fundus APTOS 0.4141 0.2000 0.8000 0.0750 0.6923 CT LNDB 0.7009 0.4960 0.4960 0.4395 0.7841 ISIC 2020 0.2676 0.5000 0.5000 0.4931 0.9726 CBIS-DDSM 0.8333 0.5000 0.5000 0.3000 0.4286 BUSI 0.1111 0.5000 0.5000 0.3333 0.5000 LC25000 0.9448 0.4444 0.8631 0.3218 0.7836 HAM10000 0.6446 0.2500 0.7500 0.2143 0.8750 VinDr CXR 0.6546 0.1607 0.9005 0.1278 0.8587 CoronaHack 0.6907 0.4583 0.7526 0.4242 0.7333 BCSS 0.5833 0.3333 0.6667 0.2727 0.7949 VinDr Mammo 0.4205 0.3333 0.6667 0.2716 ",
    "full_text_length": 166013,
    "chunk_length": 1398
  },
  {
    "chunk_id": 3949,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 139,
    "total_chunks": 159,
    "text_content": "medical imaging datasets. PMC CLIP Dataset AUC Sensitivity Specificity F1 Score Accuracy CT RSPECT 0.7928 0.3333 0.6667 0.3241 0.9639 CT INSPECT 0.5484 0.2000 0.8000 0.1779 0.9205 MIMIC-CXR 0.6613 0.1261 0.9411 0.1234 0.8438 Fundus JINCHI 0.5492 0.2560 0.7513 0.2110 0.8308 Fundus APTOS 0.5863 0.2033 0.8023 0.0928 0.7124 CT LNDB 0.7012 0.5000 0.5000 0.4515 0.8232 ISIC 2020 0.7351 0.5000 0.5000 0.4955 0.9823 CBIS-DDSM 0.6475 0.2069 0.8064 0.1798 0.8024 BUSI 0.5371 0.3233 0.6596 0.2847 0.6616 LC250",
    "full_text_length": 166013,
    "chunk_length": 1396
  },
  {
    "chunk_id": 3950,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 140,
    "total_chunks": 159,
    "text_content": "0.5226 0.5226 0.0499 0.0513 COVID-US 0.6091 0.3333 0.6730 0.1961 0.6000 Messidor-2 0.3898 0.2000 0.8000 0.1473 0.8331 CT Hemorrhage 0.5805 0.5000 0.5000 0.4550 0.8350 Brain Tumor 2 0.5848 0.5441 0.5441 0.5433 0.5882 Overall 0.6346 0.3408 0.7237 0.2727 0.7518 36 CLIMB: Data Foundations for Large Scale Multimodal Clinical Foundation Models Table 12. Performance metrics of RAD-DINO across different medical imaging datasets. RAD-DINO Dataset AUC Sensitivity Specificity F1 Score Accuracy CT RSPECT 0.",
    "full_text_length": 166013,
    "chunk_length": 1411
  },
  {
    "chunk_id": 3951,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 141,
    "total_chunks": 159,
    "text_content": "0.5739 0.7436 BCSS 0.6502 0.2622 0.7535 0.1526 0.6768 VinDr Mammo 0.5756 0.2000 0.8000 0.1606 0.8682 COVID-BLUES 0.5775 0.5000 0.5000 0.2066 0.2604 Brain Tumor 0.6725 0.3334 0.7760 0.2437 0.6459 KiTS23 0.4565 0.5000 0.5000 0.4658 0.8718 CheXpert 0.7516 0.2124 0.9264 0.2347 0.8068 PAD-UFES-20 0.6125 0.2666 0.8217 0.1722 0.7028 COVID-19 CXR 0.8946 0.4199 0.8357 0.3952 0.7753 CMMD 0.5065 0.5000 0.5000 0.0064 0.0064 COVID-US 0.6750 0.5803 0.7397 0.5056 0.6800 Messidor-2 0.5311 0.2000 0.8000 0.1473 0",
    "full_text_length": 166013,
    "chunk_length": 1398
  },
  {
    "chunk_id": 3952,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 142,
    "total_chunks": 159,
    "text_content": "APTOS 0.8212 0.3514 0.8927 0.3292 0.8647 CT LNDB 0.6932 0.5000 0.5000 0.4515 0.8232 ISIC 2020 0.8026 0.5000 0.5000 0.4955 0.9823 CBIS-DDSM 0.6366 0.1994 0.8007 0.1437 0.8232 BUSI 0.6649 0.3561 0.6773 0.3040 0.7056 LC25000 0.9977 0.9622 0.9906 0.9622 0.9849 HAM10000 0.8641 0.2797 0.8438 0.2902 0.8824 VinDr CXR 0.6377 0.0845 0.9188 0.0717 0.9261 CoronaHack 0.9127 0.7521 0.8830 0.7532 0.8462 BCSS 0.7249 0.4504 0.8171 0.3938 0.7550 VinDr Mammo 0.5981 0.2003 0.8013 0.1668 0.8653 COVID-BLUES 0.6890 0.",
    "full_text_length": 166013,
    "chunk_length": 1412
  },
  {
    "chunk_id": 3953,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 143,
    "total_chunks": 159,
    "text_content": "Foundations for Large Scale Multimodal Clinical Foundation Models Table 14. Performance metrics of Swin Transformer across different medical imaging datasets. Swin Transformer Dataset AUC Sensitivity Specificity F1 Score Accuracy CT RSPECT 0.8962 0.4318 0.7605 0.4613 0.9683 CT INSPECT 0.6468 0.2049 0.8035 0.1878 0.9219 MIMIC-CXR 0.7620 0.1748 0.9459 0.2113 0.8571 Fundus JICHI 0.7821 0.3663 0.7967 0.3392 0.8396 Fundus APTOS 0.8636 0.3632 0.9089 0.3123 0.8849 CT LNDB 0.6660 0.5113 0.5113 0.4778 0.",
    "full_text_length": 166013,
    "chunk_length": 1415
  },
  {
    "chunk_id": 3954,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 144,
    "total_chunks": 159,
    "text_content": "0.2395 0.8140 PAD-UFES-20 0.8042 0.3996 0.8542 0.3654 0.8100 COVID-19 CXR 0.9319 0.7029 0.9268 0.7396 0.8978 CT Hemorrhage 0.7562 0.5000 0.5000 0.4550 0.8350 COVID-US 0.8721 0.6727 0.8619 0.6672 0.8133 Messidor-2 0.6645 0.2524 0.8072 0.2183 0.8366 Brain Tumor 2 0.9360 0.5588 0.5588 0.5149 0.7059 Overall 0.7649 0.4364 0.7753 0.4154 0.8369 39 CLIMB: Data Foundations for Large Scale Multimodal Clinical Foundation Models Table 15. Performance metrics of EV A-2 across different medical imaging datase",
    "full_text_length": 166013,
    "chunk_length": 1403
  },
  {
    "chunk_id": 3955,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 145,
    "total_chunks": 159,
    "text_content": "0.5196 0.0958 0.1624 VinDr CXR 0.6986 0.1136 0.9326 0.4718 0.9223 Messidor-2 0.5972 0.2000 0.8000 0.4293 0.8331 CoronaHack 0.8826 0.4654 0.7472 0.4578 0.6902 Fundus APTOS 0.8674 0.2773 0.8562 0.4195 0.8117 PAD-UFES-20 0.5795 0.2386 0.8073 0.2656 0.7673 CheXpert 0.8056 0.2596 0.8940 0.3897 0.8034 CMMD 0.4932 0.5000 0.5000 0.0001 0.0064 BUSI 0.5925 0.3333 0.6667 0.4001 0.7056 Brain Tumor 2 0.6739 0.5000 0.5000 0.5333 0.6667 Overall 0.6871 0.3452 0.7239 0.4580 0.7366 40 CLIMB: Data Foundations for ",
    "full_text_length": 166013,
    "chunk_length": 1411
  },
  {
    "chunk_id": 3956,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 146,
    "total_chunks": 159,
    "text_content": "0.8299 BUSI 0.7653 0.4862 0.7431 0.4623 0.6853 LC25000 0.9999 0.9926 0.9982 0.9926 0.9971 HAM10000 0.9423 0.6485 0.9298 0.6511 0.9251 VinDr CXR 0.5609 0.0852 0.9175 0.0726 0.9270 CoronaHack 0.9573 0.8369 0.9260 0.8381 0.9017 BCSS 0.8098 0.5076 0.8180 0.4981 0.7773 VinDr Mammo 0.6732 0.2536 0.8069 0.2414 0.8693 COVID-BLUES 0.7318 0.6814 0.6814 0.6742 0.7396 Brain Tumor 0.9293 0.7257 0.9069 0.7136 0.8655 KiTS23 0.4108 0.5000 0.5000 0.4658 0.8718 CheXpert 0.8037 0.2722 0.9521 0.3160 0.8236 PAD-UFES",
    "full_text_length": 166013,
    "chunk_length": 1404
  },
  {
    "chunk_id": 3957,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 147,
    "total_chunks": 159,
    "text_content": "Score Accuracy CT RSPECT 0.9005 0.4407 0.7597 0.4791 0.9679 CT INSPECT 0.5954 0.2013 0.8008 0.1808 0.9206 MIMIC-CXR 0.7780 0.2196 0.9426 0.2397 0.8667 Fundus JINCHI 0.8258 0.5438 0.8224 0.4625 0.7293 Fundus APTOS 0.8786 0.4505 0.9260 0.4115 0.8941 CT LNDB 0.6810 0.5448 0.5448 0.5470 0.8050 ISIC 2020 0.8427 0.5040 0.5040 0.5037 0.9820 CBIS-DDSM 0.6701 0.2714 0.8217 0.2488 0.8205 BUSI 0.7310 0.5016 0.7391 0.4475 0.6311 LC25000 0.9990 0.9738 0.9934 0.9737 0.9895 HAM10000 0.9012 0.6092 0.9210 0.5381",
    "full_text_length": 166013,
    "chunk_length": 1418
  },
  {
    "chunk_id": 3958,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 148,
    "total_chunks": 159,
    "text_content": "Messidor-2 0.8119 0.2977 0.8050 0.1526 0.6720 CT Hemorrhage 0.7790 0.6562 0.6562 0.6398 0.7806 Brain Tumor 2 0.9377 0.7500 0.7500 0.7733 0.8235 Overall 0.7720 0.4923 0.7888 0.4721 0.8368 Table 18. Performance evaluation of GCN across different modalities and datasets. GCN Modality DatasetPerformance Metrics AUC Sensitivity Specificity Brain NetworksPPMI 0.973 0.922 0.897 ABIDE 0.626 0.596 0.586 ABCD 0.814 0.570 0.916 Average 0.804 0.696 0.800 MolecularPPI 0.807 0.496 0.716 PROTEINS 0.718 0.568 0",
    "full_text_length": 166013,
    "chunk_length": 1458
  },
  {
    "chunk_id": 3959,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 149,
    "total_chunks": 159,
    "text_content": "different modalities and datasets. Graph Transformers Modality DatasetPerformance Metrics AUC Sensitivity Specificity Brain NetworksPPMI 0.950 0.862 0.957 ABIDE 0.743 0.707 0.683 ABCD 0.864 0.860 0.837 Average 0.852 0.810 0.826 MolecularPPI 0.997 0.606 0.873 PROTEINS 0.580 0.156 0.967 Average 0.789 0.381 0.920 Overall Average 0.820 0.595 0.873 43 CLIMB: Data Foundations for Large Scale Multimodal Clinical Foundation Models Table 21. Performance metrics across different medical imaging datasets. ",
    "full_text_length": 166013,
    "chunk_length": 1428
  },
  {
    "chunk_id": 3960,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 150,
    "total_chunks": 159,
    "text_content": "CXR - 0.3353 0.5809 0.1281 0.7099 CoronaHack 0.9461 0.8404 0.9225 0.8400 0.8462 BCSS 0.8431 0.6276 0.8571 0.6053 0.6219 VinDr Mammo 0.6864 0.3138 0.8179 0.2802 0.6747 COVID-BLUES 0.5282 0.5000 0.5000 0.2066 0.2604 Brain Tumor 0.8291 0.4801 0.8188 0.4555 0.4721 KiTS23 0.3235 0.5000 0.5000 0.4658 0.8718 CheXpert - 0.6351 0.7669 0.4516 0.7603 PAD-UFES-20 0.8112 0.3295 0.8531 0.3430 0.5425 COVID-19 CXR 0.9168 0.5689 0.8947 0.6013 0.7419 CMMD 0.5204 0.4860 0.4860 0.4913 0.9658 COVID-US 0.5000 0.3333 ",
    "full_text_length": 166013,
    "chunk_length": 1389
  },
  {
    "chunk_id": 3961,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 151,
    "total_chunks": 159,
    "text_content": "0.001 0.214 0.786 0.063 0.001 0.214 0.794 0.090 CheXpert 0.000 0.077 0.924 0.036 0.000 0.154 0.847 0.093 VinDr-CXR 0.045 0.083 0.916 0.016 0.702 0.083 0.917 0.069 COVID-19 0.009 0.250 0.750 0.005 0.455 0.250 0.750 0.156 CoronaHack 0.388 0.333 0.667 0.186 0.388 0.333 0.667 0.186 Brain Tumor 0.393 0.445 0.799 0.300 0.292 0.250 0.750 0.113 Brain Tumor 2 0.333 0.500 0.500 0.250 0.667 0.500 0.500 0.400 BUSI 0.579 0.391 0.697 0.357 0.558 0.333 0.667 0.239 COVID-BLUES 0.365 0.557 0.557 0.353 0.740 0.50",
    "full_text_length": 166013,
    "chunk_length": 1231
  },
  {
    "chunk_id": 3962,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 152,
    "total_chunks": 159,
    "text_content": "0.832 0.048 0.583 0.200 0.800 0.147 APTOS 0.492 0.200 0.800 0.132 0.492 0.200 0.800 0.132 Jichi 0.660 0.250 0.750 0.199 0.660 0.250 0.750 0.199 LNDb 0.823 0.500 0.500 0.452 0.645 0.541 0.541 0.520 Kits23 0.128 0.500 0.500 0.114 0.872 0.500 0.500 0.466 Brain CT 0.835 0.500 0.500 0.455 0.835 0.500 0.500 0.455 INSPECT 0.004 0.194 0.800 0.006 0.801 0.200 0.800 0.178 Cholec 80 0.000 1.000 0.000 0.243 0.296 0.143 0.857 0.069 Overall 0.298 0.333 0.685 0.172 0.570 0.297 0.707 0.231 Table 23. Model Perfo",
    "full_text_length": 166013,
    "chunk_length": 1111
  },
  {
    "chunk_id": 3963,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 153,
    "total_chunks": 159,
    "text_content": ".906 .347 .874 .787 .787 .789 .839 .562 .833 .528 ContraWR .832 .446 .892 .410 .847 .439 .898 .370 .872 .782 .782 .781 .850 .556 .858 .520 STTransformer .785 .412 .884 .407 .701 .371 .874 .262 .864 .785 .785 .787 .783 .522 .848 .485 BIOT .854 .510 .905 .499 .856 .466 .908 .371 .879 .798 .798 .799 .863 .591 .870 .556 BIOT-pretrain-PREST .844 .496 .902 .486 .898 .580 .918 .373 .878 .797 .797 .799 .873 .624 .872 .552 BIOT-pretrain-SHHS+PREST .848 .523 .906 .507 .880 .586 .914 .415 .882 .806 .806 .8",
    "full_text_length": 166013,
    "chunk_length": 1175
  },
  {
    "chunk_id": 3964,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 154,
    "total_chunks": 159,
    "text_content": ".740 .359 .873 full .856 .466 .908 BIOT-pretrain-PREST1 TUEV.609 .239 .835 8 .754 .372 .871 32 .781 .363 .899 full .898 .580 .918 BIOT-pretrain-SHHS+PREST1 TUEV.630 .239 .836 8 .723 .305 .861 32 .777 .382 .879 full .880 .586 .914 BIOT-pretrain-IIIC+TUEV1 TUEV.553 .179 .836 8 .759 .332 .873 32 .807 .410 .894 full .869 .510 .905 Table 25. Model Performance on Different ECG Datasets Model NamePTB-XL ChapmanShao CPSC Ga Overall AUC Sens Spe AUC Sens Spe AUC Sens Spe AUC Sens Spe AUC Sens Spe Transfo",
    "full_text_length": 166013,
    "chunk_length": 1152
  },
  {
    "chunk_id": 3965,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 155,
    "total_chunks": 159,
    "text_content": "on CLIMB dataset with PTB-XL removed. Model Pretrain Encoder Num Shots Finetune AUC Sens Spe ECG-JEPAOurs1 PTB-XL.633 .048 .956 8 .760 .113 .966 full .895 .210 .980 PTB-XL1 PTB-XL.512 .043 .956 8 .472 .043 .956 full .868 .195 .979 UniTSOurs1 PTB-XL.527 .430 .537 8 .549 .641 .370 full .673 .025 .993 PTB-XL1 PTB-XL.512 .674 .371 8 .470 .322 .658 full .688 .053 .984 46 CLIMB: Data Foundations for Large Scale Multimodal Clinical Foundation Models Table 27. Model Performance of ECG-JEPA Variants with",
    "full_text_length": 166013,
    "chunk_length": 1227
  },
  {
    "chunk_id": 3966,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 156,
    "total_chunks": 159,
    "text_content": "dataset and Original is the pretrained encoder provided by the UniTS model. Pretrain Train Eval AUC Sens Spe OriginalPTB-XL PTB-XL .669 .150 .861 CPSC CPSC .641 .143 .857 ChapmanShao ChapmanShao .656 .146 .859 Ga Ga .598 .158 .863 Original OursPTB-XL .772 .242 .886 CPSC .871 .336 .919 ChapmanShao .812 .333 .927 Ga .742 .271 .894 47 CLIMB: Data Foundations for Large Scale Multimodal Clinical Foundation Models Table 29. Performance Comparison with Dataset Specific Encoders. Ours is the result of m",
    "full_text_length": 166013,
    "chunk_length": 1257
  },
  {
    "chunk_id": 3967,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 157,
    "total_chunks": 159,
    "text_content": "(AUC) Source/Method Name Reference PTB-XL 0.895 0.896 ECG JEPA Link Chapman-Shaoxing 0.858 0.979 X3ECG w/ HC + DDI \u2013 Georgia 0.767 \u2013 \u2013 \u2013 CPSC 0.978 0.974 ECG JEPA Link MIMIC-CXR 0.800 0.834 ChexClusion Link CheXpert 0.783 0.933 CFT Link VinDr-CXR 0.631 (14 classes) 0.961 (6 classes) Paper Link VinDr-Mammo 0.673 (5 classes) 0.840 (2 classes) MaMT4 Link CBIS-DDSM 0.707 (5 classes) 0.900 (2 classes) MEWOA Link ISIC 2020 0.853 0.943 Kaggle Leaderboard Link HAM10000 0.942 0.943 Paper Link PAD-UFES-20",
    "full_text_length": 166013,
    "chunk_length": 1148
  },
  {
    "chunk_id": 3968,
    "paper_filename": "wei_2024_data_foundation_for_large_scale_mulimodal_clinical_foundation_model.pdf",
    "paper_title": "Wei 2024 Data Foundation For Large Scale Mulimodal Clinical Foundation Model",
    "chunk_index": 158,
    "total_chunks": 159,
    "text_content": "CoronaHack 0.957 \u2013 \u2013 \u2013 COVID-BLUES 0.732 \u2013 \u2013 \u2013 COVID-US 0.825 0.94 Review Link IIIC 0.862 0.580 (Balanced Acc.) \u2013 \u2013 TUAB 0.882 0.882 \u2013 \u2013 TUEV 0.898 0.528 (Balanced Acc.) \u2013 \u2013 PROTEINS 0.719 0.849 (Acc) HGP-SL Link PPI 0.997 0.997 (Acc) g2-MLP Link RSPECT 0.94 \u2013 \u2013 \u2013 LC25000 1.000 1.000 SE Networks Link BCSS 0.810 0.710 (mIoU) MLP-MF Link 48",
    "full_text_length": 166013,
    "chunk_length": 340
  },
  {
    "chunk_id": 3969,
    "paper_filename": "xiangdong_2023_review_of_the_application_of_multi\u2011modal_deep_learning_in medicine.pdf",
    "paper_title": "Xiangdong 2023 Review Of The Application Of Multi\u2011Modal Deep Learning In Medicine",
    "chunk_index": 0,
    "total_chunks": 76,
    "text_content": "Vol.:(0123456789)1 3International Journal of Computational Intelligence Systems (2023) 16:44 https://doi.org/10.1007/s44196-023-00225-6 REVIEW ARTICLE A Review of the Application of Multi\u2011modal Deep Learning in Medicine: Bibliometrics and Future Directions Xiangdong Pei1,2 \u00b7 Ke Zuo1 \u00b7 Yuan Li1 \u00b7 Zhengbin Pang1 Received: 12 May 2022 / Accepted: 16 March 2023 \u00a9 The Author(s) 2023 Abstract In recent years, deep learning has been applied in the field of clinical medicine to process large-scale medic",
    "full_text_length": 79306,
    "chunk_length": 1468
  },
  {
    "chunk_id": 3970,
    "paper_filename": "xiangdong_2023_review_of_the_application_of_multi\u2011modal_deep_learning_in medicine.pdf",
    "paper_title": "Xiangdong 2023 Review Of The Application Of Multi\u2011Modal Deep Learning In Medicine",
    "chunk_index": 1,
    "total_chunks": 76,
    "text_content": "samples, and implementation performance. Additionally, we present the main challenges and goals of the latest trends in multi-modal medical convergence. To provide a clearer perspective on new trends, we also analyzed relevant papers on the Web of Science. We obtain some meaningful results based on the annual development trends, country, institution, and journal-level research, highly cited papers, and research directions. Finally, we perform co-authorship analysis, co-citation analysis, co-occu",
    "full_text_length": 79306,
    "chunk_length": 1549
  },
  {
    "chunk_id": 3971,
    "paper_filename": "xiangdong_2023_review_of_the_application_of_multi\u2011modal_deep_learning_in medicine.pdf",
    "paper_title": "Xiangdong 2023 Review Of The Application Of Multi\u2011Modal Deep Learning In Medicine",
    "chunk_index": 2,
    "total_chunks": 76,
    "text_content": "and Language by Large- Scale Multi-Modal Pre-Training UniT Unified TransformerUNITER Universal image-text representation ViLT Vision and language Transformer CPt Colorful prompt tuning ALBEF Align before fuse ITC Image-text contrast learning MLM Masking language modeling ITM Image-text matching AUC Area under the receiver operating characteris- tic curve CNN Convolutional neural network GCN Graph convolutional network TCGA The Cancer Genome Atlas RNN Recurrent neural network GPT-3 Generative pre",
    "full_text_length": 79306,
    "chunk_length": 1507
  },
  {
    "chunk_id": 3972,
    "paper_filename": "xiangdong_2023_review_of_the_application_of_multi\u2011modal_deep_learning_in medicine.pdf",
    "paper_title": "Xiangdong 2023 Review Of The Application Of Multi\u2011Modal Deep Learning In Medicine",
    "chunk_index": 3,
    "total_chunks": 76,
    "text_content": "Long short-term memory MoCo Momentum contrast InfoNCE Info noise contrastive estimation HCPs Highly cited papers RCNN Regions with CNNs WSI Whole slide image H&E Hematoxylin and eosin 1 Introduction 1.1 Background The concept of multi-modal systems is related to the study of information representation in the field of human\u2013computer interaction. The term \u201cmode\u201d refers to the representation and exchange of information on a specific physical medium. Due to the development of medical technology and ",
    "full_text_length": 79306,
    "chunk_length": 1326
  },
  {
    "chunk_id": 3973,
    "paper_filename": "xiangdong_2023_review_of_the_application_of_multi\u2011modal_deep_learning_in medicine.pdf",
    "paper_title": "Xiangdong 2023 Review Of The Application Of Multi\u2011Modal Deep Learning In Medicine",
    "chunk_index": 4,
    "total_chunks": 76,
    "text_content": "diagnose the patient's condition; this increases the difficulty of diagnosis. Therefore, fusion processing of multi-modal medical images can be used to comprehensively analyze different medical image information in a fusion image and provide doctors with a more adequate basis for the judgment of clinical diagnosis and treatment [2 ]. Deep learning provides scientific methods for processing large-scale medical images and screening big data, as well as for the diagnosis and efficacy evaluation of ",
    "full_text_length": 79306,
    "chunk_length": 1357
  },
  {
    "chunk_id": 3974,
    "paper_filename": "xiangdong_2023_review_of_the_application_of_multi\u2011modal_deep_learning_in medicine.pdf",
    "paper_title": "Xiangdong 2023 Review Of The Application Of Multi\u2011Modal Deep Learning In Medicine",
    "chunk_index": 5,
    "total_chunks": 76,
    "text_content": "monitoring, and treatment planning for doctors and researchers. Multi-modal fusion combines the informa- tion of multiple modes for target prediction (classification or regression), which was previously understood as multi- source information fusion [3 ]. For example, videos as a type of multimedia can be subdivided into multiple single modes, such as dynamic text, dynamic images, and dynamic voice [4]. Research shows that information processing methods based on the multi-modal concept often per",
    "full_text_length": 79306,
    "chunk_length": 1359
  },
  {
    "chunk_id": 3975,
    "paper_filename": "xiangdong_2023_review_of_the_application_of_multi\u2011modal_deep_learning_in medicine.pdf",
    "paper_title": "Xiangdong 2023 Review Of The Application Of Multi\u2011Modal Deep Learning In Medicine",
    "chunk_index": 6,
    "total_chunks": 76,
    "text_content": "the application of medical image fusion. 1.2 Motivation for this Paper Multi-modal deep learning is the fusion of various types of information via deep-learning techniques. Imaging technol- ogy plays an important role in medical diagnosis. The infor - mation provided by a single-mode medical image is limited since large amounts of information need to be processed in clinical diagnosis. In multi-modal technology, a single mode of the medical image can supplement the weakness of another mode to ac",
    "full_text_length": 79306,
    "chunk_length": 1361
  },
  {
    "chunk_id": 3976,
    "paper_filename": "xiangdong_2023_review_of_the_application_of_multi\u2011modal_deep_learning_in medicine.pdf",
    "paper_title": "Xiangdong 2023 Review Of The Application Of Multi\u2011Modal Deep Learning In Medicine",
    "chunk_index": 7,
    "total_chunks": 76,
    "text_content": "different modes, thereby improving the clinical applicability of medical images in the diagnosis and evaluation of medical problems. Considering the above advantages, the application of multi-modal deep learning in medicine (MMDLM) has rapidly attracted wide attention. This study was conducted to address the following gaps in the existing literature: 1. a lack of effective literature on the multi-modal data fusion mechanisms of medical images to sort and sum- marize research points. 2. the limit",
    "full_text_length": 79306,
    "chunk_length": 1365
  },
  {
    "chunk_id": 3977,
    "paper_filename": "xiangdong_2023_review_of_the_application_of_multi\u2011modal_deep_learning_in medicine.pdf",
    "paper_title": "Xiangdong 2023 Review Of The Application Of Multi\u2011Modal Deep Learning In Medicine",
    "chunk_index": 8,
    "total_chunks": 76,
    "text_content": "Computational Intelligence Systems (2023) 16:44 1 3 Page 3 of 20 44 1. We macroscopically analyze the scope of medical multi- modal applications and mainstream pre-training meth- ods, and discuss the adaptability of each method. 2. We discuss various topics in medical multi-modal fusion methods, ranging from micro-scale methods (such as algorithm-based convolutional neural networks, deep iterations as well as fully supervised, weakly super - vised, and unsupervised learning) to process-based sta",
    "full_text_length": 79306,
    "chunk_length": 1381
  },
  {
    "chunk_id": 3978,
    "paper_filename": "xiangdong_2023_review_of_the_application_of_multi\u2011modal_deep_learning_in medicine.pdf",
    "paper_title": "Xiangdong 2023 Review Of The Application Of Multi\u2011Modal Deep Learning In Medicine",
    "chunk_index": 9,
    "total_chunks": 76,
    "text_content": "medical field. Furthermore, periodical distribu- tions are discussed to effectively help scholars search for related research topics. Co-author, co-occurrence, co-citation, and literature coupling analyses are also per - formed. The scope of the discussion in this review is shown in Fig. 1. 1.3 Structure of this Paper The remainder of the paper is organized as follows: Sect. 1 describes the development of the multi-modal concept and some of the related problems encountered thus far. Section 2 re",
    "full_text_length": 79306,
    "chunk_length": 1341
  },
  {
    "chunk_id": 3979,
    "paper_filename": "xiangdong_2023_review_of_the_application_of_multi\u2011modal_deep_learning_in medicine.pdf",
    "paper_title": "Xiangdong 2023 Review Of The Application Of Multi\u2011Modal Deep Learning In Medicine",
    "chunk_index": 10,
    "total_chunks": 76,
    "text_content": "Literature Review 2.1 Multi\u2011modal Medical Applications A modality is a particular mode wherein something exists, is experienced, or is expressed. When a research problem comprises several modes, it is characterized as a multi- modal research problem. Simultaneously, modes can also be defined in a very broad manner. For example, data regard- ing two different languages, or datasets collected under two different circumstances, can be regarded as two modes [4 ]. To better understand the world aroun",
    "full_text_length": 79306,
    "chunk_length": 1333
  },
  {
    "chunk_id": 3980,
    "paper_filename": "xiangdong_2023_review_of_the_application_of_multi\u2011modal_deep_learning_in medicine.pdf",
    "paper_title": "Xiangdong 2023 Review Of The Application Of Multi\u2011Modal Deep Learning In Medicine",
    "chunk_index": 11,
    "total_chunks": 76,
    "text_content": "(2023) 16:44 1 3 44 Page 4 of 20 At present, multi-modal learning for image, video, audio, and semantics is being deeply investigated. Par - ticularly, research is being conducted on deep neural networks to learn multi-layer representations and for the abstraction of data before it is converted into high- level abstract features of the network. Image analysis has made important research progress in various medical fields such as classification, segmentation, detection, and localization [4 ]. Dee",
    "full_text_length": 79306,
    "chunk_length": 1396
  },
  {
    "chunk_id": 3981,
    "paper_filename": "xiangdong_2023_review_of_the_application_of_multi\u2011modal_deep_learning_in medicine.pdf",
    "paper_title": "Xiangdong 2023 Review Of The Application Of Multi\u2011Modal Deep Learning In Medicine",
    "chunk_index": 12,
    "total_chunks": 76,
    "text_content": "segmentation, classification, and anomaly detection of images generated using a wide range of clinical imaging modes. For example, in 2021, Qian\u2019s team from the University of Southern California proposed a deep-learning system based on multi-modal and multi-angle medical ultrasound images, and success- fully verified the accuracy, robustness, and effectiveness of the system in the prospective clinical environment of several hospitals. The results showed that the interpret- able artificial intell",
    "full_text_length": 79306,
    "chunk_length": 1374
  },
  {
    "chunk_id": 3982,
    "paper_filename": "xiangdong_2023_review_of_the_application_of_multi\u2011modal_deep_learning_in medicine.pdf",
    "paper_title": "Xiangdong 2023 Review Of The Application Of Multi\u2011Modal Deep Learning In Medicine",
    "chunk_index": 13,
    "total_chunks": 76,
    "text_content": "includes structured test data such as hemoglobin and urine routine, as well as unstructured text data such as patient complaints and pathology texts recorded by doctors. 2. Image and waveform data, including imaging data such as ultrasound images, CT images, MRI images, and sig- nal data such as ECG and EEG. 3. Biomics data, which can be subdivided into genomic, transcriptomic, proteomic, and other categories accord- ing to different molecular levels. Each type of patient-related data is a data ",
    "full_text_length": 79306,
    "chunk_length": 1360
  },
  {
    "chunk_id": 3983,
    "paper_filename": "xiangdong_2023_review_of_the_application_of_multi\u2011modal_deep_learning_in medicine.pdf",
    "paper_title": "Xiangdong 2023 Review Of The Application Of Multi\u2011Modal Deep Learning In Medicine",
    "chunk_index": 14,
    "total_chunks": 76,
    "text_content": "of medical images and texts (electronic medical records, laboratory reports, etc.). Multi-modal matching focuses on how to align two modal features, images, and texts. Table 1 shows the main studies, key application areas, and methods of common multi-modal pre-training for comparative analy - sis. To complete the medical multi-modal fusion method and performance comparison research, we conducted a Table 1 Mainstream multi-modal pre-training models Study Model Keywords Applications 1 Zhang et al.",
    "full_text_length": 79306,
    "chunk_length": 1375
  },
  {
    "chunk_id": 3984,
    "paper_filename": "xiangdong_2023_review_of_the_application_of_multi\u2011modal_deep_learning_in medicine.pdf",
    "paper_title": "Xiangdong 2023 Review Of The Application Of Multi\u2011Modal Deep Learning In Medicine",
    "chunk_index": 15,
    "total_chunks": 76,
    "text_content": "Roberta, InfoNCE, MoCo, Faster-RCNNChinese text and text retrieval 7 Hu et al. [41] UniT Multi-modal, multitask, Transformer, DETR, BERT Natural language understanding and multi-modal reasoning 8 Chen et al. [13] UNITER Integration, BERT, Faster R-CNN, multi-modal Image and text matching 9 Li et al. [16] ALIGN Billion image-text pairs of noisy datasets, compara- tive learningAlign images and text pairs for visualization and verbal representation 10 Kim et al. [17] ViLT Image cutting, splicing, f",
    "full_text_length": 79306,
    "chunk_length": 1350
  },
  {
    "chunk_id": 3985,
    "paper_filename": "xiangdong_2023_review_of_the_application_of_multi\u2011modal_deep_learning_in medicine.pdf",
    "paper_title": "Xiangdong 2023 Review Of The Application Of Multi\u2011Modal Deep Learning In Medicine",
    "chunk_index": 16,
    "total_chunks": 76,
    "text_content": "[10], which use a two-tower structure focusing on multi-modal alignment to facilitate text matching, retrieval, and other downstream tasks. The heavy fusion approach is based on pre-trained Transformers [11], as represented by OSCAR [12], UNITER [13], VINVL [14], etc. These methods can be regarded as a single-tower structure that focuses on incorporating multi-modal infor - mation with an attention mechanism to perform additional tasks. Heavy fusion can interpret VQA [15], captions, and other do",
    "full_text_length": 79306,
    "chunk_length": 1329
  },
  {
    "chunk_id": 3986,
    "paper_filename": "xiangdong_2023_review_of_the_application_of_multi\u2011modal_deep_learning_in medicine.pdf",
    "paper_title": "Xiangdong 2023 Review Of The Application Of Multi\u2011Modal Deep Learning In Medicine",
    "chunk_index": 17,
    "total_chunks": 76,
    "text_content": "information of multiple modes for classification or regression tasks. The benefits of multi-modal fusion are as follows: (1) more robust inference results can be generated for different modal representations of the same phenomenon, (2) auxiliary information that is not visible in a single-mode can be retrieved from multiple scales, and (3) for a multi-modal system, modal fusion can operate normally, even when a certain mode disappears. The modal and optimization methods used by the neural networ",
    "full_text_length": 79306,
    "chunk_length": 1342
  },
  {
    "chunk_id": 3987,
    "paper_filename": "xiangdong_2023_review_of_the_application_of_multi\u2011modal_deep_learning_in medicine.pdf",
    "paper_title": "Xiangdong 2023 Review Of The Application Of Multi\u2011Modal Deep Learning In Medicine",
    "chunk_index": 18,
    "total_chunks": 76,
    "text_content": "(2) perform end-to-end learning of multi-modal feature representation and fusion, and (3) performs better than non-deep-learning methods and can learn a complex decision boundary [4 ]. Table 1 shows that the multi-modal pre-trained model and its variants update and iterate very quickly, covering keywords including contrast learning, text and text matching, feature space align- ment, understanding and generation, Chinese image genera- tion, and transfer learning. These key technologies can be wid",
    "full_text_length": 79306,
    "chunk_length": 1409
  },
  {
    "chunk_id": 3988,
    "paper_filename": "xiangdong_2023_review_of_the_application_of_multi\u2011modal_deep_learning_in medicine.pdf",
    "paper_title": "Xiangdong 2023 Review Of The Application Of Multi\u2011Modal Deep Learning In Medicine",
    "chunk_index": 19,
    "total_chunks": 76,
    "text_content": "out using a large amount of unlabeled data, and then fine-tuning can be done with a small amount of labeled data. For example, as the simplest single-mode model, ViLT [17] is fast in visual and text processing, which provides a good foundation for embedded devices such as clinical consultations and surgeries. The approach used for multi-modal fusion depends on the task and data, and exist - ing work often proposes various fusion methods without any real unified theoretical support. To efficientl",
    "full_text_length": 79306,
    "chunk_length": 1332
  },
  {
    "chunk_id": 3989,
    "paper_filename": "xiangdong_2023_review_of_the_application_of_multi\u2011modal_deep_learning_in medicine.pdf",
    "paper_title": "Xiangdong 2023 Review Of The Application Of Multi\u2011Modal Deep Learning In Medicine",
    "chunk_index": 20,
    "total_chunks": 76,
    "text_content": "excellent clinical results. They pro- posed a deep-learning fusion model based on Bidirectional Long Short-term Memory (BiLSTM) networks and Convo- lutional Neural Networks (CNNs). The multi-modal multi- task model, based on five modalities [i.e., Magnetic Reso- nance Imaging (MRI), PET Positron Emission Computed Tomography, neuropsychological data, cognitive score data, and evaluation data], jointly predicts variables such as Alz- heimer\u2019s disease (AD) multistage progression tasks and four key ",
    "full_text_length": 79306,
    "chunk_length": 1405
  },
  {
    "chunk_id": 3990,
    "paper_filename": "xiangdong_2023_review_of_the_application_of_multi\u2011modal_deep_learning_in medicine.pdf",
    "paper_title": "Xiangdong 2023 Review Of The Application Of Multi\u2011Modal Deep Learning In Medicine",
    "chunk_index": 21,
    "total_chunks": 76,
    "text_content": "2.4 Deep Multi\u2011modal Fusion Methods and Performance Multi-modal fusion is a key research point in multi-modal research. It integrates information extracted from differ - ent modes into a stable multi-modal representation. Multi- modal fusion is related to representations, and a process that focuses on using some architecture to merge representations of different single modes is classified as fusion. Fusion methods can be divided into late and early fusion according International Journal of Compu",
    "full_text_length": 79306,
    "chunk_length": 1318
  },
  {
    "chunk_id": 3991,
    "paper_filename": "xiangdong_2023_review_of_the_application_of_multi\u2011modal_deep_learning_in medicine.pdf",
    "paper_title": "Xiangdong 2023 Review Of The Application Of Multi\u2011Modal Deep Learning In Medicine",
    "chunk_index": 22,
    "total_chunks": 76,
    "text_content": "Fusion is based on integrating feature vectors from dif- ferent modes in simple ways, such as vector splicing, vector weighted sum, and so on. For multi-modal tasks, the first approach that comes to mind should be based on simple operations [19\u2013 21, 26\u201330], and the fusion of this approach has achieved the desired results in medical multi-modal applications. Holste et al. studied 10,185 breast enhancement MRI (DCE-MRI) data from 5248 women. They extracted clinical indications and breast density i",
    "full_text_length": 79306,
    "chunk_length": 1335
  },
  {
    "chunk_id": 3992,
    "paper_filename": "xiangdong_2023_review_of_the_application_of_multi\u2011modal_deep_learning_in medicine.pdf",
    "paper_title": "Xiangdong 2023 Review Of The Application Of Multi\u2011Modal Deep Learning In Medicine",
    "chunk_index": 23,
    "total_chunks": 76,
    "text_content": "clinical features3,891 individuals Predicting the risk of diabetic cardiovascular and cerebrovascular disease 3 Holste et al. [20] MRI, clinical features 17,046 samples of 5,248 women Classification of breast cancer 4 El-Sappagh et al. [19] MRI, PET, neuropsychology data, cognitive scores, assessment data1,536 patients from ADNI (Public) Classification of Alzheimer\u2019s disease 5 Yan et al. [21] Pathological images, clinical features3,764 samples of 153 patients (Public)Classification of breast can",
    "full_text_length": 79306,
    "chunk_length": 1394
  },
  {
    "chunk_id": 3993,
    "paper_filename": "xiangdong_2023_review_of_the_application_of_multi\u2011modal_deep_learning_in medicine.pdf",
    "paper_title": "Xiangdong 2023 Review Of The Application Of Multi\u2011Modal Deep Learning In Medicine",
    "chunk_index": 24,
    "total_chunks": 76,
    "text_content": "Yao et al. [22] Pathological images, genomic data 106 patients from TCGA-LUSC, and 126 cases from the open- source dataset TCGA-GBMSurvival prediction of patients with lung cancer and brain cancer 11 Cheerla et al. [23] Pathological images, gene expres- sion, microRNA expression, clinical features11,160 patients from the open- source dataset TCGA Survival prediction of patients with 20 types of cancer 12 Li et al. [24] Pathological images, genomic data 826 cases from the open-source dataset TCGA",
    "full_text_length": 79306,
    "chunk_length": 1327
  },
  {
    "chunk_id": 3994,
    "paper_filename": "xiangdong_2023_review_of_the_application_of_multi\u2011modal_deep_learning_in medicine.pdf",
    "paper_title": "Xiangdong 2023 Review Of The Application Of Multi\u2011Modal Deep Learning In Medicine",
    "chunk_index": 25,
    "total_chunks": 76,
    "text_content": "carcinoma 16 Cui et al. [8] CT, clinical features 397 patients Evaluation and prediction of metasta- sis of lymphocytic carcinoma 17 Li et al. [45] MRI, genomic data demography features112 patients Efficacy prediction of chemotherapy in breast cancer patients 18 Guan et al. [31] CT, clinical features 553 patients Classification of esophageal fistula risk 19 Wang et al. [7] Pathological images, genomic data 345 patients from TCGA (Public) Survival prediction of patients with breast cancer 20 Zhou",
    "full_text_length": 79306,
    "chunk_length": 1294
  },
  {
    "chunk_id": 3995,
    "paper_filename": "xiangdong_2023_review_of_the_application_of_multi\u2011modal_deep_learning_in medicine.pdf",
    "paper_title": "Xiangdong 2023 Review Of The Application Of Multi\u2011Modal Deep Learning In Medicine",
    "chunk_index": 26,
    "total_chunks": 76,
    "text_content": "coupling relationship is insufficient. The attention-based fusion approach, which is based on an attention mechanism, has been widely used in multi- modal medical applications. To enable the model to pay attention to text, such as text in medical images and medical records, the mechanism gives different parts of the image feature vector different weights according to the character - istics of the image and text features. This enables the model to extract effective features from multi-source data",
    "full_text_length": 79306,
    "chunk_length": 1298
  },
  {
    "chunk_id": 3996,
    "paper_filename": "xiangdong_2023_review_of_the_application_of_multi\u2011modal_deep_learning_in medicine.pdf",
    "paper_title": "Xiangdong 2023 Review Of The Application Of Multi\u2011Modal Deep Learning In Medicine",
    "chunk_index": 27,
    "total_chunks": 76,
    "text_content": "method-based tensors, also known as the bilinear pooling fusion method, are predominantly used to fuse visual fea - ture vectors and text feature vectors to attain a joint rep- resentation space [33]. Through the stepwise decomposi- tion of the weight tensor, an efficient multi-modal fusion model can be achieved. In recent years, the most important multi-modal fusion methods have been attention-based and bilinear pooling methods using attention-based fusion and tensor-based fusion [13, 34]. Clin",
    "full_text_length": 79306,
    "chunk_length": 1382
  },
  {
    "chunk_id": 3997,
    "paper_filename": "xiangdong_2023_review_of_the_application_of_multi\u2011modal_deep_learning_in medicine.pdf",
    "paper_title": "Xiangdong 2023 Review Of The Application Of Multi\u2011Modal Deep Learning In Medicine",
    "chunk_index": 28,
    "total_chunks": 76,
    "text_content": "an integrated multi-modal risk model, at the same time introducing a multi-modal organization loss, through a complementary embedded model to improve performance. When the DOF model predicted the overall survival of glioma patients, the median C-index was 0.788 \u00b1 0.067, which is significantly better than the best-performing single-peak model (median C-index: 0.718 \u00b1 0.064; P = 0.023) [34]. Faisal Mahmood's team used multi-modal deep learning to integrate and ana- lyze whole-section images and ge",
    "full_text_length": 79306,
    "chunk_length": 1356
  },
  {
    "chunk_id": 3998,
    "paper_filename": "xiangdong_2023_review_of_the_application_of_multi\u2011modal_deep_learning_in medicine.pdf",
    "paper_title": "Xiangdong 2023 Review Of The Application Of Multi\u2011Modal Deep Learning In Medicine",
    "chunk_index": 29,
    "total_chunks": 76,
    "text_content": "1 3 44 Page 8 of 20 been established for further exploration, as well as for bio- marker discovery and characterization [35]. Figure 3 dis- plays a performance comparison of several researchers who conducted multi-modal deep learning using three different fusion modes. Through an overall comparative analysis, the three-stage deep feature learning and fusion diagnosis framework pro- posed by Zhou et al. [ 46] is considered a good approach among the sample of studies we collected. The framework is",
    "full_text_length": 79306,
    "chunk_length": 1316
  },
  {
    "chunk_id": 3999,
    "paper_filename": "xiangdong_2023_review_of_the_application_of_multi\u2011modal_deep_learning_in medicine.pdf",
    "paper_title": "Xiangdong 2023 Review Of The Application Of Multi\u2011Modal Deep Learning In Medicine",
    "chunk_index": 30,
    "total_chunks": 76,
    "text_content": "that a balanced approach to data and fusion process is crucial for the success of multi-modal deep learning in medicine. Therefore, we recommend that future studies focus on gradually integrating or extracting the fea- tures of different modalities in a specific order during the fusion process, while also prioritizing data quality and pro- gressively making it more complete. In multi-modal deep learning, the process of collecting effective features from different modes is called \u201cmulti- modal fu",
    "full_text_length": 79306,
    "chunk_length": 1325
  },
  {
    "chunk_id": 4000,
    "paper_filename": "xiangdong_2023_review_of_the_application_of_multi\u2011modal_deep_learning_in medicine.pdf",
    "paper_title": "Xiangdong 2023 Review Of The Application Of Multi\u2011Modal Deep Learning In Medicine",
    "chunk_index": 31,
    "total_chunks": 76,
    "text_content": "modes are combined and co-learned during training. It allows for modal-specific preprocessing while capturing interactions between data modes to achieve joint representations. Late fusion is also a simple method wherein a separate model is trained for each mode and com- bined with the output probability for joint representation. However, such a fusion method misses the opportunity to extract information from the interaction between the modes. Over the past few years, deep learning has transition",
    "full_text_length": 79306,
    "chunk_length": 1439
  },
  {
    "chunk_id": 4001,
    "paper_filename": "xiangdong_2023_review_of_the_application_of_multi\u2011modal_deep_learning_in medicine.pdf",
    "paper_title": "Xiangdong 2023 Review Of The Application Of Multi\u2011Modal Deep Learning In Medicine",
    "chunk_index": 32,
    "total_chunks": 76,
    "text_content": "other relevant rules, it is highly difficult to obtain medical data. One possible solution is to use the available data from one mode to aid learning using 0.860.740.8360.7460.8710.7180.7630.74240.81960.83120.8080.69120.750.90450.5989 0.9030.8080.92620.8790.7740.8880.7370.7460.7680.77910.91190.88440.8260.75710.7840.96360.6287 00 .1 0.20 .3 0.40 .5 0.60 .7 0.80 .9 1Holste et al. 2021 [20]Lu et al. 2021[26]El-Sappagh et al. 2020 [19]Yan et al. 2021[21]Mobadersany et al. 2018[27]Yap et al. 2018[28]",
    "full_text_length": 79306,
    "chunk_length": 1458
  },
  {
    "chunk_id": 4002,
    "paper_filename": "xiangdong_2023_review_of_the_application_of_multi\u2011modal_deep_learning_in medicine.pdf",
    "paper_title": "Xiangdong 2023 Review Of The Application Of Multi\u2011Modal Deep Learning In Medicine",
    "chunk_index": 33,
    "total_chunks": 76,
    "text_content": "some studies have suggested that a Trans- former that is pre-trained on untagged language data may generalize well to other tasks. In medicine, a model architec- ture called \u201cCycleGans\u201d, which was trained using unpaired uncontrasted or contrast computed tomography (CT) scan images, is used to generate uncontrasted or contrast CT scan images [36]. 3 Discussion In the previous section, we reviewed the current models used for researching disease prognosis and diagnosis using deep learning-based app",
    "full_text_length": 79306,
    "chunk_length": 1306
  },
  {
    "chunk_id": 4003,
    "paper_filename": "xiangdong_2023_review_of_the_application_of_multi\u2011modal_deep_learning_in medicine.pdf",
    "paper_title": "Xiangdong 2023 Review Of The Application Of Multi\u2011Modal Deep Learning In Medicine",
    "chunk_index": 34,
    "total_chunks": 76,
    "text_content": "of improved Transformer models, one of which is UniT [41], which can simultane- ously handle two modes of data and seven tasks, such as natural language processing, natural language understand- ing, image recognition, and object detection. While most of these multi-tasking, multi-modal AI systems are in the research and experimental stage, some have already achieved good results in practical applications. For instance, elec- tronic health records (EHR) have a complex multi-modal structure. Xu et",
    "full_text_length": 79306,
    "chunk_length": 1408
  },
  {
    "chunk_id": 4004,
    "paper_filename": "xiangdong_2023_review_of_the_application_of_multi\u2011modal_deep_learning_in medicine.pdf",
    "paper_title": "Xiangdong 2023 Review Of The Application Of Multi\u2011Modal Deep Learning In Medicine",
    "chunk_index": 35,
    "total_chunks": 76,
    "text_content": "risk of overfitting. The attention mechanism-based method is a quite effective method for multi-modal feature fusion and can calculate the inter-modal and intra-modal importance features; hence, it is widely used in multi- modal fusion applications. Furthermore, with the help of self-attention, the steps of modal fusion do not need to be designed carefully. One can simply splice the multi-modal information into a sequence and use a Transformer encoder to learn their binary relations and merge th",
    "full_text_length": 79306,
    "chunk_length": 1306
  },
  {
    "chunk_id": 4005,
    "paper_filename": "xiangdong_2023_review_of_the_application_of_multi\u2011modal_deep_learning_in medicine.pdf",
    "paper_title": "Xiangdong 2023 Review Of The Application Of Multi\u2011Modal Deep Learning In Medicine",
    "chunk_index": 36,
    "total_chunks": 76,
    "text_content": "ConVIRT [37], which can automatically annotate X-rays with text. The self-attention mechanism does not require the user to explicitly designate the prior adjacency matrix, but simply input sufficient data (if available) and let the model learn the edge weights on its own. Compared with CNNs and RNNs, the Transformer has a larger number of parameters, stronger expression ability, and requires more training data. Accordingly, to effectively assist physicians in diagnosis and therapy, scientists sh",
    "full_text_length": 79306,
    "chunk_length": 1386
  },
  {
    "chunk_id": 4006,
    "paper_filename": "xiangdong_2023_review_of_the_application_of_multi\u2011modal_deep_learning_in medicine.pdf",
    "paper_title": "Xiangdong 2023 Review Of The Application Of Multi\u2011Modal Deep Learning In Medicine",
    "chunk_index": 37,
    "total_chunks": 76,
    "text_content": "this field. We list the main challenges below. 1. The diversity and uncertainty in medical datasets, including image and non-image data, sample size, depth of phenotypic analysis, heterogeneity and diversity of participants, degree of data standardization and harmo- nization, and degree of correlation among data sources together constitute a greater challenge than that posed by a single-mode deep-learning model. The challenge of handling the highly variable data found in real-world clinical data",
    "full_text_length": 79306,
    "chunk_length": 1365
  },
  {
    "chunk_id": 4007,
    "paper_filename": "xiangdong_2023_review_of_the_application_of_multi\u2011modal_deep_learning_in medicine.pdf",
    "paper_title": "Xiangdong 2023 Review Of The Application Of Multi\u2011Modal Deep Learning In Medicine",
    "chunk_index": 38,
    "total_chunks": 76,
    "text_content": "in this field, pose significant chal- lenges. 4. For some modes (e.g., 3D imaging and genomics), pro- cessing even a single point in time or individual instance of data requires a large amount of computing power. Hence, building models to simultaneously and rapidly process large-scale tumor pathological slides, genomics, or medical text data is an important fundamental chal- lenge. International Journal of Computational Intelligence Systems (2023) 16:44 1 3 44 Page 10 of 20 5. When collecting he",
    "full_text_length": 79306,
    "chunk_length": 1369
  },
  {
    "chunk_id": 4008,
    "paper_filename": "xiangdong_2023_review_of_the_application_of_multi\u2011modal_deep_learning_in medicine.pdf",
    "paper_title": "Xiangdong 2023 Review Of The Application Of Multi\u2011Modal Deep Learning In Medicine",
    "chunk_index": 39,
    "total_chunks": 76,
    "text_content": "This interaction is hindered by the significant challenges of collaboration. To meaningfully process and integrate the information in different medical data and increase the participation of AI in the assisted diagnosis and treatment process, joint efforts between the medical community and AI research- ers will be required to construct and validate new models and ultimately demonstrate their ability to improve diag - nosis and treatment. 4 Bibliometrics Bibliometrics is an effective quantitative",
    "full_text_length": 79306,
    "chunk_length": 1381
  },
  {
    "chunk_id": 4009,
    "paper_filename": "xiangdong_2023_review_of_the_application_of_multi\u2011modal_deep_learning_in medicine.pdf",
    "paper_title": "Xiangdong 2023 Review Of The Application Of Multi\u2011Modal Deep Learning In Medicine",
    "chunk_index": 40,
    "total_chunks": 76,
    "text_content": "of the literature related to multi-modal fusion and deep learning based on medical images. For the query, we used TS = (Multi-modal deep learning (OR (Multi-modal deep learning)) AND medi - cine). A total of 920 records were retrieved on this subject. Simultaneously, we used the refining function of the WoS to extract the key information of the remaining publica- tions, eliminate irrelevant or weakly related publications, and select a final total of 879 publications as the basic target data of t",
    "full_text_length": 79306,
    "chunk_length": 1333
  },
  {
    "chunk_id": 4010,
    "paper_filename": "xiangdong_2023_review_of_the_application_of_multi\u2011modal_deep_learning_in medicine.pdf",
    "paper_title": "Xiangdong 2023 Review Of The Application Of Multi\u2011Modal Deep Learning In Medicine",
    "chunk_index": 41,
    "total_chunks": 76,
    "text_content": "in this study, it would have been challenging to manually extract the information individually and further explore the relationship between them. Therefore, it was necessary to sort them using a bibliometric analysis, which aims to study the distribution structure, quantitative relationships, and variation in the literature using measurement methods such as mathematics and statistics [50]. Common bibliomet- ric mapping software tools include HistCite [51], CiteSpace [52], and VOSviewer [ 53, 54]",
    "full_text_length": 79306,
    "chunk_length": 1368
  },
  {
    "chunk_id": 4011,
    "paper_filename": "xiangdong_2023_review_of_the_application_of_multi\u2011modal_deep_learning_in medicine.pdf",
    "paper_title": "Xiangdong 2023 Review Of The Application Of Multi\u2011Modal Deep Learning In Medicine",
    "chunk_index": 42,
    "total_chunks": 76,
    "text_content": "collaborative publications and networks of multi-modal fusion in medical image research, as well as the collaboration and distribution among countries, research institutions and authors on the subject. We set the node types to \u201ccountry\u201d, \u201cagency\u201d, and \u201cauthor\u201d in VOSviewer soft- ware. At the same time, considering the close relationship between countries and institutions, we added nodes for coun- tries and institutions in the same graph. By setting the node type to \u201ccategory\u201d and selecting the t",
    "full_text_length": 79306,
    "chunk_length": 1271
  },
  {
    "chunk_id": 4012,
    "paper_filename": "xiangdong_2023_review_of_the_application_of_multi\u2011modal_deep_learning_in medicine.pdf",
    "paper_title": "Xiangdong 2023 Review Of The Application Of Multi\u2011Modal Deep Learning In Medicine",
    "chunk_index": 43,
    "total_chunks": 76,
    "text_content": "in the different develop- ment stages of this research field. 4.3 Literature Analysis 4.3.1 Trends in the Number of Published Papers We investigated 879 papers that were published between 2010 and 2022 (Fig. 4). The growth in the number of publi- cations can be divided into three stages, which we call prepa- ration, rise, and prosperity. International Journal of Computational Intelligence Systems (2023) 16:44 1 3 Page 11 of 20 44 4.3.2 Annual Trends and Possible Explanations By analyzing the col",
    "full_text_length": 79306,
    "chunk_length": 1325
  },
  {
    "chunk_id": 4013,
    "paper_filename": "xiangdong_2023_review_of_the_application_of_multi\u2011modal_deep_learning_in medicine.pdf",
    "paper_title": "Xiangdong 2023 Review Of The Application Of Multi\u2011Modal Deep Learning In Medicine",
    "chunk_index": 44,
    "total_chunks": 76,
    "text_content": "expand. It saw particularly strong growth in 2017. There are two possible reasons for this rise: 1. Many researchers have begun to use deep learning to solve medical problems. 2. The pioneering research achievements globally have piqued the interest and increased the confidence of researchers in this approach.4.3.3 Countries The analysis of literature data shows that scholars from 35 countries/regions have published publications about MMDLM, but over 85% of the publications were contributed by s",
    "full_text_length": 79306,
    "chunk_length": 1360
  },
  {
    "chunk_id": 4014,
    "paper_filename": "xiangdong_2023_review_of_the_application_of_multi\u2011modal_deep_learning_in medicine.pdf",
    "paper_title": "Xiangdong 2023 Review Of The Application Of Multi\u2011Modal Deep Learning In Medicine",
    "chunk_index": 45,
    "total_chunks": 76,
    "text_content": "publication contributors at the national level, we further analyzed the outstanding publication contributors at the institutional level. Accord- ing to the collected data, the League of European Research Universities (LERU) is a prominent institution that has con- tributed 79 publications, accounting for 8.957% of the global publication volume on this topic. As an important contribut- ing country, China ranked second and third in terms of the intellectual output of the University of Chinese Acad",
    "full_text_length": 79306,
    "chunk_length": 1330
  },
  {
    "chunk_id": 4015,
    "paper_filename": "xiangdong_2023_review_of_the_application_of_multi\u2011modal_deep_learning_in medicine.pdf",
    "paper_title": "Xiangdong 2023 Review Of The Application Of Multi\u2011Modal Deep Learning In Medicine",
    "chunk_index": 46,
    "total_chunks": 76,
    "text_content": "MMDLM research, we selected 12 highly cited papers (HCPs) and 1 \u201chot\u201d paper from the WoS and ranked them according to their total citation frequency. Table 4 lists authors, journal names, regions, titles, and cita- tions for these HCPs. The MMDLM-related HCP proposed by Arbabsiar [55], which has been cited 381 times, can be regarded as a pioneering work in the single-discipline pre- diction of brain dysfunction based on neuroimaging. Emerg - ing trends such as multi-modal brain imaging, survival",
    "full_text_length": 79306,
    "chunk_length": 1334
  },
  {
    "chunk_id": 4016,
    "paper_filename": "xiangdong_2023_review_of_the_application_of_multi\u2011modal_deep_learning_in medicine.pdf",
    "paper_title": "Xiangdong 2023 Review Of The Application Of Multi\u2011Modal Deep Learning In Medicine",
    "chunk_index": 47,
    "total_chunks": 76,
    "text_content": "MMDLM publications from 2010 to 2022 Fig. 5 Author distribution by country International Journal of Computational Intelligence Systems (2023) 16:44 1 3 44 Page 12 of 20 deep-learning methods for solving medical processes, such as different fusion models for physical medical image seg- mentation [6 ], brain tumor segmentation [24], multi-organ detection [56], anatomical education [57], multiple diag- noses of Alzheimer's disease [58], the prognosis of rectal cancer [59], breast mass detection [60",
    "full_text_length": 79306,
    "chunk_length": 1303
  },
  {
    "chunk_id": 4017,
    "paper_filename": "xiangdong_2023_review_of_the_application_of_multi\u2011modal_deep_learning_in medicine.pdf",
    "paper_title": "Xiangdong 2023 Review Of The Application Of Multi\u2011Modal Deep Learning In Medicine",
    "chunk_index": 48,
    "total_chunks": 76,
    "text_content": "Technical University of Munich Germany 24 2.721 7 University of California System USA 23 2.608 8 North Carolina State University USA 23 2.608 9 University of Texas System USA 23 2.608 10 Zhejiang University China 23 2.608 11 Harvard Medical School USA 22 2.494 12 Fudan University China 21 2.381 13 The University of North Carolina at Chapel Hill USA 20 2.268 14 Imperial College London UK 19 2.154 15 Universit\u00e9 F\u00e9d\u00e9rale Toulouse Midi-Pyr\u00e9n\u00e9es France 19 2.154 Table 4 Highly cited papers Highly cite",
    "full_text_length": 79306,
    "chunk_length": 1247
  },
  {
    "chunk_id": 4018,
    "paper_filename": "xiangdong_2023_review_of_the_application_of_multi\u2011modal_deep_learning_in medicine.pdf",
    "paper_title": "Xiangdong 2023 Review Of The Application Of Multi\u2011Modal Deep Learning In Medicine",
    "chunk_index": 49,
    "total_chunks": 76,
    "text_content": "381 2 Ibtehaz et al. [6] Bangladesh MultiResUNet: Rethinking the U-Net architecture for multi- modal biomedical image segmentationNeural Networks 335a 3 Araujo et al. [24] Portugal Classification of breast cancer histology images using convo- lutional neural networksPLOS ONE 320 4 Shin et al. [56] UK Stacked autoencoders for unsupervised feature learning and multiple organ detection in a pilot study using 4D patient dataIEEE T. Pattern. Anal 327 5 Zhao et al. [57] China A deep learning model int",
    "full_text_length": 79306,
    "chunk_length": 1322
  },
  {
    "chunk_id": 4019,
    "paper_filename": "xiangdong_2023_review_of_the_application_of_multi\u2011modal_deep_learning_in medicine.pdf",
    "paper_title": "Xiangdong 2023 Review Of The Application Of Multi\u2011Modal Deep Learning In Medicine",
    "chunk_index": 50,
    "total_chunks": 76,
    "text_content": "[63] Finland Deep learning-based tissue analysis predicts outcome in colorectal cancerSci. Rep 235 10 Arevalo et al. [61] Colombia Representation learning for mammography mass lesion clas- sification with convolutional neural networksComput. Meth. Prog. Bio 214 11 Liu et al. [62] China The applications of radiomics in precision diagnosis and treatment of oncology: Opportunities and challengesTheranostics 219 12 Zeng et al. [64] USA DeepDR: A network-based deep-learning approach to in silico drug",
    "full_text_length": 79306,
    "chunk_length": 1442
  },
  {
    "chunk_id": 4020,
    "paper_filename": "xiangdong_2023_review_of_the_application_of_multi\u2011modal_deep_learning_in medicine.pdf",
    "paper_title": "Xiangdong 2023 Review Of The Application Of Multi\u2011Modal Deep Learning In Medicine",
    "chunk_index": 51,
    "total_chunks": 76,
    "text_content": "Science Photographic Technology\u201d, and \u201cArtificial Intelligence\u201d are the biggest categories, contain- ing approximately 50% of the related documents. Figure 6 shows the primary WoS categories that belong to MMDLM- related documents. In addition to the \u201cComputer Science\u201d and \u201cMedical\u201d categories, MMDLM-related documents were also seen in the \u201cNeurosciences\u201d, \u201cMathematical Com- putational Biology\u201d, \u201cMultidisciplinary Sciences\u201d, \u201cOptics\u201d, \u201cTelecommunications\u201d, \u201cClinical Neurology\u201d, \u201cNeuroimag- ing\u201d,",
    "full_text_length": 79306,
    "chunk_length": 1449
  },
  {
    "chunk_id": 4021,
    "paper_filename": "xiangdong_2023_review_of_the_application_of_multi\u2011modal_deep_learning_in medicine.pdf",
    "paper_title": "Xiangdong 2023 Review Of The Application Of Multi\u2011Modal Deep Learning In Medicine",
    "chunk_index": 52,
    "total_chunks": 76,
    "text_content": "the visual platform keyword representing density has a special color, which is closely related to the link den- sity on the node, and the color of the node depends on the degree of closeness of the node neighbor relationship. The red component in the keywords indicates its frequency is high. In contrast, keywords that appear less frequently have an amber color. Density visualization is very effective for understanding the overall structure and focusing on the most important components. Table 5 s",
    "full_text_length": 79306,
    "chunk_length": 1625
  },
  {
    "chunk_id": 4022,
    "paper_filename": "xiangdong_2023_review_of_the_application_of_multi\u2011modal_deep_learning_in medicine.pdf",
    "paper_title": "Xiangdong 2023 Review Of The Application Of Multi\u2011Modal Deep Learning In Medicine",
    "chunk_index": 53,
    "total_chunks": 76,
    "text_content": "IntelligenceImaging Science Photographic TechnologyEngineering BiomedicalRadiology Nuclear Medicine Medical Imagin g Publica/g415on sS hare %Table 5 Top-10 keywords of MMDLM-related publications Rank Keyword Total link strengthOccurrences 1 Deep learning 1395 451 2 Classification 518 119 3 Segmentation 312 86 4 MRI 275 63 5 Diagnosis 210 51 6 Prediction 206 39 7 Magnetic resonance imaging 197 34 8 Feature extraction 180 32 9 Model 145 34 10 Images 141 33 11 Cancer 129 32 12 Alzheimer\u2019s disease 1",
    "full_text_length": 79306,
    "chunk_length": 1269
  },
  {
    "chunk_id": 4023,
    "paper_filename": "xiangdong_2023_review_of_the_application_of_multi\u2011modal_deep_learning_in medicine.pdf",
    "paper_title": "Xiangdong 2023 Review Of The Application Of Multi\u2011Modal Deep Learning In Medicine",
    "chunk_index": 54,
    "total_chunks": 76,
    "text_content": "is a traditional deep-learning application. 3. Tumor and brain science are important areas of concern for researchers. The node and phrase font sizes in Fig. 7 represent weights. As can be seen from the network diagram, the larger the node (keyword) font, the larger the correspond- ing weight is. At the same time, the Euclidean distance of two nodes indicates the strength of the coupling. A direct link between two keywords indicates that they occur simul- taneously. The more densely connected th",
    "full_text_length": 79306,
    "chunk_length": 1304
  },
  {
    "chunk_id": 4024,
    "paper_filename": "xiangdong_2023_review_of_the_application_of_multi\u2011modal_deep_learning_in medicine.pdf",
    "paper_title": "Xiangdong 2023 Review Of The Application Of Multi\u2011Modal Deep Learning In Medicine",
    "chunk_index": 55,
    "total_chunks": 76,
    "text_content": "the link strength between two nodes represents the co-occurrence frequency strength, which shows the quantitative parameters of the coupling relationship between two nodes. Calculating the total link strength of a node is the sum of the link strength of that node and the link strength of all other associated nodes. By observing this intensity, it is possible to intuitively calculate the degree of closeness of correlated studies, which may lead researchers to pay atten- tion to the subcategories ",
    "full_text_length": 79306,
    "chunk_length": 1319
  },
  {
    "chunk_id": 4025,
    "paper_filename": "xiangdong_2023_review_of_the_application_of_multi\u2011modal_deep_learning_in medicine.pdf",
    "paper_title": "Xiangdong 2023 Review Of The Application Of Multi\u2011Modal Deep Learning In Medicine",
    "chunk_index": 56,
    "total_chunks": 76,
    "text_content": "research area can be intuitively calculated. At the same time, co-citation analysis is widely used to dis- close the coupling of authors, literature, and journals in the research field. In this section, we introduce the co-citation of authors, literature, and journals in the medical applications of multi-modal deep learning. Figure 8 shows the journal collaboration network for MMDLM-related publications. By analyzing and summarizing the co-citation relationships between authors and publications ",
    "full_text_length": 79306,
    "chunk_length": 1308
  },
  {
    "chunk_id": 4026,
    "paper_filename": "xiangdong_2023_review_of_the_application_of_multi\u2011modal_deep_learning_in medicine.pdf",
    "paper_title": "Xiangdong 2023 Review Of The Application Of Multi\u2011Modal Deep Learning In Medicine",
    "chunk_index": 57,
    "total_chunks": 76,
    "text_content": "3 Page 15 of 20 44 4.3.9 Bibliographic Coupling Analysis In citation network, the bibliographic coupling of two arti- cles refers to the number of the same articles in the refer - ences of both articles, that is, the number of other articles cited by both articles at the same time. We can define the undirected document coupling network corresponding to the directed citation network as follows: if two articles have at least one identical reference, there will be an edge between the corresponding ",
    "full_text_length": 79306,
    "chunk_length": 1287
  },
  {
    "chunk_id": 4027,
    "paper_filename": "xiangdong_2023_review_of_the_application_of_multi\u2011modal_deep_learning_in medicine.pdf",
    "paper_title": "Xiangdong 2023 Review Of The Application Of Multi\u2011Modal Deep Learning In Medicine",
    "chunk_index": 58,
    "total_chunks": 76,
    "text_content": "showing the publication coupling network relationships of 879 articles by author (see Fig. 10), journal (see Fig. 11), institute (see Fig. 12), and country (see Fig. 13).5 Challenges and Perspectives The successful advancement of multi-modal deep learning in medicine requires a large amount of data, and chal - lenges are encountered when applying data, models, and performing complex tasks in this field. We list the main challenges below. 1. Lack of standardized data collection and annotation pro",
    "full_text_length": 79306,
    "chunk_length": 1371
  },
  {
    "chunk_id": 4028,
    "paper_filename": "xiangdong_2023_review_of_the_application_of_multi\u2011modal_deep_learning_in medicine.pdf",
    "paper_title": "Xiangdong 2023 Review Of The Application Of Multi\u2011Modal Deep Learning In Medicine",
    "chunk_index": 59,
    "total_chunks": 76,
    "text_content": "publications Fig. 9 Co-authorship network of MMDLM-related publica- tions International Journal of Computational Intelligence Systems (2023) 16:44 1 3 44 Page 16 of 20 and data sharing agreements to collect and integrate data from multiple sources. 5. Model overfitting, which can lead to poor generalization performance on new data and be particularly problem- atic in medical applications. 6. Ethical implications of using deep-learning models in medical decision-making, such as potential biases a",
    "full_text_length": 79306,
    "chunk_length": 1396
  },
  {
    "chunk_id": 4029,
    "paper_filename": "xiangdong_2023_review_of_the_application_of_multi\u2011modal_deep_learning_in medicine.pdf",
    "paper_title": "Xiangdong 2023 Review Of The Application Of Multi\u2011Modal Deep Learning In Medicine",
    "chunk_index": 60,
    "total_chunks": 76,
    "text_content": "International Journal of Computational Intelligence Systems (2023) 16:44 1 3 Page 17 of 20 44 as cancer and Alzheimer's disease, and help clinicians predict disease risk and personalize treatment plans. 3. Multi-modal deep learning can provide more accurate and timely diagnoses and treatment recommendations, aiding clinical decision-making. 4. Approaches being explored include developing more standardized protocols for data collection and annota- tion, developing more interpretable and transpare",
    "full_text_length": 79306,
    "chunk_length": 1462
  },
  {
    "chunk_id": 4030,
    "paper_filename": "xiangdong_2023_review_of_the_application_of_multi\u2011modal_deep_learning_in medicine.pdf",
    "paper_title": "Xiangdong 2023 Review Of The Application Of Multi\u2011Modal Deep Learning In Medicine",
    "chunk_index": 61,
    "total_chunks": 76,
    "text_content": "for patients.Fig. 12 Institute bibliographic coupling network of MMDLM- related publications Fig. 13 Country bibliographic coupling network of MMDLM- related publications International Journal of Computational Intelligence Systems (2023) 16:44 1 3 44 Page 18 of 20 6 Conclusion In this report, we comprehensively discussed the perfor - mance of medical multi-modal deep learning from the aspects of pre-trained networks, fusion approaches, and models in clinical and application studies, and we com- ",
    "full_text_length": 79306,
    "chunk_length": 1338
  },
  {
    "chunk_id": 4031,
    "paper_filename": "xiangdong_2023_review_of_the_application_of_multi\u2011modal_deep_learning_in medicine.pdf",
    "paper_title": "Xiangdong 2023 Review Of The Application Of Multi\u2011Modal Deep Learning In Medicine",
    "chunk_index": 62,
    "total_chunks": 76,
    "text_content": "ity, from 2020 to the present. Aiming at the current situa- tion of insufficient polymorphic data fusion in deep learn - ing in medical applications and to address the needs of human\u2013machine collaborative medical cross-modal aux - iliary diagnosis and treatment applications, we reviewed the use of deep learning to perform multi-modal medical data and medical knowledge fusion analysis research and establish a medical heterogeneous multi-dimensional data retrieval and matching mechanism. To this e",
    "full_text_length": 79306,
    "chunk_length": 1384
  },
  {
    "chunk_id": 4032,
    "paper_filename": "xiangdong_2023_review_of_the_application_of_multi\u2011modal_deep_learning_in medicine.pdf",
    "paper_title": "Xiangdong 2023 Review Of The Application Of Multi\u2011Modal Deep Learning In Medicine",
    "chunk_index": 63,
    "total_chunks": 76,
    "text_content": "on diverse approaches to model integration. In addition, it is hoped that deep-learning models can be applied to long-term health monitoring and disease prevention to ensure suf- ficient data support for the in-depth development of AI in the medical field and further improve multi-modal sys- tems. We believe that the findings of this report will play an important role in guiding and developing the impor - tance of AI in clinical medicine and research. Acknowledgements This work was funded by a s",
    "full_text_length": 79306,
    "chunk_length": 1300
  },
  {
    "chunk_id": 4033,
    "paper_filename": "xiangdong_2023_review_of_the_application_of_multi\u2011modal_deep_learning_in medicine.pdf",
    "paper_title": "Xiangdong 2023 Review Of The Application Of Multi\u2011Modal Deep Learning In Medicine",
    "chunk_index": 64,
    "total_chunks": 76,
    "text_content": "analyzed during the current study are available from the corresponding author on reasonable request.Code Availability Not applicable. Declarations Conflict of interest We declare that we have no conflicts of interest. Ethics approval This article does not contain any studies with human participants or animals performed by any of the authors. Consent to participate Not applicable. Consent for publication Not applicable. Open Access This article is licensed under a Creative Commons Attri- bution 4",
    "full_text_length": 79306,
    "chunk_length": 1296
  },
  {
    "chunk_id": 4034,
    "paper_filename": "xiangdong_2023_review_of_the_application_of_multi\u2011modal_deep_learning_in medicine.pdf",
    "paper_title": "Xiangdong 2023 Review Of The Application Of Multi\u2011Modal Deep Learning In Medicine",
    "chunk_index": 65,
    "total_chunks": 76,
    "text_content": "in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/. References 1. Du, J., et al.: An overview of multi-modal medical image fusion. Neurocomputing 215, 3\u201320 (2016) 2. Litjens, G., et al.: A survey on deep learning in medical image analysis. Med. Image Anal",
    "full_text_length": 79306,
    "chunk_length": 1304
  },
  {
    "chunk_id": 4035,
    "paper_filename": "xiangdong_2023_review_of_the_application_of_multi\u2011modal_deep_learning_in medicine.pdf",
    "paper_title": "Xiangdong 2023 Review Of The Application Of Multi\u2011Modal Deep Learning In Medicine",
    "chunk_index": 66,
    "total_chunks": 76,
    "text_content": "MultiResUNet: rethinking the U-Net archi- tecture for multi-modal biomedical image segmentation. Neural Netw. 121, 74\u201387 (2020) 7. Wang, Z., et al.: GPDBN: deep bilinear network integrating both genomic data and pathological images for breast cancer prognosis prediction. Bioinform. 37(18), 2963\u20132970 (2021) 8. Cui, H., et al.: Co-graph attention reasoning based imaging and clinical features integration for lymph node metastasis predic- tion. In: Proc. Int. Conf. MICCAI (pp. 657\u2013666). Springer, Ch",
    "full_text_length": 79306,
    "chunk_length": 1335
  },
  {
    "chunk_id": 4036,
    "paper_filename": "xiangdong_2023_review_of_the_application_of_multi\u2011modal_deep_learning_in medicine.pdf",
    "paper_title": "Xiangdong 2023 Review Of The Application Of Multi\u2011Modal Deep Learning In Medicine",
    "chunk_index": 67,
    "total_chunks": 76,
    "text_content": "Process. Syst. 34, 19822\u201319835 (2021) International Journal of Computational Intelligence Systems (2023) 16:44 1 3 Page 19 of 20 44 12. Li, X., Yin, X., Li, C., Zhang, P., Hu, X., Zhang, L., Wang, L., Hu, H., Dong, L., Wei, F. In Oscar: Object-semantics aligned pre-training for vision-language tasks. In: European Conference on Computer Vision, pp. 121\u2013137. Springer, Berlin (2020) 13. Chen, Y.C., et al.: UNITER: UNiversal Image-TExt Represen- tation Learning. In: Proc. ECCV, pp. 104\u2013120. Springer",
    "full_text_length": 79306,
    "chunk_length": 1301
  },
  {
    "chunk_id": 4037,
    "paper_filename": "xiangdong_2023_review_of_the_application_of_multi\u2011modal_deep_learning_in medicine.pdf",
    "paper_title": "Xiangdong 2023 Review Of The Application Of Multi\u2011Modal Deep Learning In Medicine",
    "chunk_index": 68,
    "total_chunks": 76,
    "text_content": "Vision, pp. 2425\u20132433 (2015) 16. Li, J., et al.: Align before fuse: vision and language representation learning with momentum distillation. Adv. Neural Inf. Process. Syst. 34 (2021) 17. Kim, W., et al.: ViLT: vision-and-language Transformer without convolution or region supervision. In: ICML, pp. 5583\u20135594. PMLR (2021) 18. Zoph, B., Le, Q.V.: Neural architecture search with reinforcement learning. arXiv preprint arXiv: 1611. 01578 (2016) 19. El-Sappagh, S., et al.: Ultimodal multitasks deep lear",
    "full_text_length": 79306,
    "chunk_length": 1380
  },
  {
    "chunk_id": 4038,
    "paper_filename": "xiangdong_2023_review_of_the_application_of_multi\u2011modal_deep_learning_in medicine.pdf",
    "paper_title": "Xiangdong 2023 Review Of The Application Of Multi\u2011Modal Deep Learning In Medicine",
    "chunk_index": 69,
    "total_chunks": 76,
    "text_content": "predic- tion from multi-modality data. In: Proc. MICCAI, pp. 406\u2013414. Springer, Cham (2017) 23. Cheerla, A., et al.: Deep learning with multi-modal representation for pan-cancer prognosis prediction. Bioinform. 35(14), 446\u2013454 (2019) 24. Ara\u00fajo, T., et al.: Classification of breast cancer histology images using convolutional neural networks. PLoS ONE 12(6), e0177544 (2017) 25. Schulz, S., et al.: Multi-modal deep learning for prognosis predic- tion in renal cancer. Front. Oncol. 11 (2021) 26. Lu",
    "full_text_length": 79306,
    "chunk_length": 1372
  },
  {
    "chunk_id": 4039,
    "paper_filename": "xiangdong_2023_review_of_the_application_of_multi\u2011modal_deep_learning_in medicine.pdf",
    "paper_title": "Xiangdong 2023 Review Of The Application Of Multi\u2011Modal Deep Learning In Medicine",
    "chunk_index": 70,
    "total_chunks": 76,
    "text_content": "Inform. 23(2), 538\u2013546 (2018) 30. Yoo, Y., et al.: Deep learning of brain lesion patterns and user- defined clinical and MRI features for predicting conversion to multiple sclerosis from the clinically isolated syndrome. Com- put. Methods Biomech. Biomed. Eng. Imaging Vis. 7 (3), 250\u2013259 (2019) 31. Guan, Y., et al.: Predicting esophageal fistula risks using multi- modal self-attention network. In: Proc. Int. Conf. MICCAI, pp. 721\u2013730. Springer, Cham (2021) 32. Silva, L., et al.: Pan-cancer progn",
    "full_text_length": 79306,
    "chunk_length": 1371
  },
  {
    "chunk_id": 4040,
    "paper_filename": "xiangdong_2023_review_of_the_application_of_multi\u2011modal_deep_learning_in medicine.pdf",
    "paper_title": "Xiangdong 2023 Review Of The Application Of Multi\u2011Modal Deep Learning In Medicine",
    "chunk_index": 71,
    "total_chunks": 76,
    "text_content": "genomic features for cancer diagnosis and prognosis. IEEE Trans. Med. Imaging 41(4), 757\u2013770 (2022) 36. Sandfort, V., Yan, K., Pickhardt, P.J., Summers, R.M.: Data aug- mentation using generative adversarial networks (CycleGAN) to improve generalizability in CT segmentation tasks. Sci. Rep. 9(1), 16884 (2019) 37. Zhang, Y., Jiang, H., Miura, Y., Manning, C.D., Langlotz, C.P.: Contrastive learning of medical visual representations from paired images and text. arXiv preprint arXiv: 2010. 00747 (20",
    "full_text_length": 79306,
    "chunk_length": 1388
  },
  {
    "chunk_id": 4041,
    "paper_filename": "xiangdong_2023_review_of_the_application_of_multi\u2011modal_deep_learning_in medicine.pdf",
    "paper_title": "Xiangdong 2023 Review Of The Application Of Multi\u2011Modal Deep Learning In Medicine",
    "chunk_index": 72,
    "total_chunks": 76,
    "text_content": "al.: Modality-aware mutual learning for multi-modal medical image segmentation. In: Proc. MICCAI, pp. 589\u2013599. Springer, Cham (2021) 43. Li, S., et al.: A novel pathological images and genomic data fusion framework for breast cancer survival prediction. In: Proc. Int. Conf. EMBC, pp. 1384\u20131387. IEEE (2020) 44. Zhou, J., et al.: Cohesive multi-modality feature learning and fusion for COVID-19 patient severity prediction. IEEE Trans. Circuits Syst. Video. Technol. (2021) 45. Li, H., et al.: Multi-",
    "full_text_length": 79306,
    "chunk_length": 1396
  },
  {
    "chunk_id": 4042,
    "paper_filename": "xiangdong_2023_review_of_the_application_of_multi\u2011modal_deep_learning_in medicine.pdf",
    "paper_title": "Xiangdong 2023 Review Of The Application Of Multi\u2011Modal Deep Learning In Medicine",
    "chunk_index": 73,
    "total_chunks": 76,
    "text_content": "Chiriatti, M.J.M.: Machines, GPT-3: its nature, scope. Lim. Conseq. 30(4), 681\u2013694 (2020) 49. Xu, Z., So, D., Dai, A.: MUFASA: Multi-modal fusion architec- ture search for electronic health records. Proc. AAAI Conf. Artif. Intell. 35(12), 10532\u201310540 (2021) 50. Adams, J.: Information and misinformation in bibliometric time- trend analysis. J. Infometr. 12(4), 1063\u20131071 (2018) 51. Garfield, E.: From the science of science to Scientometrics: visual- izing the history of science with HistCite softw",
    "full_text_length": 79306,
    "chunk_length": 1402
  },
  {
    "chunk_id": 4043,
    "paper_filename": "xiangdong_2023_review_of_the_application_of_multi\u2011modal_deep_learning_in medicine.pdf",
    "paper_title": "Xiangdong 2023 Review Of The Application Of Multi\u2011Modal Deep Learning In Medicine",
    "chunk_index": 74,
    "total_chunks": 76,
    "text_content": "Single sub- ject prediction of brain disorders in neuroimaging: promises and pitfalls. Neuroimage 145, 137\u2013165 (2017) 56. Shin, H.C., et al.: Stacked autoencoders for unsupervised feature learning and multiple organ detection in a pilot study using 4D International Journal of Computational Intelligence Systems (2023) 16:44 1 3 44 Page 20 of 20 patient data. IEEE Trans. Pattern Anal. Mach. Intel. 35(8), 1930\u2013 1943 (2012) 57. Zhao, X., Wu, Y., Song, G., Li, Z., Zhang, Y., Fan, Y.: A deep learning ",
    "full_text_length": 79306,
    "chunk_length": 1339
  },
  {
    "chunk_id": 4044,
    "paper_filename": "xiangdong_2023_review_of_the_application_of_multi\u2011modal_deep_learning_in medicine.pdf",
    "paper_title": "Xiangdong 2023 Review Of The Application Of Multi\u2011Modal Deep Learning In Medicine",
    "chunk_index": 75,
    "total_chunks": 76,
    "text_content": "neuroimaging feature learning for mul- ticlass diagnosis of Alzheimer\u2019s disease. IEEE Trans. Biomed. Eng. 62(4), 1132\u20131140 (2014) 61. Arevalo, J., Gonz\u00e1lez, F.A., Ramos-Poll\u00e1n, R., Oliveira, J.L., Lopez, M.A.G.: Representation learning for mammography mass lesion classification with convolutional neural networks. Comput. Methods. Programs Biomed. 127, 248\u2013257 (2016) 62. Liu, Z., et al.: The applications of radiomics in precision diagnosis and treatment of oncology: Opportunities and challenges. ",
    "full_text_length": 79306,
    "chunk_length": 1132
  },
  {
    "chunk_id": 4045,
    "paper_filename": "xin_2020_mammography_interpretatiaon_framework.pdf",
    "paper_title": "Xin 2020 Mammography Interpretatiaon Framework",
    "chunk_index": 0,
    "total_chunks": 98,
    "text_content": "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 1 ViKL: A Mammography Interpretation Framework via Multimodal Aggregation of Visual-knowledge-linguistic Features Xin Wei, Y aling Tao, Changde Du, Gangming Zhao, Yizhou Yu, Fellow, IEEE , and Jinpeng Li, Member, IEEE Abstract \u2014Mammography is the primary imaging tool for breast cancer diagnosis. Despite significant strides in applying deep learning to interpret mammography images, efforts that focus predominantly on visual features often ",
    "full_text_length": 101240,
    "chunk_length": 1510
  },
  {
    "chunk_id": 4046,
    "paper_filename": "xin_2020_mammography_interpretatiaon_framework.pdf",
    "paper_title": "Xin 2020 Mammography Interpretatiaon Framework",
    "chunk_index": 1,
    "total_chunks": 98,
    "text_content": "features. This framework relies solely on pairing information without the necessity for pathology labels, which are often challanging to acquire. ViKL employs a triple contrastive learning approach to merge linguistic and knowledge-based insights with visual data, enabling both inter-modality and intra-modality feature enhancement. Our research yields significant findings: 1) Integrating reports and manifestations with unsupervised visual pretraining, ViKL substantially enhances the pathological",
    "full_text_length": 101240,
    "chunk_length": 1552
  },
  {
    "chunk_id": 4047,
    "paper_filename": "xin_2020_mammography_interpretatiaon_framework.pdf",
    "paper_title": "Xin 2020 Mammography Interpretatiaon Framework",
    "chunk_index": 2,
    "total_chunks": 98,
    "text_content": "are critical to improving the prognosis, as emphasized in existing literature [1]. Mammography stands as the principal tool for early breast cancer screening. Yet, the early indicators of breast cancer are subtle and subject to varying interpretations by different radiologists, presenting challenges in screening accuracy. In the past five years, the advent of deep learning in medical image analysis has sparked a wave of innovation. Models such as convolutional neural networks (CNN) [2, 3] and tr",
    "full_text_length": 101240,
    "chunk_length": 1369
  },
  {
    "chunk_id": 4048,
    "paper_filename": "xin_2020_mammography_interpretatiaon_framework.pdf",
    "paper_title": "Xin 2020 Mammography Interpretatiaon Framework",
    "chunk_index": 3,
    "total_chunks": 98,
    "text_content": "and Engineering, South China University of Technology, Guangzhou 510641, Guangdong, China. \u2022Du, C is with the Research Center for Brain-Inspired Intelligence, State Key Laboratory of Multimodal Artificial Intelligence Systems, Institute of Automation, Chinese Academy of Sciences, Beijing 100190, China. \u2022Zhao, G and Yu, Y are with the Department of Computer Science, University of Hong Kong, Pokfulam, Hong Kong. This work was supported by the National Natural Science Foundation of China under Gran",
    "full_text_length": 101240,
    "chunk_length": 1463
  },
  {
    "chunk_id": 4049,
    "paper_filename": "xin_2020_mammography_interpretatiaon_framework.pdf",
    "paper_title": "Xin 2020 Mammography Interpretatiaon Framework",
    "chunk_index": 4,
    "total_chunks": 98,
    "text_content": "the disease-related features in these images are often complex and subtle, making accurate in- terpretation challenging. 2) Manifestations representing the expert knowledge in analyzing breast lesions, which are rich in dense, semi-semantic information. 3) Mammography Reports standing for the linguistic summaries of the radi- ologist\u2019s observations with containing highly sparse and abstract semantic information. The reports encapsulate the radiologist\u2019s final interpretation and conclusions based",
    "full_text_length": 101240,
    "chunk_length": 1506
  },
  {
    "chunk_id": 4050,
    "paper_filename": "xin_2020_mammography_interpretatiaon_framework.pdf",
    "paper_title": "Xin 2020 Mammography Interpretatiaon Framework",
    "chunk_index": 5,
    "total_chunks": 98,
    "text_content": "techniques, which shows the possibility ofarXiv:2409.15744v1 [eess.IV] 24 Sep 2024 JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 2 Manifestation ? Round Ovoid Lobulated Irregular Microlobulated Obscured Branching Crescentic Thread -likeGritty Large Rod-like YesShape Edge Calcification ShapeGranular Popcorn -like Eggshell -like Comet TailSign No \u2026\u2026Mammography Image A mass -like shadow in the posterior and upper outer quadrant of the left nipple with calcified shadow in the upper outer",
    "full_text_length": 101240,
    "chunk_length": 1441
  },
  {
    "chunk_id": 4051,
    "paper_filename": "xin_2020_mammography_interpretatiaon_framework.pdf",
    "paper_title": "Xin 2020 Mammography Interpretatiaon Framework",
    "chunk_index": 6,
    "total_chunks": 98,
    "text_content": "paradigm of ViKL, which aggregates visual, knowledge and linguistic features to build intelligent machines for mammography analysis. extracting manifestations in reports [7]. In contrast, mam- mography reports are very succinct, and the diagnostic manifestations are not fully reflected in the report. For exam- ple, a manifestation\u2019s absence in the report does not imply its absence in the image. Therefore, the keywords extracted from the report are far from representing the expertise of radiologi",
    "full_text_length": 101240,
    "chunk_length": 1429
  },
  {
    "chunk_id": 4052,
    "paper_filename": "xin_2020_mammography_interpretatiaon_framework.pdf",
    "paper_title": "Xin 2020 Mammography Interpretatiaon Framework",
    "chunk_index": 7,
    "total_chunks": 98,
    "text_content": "For example, REFERS [14] leveraged supervision signals from X-ray re- ports and employed self-supervised contrastive learning to develop joint representations. Demonstrating exceptional performance under limited supervision, REFERS even out- performs approaches reliant on structured labels manually added to X-rays. However, REFERS does not incorporate manifestations, which are rich in dense semantic infor- mation. We posit that the role of manifestations warrants independent and systematic inves",
    "full_text_length": 101240,
    "chunk_length": 1556
  },
  {
    "chunk_id": 4053,
    "paper_filename": "xin_2020_mammography_interpretatiaon_framework.pdf",
    "paper_title": "Xin 2020 Mammography Interpretatiaon Framework",
    "chunk_index": 8,
    "total_chunks": 98,
    "text_content": "label smoothing and manifestation deduplication. 2) We propose a novel hard negative sample selec- tion method based on manifestations, termed ManiNeg. It estimates the hardness of negative samples through the Hamming distance between manifestations, effectively ad-dressing the issues of limited sampling space and mismatch between representation and semantics, which exist in tradi- tional representation-based hard negative sample selection methods. 3) We contribute MVKL, the first trimodal mammo",
    "full_text_length": 101240,
    "chunk_length": 1487
  },
  {
    "chunk_id": 4054,
    "paper_filename": "xin_2020_mammography_interpretatiaon_framework.pdf",
    "paper_title": "Xin 2020 Mammography Interpretatiaon Framework",
    "chunk_index": 9,
    "total_chunks": 98,
    "text_content": "to ViKL. Section 3 introduces the MVKL dataset. Section 4 presents the ViKL framework and optimization method. Section 5 and Section 6 include experimental results with in-depth analysis. Section 7 sum- marizes take-away information, with limitations and future directions for follow-up studies. 2 R ELATED WORKS Self-supervised Learning (SSL) . SSL operates without tar- get task annotations, positioning it within the domain of unsupervised learning. It relies on pretext tasks, which can vary in n",
    "full_text_length": 101240,
    "chunk_length": 1322
  },
  {
    "chunk_id": 4055,
    "paper_filename": "xin_2020_mammography_interpretatiaon_framework.pdf",
    "paper_title": "Xin 2020 Mammography Interpretatiaon Framework",
    "chunk_index": 10,
    "total_chunks": 98,
    "text_content": "for the target task. A distinct category within SSL is instance discrimination , which treats each instance as a unique class, leading to the emergence of contrastive learning exemplified by SimCLR [21] and MOCO [22]. In this framework, an image is aug- mented through rotation, flipping, colorization, etc., and in JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 3 Image\ud835\udc38\ud835\udc3c \ud835\udc38\ud835\udc3c \ud835\udc38\ud835\udc3c Fuse Projector Image Text Manifestation\ud835\udc38\ud835\udc3c \ud835\udc38\ud835\udc47 \ud835\udc38\ud835\udc40 Fuse Projector Image Text\ud835\udc38\ud835\udc3c \ud835\udc38\ud835\udc47 \ud835\udc38\ud835\udc40 Fuse Projector Attract Repel",
    "full_text_length": 101240,
    "chunk_length": 1400
  },
  {
    "chunk_id": 4056,
    "paper_filename": "xin_2020_mammography_interpretatiaon_framework.pdf",
    "paper_title": "Xin 2020 Mammography Interpretatiaon Framework",
    "chunk_index": 11,
    "total_chunks": 98,
    "text_content": "projected into the same embedding space using respective encoders. Matched modalities of the same instance are attracted to each other, ensuring inter-modality alignment. C. Features from distinct instances are repelled to enhance uniformity across the hypersphere, preserving information effectively. D. The image encoder is capable of improved pathological classification of breast lumps through a simple task head. ViKL model is also versatile, suitable for multimodal tasks like image-report retr",
    "full_text_length": 101240,
    "chunk_length": 1374
  },
  {
    "chunk_id": 4057,
    "paper_filename": "xin_2020_mammography_interpretatiaon_framework.pdf",
    "paper_title": "Xin 2020 Mammography Interpretatiaon Framework",
    "chunk_index": 12,
    "total_chunks": 98,
    "text_content": "of VLP is CLIP [11]. During pretraining, CLIP uses image and text encoders to map these modalities into an embedding space, training the encoders to recognize paired images and text. During test, simple classifiers are added to the image encoder to achieve target tasks. CLIP has inspired numerous follow-ups, most utilizing BERT [23] as the lan- guage encoder, while adopting various vision encoders. For instance, region-based encoders like Faster R-CNN [24] extract regions for VLP , offering high",
    "full_text_length": 101240,
    "chunk_length": 1369
  },
  {
    "chunk_id": 4058,
    "paper_filename": "xin_2020_mammography_interpretatiaon_framework.pdf",
    "paper_title": "Xin 2020 Mammography Interpretatiaon Framework",
    "chunk_index": 13,
    "total_chunks": 98,
    "text_content": "images. VLMo [27] of- fers a unified vision-language model, leveraging a modular transformer network to jointly learn a dual encoder and afusion encoder. BLIP [28] adapts to both understanding and generation tasks in vision-language by bootstrapping web data captions, filtering out noise to enhance learning. CoCa [29] combines contrastive and captioning loss in an image- text encoder-decoder foundation model, encompassing both contrastive and generative methods. BEiT [30]pretrains vi- sion trans",
    "full_text_length": 101240,
    "chunk_length": 1422
  },
  {
    "chunk_id": 4059,
    "paper_filename": "xin_2020_mammography_interpretatiaon_framework.pdf",
    "paper_title": "Xin 2020 Mammography Interpretatiaon Framework",
    "chunk_index": 14,
    "total_chunks": 98,
    "text_content": "like ImageNet. However, the significant disparity between natural and medical images often renders this approach less than ideal. 2) Another method involves seeking free super- visory signals from unannotated data. For instance, Models Genesis utilizes an AE-based SSL approach to reconstruct CT images from their augmented versions [17, 18], where the encoder is then adapted for downstream tasks. Simi- larly, our preliminary work with MVCNet leverages mul- tiview contrastive learning to aggregate",
    "full_text_length": 101240,
    "chunk_length": 1417
  },
  {
    "chunk_id": 4060,
    "paper_filename": "xin_2020_mammography_interpretatiaon_framework.pdf",
    "paper_title": "Xin 2020 Mammography Interpretatiaon Framework",
    "chunk_index": 15,
    "total_chunks": 98,
    "text_content": "to encode X-ray images and minimizes contrastive alignment loss to pair images with their corresponding free-text reports [14]. This approach, capitalizing on the complex logic and abstract reasoning in reports, shows promise in supplanting traditional pretraining methods. The existing studies rarely take the manifestation modality into consideration, but we argue that while images are rich repositories of fundamental yet latent disease features and reports encapsulate unstruc- tured, abstract, ",
    "full_text_length": 101240,
    "chunk_length": 1537
  },
  {
    "chunk_id": 4061,
    "paper_filename": "xin_2020_mammography_interpretatiaon_framework.pdf",
    "paper_title": "Xin 2020 Mammography Interpretatiaon Framework",
    "chunk_index": 16,
    "total_chunks": 98,
    "text_content": "refined modeling. The LIDC-IDRI dataset includes seven attributes for benign/malignant classification of pulmonary nodules. Building on these attributes, various studies have advanced pulmonary nodule diagnosis, such as employing Bayesian frameworks for attribute-based reasoning [35] or integrating these attributes into neural network inputs [36]. In our preliminary work, we developed the first tuberculo- sis dataset featuring seven manifestations and employed a multi-scale feature interaction m",
    "full_text_length": 101240,
    "chunk_length": 1540
  },
  {
    "chunk_id": 4062,
    "paper_filename": "xin_2020_mammography_interpretatiaon_framework.pdf",
    "paper_title": "Xin 2020 Mammography Interpretatiaon Framework",
    "chunk_index": 17,
    "total_chunks": 98,
    "text_content": "tions distinctly differ from radiomics features in their na- ture and application. While radiomics involves the high- throughput extraction of quantitative image features, man- ifestations offer a more concise and disease-specific ap- proach. They encapsulate higher-level features compared to radiomics, focusing on specific characteristics closely asso- ciated with particular diseases. This distinction highlights manifestations as a more targeted and nuanced method in medical imaging, in contras",
    "full_text_length": 101240,
    "chunk_length": 1470
  },
  {
    "chunk_id": 4063,
    "paper_filename": "xin_2020_mammography_interpretatiaon_framework.pdf",
    "paper_title": "Xin 2020 Mammography Interpretatiaon Framework",
    "chunk_index": 18,
    "total_chunks": 98,
    "text_content": "bilateral views [44]. Also, in light of the limited availability of la- beled data, weakly-supervised learning approaches such as multi-instance learning have shown promise in improving sensitivity, utilizing labels of varying granularity [45]. 3 D ATASET We have compiled the first trimodal dataset for mammog- raphy: The Mammography Visual- Knowledge- Linguistic dataset (MVKL). The dataset consists of 2671 mammog- raphy examinations containing breast lumps, along with their imaging reports, mani",
    "full_text_length": 101240,
    "chunk_length": 1467
  },
  {
    "chunk_id": 4064,
    "paper_filename": "xin_2020_mammography_interpretatiaon_framework.pdf",
    "paper_title": "Xin 2020 Mammography Interpretatiaon Framework",
    "chunk_index": 19,
    "total_chunks": 98,
    "text_content": "Yet, other less emphasized manifestations can also be crucial for rep- resentation learning. Therefore, we developed a compre- hensive manifestation table to capture a broad spectrum of relevant breast lump traits. The annotation process involved two phases for accuracy. Initially, five attending and chief physicians annotated the breast lumps and their manifesta- tions, referencing the reports and pathology results. In the second phase, a different physician reviewed these annota- tions, resolv",
    "full_text_length": 101240,
    "chunk_length": 1478
  },
  {
    "chunk_id": 4065,
    "paper_filename": "xin_2020_mammography_interpretatiaon_framework.pdf",
    "paper_title": "Xin 2020 Mammography Interpretatiaon Framework",
    "chunk_index": 20,
    "total_chunks": 98,
    "text_content": "oblique (LMLO), left craniocaudal (LCC), right mediolateral oblique (RMLO), and right craniocaudal (RCC). To ensure dataset completeness and versatility for various tasks, we retained all views in their original form, irrespective of the presence breast lumps. However, views without breast lumps are not involved in this study. Manifestations. We crafted a generic manifestation table based on radiological expertise to describe breast lumps, detailed in Table 1. This table encompasses 8 primary tr",
    "full_text_length": 101240,
    "chunk_length": 1406
  },
  {
    "chunk_id": 4066,
    "paper_filename": "xin_2020_mammography_interpretatiaon_framework.pdf",
    "paper_title": "Xin 2020 Mammography Interpretatiaon Framework",
    "chunk_index": 21,
    "total_chunks": 98,
    "text_content": "manifestation table. Options are separated with commas, and slash separators indicate these traits are treated as the same option. The options in the miscellaneous row are independent, while the options in other rows are mutually exclusive. Manifestations Options mass shape irregular, lobulated, ovoid, round mass edge microlobulated, obscured, spiculated, well-circumscribed mass density low, median, high mass size \u22642cm, 2-5cm, >5cm calcification shape branching, crescentic/annular/gritty/thread-",
    "full_text_length": 101240,
    "chunk_length": 1599
  },
  {
    "chunk_id": 4067,
    "paper_filename": "xin_2020_mammography_interpretatiaon_framework.pdf",
    "paper_title": "Xin 2020 Mammography Interpretatiaon Framework",
    "chunk_index": 22,
    "total_chunks": 98,
    "text_content": "malignancy. These categories reflect radiologists\u2019 assessments based on imaging alone, akin to the reports. Privacy Statement. Upon collection, patient-identifiable data was promptly removed. This includes specific details and identifiers in the DICOM headers such as patient and physician names, birth dates, institution names and addresses, as well as various identification numbers like Accession Number, Patient ID, Study ID, Study Instance UID, Series Instance UID, and SOP Instance UID. 4 M ETH",
    "full_text_length": 101240,
    "chunk_length": 1344
  },
  {
    "chunk_id": 4068,
    "paper_filename": "xin_2020_mammography_interpretatiaon_framework.pdf",
    "paper_title": "Xin 2020 Mammography Interpretatiaon Framework",
    "chunk_index": 23,
    "total_chunks": 98,
    "text_content": "employ a BERT [23] encoder. Given that manifestations can be represented as vectors, they are encoded using two linear layers. Ad- ditionally, an alignment linear layer is appended to each unimodal encoder, standardizing the output dimensions to 128 in our experiments. For both the manifestation and text branches, an extra dropout layer follows the alignment layer, acting as a feature augmenter akin to data augmenta- tion, with a dropout probability of 0.5. The multimodal fuse projector is pivot",
    "full_text_length": 101240,
    "chunk_length": 1366
  },
  {
    "chunk_id": 4069,
    "paper_filename": "xin_2020_mammography_interpretatiaon_framework.pdf",
    "paper_title": "Xin 2020 Mammography Interpretatiaon Framework",
    "chunk_index": 24,
    "total_chunks": 98,
    "text_content": "Pretraining Objectives We pretrain ViKL with dual objectives: image multi-view con- trastive learning (IMC) and image-text-manifestation contrastive learning (ITM). To establish an effective feature space, spe- cific strategies like label smoothing and hard negative sample selection are incorporated into the loss functions, tailored to the characteristics of each modality. The subsequent sections will delve into these aspects in greater detail. General Objective. Before proceeding, it\u2019s crucial ",
    "full_text_length": 101240,
    "chunk_length": 1430
  },
  {
    "chunk_id": 4070,
    "paper_filename": "xin_2020_mammography_interpretatiaon_framework.pdf",
    "paper_title": "Xin 2020 Mammography Interpretatiaon Framework",
    "chunk_index": 25,
    "total_chunks": 98,
    "text_content": "21, 22, 11], we can distill the essence of contrastive learning into a fundamental loss function. For two modalities m1andm2, and a mini batch of Npairs, the contrastive loss for a pair of positive sample pair\u0000zi m1,zi m2\u0001 is defined as Lp\u0000zi m1,zi m2\u0001=\u2212logexp\u0000sim\u0000zi m1,zi m2\u0001/\u03c4\u0001 P k\u2208N m\u2208{m1,m2} k\u0338=iexp\u0000sim\u0000zim1,zkm\u0001/\u03c4\u0001, (1) where \u03c4represents a temperature parameter, crucial in modulating the loss function\u2019s dynamic range. In this study, we utilize cosine similarity (denoted as sim(\u00b7)) as the me",
    "full_text_length": 101240,
    "chunk_length": 1355
  },
  {
    "chunk_id": 4071,
    "paper_filename": "xin_2020_mammography_interpretatiaon_framework.pdf",
    "paper_title": "Xin 2020 Mammography Interpretatiaon Framework",
    "chunk_index": 26,
    "total_chunks": 98,
    "text_content": "LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 6 The following sections will delve deeper into the applica- tion of these contrastive learning principles across the three modalities in our study, employing these loss functions. Image Multi-view Contrastive Learning is designed to develop a representation space that effectively extracts meaningful image representations. For mammography im- ages, we leverage the contrastive loss to amplify the sim- ilarity within ipsilateral breasts and increase s",
    "full_text_length": 101240,
    "chunk_length": 1418
  },
  {
    "chunk_id": 4072,
    "paper_filename": "xin_2020_mammography_interpretatiaon_framework.pdf",
    "paper_title": "Xin 2020 Mammography Interpretatiaon Framework",
    "chunk_index": 27,
    "total_chunks": 98,
    "text_content": "of IMC loss function, where Irepresents the image modality. Lbasic IMC(zI) =Lcl(zcc,zmlo). (3) Image-Text-Manifestation Contrastive Learning ex- tends the scope of image contrastive learning to encompass additional modalities. By aligning the representations of image, text, and manifestations, it enables the seamless application of the unimodal image multi-view loss function to multiple modalities: Lbasic ITM(zI,zT,zM) = 1 3(Lcl(zI,zT) +Lcl(zI,zM) +Lcl(zT,zM)),(4) where I,T,Mrepresent the image,",
    "full_text_length": 101240,
    "chunk_length": 1388
  },
  {
    "chunk_id": 4073,
    "paper_filename": "xin_2020_mammography_interpretatiaon_framework.pdf",
    "paper_title": "Xin 2020 Mammography Interpretatiaon Framework",
    "chunk_index": 28,
    "total_chunks": 98,
    "text_content": "pairs are invariably negative samples. However, we retain this term, as in a data-limited context, it can aid in achieving a more uniform distribution of representations, thereby preserving information more effectively [48]. While we have established the basic form of these loss functions, it is crucial to acknowledge that different modal- ities exhibit varying densities of information, resulting in distinct correspondence levels between them. To address these nuances and ensure model convergenc",
    "full_text_length": 101240,
    "chunk_length": 1406
  },
  {
    "chunk_id": 4074,
    "paper_filename": "xin_2020_mammography_interpretatiaon_framework.pdf",
    "paper_title": "Xin 2020 Mammography Interpretatiaon Framework",
    "chunk_index": 29,
    "total_chunks": 98,
    "text_content": "their predictions. Similarly, in contrastive learn- ing, which optimizes inter-sample relationships via cross- entropy, there\u2019s a risk of over-emphasizing correlations and exhibiting over-confidence. To address this, we integrate label smoothing into the loss function for text, a method proven effective in mitigating over-confidence in the litera- ture [50, 51]. Label smoothing works by softening classification tar- gets, thereby reducing over-confidence. It modifies the basic contrastive loss f",
    "full_text_length": 101240,
    "chunk_length": 1375
  },
  {
    "chunk_id": 4075,
    "paper_filename": "xin_2020_mammography_interpretatiaon_framework.pdf",
    "paper_title": "Xin 2020 Mammography Interpretatiaon Framework",
    "chunk_index": 30,
    "total_chunks": 98,
    "text_content": "to learn nuanced distinctions without generate overly confident predictions. Hard Negative Sample Selection. The selection of nega- tive samples is critical for the success of contrastive learning. For an anchor sample zaand its set of negative samples z\u2212\u2208 {zi m|i\u0338=a\u2200m}, the standard contrastive loss function selects negatives uniformly, which often leads to optimiza- tion dominated by more pronounced features. However, in mammography, breast lumps typically occupy a small area and may lack dist",
    "full_text_length": 101240,
    "chunk_length": 1345
  },
  {
    "chunk_id": 4076,
    "paper_filename": "xin_2020_mammography_interpretatiaon_framework.pdf",
    "paper_title": "Xin 2020 Mammography Interpretatiaon Framework",
    "chunk_index": 31,
    "total_chunks": 98,
    "text_content": "strictly align with their semantics, which allow the representation to be uti- lized as the basis for hard negative sample selection. The second is that there will always be ideal hard negative samples in the mini batch. However, for mammography, due to the small and concealed features of lesions, the rep- resentations during training often cannot strictly align with semantics, which can lead to the selection of suboptimal hard negative samples, thereby harming model optimizing. Worse still, the",
    "full_text_length": 101240,
    "chunk_length": 1317
  },
  {
    "chunk_id": 4077,
    "paper_filename": "xin_2020_mammography_interpretatiaon_framework.pdf",
    "paper_title": "Xin 2020 Mammography Interpretatiaon Framework",
    "chunk_index": 32,
    "total_chunks": 98,
    "text_content": "binary mani- festations can be easily measured using Hamming distance, JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 7 ensuring computational efficiency in the selection of hard negative samples. Lastly, unlike representations that may change with model optimizes, manifestation is an intrinsic attributes of breast lumps and remain constant. This allows for the selection of hard negative samples from the entire dataset rather than just from mini batches. Based on these observations, w",
    "full_text_length": 101240,
    "chunk_length": 1376
  },
  {
    "chunk_id": 4078,
    "paper_filename": "xin_2020_mammography_interpretatiaon_framework.pdf",
    "paper_title": "Xin 2020 Mammography Interpretatiaon Framework",
    "chunk_index": 33,
    "total_chunks": 98,
    "text_content": "pling, as detailed in Eq. (7). f(x;\u00b5, \u03c32, a, b) =(\u03d5(x;\u00b5,\u03c32) \u03a6(b;\u00b5,\u03c32)\u2212\u03a6(a;\u00b5,\u03c32),ifa\u2264x\u2264b 0, others(7) In the formula, xrepresents the Hamming distance between the sample and the anchor\u2019s manifestations. \u03a6and\u03d5are the cumulative distribution function and the probability density function of the Gaussian distribution N(\u00b5, \u03c32), respectively. aandbrepresent the lower and upper bounds of the trun- cated Gaussian distribution. Specifically, we set the lower bound ato 1 to prevent the anchor itself from b",
    "full_text_length": 101240,
    "chunk_length": 1298
  },
  {
    "chunk_id": 4079,
    "paper_filename": "xin_2020_mammography_interpretatiaon_framework.pdf",
    "paper_title": "Xin 2020 Mammography Interpretatiaon Framework",
    "chunk_index": 34,
    "total_chunks": 98,
    "text_content": "mini batch. 4) Finally, we will perform deduplication based on man- ifestations within the batch. Specifically, if multiple samples in the mini batch have the same manifestation, only one sample will be randomly retained. During the training pro- cess, every pair of samples in the mini batch will be treated as a negative sample pair. Deduplication helps prevent the model from mistakenly treating potential positive sample pairs as negative ones. Additionally, we will incorporate a hardness anneal",
    "full_text_length": 101240,
    "chunk_length": 1370
  },
  {
    "chunk_id": 4080,
    "paper_filename": "xin_2020_mammography_interpretatiaon_framework.pdf",
    "paper_title": "Xin 2020 Mammography Interpretatiaon Framework",
    "chunk_index": 35,
    "total_chunks": 98,
    "text_content": "is articulated as follows:LIMC(zI) =Lcl(zcc,zmlo), LITM(zI,zT,zM) = 1 3\u0010 Lls cl(zI,zT) +Lcl(zI,zM) +Lls cl(zT,zM)\u0011 , Lall(zI,zT,zM) =LIMC(zI) +LITM(zI,zT,zM).(8) In these equations, lsrepresents the use of label smoothing. The cumulative function, Lall, forms the core of ViKL\u2019s pretraining objective. The model underwent pretraining for 300 epochs on a single A100 GPU, totaling approximately 3 hours. For the text branch, we used bert-base-chinese [23] as the initial weight. In the image branch, w",
    "full_text_length": 101240,
    "chunk_length": 1369
  },
  {
    "chunk_id": 4081,
    "paper_filename": "xin_2020_mammography_interpretatiaon_framework.pdf",
    "paper_title": "Xin 2020 Mammography Interpretatiaon Framework",
    "chunk_index": 36,
    "total_chunks": 98,
    "text_content": "we focus on evaluating how ViKL augments the image branch via multimodal con- trastive learning, for this is essential clinically. Inspired by CLIP [11] and SimCLR [21], we assess the model\u2019s ability to extract features from mammography images using the following protocols: Linear Evaluation (LE) involves maintaining the pre- trained weights of the image branch and adding a trainable linear layer for the classification task. This method evaluates the quality of the extracted features by observin",
    "full_text_length": 101240,
    "chunk_length": 1361
  },
  {
    "chunk_id": 4082,
    "paper_filename": "xin_2020_mammography_interpretatiaon_framework.pdf",
    "paper_title": "Xin 2020 Mammography Interpretatiaon Framework",
    "chunk_index": 37,
    "total_chunks": 98,
    "text_content": "an added linear layer for task-specific purposes. This approach assesses ViKL\u2019s adaptability and performance in real-world scenarios. Both LE and LP limit the number of trainable parame- ters, thus serving as robust methods to assess the intrinsic quality of ViKL\u2019s features while minimizing the influence of hyperparameters. On the other hand, FT provides insights into the model\u2019s practical applicability and performance enhancement in real-world tasks. 5.2 Experimental Setups We now delve into th",
    "full_text_length": 101240,
    "chunk_length": 1379
  },
  {
    "chunk_id": 4083,
    "paper_filename": "xin_2020_mammography_interpretatiaon_framework.pdf",
    "paper_title": "Xin 2020 Mammography Interpretatiaon Framework",
    "chunk_index": 38,
    "total_chunks": 98,
    "text_content": "for these tasks are de- rived from pathologically validated benign-malignant eval- uations, ensuring an objective analysis. Metric. To ensure consistent and clear comparisons be- tween models, we exclusively use the area under the re- ceiver operating characteristic (AUC). This metric provides a straightforward measure of performance without the com- plexities introduced by thresholding. Data Augmentation. The same data augmentation strat- egy is applied in both pretraining and downstream task p",
    "full_text_length": 101240,
    "chunk_length": 1384
  },
  {
    "chunk_id": 4084,
    "paper_filename": "xin_2020_mammography_interpretatiaon_framework.pdf",
    "paper_title": "Xin 2020 Mammography Interpretatiaon Framework",
    "chunk_index": 39,
    "total_chunks": 98,
    "text_content": "in a subsequent section. Hyperparameters. Specific hyperparameters for each protocol are listed in Appendix A. 5.3 Pulic Datasets for External Evaluation In addition to the in-house MVKL dataset, we extend our experiments to two publicly available datasets, CBIS-DDSM [54] and INbreast [55], to assess the transferability of ViKL. CBIS-DDSM. This dataset comprises mammograms specifically annotated for breast cancer diagnosis, including cases marked for calcifications and masses. We adhere to the d",
    "full_text_length": 101240,
    "chunk_length": 1362
  },
  {
    "chunk_id": 4085,
    "paper_filename": "xin_2020_mammography_interpretatiaon_framework.pdf",
    "paper_title": "Xin 2020 Mammography Interpretatiaon Framework",
    "chunk_index": 40,
    "total_chunks": 98,
    "text_content": "assign BI-RADS \u2208 {1,2,3}as non-malignant and BI-RADS \u2208 {4,5,6}as malignant. Notably, BI-RADS 0 is not present in the INbreast dataset. To ensure comparability, we apply the same experi- mental protocols to these public datasets as we did with MVKL. The initial weights are pretrained on MVKL to eval- uate ViKL\u2019s cross-dataset effectiveness. Given the variations in collection years and equipment, these public datasets present mammograms with notable differences in clarity and appearance compared t",
    "full_text_length": 101240,
    "chunk_length": 1246
  },
  {
    "chunk_id": 4086,
    "paper_filename": "xin_2020_mammography_interpretatiaon_framework.pdf",
    "paper_title": "Xin 2020 Mammography Interpretatiaon Framework",
    "chunk_index": 41,
    "total_chunks": 98,
    "text_content": "384 CBIS-DDSM 1494 1083 189 98 428 276 INbreast* 212 75 32 9 66 16 *BI-RADS 1 category is presented in the INbreast dataset, which typically indicates that no lumps were found in the mammogram. For readability, we merged it into class Benign . modality in ViKL when applied to the downstream task. This involved using the untrained encoders of each modal- ity, followed by a linear layer for binary classification. We employed the Adam optimizer [56] with optimally chosen learning rates to ensure co",
    "full_text_length": 101240,
    "chunk_length": 1223
  },
  {
    "chunk_id": 4087,
    "paper_filename": "xin_2020_mammography_interpretatiaon_framework.pdf",
    "paper_title": "Xin 2020 Mammography Interpretatiaon Framework",
    "chunk_index": 42,
    "total_chunks": 98,
    "text_content": "results to minimize the impact of classification thresholding. Interestingly, we ob- served that despite both text and manifestations originating from images, the classification performance using images alone was the weakest. This suggests that neural networks might struggle to extract sufficient useful information from images using simple binary labels. Text and manifestations, in this context, act as refined versions of the original images, potentially guiding the network towards more effectiv",
    "full_text_length": 101240,
    "chunk_length": 1478
  },
  {
    "chunk_id": 4088,
    "paper_filename": "xin_2020_mammography_interpretatiaon_framework.pdf",
    "paper_title": "Xin 2020 Mammography Interpretatiaon Framework",
    "chunk_index": 43,
    "total_chunks": 98,
    "text_content": "volume and label diversity often hinder the generalization capability of models pretrained in a supervised manner. ViKL, with its unsupervised multimodal contrastive learning approach, adeptly overcomes these challenges. To empirically assess model generalization, we con- ducted a cross-dataset evaluation using the three datasets. Models were pretrained on each dataset using supervised learning, then tested across all three datasets on a down- stream task. We employed the LP protocol for a robus",
    "full_text_length": 101240,
    "chunk_length": 1449
  },
  {
    "chunk_id": 4089,
    "paper_filename": "xin_2020_mammography_interpretatiaon_framework.pdf",
    "paper_title": "Xin 2020 Mammography Interpretatiaon Framework",
    "chunk_index": 44,
    "total_chunks": 98,
    "text_content": "un- derscores the limited generalization capacity of supervised pretraining methods. In contrast, ViKL demonstrates strong, consistent performance across various datasets, indicating its superior ability in generating robust and generalizable representations. TABLE 4: Cross-dataset evaluation results with the LP protocol. Results are in AUC ( Mean \u00b1Std). Column headers represent the datasets for downstream evaluation, while row headers indicate the datasets for supervised pretraining. The last r",
    "full_text_length": 101240,
    "chunk_length": 1427
  },
  {
    "chunk_id": 4090,
    "paper_filename": "xin_2020_mammography_interpretatiaon_framework.pdf",
    "paper_title": "Xin 2020 Mammography Interpretatiaon Framework",
    "chunk_index": 45,
    "total_chunks": 98,
    "text_content": "and FT protocols across the in-house MVKL, CBIS-DDSM, and INbreast datasets. The findings from these experiments are presented in Tables 5 and 6. 5.6.1 Comparison with Supervised Models In our analysis, we primarily compare three key results from Table 5: ViKL, ViKL-R, and IM. ViKL represents our proposed model, where the image branch is preinitialized with ImageNet weights prior to multimodal contrastive learning. In contrast, ViKL-R denotes a variant with a ran- domly initialized image branch ",
    "full_text_length": 101240,
    "chunk_length": 1399
  },
  {
    "chunk_id": 4091,
    "paper_filename": "xin_2020_mammography_interpretatiaon_framework.pdf",
    "paper_title": "Xin 2020 Mammography Interpretatiaon Framework",
    "chunk_index": 46,
    "total_chunks": 98,
    "text_content": "the incorpo- ration of text and manifestation data through contrastive learning effectively enhances the image branch\u2019s sensitivity to lesions. This improvement is not only evident in the MVKL dataset used for pretraining but also extends to CBIS-DDSM and INbreast datasets, suggesting the model\u2019s adaptability across diverse datasets.5.6.2 Ablation Study In the ablation study, we explore two crucial aspects: 1) the contribution of each modality, and 2) the effectiveness of the loss function mecha",
    "full_text_length": 101240,
    "chunk_length": 1424
  },
  {
    "chunk_id": 4092,
    "paper_filename": "xin_2020_mammography_interpretatiaon_framework.pdf",
    "paper_title": "Xin 2020 Mammography Interpretatiaon Framework",
    "chunk_index": 47,
    "total_chunks": 98,
    "text_content": "lesion-related knowledge during pretraining. We hypothesize that this is because, although contrastive learn- ing can effectively derive image representations, these rep- resentations may not necessarily correlate well with lesion characteristics, especially in scenarios with limited data or small lesions. This phenomenon highlights a crucial consid- eration in medical image analysis: the potential limitations of applying contrastive learning, successful in natural image domains, might not direc",
    "full_text_length": 101240,
    "chunk_length": 1509
  },
  {
    "chunk_id": 4093,
    "paper_filename": "xin_2020_mammography_interpretatiaon_framework.pdf",
    "paper_title": "Xin 2020 Mammography Interpretatiaon Framework",
    "chunk_index": 48,
    "total_chunks": 98,
    "text_content": "comprehensive manifestations entails some ad- ditional annotation effort. Given the emphasis on reducing annotation costs in medical image analysis, justifying such an expense is crucial. Our experiment provides this justi- fication, showing the value of including manifestations in enhancing the learning process. Finally, we also demonstrate the effectiveness of Ma- niNeg through ablation study. Table 6 shows the results without a specific hard negative sample selection (i.e., weighting hard neg",
    "full_text_length": 101240,
    "chunk_length": 1391
  },
  {
    "chunk_id": 4094,
    "paper_filename": "xin_2020_mammography_interpretatiaon_framework.pdf",
    "paper_title": "Xin 2020 Mammography Interpretatiaon Framework",
    "chunk_index": 49,
    "total_chunks": 98,
    "text_content": "pairs. Follow the afore- mentioned protocols, we use the image branches from these models and apply the three experimental protocols (LE, LP , FT) across the three datasets, with results detailed in Table 5. The results indicate that comparative methods exhibit less optimal classification performance compared to ViKL. This disparity can partly be attributed to the differing JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 10 TABLE 5: Comparative AUC percentages (%) across the MVKL, CBIS",
    "full_text_length": 101240,
    "chunk_length": 1298
  },
  {
    "chunk_id": 4095,
    "paper_filename": "xin_2020_mammography_interpretatiaon_framework.pdf",
    "paper_title": "Xin 2020 Mammography Interpretatiaon Framework",
    "chunk_index": 50,
    "total_chunks": 98,
    "text_content": "and fine-tuning, respectively. The top-performing results in each category are highlighted in bold for easy reference. MVKL CBIS-DDSM INbreast Methods LP LE FT LP LE FT LP LE FT Mean IM 59.64 \u00b11.76 71.42 \u00b11.04 72.74 \u00b11.51 59.97 \u00b11.24 67.04 \u00b11.11 75.52\u00b11.15 55.68\u00b13.32 52.08 \u00b18.06 73.90 \u00b14.85 65.33 CLIP [11] 51.60 \u00b10.55 66.57 \u00b10.66 75.70 \u00b11.20 57.57 \u00b10.52 64.75 \u00b10.17 73.02 \u00b11.23 52.58 \u00b13.61 57.77 \u00b17.52 74.62 \u00b15.48 63.79 MedCLIP [57] 52.90 \u00b10.92 65.27 \u00b10.26 70.03 \u00b11.12 55.90 \u00b11.41 61.85 \u00b10.33 72.41",
    "full_text_length": 101240,
    "chunk_length": 1207
  },
  {
    "chunk_id": 4096,
    "paper_filename": "xin_2020_mammography_interpretatiaon_framework.pdf",
    "paper_title": "Xin 2020 Mammography Interpretatiaon Framework",
    "chunk_index": 51,
    "total_chunks": 98,
    "text_content": "61.57 \u00b11.79 59.07 BEiT3 [58] 57.46 \u00b11.77 70.94 \u00b10.58 75.33 \u00b11.40 59.05 \u00b10.90 62.88 \u00b10.42 69.65 \u00b11.53 60.18 \u00b11.92 69.98 \u00b10.73 78.58 \u00b13.86 67.11 MedKLIP [59] 52.55 \u00b12.09 67.14 \u00b10.64 70.83 \u00b11.95 53.95 \u00b10.77 61.35 \u00b10.37 70.53 \u00b11.77 52.46 \u00b12.85 57.33 \u00b13.48 69.10 \u00b14.15 61.69 PubMedCLIP [60] 60.74 \u00b12.60 71.26 \u00b10.53 70.41 \u00b14.06 61.89 \u00b10.99 64.63 \u00b10.32 63.72 \u00b11.22 64.96\u00b15.47 66.52\u00b15.45 69.37 \u00b14.40 65.94 BiomedCLIP [61] 60.54 \u00b10.73 73.62 \u00b10.66 75.95 \u00b11.30 62.03 \u00b11.00 68.54 \u00b10.67 68.86 \u00b10.78 61.98 \u00b10.20 74",
    "full_text_length": 101240,
    "chunk_length": 1236
  },
  {
    "chunk_id": 4097,
    "paper_filename": "xin_2020_mammography_interpretatiaon_framework.pdf",
    "paper_title": "Xin 2020 Mammography Interpretatiaon Framework",
    "chunk_index": 52,
    "total_chunks": 98,
    "text_content": "75.32 \u00b10.67 63.21 \u00b11.01 65.76 \u00b10.40 72.15 \u00b10.82 58.31 \u00b12.52 72.15 \u00b13.72 76.52 \u00b13.71 68.12 MedKLIP-M 57.12 \u00b11.04 69.43 \u00b10.74 74.49 \u00b10.95 56.85 \u00b10.97 62.55 \u00b10.77 73.10 \u00b11.12 54.31 \u00b13.12 66.53 \u00b14.28 72.57 \u00b13.04 65.22 PubMedCLIP-M 60.36 \u00b11.15 72.17 \u00b10.74 75.13 \u00b10.68 62.16 \u00b10.75 65.75 \u00b10.64 70.21 \u00b11.01 62.15 \u00b14.47 67.15 \u00b14.43 71.53 \u00b13.34 67.40 BiomedCLIP-M 61.66 \u00b11.10 74.15 \u00b10.76 76.13 \u00b10.57 63.64 \u00b10.68 67.98 \u00b10.49 71.84 \u00b11.02 59.62 \u00b13.46 75.15 \u00b13.75 78.69 \u00b14.12 69.87 ViKL-R 61.16 \u00b12.04 72.50 \u00b10.27 7",
    "full_text_length": 101240,
    "chunk_length": 1247
  },
  {
    "chunk_id": 4098,
    "paper_filename": "xin_2020_mammography_interpretatiaon_framework.pdf",
    "paper_title": "Xin 2020 Mammography Interpretatiaon Framework",
    "chunk_index": 53,
    "total_chunks": 98,
    "text_content": "negative sample selection. In the HN section, \u25b3indicates the method proposed by Robinson et al. [52], while \u2713indicates ManiNeg. Other abbreviations used in this table are consistent with those in Table 5. MVKL CBIS-DDSM INbreast I M T LS HN LP LE FT LP LE FT LP LE FT Mean \u2713 \u2713 55.82\u00b10.83 70.38 \u00b10.96 75.45 \u00b10.39 62.54 \u00b10.40 66.50 \u00b10.13 71.00 \u00b10.40 50.31 \u00b12.61 59.90 \u00b19.03 76.30 \u00b11.44 65.35 \u2713 \u2713 \u2713 61.69\u00b11.41 71.24 \u00b10.43 76.38 \u00b10.26 60.80 \u00b10.71 65.21 \u00b10.32 69.59 \u00b10.98 65.08 \u00b12.15 63.66 \u00b14.00 78.72 \u00b11.",
    "full_text_length": 101240,
    "chunk_length": 1099
  },
  {
    "chunk_id": 4099,
    "paper_filename": "xin_2020_mammography_interpretatiaon_framework.pdf",
    "paper_title": "Xin 2020 Mammography Interpretatiaon Framework",
    "chunk_index": 54,
    "total_chunks": 98,
    "text_content": "59.10 \u00b12.21 67.93 \u00b13.91 80.22 \u00b11.21 68.48 \u2713 \u2713 \u2713 \u2713 \u25b3 61.24\u00b11.84 75.56 \u00b10.44 77.46 \u00b10.53 65.58 \u00b10.66 68.24 \u00b10.49 72.48 \u00b11.12 64.11 \u00b10.63 68.68 \u00b11.78 79.16 \u00b13.60 70.28 \u2713 \u2713 \u2713 \u2713 \u2713 63.81\u00b11.51 75.14 \u00b10.53 79.06 \u00b10.53 64.59 \u00b11.02 70.94 \u00b10.35 72.52 \u00b10.81 64.55 \u00b12.49 75.86 \u00b10.81 79.47 \u00b12.15 71.77 nature of the data used during pretraining. Comparative methods often focus on extracting representations of the salient objects in the images, while ViKL leverages text and manifestations that are finely tuned t",
    "full_text_length": 101240,
    "chunk_length": 1276
  },
  {
    "chunk_id": 4100,
    "paper_filename": "xin_2020_mammography_interpretatiaon_framework.pdf",
    "paper_title": "Xin 2020 Mammography Interpretatiaon Framework",
    "chunk_index": 55,
    "total_chunks": 98,
    "text_content": "), and combined it with imaging reports to form the text modality. The results of downstream task evaluations for such models are marked with a -Msuffix in Table 5. It can be observed that after pretraining with MVKL, themodels showed improvements in their capability of extract- ing mammographic representations. ViKL, which utilizes a dedicated manifestation encoder in conjunction with label smoothing and ManiNeg mechanism, demonstrated the best feature extraction capability. 5.6.4 Comparison wi",
    "full_text_length": 101240,
    "chunk_length": 1337
  },
  {
    "chunk_id": 4101,
    "paper_filename": "xin_2020_mammography_interpretatiaon_framework.pdf",
    "paper_title": "Xin 2020 Mammography Interpretatiaon Framework",
    "chunk_index": 56,
    "total_chunks": 98,
    "text_content": "determine whether they JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 11 contained malignant breast lumps. To provide a more di- versified comparison, the two radiologists were asked to read the mammograms in different scenarios, henceforth referred to as Radiologist A and Radiologist B. Radiologist A was tasked with judging the nature of the breast lumps as accurately as possible, while Radiologist B was instructed to record results based on first impressions and to make diagnosis ab",
    "full_text_length": 101240,
    "chunk_length": 1239
  },
  {
    "chunk_id": 4102,
    "paper_filename": "xin_2020_mammography_interpretatiaon_framework.pdf",
    "paper_title": "Xin 2020 Mammography Interpretatiaon Framework",
    "chunk_index": 57,
    "total_chunks": 98,
    "text_content": "65.17 Radiologist B 5.935s - 39.68 97.44 56.55 ViKL 0.005s 0.87 40.24 96.62 56.13 ViKL 0.005s 0.74 58.50 86.12 67.76 ViKL 0.005s 0.50 89.70 45.33 76.46 0.0 0.2 0.4 0.6 0.8 1.0 False Positive Rate0.00.20.40.60.81.0True Positive Rate ViKL ROC (area = 0.79) Radiologist A Radiologist B Fig. 3: The ROC curve for ViKL, along with the metrics from two radiologists. When diagnosing with single-view mammograms, their classification performance is comparable, but the probabilistic outputs from ViKL provid",
    "full_text_length": 101240,
    "chunk_length": 1324
  },
  {
    "chunk_id": 4103,
    "paper_filename": "xin_2020_mammography_interpretatiaon_framework.pdf",
    "paper_title": "Xin 2020 Mammography Interpretatiaon Framework",
    "chunk_index": 58,
    "total_chunks": 98,
    "text_content": "distinct advantage in terms of time cost for analyzing large datasets, such as retrospective analyses of historical data. Secondly, diagnostic habits of radiologists are shaped over years of learning and practice, implying a unchanging preference forsensitivity and specificity. For example, in our experiments, radiologists tend to diagnose images as benign in the ab- sence of sufficient evidence, leading to higher specificity and lower sensitivity. Since ViKL outputs probabilities of benignity o",
    "full_text_length": 101240,
    "chunk_length": 1427
  },
  {
    "chunk_id": 4104,
    "paper_filename": "xin_2020_mammography_interpretatiaon_framework.pdf",
    "paper_title": "Xin 2020 Mammography Interpretatiaon Framework",
    "chunk_index": 59,
    "total_chunks": 98,
    "text_content": "lumps. This also means that there is significant potential for further improvement in the model\u2019s perfor- mance. For instance, ViKL could leverage transfer learning techniques to transfer weights to models that involve mul- tiple mammographic views or lump localization, thereby further optimizing the model\u2019s usability and performance. It is noteworthy that diagnosing breast lumps with a single-view mammogram is a challenging task. This chal- lenge arises from factors such as overlapping breast t",
    "full_text_length": 101240,
    "chunk_length": 1404
  },
  {
    "chunk_id": 4105,
    "paper_filename": "xin_2020_mammography_interpretatiaon_framework.pdf",
    "paper_title": "Xin 2020 Mammography Interpretatiaon Framework",
    "chunk_index": 60,
    "total_chunks": 98,
    "text_content": "disparate modalities within a shared space unlocks the potential for complex tasks like retrieval and generation across these modalities. However, the relatively small scale of the MVKL dataset imposes a limitation, resulting in a representation space that is sparse. This sparsity potentially hampers the robustness and practicality of intermodal inter- actions. Nonetheless, even within this limited representation space, we can still illustrate the existence of meaningful cor- relations between m",
    "full_text_length": 101240,
    "chunk_length": 1456
  },
  {
    "chunk_id": 4106,
    "paper_filename": "xin_2020_mammography_interpretatiaon_framework.pdf",
    "paper_title": "Xin 2020 Mammography Interpretatiaon Framework",
    "chunk_index": 61,
    "total_chunks": 98,
    "text_content": "the image representation, mirroring the distance metric used in pretraining. The structured and discrete attributes of manifestations contribute to their more sparse distribution in the represen- tation space, compared to other modalities. To counteract this and achieve robust retrieval outcomes, we implement a distance threshold strategy. This involves selecting nearby manifestations relative to the input image and determin- ing the classification result based on the proportion of JOURNAL OF LA",
    "full_text_length": 101240,
    "chunk_length": 1438
  },
  {
    "chunk_id": 4107,
    "paper_filename": "xin_2020_mammography_interpretatiaon_framework.pdf",
    "paper_title": "Xin 2020 Mammography Interpretatiaon Framework",
    "chunk_index": 62,
    "total_chunks": 98,
    "text_content": "regarded as positive. Tdistance Tclassification Accuracy Sensitivity Specificity 0.4 0.15 72.72 73.30 72.62 0.4 0.12 66.33 80.55 63.81 0.4 0.25 82.80 50.21 88.57 Image-Report Retrieval. In this experiment, we compute the results by identifying the single nearest neighbor report in the representation space, leveraging the relatively dense distribution of reports. This process involves pre-computing representations for all reports in the training set and then locating the nearest neighbors within ",
    "full_text_length": 101240,
    "chunk_length": 1204
  },
  {
    "chunk_id": 4108,
    "paper_filename": "xin_2020_mammography_interpretatiaon_framework.pdf",
    "paper_title": "Xin 2020 Mammography Interpretatiaon Framework",
    "chunk_index": 63,
    "total_chunks": 98,
    "text_content": "retrieval task. 0 1 2 3 4 5 Ground-Truth BI-RADS0 1 2 3 4 5Retrieved BI-RADS0 0 1 2 1 0 0 3 6 8 1 1 0 6 11 28 6 0 0 6 30 122 21 0 0 2 5 37 408 28 0 0 0 3 33 5 Fig. 4: Confusion matrix of image-report retrieval experiment. The confusion matrix is depicted in Fig. 4. Notably, the retrieval results predominantly cluster near the diagonal, indicating that ViKL effectively matches images with reports through multimodal contrastive learning. 5.8 ViKL Reduces the Data Requirement in Fine-tuning A key g",
    "full_text_length": 101240,
    "chunk_length": 1151
  },
  {
    "chunk_id": 4109,
    "paper_filename": "xin_2020_mammography_interpretatiaon_framework.pdf",
    "paper_title": "Xin 2020 Mammography Interpretatiaon Framework",
    "chunk_index": 64,
    "total_chunks": 98,
    "text_content": "subsets to assess how ViKL and the supervised IM model adapt to varying data availability in downstream tasks. The outcomes of these experiments are illustrated in Fig. 5. As anticipated, ViKL demonstrates a markedly lower sensitivity to reductions in training data volume compared to the supervised IM model. This finding underscores ViKL\u2019s efficiency in leveraging limited data for training. Moreover, even during pretraining, ViKL exhibits an ability to reduce data requirements. As discussed earl",
    "full_text_length": 101240,
    "chunk_length": 1355
  },
  {
    "chunk_id": 4110,
    "paper_filename": "xin_2020_mammography_interpretatiaon_framework.pdf",
    "paper_title": "Xin 2020 Mammography Interpretatiaon Framework",
    "chunk_index": 65,
    "total_chunks": 98,
    "text_content": "fine-tuning on the MKVL dataset with reduced train- ing data, illustrating less performance reduction as the availability of downstream data decreases. 5.9 ViKL Provides Evidence for Clinical Decision In our earlier sections, we discussed how ViKL enhances the image branch\u2019s capability for detailed feature extraction by using text and manifestations that describe localized lumps. To directly observe this enhanced feature extraction ability, we utilize the class activation map (CAM) [62], a commo",
    "full_text_length": 101240,
    "chunk_length": 1386
  },
  {
    "chunk_id": 4111,
    "paper_filename": "xin_2020_mammography_interpretatiaon_framework.pdf",
    "paper_title": "Xin 2020 Mammography Interpretatiaon Framework",
    "chunk_index": 66,
    "total_chunks": 98,
    "text_content": "of small lesions, highlighting its refined ability to capture minute features in medical imaging. 5.10 ViKL Enhances Confidence Calibration Guo et al. [49] delves into the critical issue of confidence calibration in neural networks, emphasizing that an ideal classifier\u2019s predicted confidence should accurately reflect the actual likelihood of a correct prediction. They noted that modern neural networks, despite improved classifi- cation metrics, often suffer from miscalibrated confidence estimate",
    "full_text_length": 101240,
    "chunk_length": 1426
  },
  {
    "chunk_id": 4112,
    "paper_filename": "xin_2020_mammography_interpretatiaon_framework.pdf",
    "paper_title": "Xin 2020 Mammography Interpretatiaon Framework",
    "chunk_index": 67,
    "total_chunks": 98,
    "text_content": "method outlined by Guo et al. [49], we assess calibration by categorizing results into bins based on predicted confidence, and then calculating classification accuracy within each bin. This approach allows us to create reliability diagrams , where a line graph plots confidence against accuracy. Additionally, we calculate the expected calibration error (ECE) by weighting the absolute difference between confidence and accuracy in each bin by the bin\u2019s sample size. Ideally, in these diagrams, the p",
    "full_text_length": 101240,
    "chunk_length": 1372
  },
  {
    "chunk_id": 4113,
    "paper_filename": "xin_2020_mammography_interpretatiaon_framework.pdf",
    "paper_title": "Xin 2020 Mammography Interpretatiaon Framework",
    "chunk_index": 68,
    "total_chunks": 98,
    "text_content": "stability. The resulting reliability diagrams and ECEs are illustrated in Fig. 7. The analysis reveals that multimodal contrastive learning inherently improves model calibration. This may stem from the limitations of IM in building a semantically meaningful representation space using one-hot labels in supervised training, which undermines the relia- bility of confidence scores. Conversely, integrating text and manifestations appears to alleviate this limitation. Further enhancing this, label smo",
    "full_text_length": 101240,
    "chunk_length": 1465
  },
  {
    "chunk_id": 4114,
    "paper_filename": "xin_2020_mammography_interpretatiaon_framework.pdf",
    "paper_title": "Xin 2020 Mammography Interpretatiaon Framework",
    "chunk_index": 69,
    "total_chunks": 98,
    "text_content": "be proxi- mal in the representation space, while uniformity suggests that representations should be evenly dispersed throughoutthe space to maximize information preservation. Balancing these two, often conflicting criteria, is crucial in the realm of representation learning. Adopting the methodology of Wang and Isola [48], we aim for an intuitive understanding of these properties. We configured ViKL\u2019s representation dimension to 2 (i.e. z\u2208S2) and trained it using both multimodal contrastive and ",
    "full_text_length": 101240,
    "chunk_length": 1407
  },
  {
    "chunk_id": 4115,
    "paper_filename": "xin_2020_mammography_interpretatiaon_framework.pdf",
    "paper_title": "Xin 2020 Mammography Interpretatiaon Framework",
    "chunk_index": 70,
    "total_chunks": 98,
    "text_content": "the direct train- ing objective, showcases superior alignment. Additionally, its representations are more uniformly distributed across the space, indicating a richer preservation of information from the input images and thus enhanced adaptability and robustness in downstream tasks. Beyond a general visual- ization of the entire test set, we also conducted separate vi- sualizations for samples classified as benign and malignant. Intriguingly, despite benign-malignant classification not be- ing a ",
    "full_text_length": 101240,
    "chunk_length": 1420
  },
  {
    "chunk_id": 4116,
    "paper_filename": "xin_2020_mammography_interpretatiaon_framework.pdf",
    "paper_title": "Xin 2020 Mammography Interpretatiaon Framework",
    "chunk_index": 71,
    "total_chunks": 98,
    "text_content": "like BERT and a diverse data set, is challenging in the medical field. This often results in text features incorporating noise that can hinder representation learning. Additionally, the issue of a single report corresponding to multiple images, albeit mitigated by label smoothing, still presents a challenge for contrastive learning. Manifestations effectively address these concerns. Each dimension of a manifestation is closely linked to specific lesion characteristics, reducing potential noise. ",
    "full_text_length": 101240,
    "chunk_length": 1433
  },
  {
    "chunk_id": 4117,
    "paper_filename": "xin_2020_mammography_interpretatiaon_framework.pdf",
    "paper_title": "Xin 2020 Mammography Interpretatiaon Framework",
    "chunk_index": 72,
    "total_chunks": 98,
    "text_content": "annotating manifestations could be seamlessly incorporated into routine assessments, possibly through a simple check- box tool. However, depending solely on manifestations has JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 14 Fig. 6: Exemplary Grad-CAM visualizations from ViKL. The first row displays the original mammogram images, while the second row presents the corresponding Grad-CAMs generated from layer 3 in ViKL. In these second-row images, breast lumps, as annotated by professi",
    "full_text_length": 101240,
    "chunk_length": 1503
  },
  {
    "chunk_id": 4118,
    "paper_filename": "xin_2020_mammography_interpretatiaon_framework.pdf",
    "paper_title": "Xin 2020 Mammography Interpretatiaon Framework",
    "chunk_index": 73,
    "total_chunks": 98,
    "text_content": "particularly in limiting the flexibility of infor- mation extraction. In our comparative analysis across three datasets, ViKL consistently outperformed models pretrained on ImageNet. This is particularly noteworthy considering the scale of data used: ViKL was trained on about two thousand samples, while ImageNet pretraining involved million-level labeled images; ViKL operates under an unsupervised learning paradigm, relying solely on pairing information without the need for pathology labels, whe",
    "full_text_length": 101240,
    "chunk_length": 1471
  },
  {
    "chunk_id": 4119,
    "paper_filename": "xin_2020_mammography_interpretatiaon_framework.pdf",
    "paper_title": "Xin 2020 Mammography Interpretatiaon Framework",
    "chunk_index": 74,
    "total_chunks": 98,
    "text_content": "are defined for specific organs or diseases, which restricts ViKL from being entirely task-agnostic. In an era that foun- dation models are increasingly prevalent, possessing task- agnostic capabilities is highly advantageous. It facilitates seamless adaptation across diverse datasets and paves the way for the development of general models capable of zero- shot learning across tasks. However, this challenge is not unmanageable. Considering the high information density of tabular data and the fin",
    "full_text_length": 101240,
    "chunk_length": 1416
  },
  {
    "chunk_id": 4120,
    "paper_filename": "xin_2020_mammography_interpretatiaon_framework.pdf",
    "paper_title": "Xin 2020 Mammography Interpretatiaon Framework",
    "chunk_index": 75,
    "total_chunks": 98,
    "text_content": "annotating manifestations is noteworthy. ViKL was trained with just over two thousand complete trimodal data pairs, yet it outperforms models pretrained on ImageNet. There\u2019s room for improvement as data volumes increase. Clinically, many image-report pairs exist, and we can infer manifestations from these to expand our dataset. This expansion could further train and enhance ViKL, leveraging clinical data abundance for greater performance. 3) While our primary focus has been on unimodal image JOU",
    "full_text_length": 101240,
    "chunk_length": 1578
  },
  {
    "chunk_id": 4121,
    "paper_filename": "xin_2020_mammography_interpretatiaon_framework.pdf",
    "paper_title": "Xin 2020 Mammography Interpretatiaon Framework",
    "chunk_index": 76,
    "total_chunks": 98,
    "text_content": "0.5 1.01.0 0.5 0.00.51.0Uniformity Feature Distribution 1.0 0.5 0.0 0.5 1.01.0 0.5 0.00.51.0Benign Feature Distribution 1.0 0.5 0.0 0.5 1.01.0 0.5 0.00.51.0Malignant Feature Distribution /2 0/2 Angle0.000.250.50DensityAngular Distribution /2 0/2 Angle0.000.250.50DensityAngular Distribution /2 0/2 Angle0.000.250.50DensityAngular Distribution Fig. 8: Alignment and uniformity analyses of the representation spaces. Panel (a) illustrates the model trained via supervised learning, and panel (b) depict",
    "full_text_length": 101240,
    "chunk_length": 1549
  },
  {
    "chunk_id": 4122,
    "paper_filename": "xin_2020_mammography_interpretatiaon_framework.pdf",
    "paper_title": "Xin 2020 Mammography Interpretatiaon Framework",
    "chunk_index": 77,
    "total_chunks": 98,
    "text_content": "applications of multimodal pretrained models are vast and extend well beyond this scope. We can delve into more intricate mul- timodal tasks, such as inter-modality generation and re- trieval. Particularly intriguing is the possibility of gener- ating more comprehensive, reliable, and information-dense reports by transitioning from images to manifestations, and ultimately to detailed reports. 4) Although ViKL has showcased implicit capabilities in lump localization with full mammography images a",
    "full_text_length": 101240,
    "chunk_length": 1469
  },
  {
    "chunk_id": 4123,
    "paper_filename": "xin_2020_mammography_interpretatiaon_framework.pdf",
    "paper_title": "Xin 2020 Mammography Interpretatiaon Framework",
    "chunk_index": 78,
    "total_chunks": 98,
    "text_content": "The manifestation-driven reasoning, particularly in- tegrating probabilistic graphical models, like Bayesian net- works, with deep learning for intelligent inference modelsthat more closely align with the reasoning and decision- making processes of imaging experts. ViKL paves the way for future studies that integrate data-driven and knowledge-driven approaches in medical AI, offering extensive opportunities for advancing the field. 7 C ONCLUSION We have introduced ViKL, an innovative multimodal ",
    "full_text_length": 101240,
    "chunk_length": 1496
  },
  {
    "chunk_id": 4124,
    "paper_filename": "xin_2020_mammography_interpretatiaon_framework.pdf",
    "paper_title": "Xin 2020 Mammography Interpretatiaon Framework",
    "chunk_index": 79,
    "total_chunks": 98,
    "text_content": "is often a challenging and resource-intensive endeavor. Benefiting from its capability to construct a high-quality representation space, ViKL effectively calibrates the model\u2019s accuracy and confidence, significantly reducing the risk JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 16 of over-confidence. Additionally, ViKL achieves a feature space characterized by excellent alignment and uniformity. This well-structured representation space is fundamental to ViKL\u2019s success in downstream",
    "full_text_length": 101240,
    "chunk_length": 1367
  },
  {
    "chunk_id": 4125,
    "paper_filename": "xin_2020_mammography_interpretatiaon_framework.pdf",
    "paper_title": "Xin 2020 Mammography Interpretatiaon Framework",
    "chunk_index": 80,
    "total_chunks": 98,
    "text_content": "to the advance- ment of medical AI, with the ultimate goal of improving healthcare outcomes. ViKL thus stands as a testament to the power of integrating diverse data modalities and a beacon for future innovations in medical imaging and analysis. REFERENCES [1] A. G. Waks and E. P . Winer, \u201cBreast cancer treatment: a review,\u201d Jama , vol. 321, no. 3, pp. 288\u2013300, 2019. [2] F. F. Ting, Y. J. Tan, and K. S. Sim, \u201cConvolutional neural network improvement for breast cancer classification,\u201d Expert Syst",
    "full_text_length": 101240,
    "chunk_length": 1185
  },
  {
    "chunk_id": 4126,
    "paper_filename": "xin_2020_mammography_interpretatiaon_framework.pdf",
    "paper_title": "Xin 2020 Mammography Interpretatiaon Framework",
    "chunk_index": 81,
    "total_chunks": 98,
    "text_content": "A. Alhudhaif, and F. Alenezi, \u201cDeconv-transformer (dect): A histopathological image classifica- tion model for breast cancer based on color deconvolution and transformer architecture,\u201d Information Sciences , vol. 608, pp. 1093\u2013 1112, 2022. [5] Y. Mo, C. Han, Y. Liu, M. Liu, Z. Shi, J. Lin, B. Zhao, C. Huang, B. Qiu, Y. Cui et al. , \u201cHover-trans: Anatomy-aware hover- transformer for roi-free breast cancer diagnosis in ultrasound images,\u201d IEEE Transactions on Medical Imaging , 2023. [6] S. G. Arma",
    "full_text_length": 101240,
    "chunk_length": 1235
  },
  {
    "chunk_id": 4127,
    "paper_filename": "xin_2020_mammography_interpretatiaon_framework.pdf",
    "paper_title": "Xin 2020 Mammography Interpretatiaon Framework",
    "chunk_index": 82,
    "total_chunks": 98,
    "text_content": "Bagheri, and R. M. Summers, \u201cChestx-ray8: Hospital-scale chest x-ray database and benchmarks on weakly-supervised classification and localization of common thorax diseases,\u201d in Proceedings of the IEEE conference on computer vision and pattern recognition , 2017, pp. 2097\u20132106. [8] A. Mogadala, M. Kalimuthu, and D. Klakow, \u201cTrends in integra- tion of vision and language research: A survey of tasks, datasets, and methods,\u201d Journal of Artificial Intelligence Research , vol. 71, pp. 1183\u20131317, 2021.",
    "full_text_length": 101240,
    "chunk_length": 1323
  },
  {
    "chunk_id": 4128,
    "paper_filename": "xin_2020_mammography_interpretatiaon_framework.pdf",
    "paper_title": "Xin 2020 Mammography Interpretatiaon Framework",
    "chunk_index": 83,
    "total_chunks": 98,
    "text_content": "Multimedia , 2021, pp. 797\u2013806. [11] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agar- wal, G. Sastry, A. Askell, P . Mishkin, J. Clark et al. , \u201cLearning transferable visual models from natural language supervision,\u201d inInternational conference on machine learning . PMLR, 2021, pp. 8748\u20138763. [12] J. Li, R. Selvaraju, A. Gotmare, S. Joty, C. Xiong, and S. C. H. Hoi, \u201cAlign before fuse: Vision and language representation learning with momentum distillation,\u201d Advances in neural inform",
    "full_text_length": 101240,
    "chunk_length": 1272
  },
  {
    "chunk_id": 4129,
    "paper_filename": "xin_2020_mammography_interpretatiaon_framework.pdf",
    "paper_title": "Xin 2020 Mammography Interpretatiaon Framework",
    "chunk_index": 84,
    "total_chunks": 98,
    "text_content": "eralized radiograph representation learning via cross-supervision between images and free-text radiology reports,\u201d Nature Machine Intelligence , vol. 4, no. 1, pp. 32\u201340, 2022. [15] N. Komodakis and S. Gidaris, \u201cUnsupervised representation learning by predicting image rotations,\u201d in International conference on learning representations (ICLR) , 2018. [16] L. Chen, P . Bentley, K. Mori, K. Misawa, M. Fujiwara, and D. Rueckert, \u201cSelf-supervised learning for medical image analysis using image contex",
    "full_text_length": 101240,
    "chunk_length": 1341
  },
  {
    "chunk_id": 4130,
    "paper_filename": "xin_2020_mammography_interpretatiaon_framework.pdf",
    "paper_title": "Xin 2020 Mammography Interpretatiaon Framework",
    "chunk_index": 85,
    "total_chunks": 98,
    "text_content": "Liang, \u201cModels genesis,\u201d Medical image analysis , vol. 67, p. 101840, 2021. [19] M. Noroozi and P . Favaro, \u201cUnsupervised learning of visual representations by solving jigsaw puzzles,\u201d in Computer Vision\u2013 ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part VI . Springer, 2016, pp. 69\u201384. [20] J. Zhu, Y. Li, Y. Hu, K. Ma, S. K. Zhou, and Y. Zheng, \u201cRubik\u2019s cube+: A self-supervised feature learning framework for 3d medi- cal image analysis,\u201d Medi",
    "full_text_length": 101240,
    "chunk_length": 1291
  },
  {
    "chunk_id": 4131,
    "paper_filename": "xin_2020_mammography_interpretatiaon_framework.pdf",
    "paper_title": "Xin 2020 Mammography Interpretatiaon Framework",
    "chunk_index": 86,
    "total_chunks": 98,
    "text_content": "2020, pp. 9729\u20139738. [23] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, \u201cBert: Pre- training of deep bidirectional transformers for language under- standing,\u201d arXiv preprint arXiv:1810.04805 , 2018. [24] S. Ren, K. He, R. Girshick, and J. Sun, \u201cFaster r-cnn: Towards real- time object detection with region proposal networks,\u201d Advances in neural information processing systems , vol. 28, 2015. [25] W. Kim, B. Son, and I. Kim, \u201cVilt: Vision-and-language transformer without convolution or region ",
    "full_text_length": 101240,
    "chunk_length": 1353
  },
  {
    "chunk_id": 4132,
    "paper_filename": "xin_2020_mammography_interpretatiaon_framework.pdf",
    "paper_title": "Xin 2020 Mammography Interpretatiaon Framework",
    "chunk_index": 87,
    "total_chunks": 98,
    "text_content": ", 2021. [28] J. Li, D. Li, C. Xiong, and S. Hoi, \u201cBlip: Bootstrapping language- image pre-training for unified vision-language understanding and generation,\u201d in International Conference on Machine Learning . PMLR, 2022, pp. 12 888\u201312 900. [29] J. Yu, Z. Wang, V . Vasudevan, L. Yeung, M. Seyedhosseini, and Y. Wu, \u201cCoca: Contrastive captioners are image-text foundation models,\u201d arXiv preprint arXiv:2205.01917 , 2022. [30] H. Bao, L. Dong, S. Piao, and F. Wei, \u201cBeit: Bert pre-training of image tran",
    "full_text_length": 101240,
    "chunk_length": 1241
  },
  {
    "chunk_id": 4133,
    "paper_filename": "xin_2020_mammography_interpretatiaon_framework.pdf",
    "paper_title": "Xin 2020 Mammography Interpretatiaon Framework",
    "chunk_index": 88,
    "total_chunks": 98,
    "text_content": "Neural Networks and Learning Systems , 2022. [33] Y. Xie, J. Zhang, L. Liu, H. Wang, Y. Ye, V . Johan, and Y. Xia, \u201cRefs: A hybrid pre-training paradigm for 3d medical image segmentation,\u201d Medical Image Analysis , p. 103023, 2023. JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 17 [34] Y. Xie, Y. Xia, J. Zhang, Y. Song, D. Feng, M. Fulham, and W. Cai, \u201cKnowledge-based collaborative deep learning for benign- malignant lung nodule classification on chest ct,\u201d IEEE transactions on medical",
    "full_text_length": 101240,
    "chunk_length": 1190
  },
  {
    "chunk_id": 4134,
    "paper_filename": "xin_2020_mammography_interpretatiaon_framework.pdf",
    "paper_title": "Xin 2020 Mammography Interpretatiaon Framework",
    "chunk_index": 89,
    "total_chunks": 98,
    "text_content": "nodules using 3d cnn-based multi-task learning,\u201d in Infor- mation Processing in Medical Imaging: 25th International Conference, IPMI 2017, Boone, NC, USA, June 25-30, 2017, Proceedings 25 . Springer, 2017, pp. 249\u2013260. [37] C. Pan, G. Zhao, J. Fang, B. Qi, J. Liu, C. Fang, D. Zhang, J. Li, and Y. Yu, \u201cComputer-aided tuberculosis diagnosis with attribute reasoning assistance,\u201d in Medical Image Computing and Computer Assisted Intervention\u2013MICCAI 2022: 25th International Conference, Singapore, Sept",
    "full_text_length": 101240,
    "chunk_length": 1377
  },
  {
    "chunk_id": 4135,
    "paper_filename": "xin_2020_mammography_interpretatiaon_framework.pdf",
    "paper_title": "Xin 2020 Mammography Interpretatiaon Framework",
    "chunk_index": 90,
    "total_chunks": 98,
    "text_content": "International Conference, Strasbourg, France, September 27\u2013October 1, 2021, Proceedings, Part II 24 . Springer, 2021, pp. 252\u2013261. [40] Y. Han, C. Chen, A. Tewfik, B. Glicksberg, Y. Ding, Y. Peng, and Z. Wang, \u201cCross-modal contrastive learning for abnormality classification and localization in chest x-rays with radiomics using a feedback loop,\u201d arXiv preprint arXiv:2104.04968 , 2021. [41] B. Gecer, S. Aksoy, E. Mercan, L. G. Shapiro, D. L. Weaver, and J. G. Elmore, \u201cDetection and classification ",
    "full_text_length": 101240,
    "chunk_length": 1243
  },
  {
    "chunk_id": 4136,
    "paper_filename": "xin_2020_mammography_interpretatiaon_framework.pdf",
    "paper_title": "Xin 2020 Mammography Interpretatiaon Framework",
    "chunk_index": 91,
    "total_chunks": 98,
    "text_content": "L. Huang, M. Han, Y. Zhang, and J. Ma, \u201cDeeplima: Deep learning based lesion identification in mammograms,\u201d in Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops , 2019, pp. 0\u2013 0. [44] Y. Liu, F. Zhang, C. Chen, S. Wang, Y. Wang, and Y. Yu, \u201cAct like a radiologist: towards reliable multi-view correspondence reasoning for mammogram mass detection,\u201d IEEE Transactions on Pattern Analysis and Machine Intelligence , vol. 44, no. 10, pp. 5947\u20135961, 2021. [45] M. Kallenbe",
    "full_text_length": 101240,
    "chunk_length": 1229
  },
  {
    "chunk_id": 4137,
    "paper_filename": "xin_2020_mammography_interpretatiaon_framework.pdf",
    "paper_title": "Xin 2020 Mammography Interpretatiaon Framework",
    "chunk_index": 92,
    "total_chunks": 98,
    "text_content": "IEEE conference on computer vision and pattern recognition , 2016, pp. 770\u2013778. [47] Y. Tian, D. Krishnan, and P . Isola, \u201cContrastive multiview coding,\u201d inComputer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part XI 16 . Springer, 2020, pp. 776\u2013794. [48] T. Wang and P . Isola, \u201cUnderstanding contrastive representation learning through alignment and uniformity on the hypersphere,\u201d inInternational Conference on Machine Learning . PMLR, 2020, pp. 9929\u2013",
    "full_text_length": 101240,
    "chunk_length": 1322
  },
  {
    "chunk_id": 4138,
    "paper_filename": "xin_2020_mammography_interpretatiaon_framework.pdf",
    "paper_title": "Xin 2020 Mammography Interpretatiaon Framework",
    "chunk_index": 93,
    "total_chunks": 98,
    "text_content": "on computer vision and pattern recognition , 2016, pp. 2818\u20132826. [52] J. Robinson, C.-Y. Chuang, S. Sra, and S. Jegelka, \u201cContrastive learning with hard negative samples,\u201d International Conference on Learning Representations , 2021. [53] E. D. Cubuk, B. Zoph, J. Shlens, and Q. V . Le, \u201cRandaugment: Practical automated data augmentation with a reduced search space,\u201d in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops , 2020, pp. 702\u2013703. [54] R. S. Lee,",
    "full_text_length": 101240,
    "chunk_length": 1240
  },
  {
    "chunk_id": 4139,
    "paper_filename": "xin_2020_mammography_interpretatiaon_framework.pdf",
    "paper_title": "Xin 2020 Mammography Interpretatiaon Framework",
    "chunk_index": 94,
    "total_chunks": 98,
    "text_content": "Kingma and J. Ba, \u201cAdam: A method for stochastic optimiza- tion,\u201d in International Conference on Learning Representations (ICLR) , San Diega, CA, USA, 2015. [57] Z. Wang, Z. Wu, D. Agarwal, and J. Sun, \u201cMedclip: Contrastive learning from unpaired medical images and text,\u201d in Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing , 2022, pp. 3876\u20133887. [58] W. Wang, H. Bao, L. Dong, J. Bjorck, Z. Peng, Q. Liu, K. Aggarwal, O. K. Mohammed, S. Singhal, S. Som et al. ",
    "full_text_length": 101240,
    "chunk_length": 1225
  },
  {
    "chunk_id": 4140,
    "paper_filename": "xin_2020_mammography_interpretatiaon_framework.pdf",
    "paper_title": "Xin 2020 Mammography Interpretatiaon Framework",
    "chunk_index": 95,
    "total_chunks": 98,
    "text_content": "372\u201321 383. [60] S. Eslami, C. Meinel, and G. De Melo, \u201cPubmedclip: How much does clip benefit visual question answering in the medical do- main?\u201d in Findings of the Association for Computational Linguistics: EACL 2023 , 2023, pp. 1181\u20131193. [61] S. Zhang, Y. Xu, N. Usuyama, J. Bagga, R. Tinn, S. Preston, R. Rao, M. Wei, N. Valluri, C. Wong, M. Lungren, T. Naumann, and H. Poon, \u201cLarge-scale domain-specific pretraining for biomedical vision-language processing,\u201d 2023. [Online]. Available: https:/",
    "full_text_length": 101240,
    "chunk_length": 1358
  },
  {
    "chunk_id": 4141,
    "paper_filename": "xin_2020_mammography_interpretatiaon_framework.pdf",
    "paper_title": "Xin 2020 Mammography Interpretatiaon Framework",
    "chunk_index": 96,
    "total_chunks": 98,
    "text_content": "618\u2013626. JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 18 APPENDIX A HYPERPARAMETERS Hyperparameters. Hyperparameters Value Pretraining Peak learning rate 1e-4 Minimal learning rate 1e-7 Learning rate schedule Cosine annealing Training steps 9000 Warmup steps 300 Batch size 64 Temperature \u03c41for img-mani and img-intra 0.03 Temperature \u03c42for img-text and mani-text 0.3 Weight decay 1e-4 Input resolution 2562 ManiNeg, maximum \u00b5 11 ManiNeg, minimum \u00b5 0 ManiNeg, \u03c3 3 ManiNeg, annealing sche",
    "full_text_length": 101240,
    "chunk_length": 1357
  },
  {
    "chunk_id": 4142,
    "paper_filename": "xin_2020_mammography_interpretatiaon_framework.pdf",
    "paper_title": "Xin 2020 Mammography Interpretatiaon Framework",
    "chunk_index": 97,
    "total_chunks": 98,
    "text_content": "probe L2regularization strength \u03bb 3.16 Maximum iteration 1000 \u2020During the training process, \u00b5starts from a maximum value of 11 and stops at a minimum value of 0 at the 50th training step, remaining unchanged thereafter. The decrease in \u00b5is linear. \u2021The learning rate on the pretrained parameters is 0.1 times that of the learning rate on the appended randomly initialized linear layer.APPENDIX B DETAILED NETWORK STRUCTURE \ud835\udc38\ud835\udc3c:ResNet50 \ud835\udc38\ud835\udc47:BERT \ud835\udc38\ud835\udc40:MLPsPretraining 512 768 512Image aligner Text aligner ",
    "full_text_length": 101240,
    "chunk_length": 858
  },
  {
    "chunk_id": 4143,
    "paper_filename": "xue_2024_breast_suspicious_microcalcifications_on_contrast_enhanced_mammograms_practice_and_reflection.pdf",
    "paper_title": "Xue 2024 Breast Suspicious Microcalcifications On Contrast Enhanced Mammograms Practice And Reflection",
    "chunk_index": 0,
    "total_chunks": 23,
    "text_content": "International Journal of General Medicine ISSN: 1178-7074 (Online) Journal homepage: www.tandfonline.com/journals/dijg20 Breast Suspicious Microcalci\ufb01cations on Contrast- Enhanced Mammograms: Practice and Re\ufb02ection Xue Zhao To cite this article: Xue Zhao (2025) Breast Suspicious Microcalci\ufb01cations on Contrast- Enhanced Mammograms: Practice and Re\ufb02ection, International Journal of General Medicine, , 273-280, DOI: 10.2147/IJGM.S494188 To link to this article: https://doi.org/10.2147/IJGM.S494188 \u00a9",
    "full_text_length": 28160,
    "chunk_length": 1650
  },
  {
    "chunk_id": 4144,
    "paper_filename": "xue_2024_breast_suspicious_microcalcifications_on_contrast_enhanced_mammograms_practice_and_reflection.pdf",
    "paper_title": "Xue 2024 Breast Suspicious Microcalcifications On Contrast Enhanced Mammograms Practice And Reflection",
    "chunk_index": 1,
    "total_chunks": 23,
    "text_content": "200011, People\u2019s Republic of China, T el +86 17612189294, Email zhaoxue@alumni.sjtu.edu.cn Purpose: To evaluate the use of contrast enhanced mammography (CEM) in suspicious microcalcifications and to discuss strategies to cope with its diagnostic limitations. Methods: We retrospectively evaluated patients with suspicious calcifications who underwent CEM at our institution. We collected and analyzed morphological findings, enhancement patterns and pathological findings of suspicious microcalcific",
    "full_text_length": 28160,
    "chunk_length": 1421
  },
  {
    "chunk_id": 4145,
    "paper_filename": "xue_2024_breast_suspicious_microcalcifications_on_contrast_enhanced_mammograms_practice_and_reflection.pdf",
    "paper_title": "Xue 2024 Breast Suspicious Microcalcifications On Contrast Enhanced Mammograms Practice And Reflection",
    "chunk_index": 2,
    "total_chunks": 23,
    "text_content": "in 9 (45%), and non-mass enhancement (NME) in 11 (55%). DCE-MRI was performed in 13 cases. One case of invasive ductal carcinoma (IDC) showed enhancement on MRI but was indeterminate on CEM due to the masking effect of background parenchymal enhancement (BPE), and one case of ductal carcinoma in situ (DCIS) lacked enhancement on CEM but had significant enhancement on MRI. Conclusion: CEM provides additional information on the enhancement associated with breast suspicious microcalcifications. It ",
    "full_text_length": 28160,
    "chunk_length": 1477
  },
  {
    "chunk_id": 4146,
    "paper_filename": "xue_2024_breast_suspicious_microcalcifications_on_contrast_enhanced_mammograms_practice_and_reflection.pdf",
    "paper_title": "Xue 2024 Breast Suspicious Microcalcifications On Contrast Enhanced Mammograms Practice And Reflection",
    "chunk_index": 3,
    "total_chunks": 23,
    "text_content": "malignancy risk of microcalcifications detected by digital mammography (DM). However, the positive predictive value (PPV) of microcalcifications assessed by mammography varies widely and is usually less than 30%.5 Dynamic contrast-enhanced breast magnetic resonance imaging (DCE-MRI) is the most sensitive imaging modality currently available.6 However, the disadvantage of MRI is the inadequate display of microcalcifications. Contrast- enhanced mammography (CEM) is a relatively new breast imaging ",
    "full_text_length": 28160,
    "chunk_length": 1550
  },
  {
    "chunk_id": 4147,
    "paper_filename": "xue_2024_breast_suspicious_microcalcifications_on_contrast_enhanced_mammograms_practice_and_reflection.pdf",
    "paper_title": "Xue 2024 Breast Suspicious Microcalcifications On Contrast Enhanced Mammograms Practice And Reflection",
    "chunk_index": 4,
    "total_chunks": 23,
    "text_content": "Limited. The full terms of this license are available at https://www.dovepress.com/terms.php and incorporate the Creative Commons Attribution \u2013 Non Commercial (unported, v3.0) License (http://creativecommons.org/licenses/by-nc/3.0/). By accessing the work you hereby accept the Terms. Non-commercial uses of the work are permitted without any further permission from Dove Medical Press Limited, provided the work is properly attributed. For permission for commercial use of this work, please see para",
    "full_text_length": 28160,
    "chunk_length": 1406
  },
  {
    "chunk_id": 4148,
    "paper_filename": "xue_2024_breast_suspicious_microcalcifications_on_contrast_enhanced_mammograms_practice_and_reflection.pdf",
    "paper_title": "Xue 2024 Breast Suspicious Microcalcifications On Contrast Enhanced Mammograms Practice And Reflection",
    "chunk_index": 5,
    "total_chunks": 23,
    "text_content": "than MRI.14 Therefore, to categorize them in more detail, this study divides the enhancement patterns into three categories: enhancement, no enhancement, and indeterminate, which may be more in line with practical applications. This study aimed to evaluate the use of CEM in suspicious microcalcifications and to discuss strategies to cope with its diagnostic limitations. Methods This study was approved by the Ethics Committee of Huangpu Branch, Shanghai Ninth People\u2019s Hospital, Shanghai Jiaotong ",
    "full_text_length": 28160,
    "chunk_length": 1417
  },
  {
    "chunk_id": 4149,
    "paper_filename": "xue_2024_breast_suspicious_microcalcifications_on_contrast_enhanced_mammograms_practice_and_reflection.pdf",
    "paper_title": "Xue 2024 Breast Suspicious Microcalcifications On Contrast Enhanced Mammograms Practice And Reflection",
    "chunk_index": 6,
    "total_chunks": 23,
    "text_content": "medical records in our hospital. Exclusion criteria were impaired renal function or a history of contrast allergy. CEM Examination CEM examinations were performed using the Senographe Pristina mammography system (GE Healthcare). Examination was standardized and performed with intermittent exposure to low and high energy during a single breast-compressed position after the injection of nonionic contrast medium (iodixanol 350 mg I/mL, Yangzijiang Pharmaceutical Co. Ltd) at a rate of 3 mL/s for a t",
    "full_text_length": 28160,
    "chunk_length": 1285
  },
  {
    "chunk_id": 4150,
    "paper_filename": "xue_2024_breast_suspicious_microcalcifications_on_contrast_enhanced_mammograms_practice_and_reflection.pdf",
    "paper_title": "Xue 2024 Breast Suspicious Microcalcifications On Contrast Enhanced Mammograms Practice And Reflection",
    "chunk_index": 7,
    "total_chunks": 23,
    "text_content": "by one of the radiologists. MRI Examination Out of the 46 cases included in our study, 13 of them were imaged using the 3.0 T MRI scanner (uMR780, China) with a dedicated breast coil was used for breast MR imaging. In our institution, 6 dynamic sequences are performed: 1 basal pre-contrast acquisition followed by 5 continuous acquisitions after contrast agent administration (0.1 mmol/kg at a flow rate of 2 mL/s) injected via an automated injector, followed by a saline flush (20 mL at the same fl",
    "full_text_length": 28160,
    "chunk_length": 1285
  },
  {
    "chunk_id": 4151,
    "paper_filename": "xue_2024_breast_suspicious_microcalcifications_on_contrast_enhanced_mammograms_practice_and_reflection.pdf",
    "paper_title": "Xue 2024 Breast Suspicious Microcalcifications On Contrast Enhanced Mammograms Practice And Reflection",
    "chunk_index": 8,
    "total_chunks": 23,
    "text_content": "46 lesions were collected with CEM examination from January 2022 to July 2024. The mean age of the patients was 51 years \u00b1 12 (standard deviation) (range, 26\u201370 years). The most common breast density was heterogeneously dense (50.0%, Table 1 ), and the most prevalent calcification type was pleomorphic calcification (37.0%, Table 1 ), followed by amorphous calcification (32.6%). BPE were prevalent in mild (39.1%) and moderate (28.3%). https: //doi.org/10.2147/IJGM.S494188 International Journal of",
    "full_text_length": 28160,
    "chunk_length": 1247
  },
  {
    "chunk_id": 4152,
    "paper_filename": "xue_2024_breast_suspicious_microcalcifications_on_contrast_enhanced_mammograms_practice_and_reflection.pdf",
    "paper_title": "Xue 2024 Breast Suspicious Microcalcifications On Contrast Enhanced Mammograms Practice And Reflection",
    "chunk_index": 9,
    "total_chunks": 23,
    "text_content": "and BI-RADS categories is shown in Table 2 . Of the 20 cases with enhancement of suspicious microcalcifications, 18 were malignant ( Figure 1 , Table 3 ) and 2 were benign. Of the 23 cases with no enhancement, 3 were malignant and 20 were benign ( Figure 2 , Table 3 ). Out of the 46 cases included in our study, 13 of them underwent DCE-MRI, of which 8 showed enhancement and 5 did not ( Table 3 ). One case of invasive ductal carcinoma (IDC) showed enhancement on MRI but was indeterminate on Table",
    "full_text_length": 28160,
    "chunk_length": 1398
  },
  {
    "chunk_id": 4153,
    "paper_filename": "xue_2024_breast_suspicious_microcalcifications_on_contrast_enhanced_mammograms_practice_and_reflection.pdf",
    "paper_title": "Xue 2024 Breast Suspicious Microcalcifications On Contrast Enhanced Mammograms Practice And Reflection",
    "chunk_index": 10,
    "total_chunks": 23,
    "text_content": "15(32.6%) Coarse heterogeneous 6(13.0%) Fine pleomorphic 17(37.0%) Fine linear/ fine linear branching 8(17.4%) Abbreviations : BPE, background parenchymal enhancement; NME, non-mass enhancement. Table 2 Distribution of Enhancement Patterns, Pathological Diagnosis, and BI-RADS Category T ypes Benign (n=24) DCIS (n=17) Invasive Carcinoma (n=5) BI-RADS categoryBI-RADS4B(n=38) 22(91.7%) 12(70.6%) 4(80.0%) BI-RADS4C(n=8) 2(8.3%) 5(29.4%) 1(20.0%) Enhancement presenceYES (n=20) 2(8.3%) 14(82.3%) 4(80.",
    "full_text_length": 28160,
    "chunk_length": 1462
  },
  {
    "chunk_id": 4154,
    "paper_filename": "xue_2024_breast_suspicious_microcalcifications_on_contrast_enhanced_mammograms_practice_and_reflection.pdf",
    "paper_title": "Xue 2024 Breast Suspicious Microcalcifications On Contrast Enhanced Mammograms Practice And Reflection",
    "chunk_index": 11,
    "total_chunks": 23,
    "text_content": "27.7% (N=5) were invasive carcinoma. The benign lesions that enhanced on CEM included one case of sclerosing adenopathy, one case of atypical ductal hyperplasia (ADH). 22 benign lesions that did not enhance, including proliferative and nonproliferative disease. Figure 1 A 36-year-old female. ( A)Mediolateral oblique (MLO) view of low energy image (LE) shows heterogeneously dense breast, and fine pleomorphic calcification (BI- RADS 4B) in the upper quadrant of the left breast (white circle). ( B)",
    "full_text_length": 28160,
    "chunk_length": 1407
  },
  {
    "chunk_id": 4155,
    "paper_filename": "xue_2024_breast_suspicious_microcalcifications_on_contrast_enhanced_mammograms_practice_and_reflection.pdf",
    "paper_title": "Xue 2024 Breast Suspicious Microcalcifications On Contrast Enhanced Mammograms Practice And Reflection",
    "chunk_index": 12,
    "total_chunks": 23,
    "text_content": "5(38.4%) Non-mass enhancement 1(16.7%) 2(28.6%) 3(23.1%) No enhancement 5(83.3%) 0 5(38.4%) https: //doi.org/10.2147/IJGM.S494188 International Journal of General Medicine 2025:18 276Zhao Powered by TCPDF (www.tcpdf.org) Figure 2 A 26-year-old female. ( A)Mediolateral oblique (MLO) view of low energy image (LE) shows heterogeneously dense breast, fine linear calcifications (BI-RADS 4C) in the upper quadrant of the right breast (white rectangle). ( B) Recombined image (RC) shows a mild level of b",
    "full_text_length": 28160,
    "chunk_length": 1457
  },
  {
    "chunk_id": 4156,
    "paper_filename": "xue_2024_breast_suspicious_microcalcifications_on_contrast_enhanced_mammograms_practice_and_reflection.pdf",
    "paper_title": "Xue 2024 Breast Suspicious Microcalcifications On Contrast Enhanced Mammograms Practice And Reflection",
    "chunk_index": 13,
    "total_chunks": 23,
    "text_content": "rectangle). ( C) Magnetic resonance imaging (MRI) shows non-mass enhancement in the same position of the left breast (white rectangle). The histological diagnosis was invasive ductal carcinoma. International Journal of General Medicine 2025:18 https: //doi.org/10.2147/IJGM.S494188 277Zhao Powered by TCPDF (www.tcpdf.org) Discussion Early detection of breast cancer depends to a large extent on the correct interpretation of suspicious microcalcifications. Particularly, most DCIS cases are identifi",
    "full_text_length": 28160,
    "chunk_length": 1354
  },
  {
    "chunk_id": 4157,
    "paper_filename": "xue_2024_breast_suspicious_microcalcifications_on_contrast_enhanced_mammograms_practice_and_reflection.pdf",
    "paper_title": "Xue 2024 Breast Suspicious Microcalcifications On Contrast Enhanced Mammograms Practice And Reflection",
    "chunk_index": 14,
    "total_chunks": 23,
    "text_content": "the masking effect of the BPE, and one IDC was confirmed by MRI to have enhancement at the corresponding site. Since DCE-MRI is the most sensitive imaging modality currently available, it can be inferred that BPE is related to false negative CEM exams. In addition, BPE may have a greater impact on CEM than MRI, because additional information is not available for CEM.14 Current research findings on CEM are discrepant. Depretto et al reported that the total of DCIS G1 lesions 100% (N=6) lacked enh",
    "full_text_length": 28160,
    "chunk_length": 1260
  },
  {
    "chunk_id": 4158,
    "paper_filename": "xue_2024_breast_suspicious_microcalcifications_on_contrast_enhanced_mammograms_practice_and_reflection.pdf",
    "paper_title": "Xue 2024 Breast Suspicious Microcalcifications On Contrast Enhanced Mammograms Practice And Reflection",
    "chunk_index": 15,
    "total_chunks": 23,
    "text_content": "regarding inconsistencies in CEM results. To the best of our knowledge, comparative CEM and MRI studies of breast suspicious microcalcifications are currently rare. According to a report specializing in DCIS, all DCIS lesions enhance on MRI, while on CEM, 5 calcified DCIS lesions were unenhanced in CEM,16 which also suggests that calcified DCIS that does not enhance on CEM may show enhancement on MRI. The demonstration of the presence or lack of enhancement of microcalcifications by MRI and CEM ",
    "full_text_length": 28160,
    "chunk_length": 1233
  },
  {
    "chunk_id": 4159,
    "paper_filename": "xue_2024_breast_suspicious_microcalcifications_on_contrast_enhanced_mammograms_practice_and_reflection.pdf",
    "paper_title": "Xue 2024 Breast Suspicious Microcalcifications On Contrast Enhanced Mammograms Practice And Reflection",
    "chunk_index": 16,
    "total_chunks": 23,
    "text_content": "of data from a large sample to draw definitive conclusions. Limitations of CEM are associated with false-positive and false-negative results. Benign lesions with blood vessels, infections or inflammation, benign skin lesions, and imaging artefacts may cause false-positive. False negatives may be caused by inadequate or complete incorporation of the lesion, the masking effect of BPE, and the absence of contrast enhancement in malignant tumours.7 In addition, some lesion features on the subtracted",
    "full_text_length": 28160,
    "chunk_length": 1338
  },
  {
    "chunk_id": 4160,
    "paper_filename": "xue_2024_breast_suspicious_microcalcifications_on_contrast_enhanced_mammograms_practice_and_reflection.pdf",
    "paper_title": "Xue 2024 Breast Suspicious Microcalcifications On Contrast Enhanced Mammograms Practice And Reflection",
    "chunk_index": 17,
    "total_chunks": 23,
    "text_content": "left breast (white circle). ( B) Recombined image (RC) shows a mild level of background parenchymal enhancement (BPE) and lack of enhancement at the area of the calcification. ( C) Sagittal magnetic resonance imaging (MRI) shows mass enhancement in the same position of the left breast (arrow). The histological diagnosis was ductal carcinoma in situ. https: //doi.org/10.2147/IJGM.S494188 International Journal of General Medicine 2025:18 278Zhao Powered by TCPDF (www.tcpdf.org) with regular menstr",
    "full_text_length": 28160,
    "chunk_length": 1315
  },
  {
    "chunk_id": 4161,
    "paper_filename": "xue_2024_breast_suspicious_microcalcifications_on_contrast_enhanced_mammograms_practice_and_reflection.pdf",
    "paper_title": "Xue 2024 Breast Suspicious Microcalcifications On Contrast Enhanced Mammograms Practice And Reflection",
    "chunk_index": 18,
    "total_chunks": 23,
    "text_content": "the level of enhancement is low according to newly published BI-RADS CEM (2022). Our study has several limitations. The number of patients was limited, and the number of cases with histological diagnoses might have selective bias. This is a preliminary analysis of the comparison of CEM and MRI for suspicious microcalcifications, and a large sample study is needed for further evaluation. In conclusion, CEM provides additional information on the enhancement associated with breast microcalcificatio",
    "full_text_length": 28160,
    "chunk_length": 1422
  },
  {
    "chunk_id": 4162,
    "paper_filename": "xue_2024_breast_suspicious_microcalcifications_on_contrast_enhanced_mammograms_practice_and_reflection.pdf",
    "paper_title": "Xue 2024 Breast Suspicious Microcalcifications On Contrast Enhanced Mammograms Practice And Reflection",
    "chunk_index": 19,
    "total_chunks": 23,
    "text_content": "mammography. J Xray Sci Technol . 2024 ;32(3):583\u2013596. doi:10.3233/XST-230332 2. Covington MF. Contrast-enhanced mammography implementation, performance, and use for supplemental breast cancer screening. Radiol Clin North Am . 2021 ;59(1):113\u2013128. doi:10.1016/j.rcl.2020.08.006 3. Kornecki A. Current status of contrast enhanced mammography: a comprehensive review. Can Assoc Radiol J . 2022 ;73(1):141\u2013156. doi:10.1177/ 08465371211029047 4. Spak DA, Plaxco JS, Santiago L, et al. BI-RADS(\u00ae) fifth ed",
    "full_text_length": 28160,
    "chunk_length": 1588
  },
  {
    "chunk_id": 4163,
    "paper_filename": "xue_2024_breast_suspicious_microcalcifications_on_contrast_enhanced_mammograms_practice_and_reflection.pdf",
    "paper_title": "Xue 2024 Breast Suspicious Microcalcifications On Contrast Enhanced Mammograms Practice And Reflection",
    "chunk_index": 20,
    "total_chunks": 23,
    "text_content": "Houben IP, Vanwetswinkel S, Kalia V , et al. Contrast-enhanced spectral mammography in the evaluation of breast suspicious calcifications: diagnostic accuracy and impact on surgical management. Acta Radiol . 2019 ;60(9):1110\u20131117. doi:10.1177/0284185118822639 9. Depretto C, Borelli A, Liguori A, et al. Contrast-enhanced mammography in the evaluation of breast calcifications: preliminary experience. Tumori . 2020 ;106(6):491\u2013496. doi:10.1177/0300891620919170 10. Depretto C, D\u2019ascoli E, Della Pepa",
    "full_text_length": 28160,
    "chunk_length": 1563
  },
  {
    "chunk_id": 4164,
    "paper_filename": "xue_2024_breast_suspicious_microcalcifications_on_contrast_enhanced_mammograms_practice_and_reflection.pdf",
    "paper_title": "Xue 2024 Breast Suspicious Microcalcifications On Contrast Enhanced Mammograms Practice And Reflection",
    "chunk_index": 21,
    "total_chunks": 23,
    "text_content": "Siegel A, et al. Machine learning assessment of background parenchymal enhancement in breast cancer and clinical applications: a literature review. Cancers . 2024 ;16(21):3681. doi:10.3390/cancers16213681 14. Wang S, Sun Y, You C, et al. Association of clinical factors and degree of early background parenchymal enhancement on contrast-enhanced mammography. AJR Am J Roentgenol . 2023 ;221(1):45\u201355. doi:10.2214/AJR.22.28769 15. Holland R, Hendriks JH. Microcalcifications associated with ductal car",
    "full_text_length": 28160,
    "chunk_length": 1542
  },
  {
    "chunk_id": 4165,
    "paper_filename": "xue_2024_breast_suspicious_microcalcifications_on_contrast_enhanced_mammograms_practice_and_reflection.pdf",
    "paper_title": "Xue 2024 Breast Suspicious Microcalcifications On Contrast Enhanced Mammograms Practice And Reflection",
    "chunk_index": 22,
    "total_chunks": 23,
    "text_content": "Liu F, Peacock S, et al. Background parenchymal enhancement on breast MRI: impact on diagnostic performance. AJR Am J Roentgenol . 2012 ;198(4):W373\u201380. doi:10.2214/AJR.10.6272 International Journal of General Medicine 2025:18 https: //doi.org/10.2147/IJGM.S494188 279Zhao Powered by TCPDF (www.tcpdf.org) International Journal of General Medicine Publish your work in this journal The International Journal of General Medicine is an international, peer-reviewed open-access journal that focuses on g",
    "full_text_length": 28160,
    "chunk_length": 1156
  },
  {
    "chunk_id": 4166,
    "paper_filename": "zhenjie_2024_mammoVLM_a_ generative_large_vision_language_model_for_mammography.pdf",
    "paper_title": "Zhenjie 2024 Mammovlm A  Generative Large Vision Language Model For Mammography",
    "chunk_index": 0,
    "total_chunks": 65,
    "text_content": "Contents lists available at ScienceDirect Information Fusion journal homepage: www.elsevier.com/locate/inffus MammoVLM: A generative large vision\u2013language model for mammography-related diagnostic assistance Zhenjie Caoa,b,Zhuo Denga,Jie Mad,Jintao Huc,d,Lan Maa,\u2217 aShenzhen International Graduate School, Tsinghua University, Shenzhen, 518055, PR China bAI Lab, Pingan Tech, Shenzhen, PR China cDepartment of Anatomical and Cellular Pathology, The Chinese University of Hong Kong, Hong Kong dRadiolog",
    "full_text_length": 64867,
    "chunk_length": 1431
  },
  {
    "chunk_id": 4167,
    "paper_filename": "zhenjie_2024_mammoVLM_a_ generative_large_vision_language_model_for_mammography.pdf",
    "paper_title": "Zhenjie 2024 Mammovlm A  Generative Large Vision Language Model For Mammography",
    "chunk_index": 1,
    "total_chunks": 65,
    "text_content": "There is a practical need for patients to have a diagnostic assistant for their follow-up Q&A regarding their mammography screening. We believe large vision\u2013language models have great potential to address this need. However, applying off-the-shelf large models directly in medical scenarios normally provides unsatisfactory results. In this work, we present MammoVLM, a large vision\u2013language model to assist patients with problems related to mammograms. MammoVLM has a sparse visual-MoE module that a",
    "full_text_length": 64867,
    "chunk_length": 1384
  },
  {
    "chunk_id": 4168,
    "paper_filename": "zhenjie_2024_mammoVLM_a_ generative_large_vision_language_model_for_mammography.pdf",
    "paper_title": "Zhenjie 2024 Mammovlm A  Generative Large Vision Language Model For Mammography",
    "chunk_index": 2,
    "total_chunks": 65,
    "text_content": "experimental results show that it has not only beaten other leading VLMs but also shows a professional capability similar to that of a junior radiologist. 1. Introduction According to WHO, breast cancer has surpassed lung cancer as the world\u2019s number one cancer in terms of morbidity and mortality in 2020 [1]. Mammography screening is the most cost-effective method for early detection of breast cancer, with approximately 48 million mammograms performed annually in the U.S. It has been reported th",
    "full_text_length": 64867,
    "chunk_length": 1358
  },
  {
    "chunk_id": 4169,
    "paper_filename": "zhenjie_2024_mammoVLM_a_ generative_large_vision_language_model_for_mammography.pdf",
    "paper_title": "Zhenjie 2024 Mammovlm A  Generative Large Vision Language Model For Mammography",
    "chunk_index": 3,
    "total_chunks": 65,
    "text_content": "over their examination. This is why a diagnostic assistant is needed for follow-up Q&A. Patients may need a Chatbot with questions related to text only (language model) or related to mammograms (vision\u2013language model). \u2217Corresponding author. E-mail address: malan@mails.tsinghua.edu.cn (L. Ma).Large language models (LLMs), such as ChatGPT, have recently achieved remarkable success, significantly enhancing applications like chatbots and AI agents. Their power stems from the ability to process and ",
    "full_text_length": 64867,
    "chunk_length": 1394
  },
  {
    "chunk_id": 4170,
    "paper_filename": "zhenjie_2024_mammoVLM_a_ generative_large_vision_language_model_for_mammography.pdf",
    "paper_title": "Zhenjie 2024 Mammovlm A  Generative Large Vision Language Model For Mammography",
    "chunk_index": 4,
    "total_chunks": 65,
    "text_content": "capabilities into a unified framework. These mod- els can understand and generate descriptions of visual content and use natural language to guide the interpretation of visual data. This allows them to perform tasks that require an understanding of both visual and linguistic content, such as image captioning, visual question answering, and text-to-image generation. Vision\u2013language models can https://doi.org/10.1016/j.inffus.2025.102998 Received 3 December 2024; Received in revised form 15 Januar",
    "full_text_length": 64867,
    "chunk_length": 1373
  },
  {
    "chunk_id": 4171,
    "paper_filename": "zhenjie_2024_mammoVLM_a_ generative_large_vision_language_model_for_mammography.pdf",
    "paper_title": "Zhenjie 2024 Mammovlm A  Generative Large Vision Language Model For Mammography",
    "chunk_index": 5,
    "total_chunks": 65,
    "text_content": "images from textual descriptions, and more. The power of large vision\u2013language models lies in their ability to bridge the gap between visual and linguistic information, enabling machines to understand and generate content in a way that is more similar to human cognition [5,6]. In this work, we aim to fully explore the cross-modality power of a large vision\u2013language model, MammoVLM, to operate as a diag- nostic assistant for Mammogram-related Q&A. Patient with her mam- mograms in hand often remai",
    "full_text_length": 64867,
    "chunk_length": 1343
  },
  {
    "chunk_id": 4172,
    "paper_filename": "zhenjie_2024_mammoVLM_a_ generative_large_vision_language_model_for_mammography.pdf",
    "paper_title": "Zhenjie 2024 Mammovlm A  Generative Large Vision Language Model For Mammography",
    "chunk_index": 6,
    "total_chunks": 65,
    "text_content": "we develop two sets of comparisons: open-source vision\u2013language models and State-Of-The-Art LLMs connected after our visual-MoE + projection module (UMiCon). We design subjective and objective experiments for comparison. For the subjective part, thirty-eight patients are invited to provide their questions regarding their mammograms. We ask eight senior radiologists to evaluate an- swers generated by each model from aspects of correctness, rationality, helpfulness, and professionalism. As for the",
    "full_text_length": 64867,
    "chunk_length": 1437
  },
  {
    "chunk_id": 4173,
    "paper_filename": "zhenjie_2024_mammoVLM_a_ generative_large_vision_language_model_for_mammography.pdf",
    "paper_title": "Zhenjie 2024 Mammovlm A  Generative Large Vision Language Model For Mammography",
    "chunk_index": 7,
    "total_chunks": 65,
    "text_content": "transferred to the LLM before a projection module with better alignment. 2.A novel projection module, UMiCon, trained with unimodal and multimodal contrastive learning that bridges the gap between visual and textual modalities. In order to perform an ideal pre- training for UMiCon, we designed a classification task between BI-RADS 3 and 4, the most difficult BI-RADS to classify, to pre- train UMiCon. Due to the lack of high-quality Q&A pairs, we turn to mammograms and their existing correspondin",
    "full_text_length": 64867,
    "chunk_length": 1291
  },
  {
    "chunk_id": 4174,
    "paper_filename": "zhenjie_2024_mammoVLM_a_ generative_large_vision_language_model_for_mammography.pdf",
    "paper_title": "Zhenjie 2024 Mammovlm A  Generative Large Vision Language Model For Mammography",
    "chunk_index": 8,
    "total_chunks": 65,
    "text_content": "are the training data for the whole MammoVLM. 4.MammoVLM has not only beaten other leading VLMs by a no- ticeable margin but also shows a professional capability similar to that of a junior radiologist. 2. Related work 2.1. Large foundation models Large foundation Models, encompassing architectures like Diffusion models [12], Transformer [13], BERT [14], DALL-E [15], GPT [16],GLM [11], and others [17\u201319], are grounded in deep learning and leverage transfer learning techniques. Often pre-trained ",
    "full_text_length": 64867,
    "chunk_length": 1339
  },
  {
    "chunk_id": 4175,
    "paper_filename": "zhenjie_2024_mammoVLM_a_ generative_large_vision_language_model_for_mammography.pdf",
    "paper_title": "Zhenjie 2024 Mammovlm A  Generative Large Vision Language Model For Mammography",
    "chunk_index": 9,
    "total_chunks": 65,
    "text_content": "of medical imaging, where they can be applied to various imaging modalities such as X-rays [23], MRI, CT scans [19], and more. By training large foundation models on a wide range of medical data from different sources and modalities, these models can accumulate a wealth of medical knowledge, making them highly adaptable for multiple tasks within medical imaging. 2.2. Vision\u2013language models Large vision\u2013language models (VLM) are part of large foundation models and have been a topic of intense res",
    "full_text_length": 64867,
    "chunk_length": 1311
  },
  {
    "chunk_id": 4176,
    "paper_filename": "zhenjie_2024_mammoVLM_a_ generative_large_vision_language_model_for_mammography.pdf",
    "paper_title": "Zhenjie 2024 Mammovlm A  Generative Large Vision Language Model For Mammography",
    "chunk_index": 10,
    "total_chunks": 65,
    "text_content": "images corresponding to complex text prompts. ALIGN [26] is pre-trained on a billion image-text pairs. It uses a simple and efficient approach for contrastive learning, which allows it to scale to such a large dataset. ALIGN performs strongly on various downstream tasks, including image classification, text-to-image retrieval, and zero-shot image classification. FLAVA [27] combines three pre-training objectives: language modeling, visual recognition, and joint image-text modeling. It aims to ali",
    "full_text_length": 64867,
    "chunk_length": 1426
  },
  {
    "chunk_id": 4177,
    "paper_filename": "zhenjie_2024_mammoVLM_a_ generative_large_vision_language_model_for_mammography.pdf",
    "paper_title": "Zhenjie 2024 Mammovlm A  Generative Large Vision Language Model For Mammography",
    "chunk_index": 11,
    "total_chunks": 65,
    "text_content": "align- ment approach bridging the representation gap between modalities. Thus, one major contribution of our proposed MammoVLM is the novel projection module, UMiCon. 2.3. Large foundation models for medical research Large foundation models for medical research are always trained/ fine-tuned with clinical data, including digital medical imaging (X-rays, CT scans, MRI, etc.), user-upload images, clinical metadata, and clinical reports. Among them, vision\u2013language models (VLMs) for VQA tasks accou",
    "full_text_length": 64867,
    "chunk_length": 1400
  },
  {
    "chunk_id": 4178,
    "paper_filename": "zhenjie_2024_mammoVLM_a_ generative_large_vision_language_model_for_mammography.pdf",
    "paper_title": "Zhenjie 2024 Mammovlm A  Generative Large Vision Language Model For Mammography",
    "chunk_index": 12,
    "total_chunks": 65,
    "text_content": "for VQA. Med-Alpaca [39], a biomedical foundational model using DePlot or Med-GIT trained for visual interpretation and LLaMA-7B for language understanding, fine-tuned on medical question\u2013answer pairs. HuaTuo tunes the LlaMA specifically with Chinese medical knowl- edge [40] . LLaVa-Med [41], on the other hand, is a single-streamInformation Fusion 118 (2025) 102998 2 Z. Cao et al. Fig. 1.Architecture of our MammoVLM. It consists of visual-MoE as the image encoder, UMiCon as the visual\u2013textual pr",
    "full_text_length": 64867,
    "chunk_length": 1400
  },
  {
    "chunk_id": 4179,
    "paper_filename": "zhenjie_2024_mammoVLM_a_ generative_large_vision_language_model_for_mammography.pdf",
    "paper_title": "Zhenjie 2024 Mammovlm A  Generative Large Vision Language Model For Mammography",
    "chunk_index": 13,
    "total_chunks": 65,
    "text_content": "Mammography screening Previous works of mammography screening are mostly developed with mammograms only [42\u201347]. In contrast, we build a multimodal model with inputs from both mammograms and diagnostic reports, namely UMiCon. UMiCon operates as the projection module within MammoVLM and is pre-trained with the classification task between BI-RADS 3 and 4. To best leverage information from both modalities, we first set up an unsupervised learning stage to improve the model\u2019s representation ability ",
    "full_text_length": 64867,
    "chunk_length": 1342
  },
  {
    "chunk_id": 4180,
    "paper_filename": "zhenjie_2024_mammoVLM_a_ generative_large_vision_language_model_for_mammography.pdf",
    "paper_title": "Zhenjie 2024 Mammovlm A  Generative Large Vision Language Model For Mammography",
    "chunk_index": 14,
    "total_chunks": 65,
    "text_content": "the MammoVLM is a VQA system that provides answers based on questions and mammograms from the patients. Fig. 1demon- strates the overall framework of MammoVLM. It has a sparse visual- MoE that processes the mammograms and generates visual features. A projection module UMiCon, pre-trained in a specific multimodal task, aligns visual features with textual features. GLM-4 9B [11] generates the final answers. Note that the same LLM without any fine-tuning is also used to prepare training and testing",
    "full_text_length": 64867,
    "chunk_length": 1333
  },
  {
    "chunk_id": 4181,
    "paper_filename": "zhenjie_2024_mammoVLM_a_ generative_large_vision_language_model_for_mammography.pdf",
    "paper_title": "Zhenjie 2024 Mammovlm A  Generative Large Vision Language Model For Mammography",
    "chunk_index": 15,
    "total_chunks": 65,
    "text_content": "densities [56]. The sparse Mixture- of-Experts (MoE) architecture offers a scalable, efficient, and flexible solution for encoder selection, enabling the construction of large foun- dation models while preserving computational efficiency. Thus, we adopt a visual-MoE to process the mammograms, as illustrated inFig. 2. We experiment with different visual encoders and select three which are most sensitive to densities. Based onTable 6, CLIP [8] performs best for category A breast, ConvNeXt-Tiny [9]",
    "full_text_length": 64867,
    "chunk_length": 1281
  },
  {
    "chunk_id": 4182,
    "paper_filename": "zhenjie_2024_mammoVLM_a_ generative_large_vision_language_model_for_mammography.pdf",
    "paper_title": "Zhenjie 2024 Mammovlm A  Generative Large Vision Language Model For Mammography",
    "chunk_index": 16,
    "total_chunks": 65,
    "text_content": "as well as the accuracy of classifier ResNet 50 have been presented in theTable 6and Section 5.2. 3.2. Pre-training the projection module UMiCon The visual features generated by Visual-MoE should be aligned with textual features in order to be sent into LLM afterward. Developing a projection module to complete this alignment is of great significance. In this section, we explain how we utilize both Unimodal andMultimodal Contrastive learning to pre-train a projection module, UMiCon. 3.2.1. Pre-tr",
    "full_text_length": 64867,
    "chunk_length": 1326
  },
  {
    "chunk_id": 4183,
    "paper_filename": "zhenjie_2024_mammoVLM_a_ generative_large_vision_language_model_for_mammography.pdf",
    "paper_title": "Zhenjie 2024 Mammovlm A  Generative Large Vision Language Model For Mammography",
    "chunk_index": 17,
    "total_chunks": 65,
    "text_content": "most challenging BI-RADS classification task, BI-RADS 3 vs. 4. Accord- ing to the ACR\u2019s Breast Imaging Reporting and Data System (BI-RADS) criteria [57], breast lesions in mammography are divided into BI- RADS 0\u223c6; BI-RADS 0 is an incomplete assessment; BI-RADS 1 finds no lesions during the examination, and its malignancy probability is almost 0, which is the same as BI-RADS 2; BI-RADS 5 lesions have malignancy possibilities of\u226595%; lesions confirmed to be malignant by biopsy are classified as B",
    "full_text_length": 64867,
    "chunk_length": 1287
  },
  {
    "chunk_id": 4184,
    "paper_filename": "zhenjie_2024_mammoVLM_a_ generative_large_vision_language_model_for_mammography.pdf",
    "paper_title": "Zhenjie 2024 Mammovlm A  Generative Large Vision Language Model For Mammography",
    "chunk_index": 18,
    "total_chunks": 65,
    "text_content": "(2025) 102998 3 Z. Cao et al. Fig. 3.UMiCon pre-training: Unsupervised learning stage. clinical scenarios, 0.9% to 7.9% of suspected benign lesions evaluated in BI-RADS 3 are diagnosed as malignant after surgery [58,59]. On the other hand, 70% of the false positives in mammography happen from BI-RADS 4. This brings problems of over-diagnosis and over-treatment. The challenge of distinguishing between BI-RADS 3 and 4 under- scores the value of this classification task. To address this, we formula",
    "full_text_length": 64867,
    "chunk_length": 1287
  },
  {
    "chunk_id": 4185,
    "paper_filename": "zhenjie_2024_mammoVLM_a_ generative_large_vision_language_model_for_mammography.pdf",
    "paper_title": "Zhenjie 2024 Mammovlm A  Generative Large Vision Language Model For Mammography",
    "chunk_index": 19,
    "total_chunks": 65,
    "text_content": "5.3to present results of UMiCon on this pre-training task and its effectiveness. 3.2.2. UMiCon structure UMiCon has a pooling layer and flatten operation that maps the visual features to a six-layer MLP within it. The overall framework of the UMiCon\u2019s pre-training is illustrated in the below figures. Fig. 3 shows theunsupervised learning stage. The resulting image encoder, text encoder, projection layer, and UMiCon are then transferred to the supervised learning stage as presented inFig. 4. We f",
    "full_text_length": 64867,
    "chunk_length": 1288
  },
  {
    "chunk_id": 4186,
    "paper_filename": "zhenjie_2024_mammoVLM_a_ generative_large_vision_language_model_for_mammography.pdf",
    "paper_title": "Zhenjie 2024 Mammovlm A  Generative Large Vision Language Model For Mammography",
    "chunk_index": 20,
    "total_chunks": 65,
    "text_content": "the image area is randomly added twice, generating a pair of input images. The mask is a square and locates strictly inside the input image. Then, the branch is carried out by the Siamese contrastive learning module, which consists of a Siamese encoder (red block in Fig. 3) and a Siamese projection module (UMiCon inFig. 3). The pair of input mammograms are simultaneously fed into the shared-weight encoders. UMiCon then projects the encoded features into two 1-D vectors representing their class\u2019s",
    "full_text_length": 64867,
    "chunk_length": 1281
  },
  {
    "chunk_id": 4187,
    "paper_filename": "zhenjie_2024_mammoVLM_a_ generative_large_vision_language_model_for_mammography.pdf",
    "paper_title": "Zhenjie 2024 Mammovlm A  Generative Large Vision Language Model For Mammography",
    "chunk_index": 21,
    "total_chunks": 65,
    "text_content": "jection module, respectively; \ud835\udc59\ud835\udc3cand\ud835\udc59\ud835\udc3c\u2032indicate the corresponding BI-RADS labels. The loss for a batch of\ud835\udc41image pairs can be simply defined as\ue238batch =\u2211\ud835\udc41 \ud835\udc56=1\ud835\udc3f(\ud835\udc3c\ud835\udc56, \ud835\udc3c\u2032 \ud835\udc56). In this case, samples always belong to the same class as they are augmented from the same image. The image encoder here is ConvNeXt-Tiny [9]. Multimodal contrastive learning branch The same input image, as that of the unimodal branch, will pass through the image encoder and UMiCon sequentially, which share weights with those in th",
    "full_text_length": 64867,
    "chunk_length": 1328
  },
  {
    "chunk_id": 4188,
    "paper_filename": "zhenjie_2024_mammoVLM_a_ generative_large_vision_language_model_for_mammography.pdf",
    "paper_title": "Zhenjie 2024 Mammovlm A  Generative Large Vision Language Model For Mammography",
    "chunk_index": 22,
    "total_chunks": 65,
    "text_content": "unimodal branch. 3.2.4. Supervised stage The inputs of this stage are mammogram-diagnostic report pairs from various patients in our dataset, of which BI-RADS labels are avail- able. The image encoder, text encoder, projection layer, and UMiCon (red, purple, gray, and black blocks inFig. 4) are transferred from the unsupervised learning stage. Cross-modality fusion Image encoder and text encoder with UMi- Con and projection layer set features from the mammogram and di- agnostic report to the sam",
    "full_text_length": 64867,
    "chunk_length": 1400
  },
  {
    "chunk_id": 4189,
    "paper_filename": "zhenjie_2024_mammoVLM_a_ generative_large_vision_language_model_for_mammography.pdf",
    "paper_title": "Zhenjie 2024 Mammovlm A  Generative Large Vision Language Model For Mammography",
    "chunk_index": 23,
    "total_chunks": 65,
    "text_content": "118 (2025) 102998 4 Z. Cao et al. Fig. 4.UMiCon pre-training: Supervised learning stage. which contains Sigmoid and cross-entropy loss calculated with the BI-RADS labels of each patient. Besides, given the subtle and hard- to-distinguish differences between BI-RADS 3 and 4, we add another cross-modal contrastive learning head upon the cross-modality 2-D vector. A pair of patients\u2019 data, in this case, a pair of 2-D vectors, are being calculated with each other using contrastive loss. From our ex-",
    "full_text_length": 64867,
    "chunk_length": 1314
  },
  {
    "chunk_id": 4190,
    "paper_filename": "zhenjie_2024_mammoVLM_a_ generative_large_vision_language_model_for_mammography.pdf",
    "paper_title": "Zhenjie 2024 Mammovlm A  Generative Large Vision Language Model For Mammography",
    "chunk_index": 24,
    "total_chunks": 65,
    "text_content": "SFT stage, the initial inputs to our MammoVLM (mammograms and patient questions) should be converted into an instruction-aware format consisting of instructions, visual input information, and ground-truth responses. The details of training samples are discussed in Section 4.1.1 . Through instruction tuning, the LLM can predict the answer given the instruction and visual input information: \ud835\udc34=\ud835\udc53(\ud835\udc3c;\ud835\udc40;\ud835\udf03) (3) where\ud835\udc34represents the answer output by LLM,\ud835\udf03denotes the LLM\u2019s parameters, \ud835\udc3c\u2208R1\u00d71\u00d7128denotes th",
    "full_text_length": 64867,
    "chunk_length": 1362
  },
  {
    "chunk_id": 4191,
    "paper_filename": "zhenjie_2024_mammoVLM_a_ generative_large_vision_language_model_for_mammography.pdf",
    "paper_title": "Zhenjie 2024 Mammovlm A  Generative Large Vision Language Model For Mammography",
    "chunk_index": 25,
    "total_chunks": 65,
    "text_content": "We have also experimented with other LLMs and results have been shown in Section 5.1andTable 2. 4. Experiment design 4.1. Data collection Our data is collected from three collaborative hospitals at distinct geographical locations using Siemens and Giotto equipment followingthe ACR standard from 2011 to 2021.1A large-scale dataset contain- ing 33,630 mammogram studies with diagnostic reports from 30,495 patients was collected. The distribution is as follows: BI-RADS 1 with 19,698 cases, BI-RADS 2",
    "full_text_length": 64867,
    "chunk_length": 1304
  },
  {
    "chunk_id": 4192,
    "paper_filename": "zhenjie_2024_mammoVLM_a_ generative_large_vision_language_model_for_mammography.pdf",
    "paper_title": "Zhenjie 2024 Mammovlm A  Generative Large Vision Language Model For Mammography",
    "chunk_index": 26,
    "total_chunks": 65,
    "text_content": "diagnosis reports. All the reports we collected have already been reviewed by at least a senior radiologist. Notably, during the pre-training stage of UMiCon as in Section 3.2, we have blanked the BI-RADS information initially contained in the reports. All data has been pre-processed and does not contain any personal or sensitive information about the patient. 4.1.1. Supervised fine-tuning We set radiologists into two groups, junior and senior, separated by their years of expertise. Junior imagi",
    "full_text_length": 64867,
    "chunk_length": 1294
  },
  {
    "chunk_id": 4193,
    "paper_filename": "zhenjie_2024_mammoVLM_a_ generative_large_vision_language_model_for_mammography.pdf",
    "paper_title": "Zhenjie 2024 Mammovlm A  Generative Large Vision Language Model For Mammography",
    "chunk_index": 27,
    "total_chunks": 65,
    "text_content": "best answer based on its correctness and professional level for each patient\u2019s case. Each training sample consisted of a question (from the patient), her mammogram(s), and an answer (picked by the senior radiologist). All these training samples are leveraged in the LLM tuning stage as in Section 3.3. 4.1.2. Visual-MoE training As described in Section 3.1, ResNet-50 operates as a selector for the visual encoders afterward based on the densities of input mammo- grams. Thus, we train the ResNet-50 ",
    "full_text_length": 64867,
    "chunk_length": 1331
  },
  {
    "chunk_id": 4194,
    "paper_filename": "zhenjie_2024_mammoVLM_a_ generative_large_vision_language_model_for_mammography.pdf",
    "paper_title": "Zhenjie 2024 Mammovlm A  Generative Large Vision Language Model For Mammography",
    "chunk_index": 28,
    "total_chunks": 65,
    "text_content": "are from Eight Senior Radiologists). Junior andSenior represent radiologists with 5 and 21 years of working experience, respectively. Aspects Correctness Rationality Helpfulness Professionalism Completeness LLaVA-1.5 [33] 81.3 77.8 91.0 80.6 96.6 CogVLM [31] 64.6 75.6 86.5 78.6 88.7 QWen-VL [28] 86.5 74.2 84.7 88.7 90.7 Mammo-CLIP [63] 80.4 81.2 88.9 89.4 79.4 BiomedGPT [38] 91.5 80.3 86.4 88.8 90.8 MammoVLM 91.5 82.6 91.2 94.6 96.5 Junior 90.6 83.0 93.4 91.1 93.0 Senior 92.7 86.1 96.3 95.1 97.1",
    "full_text_length": 64867,
    "chunk_length": 1287
  },
  {
    "chunk_id": 4195,
    "paper_filename": "zhenjie_2024_mammoVLM_a_ generative_large_vision_language_model_for_mammography.pdf",
    "paper_title": "Zhenjie 2024 Mammovlm A  Generative Large Vision Language Model For Mammography",
    "chunk_index": 29,
    "total_chunks": 65,
    "text_content": "Senior Radiologists). Method Correctness Rationality Helpfulness Professionalism Completeness CLIP only w/o MoE + Linear Mapping + LLM (freeze) 71.5 67.3 69.0 65.8 80.8 ConvNeXt only w/o MoE + Linear Mapping + LLM (freeze) 70.8 70.3 71.4 68.9 86.9 Visual-MoE + Linear Mapping + LLM (freeze) 77.9 75.4 76.3 74.0 88.4 Visual-MoE + UMiCon + LLM (freeze) 86.4 78.8 88.1 90.2 92.6 Visual-MoE + UMiCon + LLM SFT(MammoVLM) 91.5 82.6 91.2 94.6 96.5 Table 4 Number of patients in each BI-RADS category for UMi",
    "full_text_length": 64867,
    "chunk_length": 1230
  },
  {
    "chunk_id": 4196,
    "paper_filename": "zhenjie_2024_mammoVLM_a_ generative_large_vision_language_model_for_mammography.pdf",
    "paper_title": "Zhenjie 2024 Mammovlm A  Generative Large Vision Language Model For Mammography",
    "chunk_index": 30,
    "total_chunks": 65,
    "text_content": "of BI-RADS 3 is\u2264 2%. Therefore, radiologists define them as benign lesions. However, in actual clinical work, 0.9% to 7.9% of suspected benign lesions evaluated in BI-RADS 3 are diagnosed as malignant after surgery. On the other hand, 70% of false positives in mammography examinations are BI-RADS 4. These false alarms from BI-RADS 4 result in unnecessary diagnosis and over-treatment. The certain degree of false negatives and positives in BI-RADS 3 and 4 diagnoses further prove the clinical impor",
    "full_text_length": 64867,
    "chunk_length": 1272
  },
  {
    "chunk_id": 4197,
    "paper_filename": "zhenjie_2024_mammoVLM_a_ generative_large_vision_language_model_for_mammography.pdf",
    "paper_title": "Zhenjie 2024 Mammovlm A  Generative Large Vision Language Model For Mammography",
    "chunk_index": 31,
    "total_chunks": 65,
    "text_content": "proved malignant by biopsy results. As for BI-RADS 4, 1,914 of the 3,378 (56.66%) BI-RADS 4 patients were later proved benign by biopsy. According to Lee [66] and Orel et al. [67], biopsy-proven benign and malignant cases should be categorized as BI-RADS 3 and 4, respectively. Thus, we correct the BI- RADS labels, assigning 2,484 cases as BI-RADS 3 and 2,073 cases as BI-RADS 4. The dataset is split into the training and validation sets by 8:1 ratio. Our test set includes 601 patients collected w",
    "full_text_length": 64867,
    "chunk_length": 1172
  },
  {
    "chunk_id": 4198,
    "paper_filename": "zhenjie_2024_mammoVLM_a_ generative_large_vision_language_model_for_mammography.pdf",
    "paper_title": "Zhenjie 2024 Mammovlm A  Generative Large Vision Language Model For Mammography",
    "chunk_index": 32,
    "total_chunks": 65,
    "text_content": "3 patients. All data in our dataset come with biopsy-proven results. 4.2. Implementation details Visual-MoE, UMiCon, and LLM tuning share the following training parameters. All input images are resized to 1008 \u00d7800 and retain the original aspect ratio. \ue22fis 128. 6 warming-up steps are added after the start. The initial learning rates are reduced by a factor of 10 after 100 epochs. Adam is used [68], with a weight decay of5 \u00d7 10\u22124. NVIDIA A100 GPUs (40G memory each) are used. The model training no",
    "full_text_length": 64867,
    "chunk_length": 1230
  },
  {
    "chunk_id": 4199,
    "paper_filename": "zhenjie_2024_mammoVLM_a_ generative_large_vision_language_model_for_mammography.pdf",
    "paper_title": "Zhenjie 2024 Mammovlm A  Generative Large Vision Language Model For Mammography",
    "chunk_index": 33,
    "total_chunks": 65,
    "text_content": "five open-source SOTA VLMs: LLaVA-1.5 [33], CogVLM [31], QWen-VL [28], BiomedGPT [38], and Mammo-CLIP [63]. We apply the same training data and SFT recipes to build an equal comparison. Also, we invite a junior (5 years of working experience) and senior radiologist (21 years of experience) to join the experiments. Besides, we apply three open- source SOTA LLMs, Llama-3 [64], QWen [28], Mixtral 8\u00d722B [65], to replace our GLM-4 9B to prove the universality of our visual-MoE andInformation Fusion 1",
    "full_text_length": 64867,
    "chunk_length": 1259
  },
  {
    "chunk_id": 4200,
    "paper_filename": "zhenjie_2024_mammoVLM_a_ generative_large_vision_language_model_for_mammography.pdf",
    "paper_title": "Zhenjie 2024 Mammovlm A  Generative Large Vision Language Model For Mammography",
    "chunk_index": 34,
    "total_chunks": 65,
    "text_content": "selector of encoders, ResNet-50, too. 4.3.2. Projection module UMiCon To fully prove the value of UMiCon as a projection module, we conduct objective comparisons in the task of BI-RADS 3 and 4 classifi- cations, the value of which has been discussed in Section 4.1.3 . We re-implement methods as in [44\u201346], the previously reported SOTA methods for mammography screening, and apply them to our datasets. We compare the system performance using quantitative metrics, in- cluding area under the receive",
    "full_text_length": 64867,
    "chunk_length": 1356
  },
  {
    "chunk_id": 4201,
    "paper_filename": "zhenjie_2024_mammoVLM_a_ generative_large_vision_language_model_for_mammography.pdf",
    "paper_title": "Zhenjie 2024 Mammovlm A  Generative Large Vision Language Model For Mammography",
    "chunk_index": 35,
    "total_chunks": 65,
    "text_content": "human doctors We invite forty-eight patients to provide their questions regarding their mammograms. We request twelve senior radiologists to evalu- ate the answers generated by each VLM model based on correctness, rationality, helpfulness, professionalism, and completeness, which are the most crucial aspects of medical VQA, according to radiologists and researches [69,70]. Each of these metrics is scored on a scale from one to one hundred, with one being the worst and one hundred being the best.",
    "full_text_length": 64867,
    "chunk_length": 1232
  },
  {
    "chunk_id": 4202,
    "paper_filename": "zhenjie_2024_mammoVLM_a_ generative_large_vision_language_model_for_mammography.pdf",
    "paper_title": "Zhenjie 2024 Mammovlm A  Generative Large Vision Language Model For Mammography",
    "chunk_index": 36,
    "total_chunks": 65,
    "text_content": "a narrow margin. LLaVA has proved to be compre- hensive on a variety of text generation tasks [33]; this result further proves it as a reliable model. As for correctness, BiomedGPT [71] joins MammoVLM to score the highest but falls far behind MammoVLM in the other three aspects. Considering that all these comparing VLMs are the current leading open-source models, we believe MammoVLM has superior and promising capability in this task of Mammography-related Q&A. Additionally, we invite junior and ",
    "full_text_length": 64867,
    "chunk_length": 1290
  },
  {
    "chunk_id": 4203,
    "paper_filename": "zhenjie_2024_mammoVLM_a_ generative_large_vision_language_model_for_mammography.pdf",
    "paper_title": "Zhenjie 2024 Mammovlm A  Generative Large Vision Language Model For Mammography",
    "chunk_index": 37,
    "total_chunks": 65,
    "text_content": "a junior doctor in this mammography-related Q&A task.Table 6 AUC comparison of different encoders in VisualMoE for classification performance on breast densities categories A, B, C, and D. Method A B C D ResNet-50 [72] 0.68 0.71 0.84 0.85 CLIP [8] 0.88 0.85 0.79 0.89 DINOv2 [10] 0.76 0.90 0.86 0.81 ConvNeXt-base [9] 0.81 0.79 0.82 0.85 ConvNeXt-tiny [9] 0.84 0.88 0.93 0.95 5.1.2. MammoVLM with other SOTA LLMs We replace the GLM-4 9B in MammoVLM with other leading open- source LLMs and present re",
    "full_text_length": 64867,
    "chunk_length": 1206
  },
  {
    "chunk_id": 4204,
    "paper_filename": "zhenjie_2024_mammoVLM_a_ generative_large_vision_language_model_for_mammography.pdf",
    "paper_title": "Zhenjie 2024 Mammovlm A  Generative Large Vision Language Model For Mammography",
    "chunk_index": 38,
    "total_chunks": 65,
    "text_content": "MammoVLM may be successful regardless of the LLM selection for another medical VQA task. 5.1.3. Ablation study on MammoVLM Table 3is the ablation experiments of MammoVLM. The first two rows represent the single vision encoder instead of MoE, vanilla linear mapping instead of UMiCon, and GLM-4 9B without SFT. Row 3 adds Visual-MoE and improves in all aspects. The most noticeable performance jump appears in row 4, where UMiCon is applied as a projection module between visual and textual features. ",
    "full_text_length": 64867,
    "chunk_length": 1270
  },
  {
    "chunk_id": 4205,
    "paper_filename": "zhenjie_2024_mammoVLM_a_ generative_large_vision_language_model_for_mammography.pdf",
    "paper_title": "Zhenjie 2024 Mammovlm A  Generative Large Vision Language Model For Mammography",
    "chunk_index": 39,
    "total_chunks": 65,
    "text_content": "encoders on breast density categories A, B, C, and D. AUC results have been presented. CLIP [8], DINOv2 [10], and ConvNeXt-tiny [9] have each topped the classification ability on breast density categories A, B, and C &D. The results show that it is practical to use different encoders for different density mammograms, according to their classification performance. This motivates us to adopt the MoE architecture as in Sec- tion3.1instead of a single vision encoder. Using data in Section 4.1.2 , Re",
    "full_text_length": 64867,
    "chunk_length": 1279
  },
  {
    "chunk_id": 4206,
    "paper_filename": "zhenjie_2024_mammoVLM_a_ generative_large_vision_language_model_for_mammography.pdf",
    "paper_title": "Zhenjie 2024 Mammovlm A  Generative Large Vision Language Model For Mammography",
    "chunk_index": 40,
    "total_chunks": 65,
    "text_content": "this section represents our pre-training strategy of UMiCon as in Section 3.2. We invite four doc- tors to our experiments: junior imaging physicians with 5 and 6 years of experience, respectively, A and B, and two senior physicians, C and D, with 21 years of working experience and professional breast imaging training. Collaborative doctors conduct the experiments solely and with the help of UMiCon. Comparison with the State-of-the-Arts As shown inTable 7, com- pared with previous SOTA methods [",
    "full_text_length": 64867,
    "chunk_length": 1216
  },
  {
    "chunk_id": 4207,
    "paper_filename": "zhenjie_2024_mammoVLM_a_ generative_large_vision_language_model_for_mammography.pdf",
    "paper_title": "Zhenjie 2024 Mammovlm A  Generative Large Vision Language Model For Mammography",
    "chunk_index": 41,
    "total_chunks": 65,
    "text_content": "Yala et al. [44] 0.68 [0.62, 0.74] 0.67 0.72 0.59 0.61 0.72 Cao et al. [45] 0.61 [0.56, 0.66] 0.60 0.81 0.45 0.56 0.70 Mckinney et al. [46]0.64 [0.62, 0.66] 0.64 0.78 0.49 0.60 0.77 UMiCon (ours) 0.70 [0.65, 0.75] 0.69 0.81 0.59 0.62 0.79 A 0.62 [0.56, 0.69] 0.62 0.80 0.45 0.58 0.70 A + UMiCon 0.74 [0.68, 0.80] 0.72 0.91 0.58 0.61 0.89 B 0.61 [0.54, 0.68] 0.60 0.83 0.38 0.56 0.70 B + UMiCon 0.75 [0.69, 0.81] 0.73 0.86 0.64 0.64 0.86 C 0.69 [0.63, 0.76] 0.67 0.88 0.51 0.57 0.85 C + UMiCon 0.76 [0",
    "full_text_length": 64867,
    "chunk_length": 1095
  },
  {
    "chunk_id": 4208,
    "paper_filename": "zhenjie_2024_mammoVLM_a_ generative_large_vision_language_model_for_mammography.pdf",
    "paper_title": "Zhenjie 2024 Mammovlm A  Generative Large Vision Language Model For Mammography",
    "chunk_index": 42,
    "total_chunks": 65,
    "text_content": "model, respectively. Row 4 is UMiCon. Method AUC ACC Base-M (Supervised classification) 0.56 0.6 Base-M + Uni-M 0.61 0.63 Base-M + Uni-M + Multi-M 0.64 0.66 Base-M + Uni-Ml + Multi-M + Cross-M 0.70 0.69 Base-M (MHCA [53]) + Uni-M + Multi-M + Cross-M 0.69 0.67 Table 9 Detailed description of MammoVLM benign and malignant test data. Mammograms BI-RADS Count Total Malignant Mammograms4 88 102 5 6 6 8 Benign Mammograms1 1346 2300 2 728 3 226 Excluded 0 136 136 Table 10 Zero-Shot Results of the mammo",
    "full_text_length": 64867,
    "chunk_length": 1289
  },
  {
    "chunk_id": 4209,
    "paper_filename": "zhenjie_2024_mammoVLM_a_ generative_large_vision_language_model_for_mammography.pdf",
    "paper_title": "Zhenjie 2024 Mammovlm A  Generative Large Vision Language Model For Mammography",
    "chunk_index": 43,
    "total_chunks": 65,
    "text_content": "accuracy, and represents a promising overall performance. Regarding sensitivity and specificity, UMiCon surpasses the previous SOTA meth- ods by 4% and 8%, on average, claiming it can effectively identify BI-RADS 3 and 4 without sacrificing one another. Comparison with the doctors The AUC (0.70) of standalone UMi- Con is higher than that of junior doctors (A, B) and close to professional doctors (C, D). Every doctor\u2019s AUC performance improved over them- selves when combined with the UMiCon to di",
    "full_text_length": 64867,
    "chunk_length": 1293
  },
  {
    "chunk_id": 4210,
    "paper_filename": "zhenjie_2024_mammoVLM_a_ generative_large_vision_language_model_for_mammography.pdf",
    "paper_title": "Zhenjie 2024 Mammovlm A  Generative Large Vision Language Model For Mammography",
    "chunk_index": 44,
    "total_chunks": 65,
    "text_content": "increases both the AUC and accuracy by 3% (row 3). Cross- modal contrastive learning sees the most significant rise of AUC by 6% (row 4), which is the level of our SOTA UMiCon. In addition, in order to validate the effectiveness of our designed cross-modal fusion strategy as in Section 3, we replace it with multi-head cross attention and transformer architecture as in [53] (row 5). Although row 5\u2019s performance is slightly below UMiCon, UMiCon\u2019s cross-modal fusion design is much simpler and requi",
    "full_text_length": 64867,
    "chunk_length": 1261
  },
  {
    "chunk_id": 4211,
    "paper_filename": "zhenjie_2024_mammoVLM_a_ generative_large_vision_language_model_for_mammography.pdf",
    "paper_title": "Zhenjie 2024 Mammovlm A  Generative Large Vision Language Model For Mammography",
    "chunk_index": 45,
    "total_chunks": 65,
    "text_content": "due to its uncertainty of malig- nancy. We inference MammoVLM by sending it the test image and questions about its malignancy. The performance comparison among various VLMs is shown inTable 10, where the AUC values are for the non-malignant class, and a sensitivity of 20% means that 20% non-malignant mammograms are confidently screened out from all non-malignant mammograms. Although all these VLMs show strong generalization capabilities, MammoVLM still outperforms others by adapting to new, unse",
    "full_text_length": 64867,
    "chunk_length": 1379
  },
  {
    "chunk_id": 4212,
    "paper_filename": "zhenjie_2024_mammoVLM_a_ generative_large_vision_language_model_for_mammography.pdf",
    "paper_title": "Zhenjie 2024 Mammovlm A  Generative Large Vision Language Model For Mammography",
    "chunk_index": 46,
    "total_chunks": 65,
    "text_content": "densities with different visual experts, a projection module UMiCon that is pre-trained with unimodal and multimodal contrastive learning on a challenging classification task BI-RADS 3 vs. 4, and supervised fine-tuning on LLM that generates the final answers. Subjective experimental results on our large-scale dataset have shown that MammoVLM has not only beaten leading open-source VLMs but also shows competitive professional capabilities at a junior radiologist level. Besides, objective ablation",
    "full_text_length": 64867,
    "chunk_length": 1441
  },
  {
    "chunk_id": 4213,
    "paper_filename": "zhenjie_2024_mammoVLM_a_ generative_large_vision_language_model_for_mammography.pdf",
    "paper_title": "Zhenjie 2024 Mammovlm A  Generative Large Vision Language Model For Mammography",
    "chunk_index": 47,
    "total_chunks": 65,
    "text_content": "and deadly cancer. Leveraging the large foundation model MammoVLM to serve as mammography- related diagnostic assistance has huge potential to improve medical service efficiencies and effectiveness. CRediT authorship contribution statement Zhenjie Cao: Writing \u2013 review & editing, Writing \u2013 original draft, Visualization, Software, Methodology, Data curation, Conceptualiza- tion. Zhuo Deng: Visualization, Validation, Project administration, Methodology, Formal analysis, Data curation. Jie Ma: Inve",
    "full_text_length": 64867,
    "chunk_length": 1516
  },
  {
    "chunk_id": 4214,
    "paper_filename": "zhenjie_2024_mammoVLM_a_ generative_large_vision_language_model_for_mammography.pdf",
    "paper_title": "Zhenjie 2024 Mammovlm A  Generative Large Vision Language Model For Mammography",
    "chunk_index": 48,
    "total_chunks": 65,
    "text_content": "Guangdong Medical Research Foundation, China under A2024506. Zhenjie Cao and Zhuo Deng contribute equally to the article. Data availability Data will be made available on request. References [1] World Health Organization, Breast cancer, 2020, URL http://www.who.int/news- room/fact-sheets/detail/breast-cancer . [2]C.D. Lehman, R.F. Arao, B.L. Sprague, J.M. Lee, D.S. Buist, K. Kerlikowske, L.M. Henderson, T. Onega, A.N. Tosteson, G.H. Rauscher, et al., National performance benchmarks for modern sc",
    "full_text_length": 64867,
    "chunk_length": 1298
  },
  {
    "chunk_id": 4215,
    "paper_filename": "zhenjie_2024_mammoVLM_a_ generative_large_vision_language_model_for_mammography.pdf",
    "paper_title": "Zhenjie 2024 Mammovlm A  Generative Large Vision Language Model For Mammography",
    "chunk_index": 49,
    "total_chunks": 65,
    "text_content": "(3) (2024) 1\u201345. [5]J. Zhang, J. Huang, S. Jin, S. Lu, Vision-language models for vision tasks: A survey, IEEE Trans. Pattern Anal. Mach. Intell. (2024). [6]W. Dai, J. Li, D. Li, A.M.H. Tiong, J. Zhao, W. Wang, B. Li, P.N. Fung, S. Hoi, Instructblip: Towards general-purpose vision-language models with instruction tuning, Adv. Neural Inf. Process. Syst. 36 (2024). [7]M.W. Kissin, A. Subramanian, To follow-up or not to follow-up, that is the question, in: Oncoplastic Breast Surgery, CRC Press, 202",
    "full_text_length": 64867,
    "chunk_length": 1269
  },
  {
    "chunk_id": 4216,
    "paper_filename": "zhenjie_2024_mammoVLM_a_ generative_large_vision_language_model_for_mammography.pdf",
    "paper_title": "Zhenjie 2024 Mammovlm A  Generative Large Vision Language Model For Mammography",
    "chunk_index": 50,
    "total_chunks": 65,
    "text_content": "2022, pp. 11976\u201311986. [10] M. Oquab, T. Darcet, T. Moutakanni, H. Vo, M. Szafraniec, V. Khalidov, P. Fernandez, D. Haziza, F. Massa, A. El-Nouby, et al., Dinov2: Learning robust visual features without supervision, 2023, arXiv preprint arXiv:2304.07193 . [11] T. GLM, A. Zeng, B. Xu, B. Wang, C. Zhang, D. Yin, D. Rojas, G. Feng, H. Zhao, H. Lai, H. Yu, H. Wang, J. Sun, J. Zhang, J. Cheng, J. Gui, J. Tang, J. Zhang, J. Li, L. Zhao, L. Wu, L. Zhong, M. Liu, M. Huang, P. Zhang, Q. Zheng, R. Lu, S. ",
    "full_text_length": 64867,
    "chunk_length": 1047
  },
  {
    "chunk_id": 4217,
    "paper_filename": "zhenjie_2024_mammoVLM_a_ generative_large_vision_language_model_for_mammography.pdf",
    "paper_title": "Zhenjie 2024 Mammovlm A  Generative Large Vision Language Model For Mammography",
    "chunk_index": 51,
    "total_chunks": 65,
    "text_content": "Wang, ChatGLM: A family of large language models from GLM-130b to GLM-4 all tools, 2024, arXiv:2406.12793 . [12] J. Ho, A. Jain, P. Abbeel, Denoising diffusion probabilistic models, Adv. Neural Inf. Process. Syst. 33 (2020) 6840\u20136851. [13] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A.N. Gomez, \u0141. Kaiser, I. Polosukhin, Attention is all you need, Adv. Neural Inf. Process. Syst. 30 (2017). [14] J. Devlin, M.-W. Chang, K. Lee, K. Toutanova, Bert: Pre-training of deep bidirectional t",
    "full_text_length": 64867,
    "chunk_length": 1279
  },
  {
    "chunk_id": 4218,
    "paper_filename": "zhenjie_2024_mammoVLM_a_ generative_large_vision_language_model_for_mammography.pdf",
    "paper_title": "Zhenjie 2024 Mammovlm A  Generative Large Vision Language Model For Mammography",
    "chunk_index": 52,
    "total_chunks": 65,
    "text_content": "Syst. 33 (2020) 1877\u20131901. [17] Y. Zhu, F. Cong, D. Zhang, W. Gong, Q. Lin, W. Feng, Y. Dong, J. Tang, Wingnn: Dynamic graph neural networks with random gradient aggregation window, in: Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, 2023, pp. 3650\u20133662.[18] Y. Zang, W. Li, J. Han, K. Zhou, C.C. Loy, Contextual object detection with multimodal large language models, 2023, arXiv preprint arXiv:2305.18279 . [19] Z. Chen, L. Luo, Y. Bie, H. Chen, Dia-LLaMA: Tow",
    "full_text_length": 64867,
    "chunk_length": 1238
  },
  {
    "chunk_id": 4219,
    "paper_filename": "zhenjie_2024_mammoVLM_a_ generative_large_vision_language_model_for_mammography.pdf",
    "paper_title": "Zhenjie 2024 Mammovlm A  Generative Large Vision Language Model For Mammography",
    "chunk_index": 53,
    "total_chunks": 65,
    "text_content": "al., Segment anything, in: Proceedings of the IEEE/CVF International Conference on Computer Vision, 2023, pp. 4015\u20134026. [22] S. Ren, Z. Wang, H. Zhu, J. Xiao, A. Yuille, C. Xie, Rejuvenating image-gpt as strong visual representation learners, in: Forty-First International Conference on Machine Learning, 2023. [23] Z. Liu, Y. Li, P. Shu, A. Zhong, L. Yang, C. Ju, Z. Wu, C. Ma, J. Luo, C. Chen, et al., Radiology-llama2: Best-in-class large language model for radiology, 2023, arXiv preprint arXiv:",
    "full_text_length": 64867,
    "chunk_length": 1348
  },
  {
    "chunk_id": 4220,
    "paper_filename": "zhenjie_2024_mammoVLM_a_ generative_large_vision_language_model_for_mammography.pdf",
    "paper_title": "Zhenjie 2024 Mammovlm A  Generative Large Vision Language Model For Mammography",
    "chunk_index": 54,
    "total_chunks": 65,
    "text_content": "Villalba, N. Dehak, Align-denoise: Single-pass non-autoregressive speech recognition, in: Interspeech, 2021, pp. 3770\u20133774. [27] A. Singh, R. Hu, V. Goswami, G. Couairon, W. Galuba, M. Rohrbach, D. Kiela, Flava: A foundational language and vision alignment model, in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 15638\u201315650. [28] J. Bai, S. Bai, S. Yang, S. Wang, S. Tan, P. Wang, J. Lin, C. Zhou, J. Zhou, Qwen-vl: A versatile vision-language model f",
    "full_text_length": 64867,
    "chunk_length": 1206
  },
  {
    "chunk_id": 4221,
    "paper_filename": "zhenjie_2024_mammoVLM_a_ generative_large_vision_language_model_for_mammography.pdf",
    "paper_title": "Zhenjie 2024 Mammovlm A  Generative Large Vision Language Model For Mammography",
    "chunk_index": 55,
    "total_chunks": 65,
    "text_content": "Train large vision-language models diving into details through chain of manipulations, 2024, arXiv preprint arXiv:2402.04236 . [31] W. Wang, Q. Lv, W. Yu, W. Hong, J. Qi, Y. Wang, J. Ji, Z. Yang, L. Zhao, X. Song, et al., Cogvlm: Visual expert for pretrained language models, 2023, arXiv preprint arXiv:2311.03079 . [32] J. Li, D. Li, S. Savarese, S. Hoi, Blip-2: Bootstrapping language-image pre- training with frozen image encoders and large language models, in: International Conference on Machine",
    "full_text_length": 64867,
    "chunk_length": 1212
  },
  {
    "chunk_id": 4222,
    "paper_filename": "zhenjie_2024_mammoVLM_a_ generative_large_vision_language_model_for_mammography.pdf",
    "paper_title": "Zhenjie 2024 Mammovlm A  Generative Large Vision Language Model For Mammography",
    "chunk_index": 56,
    "total_chunks": 65,
    "text_content": "Peng, E. Cambria, M. Feng, Has multimodal learning delivered universal intelligence in healthcare? a comprehensive survey, Inf. Fusion (2024) 102795. [36] Z. Deng, W. Gao, C. Chen, Z. Niu, Z. Gong, R. Zhang, Z. Cao, F. Li, Z. Ma, W. Wei, et al., OphGLM: An ophthalmology large language-and-vision assistant, Artif. Intell. Med. 157 (2024) 103001. [37] S. Eslami, C. Meinel, G. De Melo, Pubmedclip: How much does clip benefit visual question answering in the medical domain? in: Findings of the Associ",
    "full_text_length": 64867,
    "chunk_length": 1266
  },
  {
    "chunk_id": 4223,
    "paper_filename": "zhenjie_2024_mammoVLM_a_ generative_large_vision_language_model_for_mammography.pdf",
    "paper_title": "Zhenjie 2024 Mammovlm A  Generative Large Vision Language Model For Mammography",
    "chunk_index": 57,
    "total_chunks": 65,
    "text_content": "of medical conversational ai models and training data, 2023, arXiv preprint arXiv:2304. 08247 . [40] H. Wang, C. Liu, N. Xi, Z. Qiang, S. Zhao, B. Qin, T. Liu, Huatuo: Tuning llama model with chinese medical knowledge, 2023, arXiv preprint arXiv:2304.06975 . [41] C. Li, C. Wong, S. Zhang, N. Usuyama, H. Liu, J. Yang, T. Naumann, H. Poon, J. Gao, Llava-med: Training a large language-and-vision assistant for biomedicine in one day, Adv. Neural Inf. Process. Syst. 36 (2024). [42] A. Rodriguez-Ruiz,",
    "full_text_length": 64867,
    "chunk_length": 1273
  },
  {
    "chunk_id": 4224,
    "paper_filename": "zhenjie_2024_mammoVLM_a_ generative_large_vision_language_model_for_mammography.pdf",
    "paper_title": "Zhenjie 2024 Mammovlm A  Generative Large Vision Language Model For Mammography",
    "chunk_index": 58,
    "total_chunks": 65,
    "text_content": "Eur. Radiol. (2020) 1\u20136.Information Fusion 118 (2025) 102998 9 Z. Cao et al. [44] A. Yala, P.G. Mikhael, F. Strand, G. Lin, K. Smith, Y.-L. Wan, L. Lamb, K. Hughes, C. Lehman, R. Barzilay, Toward robust mammography-based models for breast cancer risk, Sci. Transl. Med. 13 (578) (2021). [45] Z. Cao, Z. Yang, Y. Tang, Y. Zhang, M. Han, J. Xiao, J. Ma, P. Chang, Supervised contrastive pre-training formammographic triage screening models, in: Medical Image Computing and Computer Assisted Interventio",
    "full_text_length": 64867,
    "chunk_length": 1325
  },
  {
    "chunk_id": 4225,
    "paper_filename": "zhenjie_2024_mammoVLM_a_ generative_large_vision_language_model_for_mammography.pdf",
    "paper_title": "Zhenjie 2024 Mammovlm A  Generative Large Vision Language Model For Mammography",
    "chunk_index": 59,
    "total_chunks": 65,
    "text_content": "for detection and classification of cancer in screening mammography, in: International Conference on Medical Image Computing and Computer-Assisted Intervention, Springer, 2020, pp. 241\u2013250. [48] S. Azizi, B. Mustafa, F. Ryan, Z. Beaver, J. Freyberg, J. Deaton, A. Loh, A. Karthikesalingam, S. Kornblith, T. Chen, et al., Big self-supervised models ad- vance medical image classification, in: Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021, pp. 3478\u20133488. [49] H. Wu, F.",
    "full_text_length": 64867,
    "chunk_length": 1371
  },
  {
    "chunk_id": 4226,
    "paper_filename": "zhenjie_2024_mammoVLM_a_ generative_large_vision_language_model_for_mammography.pdf",
    "paper_title": "Zhenjie 2024 Mammovlm A  Generative Large Vision Language Model For Mammography",
    "chunk_index": 60,
    "total_chunks": 65,
    "text_content": "Wang, A. Li, Carl: Cross-aligned representation learning for multi-view lung cancer histology classification, in: International Conference on Medical Image Computing and Computer-Assisted Intervention, Springer, 2023, pp. 358\u2013367. [52] Q. Jin, C. Zou, H. Cui, C. Sun, S.-W. Huang, Y.-J. Kuo, P. Xuan, L. Cao, R. Su, L. Wei, et al., Multi-modality contrastive learning for sarcopenia screening from hip X-rays and clinical information, in: International Conference on Medical Image Computing and Compu",
    "full_text_length": 64867,
    "chunk_length": 1427
  },
  {
    "chunk_id": 4227,
    "paper_filename": "zhenjie_2024_mammoVLM_a_ generative_large_vision_language_model_for_mammography.pdf",
    "paper_title": "Zhenjie 2024 Mammovlm A  Generative Large Vision Language Model For Mammography",
    "chunk_index": 61,
    "total_chunks": 65,
    "text_content": "R.A. Hubbard, B. Geller, K. Dittus, D. Braithwaite, K.J. Wernli, D.L. Miglioretti, E.S. O\u2019Meara, B.C.S. Consortium, et al., Outcomes of screening mammography by frequency, breast density, and postmenopausal hormone therapy, JAMA Intern. Med. 173 (9) (2013) 807\u2013816. [56] M. Elshinawy, A. Badawy, W. Abdelmageed, M. Chouikha, Effect of breast density in selecting features for normal mammogram detection, in: 2011 IEEE International Symposium on Biomedical Imaging: From Nano To Macro, IEEE, 2011, pp.",
    "full_text_length": 64867,
    "chunk_length": 1359
  },
  {
    "chunk_id": 4228,
    "paper_filename": "zhenjie_2024_mammoVLM_a_ generative_large_vision_language_model_for_mammography.pdf",
    "paper_title": "Zhenjie 2024 Mammovlm A  Generative Large Vision Language Model For Mammography",
    "chunk_index": 62,
    "total_chunks": 65,
    "text_content": "[60] R. Hadsell, S. Chopra, Y. LeCun, Dimensionality reduction by learning an invariant mapping, in: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Vol. 2, 2006, pp. 1735\u20131742. [61] Y. Cui, W. Che, T. Liu, B. Qin, S. Wang, G. Hu, Revisiting pre-trained models for Chinese natural language processing, in: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings, Association for Computational Linguistics, Online, 2020, pp. 657\u2013",
    "full_text_length": 64867,
    "chunk_length": 1330
  },
  {
    "chunk_id": 4229,
    "paper_filename": "zhenjie_2024_mammoVLM_a_ generative_large_vision_language_model_for_mammography.pdf",
    "paper_title": "Zhenjie 2024 Mammovlm A  Generative Large Vision Language Model For Mammography",
    "chunk_index": 63,
    "total_chunks": 65,
    "text_content": "Martinet, M.-A. Lachaux, T. Lacroix, B. Rozi\u00e8re, N. Goyal, E. Hambro, F. Azhar, et al., Llama: Open and efficient foundation language models, 2023, arXiv preprint arXiv:2302.13971 . [65] A.Q. Jiang, A. Sablayrolles, A. Roux, A. Mensch, B. Savary, C. Bamford, D.S. Chaplot, D.d.l. Casas, E.B. Hanna, F. Bressand, et al., Mixtral of experts, 2024, arXiv preprint arXiv:2401.04088 . [66] K.A. Lee, N. Talati, R. Oudsema, S. Steinberger, L.R. Margolies, BI-RADS 3: current and future use of probably beni",
    "full_text_length": 64867,
    "chunk_length": 1285
  },
  {
    "chunk_id": 4230,
    "paper_filename": "zhenjie_2024_mammoVLM_a_ generative_large_vision_language_model_for_mammography.pdf",
    "paper_title": "Zhenjie 2024 Mammovlm A  Generative Large Vision Language Model For Mammography",
    "chunk_index": 64,
    "total_chunks": 65,
    "text_content": "R. Azevedo, Expertise in radiology: Accounting for the evidence and implications for instruction, 1999. [71] K. Zhang, J. Yu, Z. Yan, Y. Liu, E. Adhikarla, S. Fu, X. Chen, C. Chen, Y. Zhou, X. Li, et al., Biomedgpt: a unified and generalist biomedical generative pre-trained transformer for vision, language, and multimodal tasks, 2023, arXiv preprint arXiv:2305.17100 . [72] K. He, X. Zhang, S. Ren, J. Sun, Deep residual learning for image recognition, in: Proceedings of the IEEE Conference on Com",
    "full_text_length": 64867,
    "chunk_length": 936
  },
  {
    "chunk_id": 4231,
    "paper_filename": "Ziruo_2025_multi_agent_system_for_complex_resining_in radiology_visual_question_answering.pdf",
    "paper_title": "Ziruo 2025 Multi Agent System For Complex Resining In Radiology Visual Question Answering",
    "chunk_index": 0,
    "total_chunks": 46,
    "text_content": "A M ULTI -AGENT SYSTEM FOR COMPLEX REASONING IN RADIOLOGY VISUAL QUESTION ANSWERING A P REPRINT Ziruo Yi University of North Texas ziruoyi@my.unt.eduJinyu Liu University of North Texas jinyuliu@my.unt.eduTing Xiao University of North Texas ting.xiao@unt.eduMark V . Albert University of North Texas mark.albert@unt.edu August 6, 2025 ABSTRACT Radiology visual question answering (RVQA) provides precise answers to questions about chest X- ray images, alleviating radiologists\u2019 workload. While recent ",
    "full_text_length": 47501,
    "chunk_length": 1507
  },
  {
    "chunk_id": 4232,
    "paper_filename": "Ziruo_2025_multi_agent_system_for_complex_resining_in radiology_visual_question_answering.pdf",
    "paper_title": "Ziruo 2025 Multi Agent System For Complex Resining In Radiology Visual Question Answering",
    "chunk_index": 1,
    "total_chunks": 46,
    "text_content": "effectiveness of our system over strong MLLM baselines, with a case study illustrat- ing its reliability and interpretability. This work highlights the potential of multi-agent approaches to support explainable and trustworthy clinical AI applications that require complex reasoning. Keywords Radiology Visual Question Answering \u00b7Complex Reasoning \u00b7Multimodal Large Language Models \u00b7 Multi-Agent Systems \u00b7Retrieval-Augmented Generation 1 Introduction In modern healthcare, radiology plays a crucial r",
    "full_text_length": 47501,
    "chunk_length": 1495
  },
  {
    "chunk_id": 4233,
    "paper_filename": "Ziruo_2025_multi_agent_system_for_complex_resining_in radiology_visual_question_answering.pdf",
    "paper_title": "Ziruo 2025 Multi Agent System For Complex Resining In Radiology Visual Question Answering",
    "chunk_index": 2,
    "total_chunks": 46,
    "text_content": "with limited diversity in task types or reasoning complexity. Although newer datasets such as EHRXQA [5] offer larger scale and more diverse question templates, the lack of expert-level answer explanations limits their ability to evaluate models\u2019 complex reasoning capabilities. While existing benchmarks mainly contain questions requiring only basic reasoning, real-world RVQA scenarios involve subtle visual cues, multi-step inference, and domain knowledge integration, which present significant ch",
    "full_text_length": 47501,
    "chunk_length": 1408
  },
  {
    "chunk_id": 4234,
    "paper_filename": "Ziruo_2025_multi_agent_system_for_complex_resining_in radiology_visual_question_answering.pdf",
    "paper_title": "Ziruo 2025 Multi Agent System For Complex Resining In Radiology Visual Question Answering",
    "chunk_index": 3,
    "total_chunks": 46,
    "text_content": "language models (MLLMs) have shown promising results across a range of tasks including image captioning [11] and visual-language dialogue [12]. In the healthcare domain, MLLMs such as Med-PaLM 2 [13] and LLaV A-Med [14] have made notable progress in pharmaceutical research [15] and clinical support [16]. In RVQA, MLLMs integrate visual and textual information to provide precise answers that support clinical decision-making [17, 18]. However, existing MLLM approaches for RVQA often treat the mode",
    "full_text_length": 47501,
    "chunk_length": 1391
  },
  {
    "chunk_id": 4235,
    "paper_filename": "Ziruo_2025_multi_agent_system_for_complex_resining_in radiology_visual_question_answering.pdf",
    "paper_title": "Ziruo 2025 Multi Agent System For Complex Resining In Radiology Visual Question Answering",
    "chunk_index": 4,
    "total_chunks": 46,
    "text_content": "the factual accu- racy of medical MLLMs. By incorporating external knowledge, RAG enhances contextual understanding and enables more grounded responses. It has been applied to various medical tasks, including report generation [22, 23] and vi- sual question answering (VQA) [24]. However, applying RAG to RVQA presents new challenges. Retrieving too many contexts can introduce noise and redundancy, while insufficient retrieval may miss key information, ultimately reducing overall answer quality. G",
    "full_text_length": 47501,
    "chunk_length": 1427
  },
  {
    "chunk_id": 4236,
    "paper_filename": "Ziruo_2025_multi_agent_system_for_complex_resining_in radiology_visual_question_answering.pdf",
    "paper_title": "Ziruo 2025 Multi Agent System For Complex Resining In Radiology Visual Question Answering",
    "chunk_index": 5,
    "total_chunks": 46,
    "text_content": "To overcome the limitations of existing MLLM and RAG approaches and to explore the potential of multi-agent sys- tems in radiology, we propose a multi-agent system (MAS) composed of three specialized agents for RVQA: a context understanding agent (CUA), a multimodal reasoning agent (MRA), and an answer validation agent (A V A). This mod- ular design enables structured, stepwise collaboration among agents, enhancing the explainability and precision of the reasoning process while reducing hallucin",
    "full_text_length": 47501,
    "chunk_length": 1410
  },
  {
    "chunk_id": 4237,
    "paper_filename": "Ziruo_2025_multi_agent_system_for_complex_resining_in radiology_visual_question_answering.pdf",
    "paper_title": "Ziruo 2025 Multi Agent System For Complex Resining In Radiology Visual Question Answering",
    "chunk_index": 6,
    "total_chunks": 46,
    "text_content": "reasoning tasks, including answer prediction and explanation generation. (2) We construct three subsets based on ReXVQA to support our MAS and systematically evaluate its performance in challenging scenarios. (3) We con- duct extensive experiments and analyses showing that our MAS consistently outperforms strong MLLM baselines in accuracy, interpretability, and robustness on ambiguous cases. 2 Related Work MLLMs for RVQA. MLLMs have made significant progress in RVQA [32, 33, 34, 35, 36, 37], wit",
    "full_text_length": 47501,
    "chunk_length": 1427
  },
  {
    "chunk_id": 4238,
    "paper_filename": "Ziruo_2025_multi_agent_system_for_complex_resining_in radiology_visual_question_answering.pdf",
    "paper_title": "Ziruo 2025 Multi Agent System For Complex Resining In Radiology Visual Question Answering",
    "chunk_index": 7,
    "total_chunks": 46,
    "text_content": "In contrast to these black-box approaches, our MAS adopts a modular decomposition with explicit retrieval and validation stages, enhancing reasoning control, interpretability, and robustness. Retrieval-Augmented Generation. RAG has been increasingly adopted to improve factual grounding in multimodal tasks by retrieving relevant external knowledge [50, 51]. It has been applied to RVQA to reduce hallucinations and enhance factual accuracy [52]. While RAG offers clear benefits, it still struggles t",
    "full_text_length": 47501,
    "chunk_length": 1470
  },
  {
    "chunk_id": 4239,
    "paper_filename": "Ziruo_2025_multi_agent_system_for_complex_resining_in radiology_visual_question_answering.pdf",
    "paper_title": "Ziruo 2025 Multi Agent System For Complex Resining In Radiology Visual Question Answering",
    "chunk_index": 8,
    "total_chunks": 46,
    "text_content": "references to support downstream reasoning and validation using both textual and visual inputs. This design enables stronger factual grounding and cross-modal alignment, making it better suited to the complex reasoning demands of RVQA. Multi-Agent Systems. MASs have gained increasing attention in NLP and healthcare AI [55, 56, 57, 58, 59]. Typi- cally, a MAS consists of a collection of agents that interact through orchestration to enable collective intelligence via coordinated task decomposition",
    "full_text_length": 47501,
    "chunk_length": 1387
  },
  {
    "chunk_id": 4240,
    "paper_filename": "Ziruo_2025_multi_agent_system_for_complex_resining_in radiology_visual_question_answering.pdf",
    "paper_title": "Ziruo 2025 Multi Agent System For Complex Resining In Radiology Visual Question Answering",
    "chunk_index": 9,
    "total_chunks": 46,
    "text_content": "System for Complex Reasoning in Radiology Visual Question Answering A P REPRINT subtasks handled by three specialized agents: a CUA, a MRA, and an A V A. This design facilitates transparent, step- wise collaboration and enables more accurate, robust, and clinically aligned reasoning required in complex radiological scenarios. 3 Method We propose a modular multi-agent system for RVQA that decomposes complex multimodal reasoning into inter- pretable and cooperative stages. Given a multiple-choice ",
    "full_text_length": 47501,
    "chunk_length": 1412
  },
  {
    "chunk_id": 4241,
    "paper_filename": "Ziruo_2025_multi_agent_system_for_complex_resining_in radiology_visual_question_answering.pdf",
    "paper_title": "Ziruo 2025 Multi Agent System For Complex Resining In Radiology Visual Question Answering",
    "chunk_index": 10,
    "total_chunks": 46,
    "text_content": "stepwise architecture enables modular design, interpretability, and flexible integration of different LLMs and MLLMs across agents, aligning with the needs of complex reasoning in radiology MCQs. Figure 1: Overview of the proposed multi-agent system. The pipeline consists of: (1) a context understanding agent that retrieves top- krelevant QA examples and predicts task name and category; (2) a multimodal reasoning agent that generates image-grounded answers and explanations; and (3) an answer val",
    "full_text_length": 47501,
    "chunk_length": 1357
  },
  {
    "chunk_id": 4242,
    "paper_filename": "Ziruo_2025_multi_agent_system_for_complex_resining_in radiology_visual_question_answering.pdf",
    "paper_title": "Ziruo 2025 Multi Agent System For Complex Resining In Radiology Visual Question Answering",
    "chunk_index": 11,
    "total_chunks": 46,
    "text_content": "and reasoning alignment. We predict the task name and diagnostic category of the input MCQ via weighted voting over the top- k reranked examples. This step provides the predicted task name, category, and top- kQA examples most relevant to the input, which together serve as contextual inputs for the next agent. 3.2 Multimodal Reasoning Agent The MRA generates an answer and explanation by integrating the MCQ, its corresponding X-ray image(s), top- k QA examples, and the predicted task name and cat",
    "full_text_length": 47501,
    "chunk_length": 1242
  },
  {
    "chunk_id": 4243,
    "paper_filename": "Ziruo_2025_multi_agent_system_for_complex_resining_in radiology_visual_question_answering.pdf",
    "paper_title": "Ziruo 2025 Multi Agent System For Complex Resining In Radiology Visual Question Answering",
    "chunk_index": 12,
    "total_chunks": 46,
    "text_content": "step. 3.3 Answer Validation Agent The A V A assesses the reliability of the predicted answer and performs correction if its confidence score falls below a predefined threshold. It receives the MCQ, the top- kQA examples retrieved by the CUA, and the predicted answer and explanation generated by the MRA. The A V A uses an LLM to estimate the confidence score of the predicted answer. If the score exceeds the threshold, the predicted answer and explanation are accepted. Otherwise, the LLM reconside",
    "full_text_length": 47501,
    "chunk_length": 1294
  },
  {
    "chunk_id": 4244,
    "paper_filename": "Ziruo_2025_multi_agent_system_for_complex_resining_in radiology_visual_question_answering.pdf",
    "paper_title": "Ziruo 2025 Multi Agent System For Complex Resining In Radiology Visual Question Answering",
    "chunk_index": 13,
    "total_chunks": 46,
    "text_content": "MLLM baselines, and analyze the contributions of individual agents to complex reasoning in RVQA. 4.1 Experimental Setup 4.1.1 Implementation Details. Our system consists of three agents: the CUA, MRA and A V A. The CUA employs Facebook AI Similarity Search (FAISS) [69] to retrieve the top- nrelevant QA examples ( n= 10 ), which are subsequently reranked by MMed- Llama-3-8B [70]. We apply a rule-based weighted voting over the top- kexamples ( k= 5) to predict the task name and radiological catego",
    "full_text_length": 47501,
    "chunk_length": 1279
  },
  {
    "chunk_id": 4245,
    "paper_filename": "Ziruo_2025_multi_agent_system_for_complex_resining_in radiology_visual_question_answering.pdf",
    "paper_title": "Ziruo 2025 Multi Agent System For Complex Resining In Radiology Visual Question Answering",
    "chunk_index": 14,
    "total_chunks": 46,
    "text_content": "models from all components of our system to prevent evaluation bias and ensure fairness. 4.1.2 Datasets. We conduct experiments on ReXVQA, a large-scale benchmark for MCQs in chest radiology. Each example in- cludes one or more chest X-ray images, a clinically meaningful question with four answer options (A/B/C/D), and an expert-written explanation. The dataset covers five radiological reasoning tasks (e.g., presence assessment, differential diagnosis), spanning a diverse range of clinical categ",
    "full_text_length": 47501,
    "chunk_length": 1373
  },
  {
    "chunk_id": 4246,
    "paper_filename": "Ziruo_2025_multi_agent_system_for_complex_resining_in radiology_visual_question_answering.pdf",
    "paper_title": "Ziruo 2025 Multi Agent System For Complex Resining In Radiology Visual Question Answering",
    "chunk_index": 15,
    "total_chunks": 46,
    "text_content": "diverse examples from the portion of ReXVQA not included in ReXVQA-Pool. This subset is used by the CUA during the retrieval step. 4.1.3 Baseline Models. To evaluate the effectiveness of our multi-agent system, we compare its performance against several state-of-the- art (SOTA) MLLMs on ReXVQA-Hard. These include general-purpose MLLMs (e.g., Janus-Pro-7B [72], LLaV A 1.5 [73], OpenFlamingo-4B [74], Phi-3.5-Vision-Instruct [75], and Qwen2.5-VL [76]) as well as a medical-domain MLLM, MedGemma. The",
    "full_text_length": 47501,
    "chunk_length": 1418
  },
  {
    "chunk_id": 4247,
    "paper_filename": "Ziruo_2025_multi_agent_system_for_complex_resining_in radiology_visual_question_answering.pdf",
    "paper_title": "Ziruo 2025 Multi Agent System For Complex Resining In Radiology Visual Question Answering",
    "chunk_index": 16,
    "total_chunks": 46,
    "text_content": "for Complex Reasoning in Radiology Visual Question Answering A P REPRINT Qwen25VL (Qwen2.5-VL). Although Phi-3.5-Vision did not participate in the construction of the ReXVQA-Hard subset, we include it in our evaluation to enable broader comparison across diverse MLLMs. 4.1.4 Evaluation Metrics. The performance of our MAS is evaluated using accuracy for answer prediction and standard text generation metrics including BLEU [77], ROUGE-L [78], METEOR [79], and BERTScore [80] for explanation quality",
    "full_text_length": 47501,
    "chunk_length": 1447
  },
  {
    "chunk_id": 4248,
    "paper_filename": "Ziruo_2025_multi_agent_system_for_complex_resining_in radiology_visual_question_answering.pdf",
    "paper_title": "Ziruo 2025 Multi Agent System For Complex Resining In Radiology Visual Question Answering",
    "chunk_index": 17,
    "total_chunks": 46,
    "text_content": "OpenFlamingo, and Qwen25VL on ReXVQA-Pool, and select examples that are incorrectly answered by at least three of them. This approach ensures that ReXVQA-Hard contains consistently difficult examples across diverse model architectures. As shown in Table 1, the five MLLMs exhibit substantial variation in accuracy on ReXVQA-Pool, ranging from 31.87% (LLaV A) to 70.77% (MedGemma). This performance gap suggests that many examples are answered correctly by some models and in- correctly by others, ena",
    "full_text_length": 47501,
    "chunk_length": 1414
  },
  {
    "chunk_id": 4249,
    "paper_filename": "Ziruo_2025_multi_agent_system_for_complex_resining_in radiology_visual_question_answering.pdf",
    "paper_title": "Ziruo 2025 Multi Agent System For Complex Resining In Radiology Visual Question Answering",
    "chunk_index": 18,
    "total_chunks": 46,
    "text_content": "OpenFlamingo Qwen25VL Accuracy (%) 70.77 65.30 31.87 42.40 68.27 4.2 Results To evaluate the effectiveness of our MAS, we conduct both quantitative and qualitative analyses on ReXVQA-Hard. This section compares our system with several strong baseline MLLMs in terms of answer accuracy and explanation quality, and presents a case study illustrating how the agents collaborate to resolve diagnostic ambiguity. These results provide a comprehensive assessment of the pipeline\u2019s ability to handle comple",
    "full_text_length": 47501,
    "chunk_length": 1423
  },
  {
    "chunk_id": 4250,
    "paper_filename": "Ziruo_2025_multi_agent_system_for_complex_resining_in radiology_visual_question_answering.pdf",
    "paper_title": "Ziruo 2025 Multi Agent System For Complex Resining In Radiology Visual Question Answering",
    "chunk_index": 19,
    "total_chunks": 46,
    "text_content": "examples. In terms of expla- nation quality, our system obtains the highest scores in BLEU (0.1230) and ROUGE-L (0.3692), suggesting improved lexical alignment with reference explanations. For METEOR, our model achieves a strong score of 0.3449, which is competitive with the best-performing Qwen25VL (0.4125). Our pipeline also achieves a BERTScore of 0.8987, which is comparable to the top score of 0.9008 from Phi35 and reflects strong semantic consistency. These results highlight the effectivene",
    "full_text_length": 47501,
    "chunk_length": 1387
  },
  {
    "chunk_id": 4251,
    "paper_filename": "Ziruo_2025_multi_agent_system_for_complex_resining_in radiology_visual_question_answering.pdf",
    "paper_title": "Ziruo 2025 Multi Agent System For Complex Resining In Radiology Visual Question Answering",
    "chunk_index": 20,
    "total_chunks": 46,
    "text_content": "The input MCQ asks for the most likely con- 5 A Multi-Agent System for Complex Reasoning in Radiology Visual Question Answering A P REPRINT Table 2: Quantitative comparison between our MAS and SOTA MLLMs on ReXVQA-Hard. Model Accuracy BLEU ROUGE-L METEOR BERTScore MedGemma 44.03% 0.0421 0.2071 0.1903 0.8755 Janus 35.46% 0.0192 0.2118 0.2536 0.8691 LLaV A 26.08% 0.0048 0.0407 0.0521 0.8171 OpenFlamingo 28.12% 0.0612 0.2567 0.2741 0.8798 Phi35 27.94% 0.0802 0.3325 0.3130 0.9008 Qwen25VL 29.53% 0.0",
    "full_text_length": 47501,
    "chunk_length": 1318
  },
  {
    "chunk_id": 4252,
    "paper_filename": "Ziruo_2025_multi_agent_system_for_complex_resining_in radiology_visual_question_answering.pdf",
    "paper_title": "Ziruo 2025 Multi Agent System For Complex Resining In Radiology Visual Question Answering",
    "chunk_index": 21,
    "total_chunks": 46,
    "text_content": "the subtle appearance of heart failure on the image. The A V A estimates the confidence of this prediction using an LLM, and determines that it falls below the predefined threshold. It then reconsiders the question using the retrieved QA ex- amples and produces a revised answer: \u201cB. Congestive heart failure,\u201d accompanied by a clinically aligned explanation that supports the diagnosis. This case highlights how contextual knowledge, visual information, and answer validation jointly contribute to r",
    "full_text_length": 47501,
    "chunk_length": 1362
  },
  {
    "chunk_id": 4253,
    "paper_filename": "Ziruo_2025_multi_agent_system_for_complex_resining_in radiology_visual_question_answering.pdf",
    "paper_title": "Ziruo 2025 Multi Agent System For Complex Resining In Radiology Visual Question Answering",
    "chunk_index": 22,
    "total_chunks": 46,
    "text_content": "the input MCQ and its associated X-ray image(s), without access to retrieved examples or any contextual information. 6 A Multi-Agent System for Complex Reasoning in Radiology Visual Question Answering A P REPRINT 2.CUA + MRA : The CUA provides retrieved QA examples along with the predicted task name and category, which are used by the MRA in addition to the input MCQ and image(s) to generate an answer and explanation. The A V A is disabled in this configuration, so no verification or correction ",
    "full_text_length": 47501,
    "chunk_length": 1253
  },
  {
    "chunk_id": 4254,
    "paper_filename": "Ziruo_2025_multi_agent_system_for_complex_resining_in radiology_visual_question_answering.pdf",
    "paper_title": "Ziruo 2025 Multi Agent System For Complex Resining In Radiology Visual Question Answering",
    "chunk_index": 23,
    "total_chunks": 46,
    "text_content": "and BLEU, ROUGE-L, METEOR, and BERTScore assessing explanation quality. Using only the MRA results in the lowest per- formance across all metrics (e.g., accuracy: 44.03%, BLEU: 0.0421, BERTScore: 0.8755), indicating that image and question inputs alone are insufficient for reliable reasoning or explanation generation. The lack of contextual guid- ance limits the model\u2019s ability to produce clinically meaningful outputs. Introducing the CUA substantially boosts performance. Accuracy increases by o",
    "full_text_length": 47501,
    "chunk_length": 1467
  },
  {
    "chunk_id": 4255,
    "paper_filename": "Ziruo_2025_multi_agent_system_for_complex_resining_in radiology_visual_question_answering.pdf",
    "paper_title": "Ziruo 2025 Multi Agent System For Complex Resining In Radiology Visual Question Answering",
    "chunk_index": 24,
    "total_chunks": 46,
    "text_content": "value of answer validation in identifying and correcting low-confidence predictions, ultimately improving output accuracy and trustworthiness. Overall, each component plays a critical role in system performance. The full pipeline benefits from contextual retrieval, multimodal reasoning, and answer validation, highlighting the importance of a modular and cooperative architecture for complex reasoning in RVQA. Table 3: Performance comparison of different agent configurations on ReXVQA-Hard. Agents",
    "full_text_length": 47501,
    "chunk_length": 1538
  },
  {
    "chunk_id": 4256,
    "paper_filename": "Ziruo_2025_multi_agent_system_for_complex_resining_in radiology_visual_question_answering.pdf",
    "paper_title": "Ziruo 2025 Multi Agent System For Complex Resining In Radiology Visual Question Answering",
    "chunk_index": 25,
    "total_chunks": 46,
    "text_content": "for tackling radiological questions that require complex reasoning. The CUA enriches contextual information by retrieving semantically relevant QA examples and predicting the radi- ological task type and category, which provides critical guidance to downstream components. The MRA then fuses visual and textual modalities to generate clinically plausible answers and explanations, effectively addressing the complexity of radiological reasoning. Finally, the A V A estimates the confidence of each pr",
    "full_text_length": 47501,
    "chunk_length": 1414
  },
  {
    "chunk_id": 4257,
    "paper_filename": "Ziruo_2025_multi_agent_system_for_complex_resining_in radiology_visual_question_answering.pdf",
    "paper_title": "Ziruo 2025 Multi Agent System For Complex Resining In Radiology Visual Question Answering",
    "chunk_index": 26,
    "total_chunks": 46,
    "text_content": "may not generalize well across different medical domains. To address this, future work will explore dynamic thresholding strategies or learned validation mechanisms to enhance flexibility and robustness, especially for difficult or rare cases. 5 Conclusion We present a MAS for RVQA, which decomposes the task into three specialized agents for context understanding, multimodal reasoning, and answer validation. This modular design fully leverages multimodal information and en- ables interpretable a",
    "full_text_length": 47501,
    "chunk_length": 1463
  },
  {
    "chunk_id": 4258,
    "paper_filename": "Ziruo_2025_multi_agent_system_for_complex_resining_in radiology_visual_question_answering.pdf",
    "paper_title": "Ziruo 2025 Multi Agent System For Complex Resining In Radiology Visual Question Answering",
    "chunk_index": 27,
    "total_chunks": 46,
    "text_content": "Vision-language models for medical report generation and visual question answering: A review. Frontiers in artificial intelligence , 7:1430984, 2024. [2] Ziruo Yi, Ting Xiao, and Mark V Albert. A survey on multimodal large language models in radiology for report generation and visual question answering. Information , 16(2):136, 2025. [3] Jason J Lau, Soumya Gayen, Asma Ben Abacha, and Dina Demner-Fushman. A dataset of clinically generated visual questions and answers about radiology images. Scie",
    "full_text_length": 47501,
    "chunk_length": 1348
  },
  {
    "chunk_id": 4259,
    "paper_filename": "Ziruo_2025_multi_agent_system_for_complex_resining_in radiology_visual_question_answering.pdf",
    "paper_title": "Ziruo 2025 Multi Agent System For Complex Resining In Radiology Visual Question Answering",
    "chunk_index": 28,
    "total_chunks": 46,
    "text_content": "records with chest x-ray images. Advances in Neural Information Processing Systems , 36:3867\u20133880, 2023. [6] Yu Huang, Chenzhuang Du, Zihui Xue, Xuanyao Chen, Hang Zhao, and Longbo Huang. What makes multi- modal learning better than single (provably). Advances in Neural Information Processing Systems , 34:10944\u2013 10956, 2021. [7] Asim Waqas, Aakash Tripathi, Ravi P Ramachandran, Paul A Stewart, and Ghulam Rasool. Multimodal data in- tegration for oncology in the era of deep neural networks: a rev",
    "full_text_length": 47501,
    "chunk_length": 1416
  },
  {
    "chunk_id": 4260,
    "paper_filename": "Ziruo_2025_multi_agent_system_for_complex_resining_in radiology_visual_question_answering.pdf",
    "paper_title": "Ziruo 2025 Multi Agent System For Complex Resining In Radiology Visual Question Answering",
    "chunk_index": 29,
    "total_chunks": 46,
    "text_content": "Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In International conference on machine learning , pages 19730\u201319742. PMLR, 2023. [12] Yupan Huang, Zaiqiao Meng, Fangyu Liu, Yixuan Su, Nigel Collier, and Yutong Lu. Sparkles: Unlocking chats across multiple images for multimodal instruction-following models. arXiv preprint arXiv:2308.16463 , 2023. [13] Karan Singhal, Tao Tu, Juraj Gottweis, Rory Sayres, Ellery Wulczyn, Mohamed Amin, Le Hou, K",
    "full_text_length": 47501,
    "chunk_length": 1437
  },
  {
    "chunk_id": 4261,
    "paper_filename": "Ziruo_2025_multi_agent_system_for_complex_resining_in radiology_visual_question_answering.pdf",
    "paper_title": "Ziruo 2025 Multi Agent System For Complex Resining In Radiology Visual Question Answering",
    "chunk_index": 30,
    "total_chunks": 46,
    "text_content": ", 79:102527, 2023. [16] Stephen R Ali, Thomas D Dobbs, Hayley A Hutchings, and Iain S Whitaker. Using chatgpt to write patient clinic letters. The Lancet Digital Health , 5(4):e179\u2013e181, 2023. [17] Omkar Thawkar, Abdelrahman Shaker, Sahal Shaji Mullappilly, Hisham Cholakkal, Rao Muhammad Anwer, Salman Khan, Jorma Laaksonen, and Fahad Shahbaz Khan. Xraygpt: Chest radiographs summarization using medical vision-language models. arXiv preprint arXiv:2306.07971 , 2023. [18] Yakoub Bazi, Mohamad Mahmo",
    "full_text_length": 47501,
    "chunk_length": 1422
  },
  {
    "chunk_id": 4262,
    "paper_filename": "Ziruo_2025_multi_agent_system_for_complex_resining_in radiology_visual_question_answering.pdf",
    "paper_title": "Ziruo 2025 Multi Agent System For Complex Resining In Radiology Visual Question Answering",
    "chunk_index": 31,
    "total_chunks": 46,
    "text_content": "and Jianfeng Dong. Alleviating hallucination in large vision- language models with active retrieval augmentation. arXiv preprint arXiv:2408.00555 , 2024. [21] Xiaoye Qu, Jiashuo Sun, Wei Wei, and Yu Cheng. Look, compare, decide: Alleviating hallucination in large vision-language models via multi-view multi-path reasoning. arXiv preprint arXiv:2408.17150 , 2024. [22] Yogesh Kumar and Pekka Marttinen. Improving medical multi-modal contrastive learning with expert annota- tions. In European Confere",
    "full_text_length": 47501,
    "chunk_length": 1420
  },
  {
    "chunk_id": 4263,
    "paper_filename": "Ziruo_2025_multi_agent_system_for_complex_resining_in radiology_visual_question_answering.pdf",
    "paper_title": "Ziruo 2025 Multi Agent System For Complex Resining In Radiology Visual Question Answering",
    "chunk_index": 32,
    "total_chunks": 46,
    "text_content": "E Gonzalez. Gorilla: Large language model connected with massive apis. Advances in Neural Information Processing Systems , 37:126544\u2013126565, 2024. [26] Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, et al. A survey on large language model based autonomous agents. Frontiers of Computer Science , 18(6):186345, 2024. [27] Chen Qian, Wei Liu, Hongzhang Liu, Nuo Chen, Yufan Dang, Jiahao Li, Cheng Yang, Weize Chen, Yusheng Su, Xin ",
    "full_text_length": 47501,
    "chunk_length": 1321
  },
  {
    "chunk_id": 4264,
    "paper_filename": "Ziruo_2025_multi_agent_system_for_complex_resining_in radiology_visual_question_answering.pdf",
    "paper_title": "Ziruo 2025 Multi Agent System For Complex Resining In Radiology Visual Question Answering",
    "chunk_index": 33,
    "total_chunks": 46,
    "text_content": "Rong, Ryutaro Tanno, et al. Towards an ai co-scientist. arXiv preprint arXiv:2502.18864 , 2025. [30] Kyle Swanson, Wesley Wu, Nash L Bulaong, John E Pak, and James Zou. The virtual lab: Ai agents design new sars-cov-2 nanobodies with experimental validation. bioRxiv , pages 2024\u201311, 2024. [31] Ankit Pal, Jung-Oh Lee, Xiaoman Zhang, Malaikannan Sankarasubbu, Seunghyeon Roh, Won Jung Kim, Meesun Lee, and Pranav Rajpurkar. Rexvqa: A large-scale visual question answering benchmark for gener- alist c",
    "full_text_length": 47501,
    "chunk_length": 1403
  },
  {
    "chunk_id": 4265,
    "paper_filename": "Ziruo_2025_multi_agent_system_for_complex_resining_in radiology_visual_question_answering.pdf",
    "paper_title": "Ziruo 2025 Multi Agent System For Complex Resining In Radiology Visual Question Answering",
    "chunk_index": 34,
    "total_chunks": 46,
    "text_content": "and Computer-Assisted Intervention , pages 374\u2013383. Springer, 2023. [34] Tiancheng Gu, Kaicheng Yang, Dongnan Liu, and Weidong Cai. Lapa: Latent prompt assist model for med- ical visual question answering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 4971\u20134980, 2024. [35] Timothy Ossowski and Junjie Hu. Multimodal prompt retrieval for generative visual question answering. arXiv preprint arXiv:2306.17675 , 2023. [36] Jonggwon Park, Soobum Kim, Byung",
    "full_text_length": 47501,
    "chunk_length": 1394
  },
  {
    "chunk_id": 4266,
    "paper_filename": "Ziruo_2025_multi_agent_system_for_complex_resining_in radiology_visual_question_answering.pdf",
    "paper_title": "Ziruo 2025 Multi Agent System For Complex Resining In Radiology Visual Question Answering",
    "chunk_index": 35,
    "total_chunks": 46,
    "text_content": "Atilla Kiraly, Sahar Kazemzadeh, Zakkai Melamed, et al. Elixr: Towards a general purpose x-ray artificial intel- ligence system through alignment of large language models and radiology vision encoders. arXiv preprint arXiv:2308.01317 , 2023. [39] Gang Liu, Jinlong He, Pengfei Li, Genrong He, Zhaolin Chen, and Shenjun Zhong. Pefomed: Parameter efficient fine-tuning of multimodal large language models for medical imaging. arXiv preprint arXiv:2401.02797 , 2024. 9 A Multi-Agent System for Complex R",
    "full_text_length": 47501,
    "chunk_length": 1390
  },
  {
    "chunk_id": 4267,
    "paper_filename": "Ziruo_2025_multi_agent_system_for_complex_resining_in radiology_visual_question_answering.pdf",
    "paper_title": "Ziruo 2025 Multi Agent System For Complex Resining In Radiology Visual Question Answering",
    "chunk_index": 36,
    "total_chunks": 46,
    "text_content": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages 19358\u201319369, 2023. [42] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, et al. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, march 2023. URL https://lmsys. org/blog/2023-03-30-vicuna , 3(5), 2023. [43] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei,",
    "full_text_length": 47501,
    "chunk_length": 1347
  },
  {
    "chunk_id": 4268,
    "paper_filename": "Ziruo_2025_multi_agent_system_for_complex_resining_in radiology_visual_question_answering.pdf",
    "paper_title": "Ziruo 2025 Multi Agent System For Complex Resining In Radiology Visual Question Answering",
    "chunk_index": 37,
    "total_chunks": 46,
    "text_content": "Kangyu Zhu, et al. Cares: A comprehensive benchmark of trustworthiness in medical vision language models. Advances in Neural Information Processing Systems , 37:140334\u2013140365, 2024. [46] Hanchao Liu, Wenyuan Xue, Yifei Chen, Dapeng Chen, Xiutian Zhao, Ke Wang, Liping Hou, Rongjun Li, and Wei Peng. A survey on hallucination in large vision-language models. arXiv preprint arXiv:2402.00253 , 2024. [47] Hikmat Khan, Nidhal C Bouaynaya, and Ghulam Rasool. The importance of robust features in mitigati",
    "full_text_length": 47501,
    "chunk_length": 1445
  },
  {
    "chunk_id": 4269,
    "paper_filename": "Ziruo_2025_multi_agent_system_for_complex_resining_in radiology_visual_question_answering.pdf",
    "paper_title": "Ziruo 2025 Multi Agent System For Complex Resining In Radiology Visual Question Answering",
    "chunk_index": 38,
    "total_chunks": 46,
    "text_content": "preprint arXiv:2503.18968 , 2025. [50] Mohammad Mahdi Abootorabi, Amirhosein Zobeiri, Mahdi Dehghani, Mohammadali Mohammadkhani, Bardia Mohammadi, Omid Ghahroodi, Mahdieh Soleymani Baghshah, and Ehsaneddin Asgari. Ask in any modality: A comprehensive survey on multimodal retrieval-augmented generation. arXiv preprint arXiv:2502.08826 , 2025. [51] Matin Mortaheb, Mohammad A Amir Khojastepour, Srimat T Chakradhar, and Sennur Ulukus. Re-ranking the context for multimodal retrieval augmented generat",
    "full_text_length": 47501,
    "chunk_length": 1393
  },
  {
    "chunk_id": 4270,
    "paper_filename": "Ziruo_2025_multi_agent_system_for_complex_resining_in radiology_visual_question_answering.pdf",
    "paper_title": "Ziruo 2025 Multi Agent System For Complex Resining In Radiology Visual Question Answering",
    "chunk_index": 39,
    "total_chunks": 46,
    "text_content": "Peng Xia, Ruiyi Zhang, Tong Sun, Yun Li, Hongtu Zhu, and Huaxiu Yao. Mdocagent: A multi-modal multi-agent framework for document understanding. arXiv preprint arXiv:2503.13964 , 2025. [55] Yu He Ke, Rui Yang, Sui An Lie, Taylor Xin Yi Lim, Hairil Rizal Abdullah, Daniel Shu Wei Ting, and Nan Liu. Enhancing diagnostic accuracy through multi-agent conversations: using large language models to mitigate cognitive bias. arXiv preprint arXiv:2401.14589 , 2024. [56] Andries Petrus Smit, Paul Duckworth, ",
    "full_text_length": 47501,
    "chunk_length": 1361
  },
  {
    "chunk_id": 4271,
    "paper_filename": "Ziruo_2025_multi_agent_system_for_complex_resining_in radiology_visual_question_answering.pdf",
    "paper_title": "Ziruo 2025 Multi Agent System For Complex Resining In Radiology Visual Question Answering",
    "chunk_index": 40,
    "total_chunks": 46,
    "text_content": "Wu Yuan. Medco: Medical education copilots based on a multi-agent framework. arXiv preprint arXiv:2408.12496 , 2024. 10 A Multi-Agent System for Complex Reasoning in Radiology Visual Question Answering A P REPRINT [59] Ling Yue and Tianfan Fu. Ct-agent: Clinical trial multi-agent with large language model-based reasoning. arXiv e-prints , pages arXiv\u20132404, 2024. [60] Mert Cemri, Melissa Z Pan, Shuyi Yang, Lakshya A Agrawal, Bhavya Chopra, Rishabh Tiwari, Kurt Keutzer, Aditya Parameswaran, Dan Kl",
    "full_text_length": 47501,
    "chunk_length": 1405
  },
  {
    "chunk_id": 4272,
    "paper_filename": "Ziruo_2025_multi_agent_system_for_complex_resining_in radiology_visual_question_answering.pdf",
    "paper_title": "Ziruo 2025 Multi Agent System For Complex Resining In Radiology Visual Question Answering",
    "chunk_index": 41,
    "total_chunks": 46,
    "text_content": "[63] Hongxin Zhang, Weihua Du, Jiaming Shan, Qinhong Zhou, Yilun Du, Joshua B Tenenbaum, Tianmin Shu, and Chuang Gan. Building cooperative embodied agents modularly with large language models. arXiv preprint arXiv:2307.02485 , 2023. [64] Yilun Du, Shuang Li, Antonio Torralba, Joshua B Tenenbaum, and Igor Mordatch. Improving factuality and reasoning in language models through multiagent debate. In Forty-first International Conference on Machine Learning , 2023. [65] Joon Sung Park, Joseph O\u2019Brien",
    "full_text_length": 47501,
    "chunk_length": 1386
  },
  {
    "chunk_id": 4273,
    "paper_filename": "Ziruo_2025_multi_agent_system_for_complex_resining_in radiology_visual_question_answering.pdf",
    "paper_title": "Ziruo 2025 Multi Agent System For Complex Resining In Radiology Visual Question Answering",
    "chunk_index": 42,
    "total_chunks": 46,
    "text_content": "Srivastav, Md Abdul Kadir, and Daniel Sonntag. Towards interpretable radiology report generation via concept bottlenecks using a multi-agentic rag. In European Conference on Infor- mation Retrieval , pages 201\u2013209. Springer, 2025. [68] Fang Zeng, Zhiliang Lyu, Quanzheng Li, and Xiang Li. Enhancing llms for impression generation in radiology reports through a multi-agent system. arXiv preprint arXiv:2412.06828 , 2024. [69] Jeff Johnson, Matthijs Douze, and Herv \u00b4e J\u00b4egou. Billion-scale similarity",
    "full_text_length": 47501,
    "chunk_length": 1422
  },
  {
    "chunk_id": 4274,
    "paper_filename": "Ziruo_2025_multi_agent_system_for_complex_resining_in radiology_visual_question_answering.pdf",
    "paper_title": "Ziruo 2025 Multi Agent System For Complex Resining In Radiology Visual Question Answering",
    "chunk_index": 43,
    "total_chunks": 46,
    "text_content": "Pan, Wen Liu, Zhenda Xie, Xingkai Yu, and Chong Ruan. Janus-pro: Unified multimodal understanding and generation with data and model scaling. arXiv preprint arXiv:2501.17811 , 2025. [73] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems , 36:34892\u201334916, 2023. [74] Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Shiori Sagawa, et al. Op",
    "full_text_length": 47501,
    "chunk_length": 1384
  },
  {
    "chunk_id": 4275,
    "paper_filename": "Ziruo_2025_multi_agent_system_for_complex_resining_in radiology_visual_question_answering.pdf",
    "paper_title": "Ziruo 2025 Multi Agent System For Complex Resining In Radiology Visual Question Answering",
    "chunk_index": 44,
    "total_chunks": 46,
    "text_content": "preprint arXiv:2502.13923 , 2025. [77] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics , pages 311\u2013318, 2002. 11 A Multi-Agent System for Complex Reasoning in Radiology Visual Question Answering A P REPRINT [78] Chin-Yew Lin. Rouge: A package for automatic evaluation of summaries. In Text summarization branches out , pages 74\u201381, ",
    "full_text_length": 47501,
    "chunk_length": 1236
  },
  {
    "chunk_id": 4276,
    "paper_filename": "Ziruo_2025_multi_agent_system_for_complex_resining_in radiology_visual_question_answering.pdf",
    "paper_title": "Ziruo 2025 Multi Agent System For Complex Resining In Radiology Visual Question Answering",
    "chunk_index": 45,
    "total_chunks": 46,
    "text_content": "Jinyu Xiang, Fang Wu, Yilun Zhao, Chenglin Wu, Wenqi Shi, et al. Medagentsbench: Benchmarking thinking models and agent frameworks for complex medical reasoning. arXiv preprint arXiv:2503.07459 , 2025. 12",
    "full_text_length": 47501,
    "chunk_length": 204
  },
  {
    "chunk_id": 4277,
    "paper_filename": "zuhra_2023_breast_cancer_detection_in_mamography_using_cnn_with_feature_selection.pdf",
    "paper_title": "Zuhra 2023 Breast Cancer Detection In Mamography Using Cnn With Feature Selection",
    "chunk_index": 0,
    "total_chunks": 57,
    "text_content": "Citation: Jafari, Z.; Karami, E. Breast Cancer Detection in Mammography Images: A CNN-Based Approach with Feature Selection. Information 2023 ,14, 410. https://doi.org/ 10.3390/info14070410 Academic Editor: Heming Jia Received: 27 May 2023 Revised: 4 July 2023 Accepted: 14 July 2023 Published: 16 July 2023 Copyright: \u00a9 2023 by the authors. Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC B",
    "full_text_length": 54595,
    "chunk_length": 1458
  },
  {
    "chunk_id": 4278,
    "paper_filename": "zuhra_2023_breast_cancer_detection_in_mamography_using_cnn_with_feature_selection.pdf",
    "paper_title": "Zuhra 2023 Breast Cancer Detection In Mamography Using Cnn With Feature Selection",
    "chunk_index": 1,
    "total_chunks": 57,
    "text_content": "novel method based on feature extraction and reduction for the detection of breast cancer in mammography images. First, we extract features from multiple pre-trained convolutional neural network (CNN) models, and then concatenate them. The most informative features are selected based on their mutual information with the target variable. Subsequently, the selected features can be classi\ufb01ed using a machine learning algorithm. We evaluate our approach using four different machine learning algorithm",
    "full_text_length": 54595,
    "chunk_length": 1417
  },
  {
    "chunk_id": 4279,
    "paper_filename": "zuhra_2023_breast_cancer_detection_in_mamography_using_cnn_with_feature_selection.pdf",
    "paper_title": "Zuhra 2023 Breast Cancer Detection In Mamography Using Cnn With Feature Selection",
    "chunk_index": 2,
    "total_chunks": 57,
    "text_content": "as high as 94.5%, and for the DDSM dataset, an accuracy of 96% is attained. These results highlight the effectiveness of our method in accurately diagnosing breast lesions and surpassing existing approaches. Keywords: breast cancer; convolutional neural network (CNN); computer aided diagnosis (CAD); feature selection; feature classi\ufb01cation; mammography images 1. Introduction Breast cancer (BC) is a widespread form of cancer with millions of new diagnoses and deaths each year [ 1]. In 2020 alone,",
    "full_text_length": 54595,
    "chunk_length": 1379
  },
  {
    "chunk_id": 4280,
    "paper_filename": "zuhra_2023_breast_cancer_detection_in_mamography_using_cnn_with_feature_selection.pdf",
    "paper_title": "Zuhra 2023 Breast Cancer Detection In Mamography Using Cnn With Feature Selection",
    "chunk_index": 3,
    "total_chunks": 57,
    "text_content": "of false positive results. This can result in unnecessary anxiety, inconvenient follow-up care, extra imaging tests, and sometimes a need for tissue sampling (often a needle biopsy) [ 5,6]. Additionally, machine learning techniques have the potential to improve the process of evaluating multiple-view radiology images based on graph-based clustering techniques [ 7\u201310]. Deep learning as a subset of machine learning in recent years has revolutionized the interpretation of diagnostic imaging studies",
    "full_text_length": 54595,
    "chunk_length": 1403
  },
  {
    "chunk_id": 4281,
    "paper_filename": "zuhra_2023_breast_cancer_detection_in_mamography_using_cnn_with_feature_selection.pdf",
    "paper_title": "Zuhra 2023 Breast Cancer Detection In Mamography Using Cnn With Feature Selection",
    "chunk_index": 4,
    "total_chunks": 57,
    "text_content": "410 2 of 14 of breast cancer images such as ultrasound (US), magnetic resonance imaging (MRI), and X-ray as follows: US Images: Ero\u02d8 glu Y [ 14] proposed a hybrid-based CNN system based on ultrasonog- raphy images for diagnosing BC by extracting features from Alexnet, MobilenetV2, and Resnet50 then, after concatenating them, mRMR features selection method was used to select the best features. This system used machine learning algorithms to support vector machine (SVM) and k-nearest neighbors (k-",
    "full_text_length": 54595,
    "chunk_length": 1274
  },
  {
    "chunk_id": 4282,
    "paper_filename": "zuhra_2023_breast_cancer_detection_in_mamography_using_cnn_with_feature_selection.pdf",
    "paper_title": "Zuhra 2023 Breast Cancer Detection In Mamography Using Cnn With Feature Selection",
    "chunk_index": 5,
    "total_chunks": 57,
    "text_content": "cropping a region of interest, enhancing it using \ufb01lters and clustering techniques, extracting features, and performing classi\ufb01cation with a neural network and a k-NN classi\ufb01er. MRI Images: Zhou J et al. [ 17] proposed a 3D deep CNN for the detection and localization of BC in dynamic contrast-enhanced MRI data using a weakly supervised approach and achieved 83.7% accuracy. In [ 18], a multi-layer CNN was designed to classify MRI images as malignant or benign tumors using pixel information and on",
    "full_text_length": 54595,
    "chunk_length": 1300
  },
  {
    "chunk_id": 4283,
    "paper_filename": "zuhra_2023_breast_cancer_detection_in_mamography_using_cnn_with_feature_selection.pdf",
    "paper_title": "Zuhra 2023 Breast Cancer Detection In Mamography Using Cnn With Feature Selection",
    "chunk_index": 6,
    "total_chunks": 57,
    "text_content": "of mediolateral oblique (MLO) and craniocaudal (CC). Multi-scale features and a penalty term were used and achieved 82.02% accuracy on the DDSM dataset. Ridhi Hela et al. in [ 21] proposed a methodology for BC detection using the CBIS-DDSM image dataset. Image pre-processing was done, followed by feature extraction using multiple CNN models (AlexNet, VGG16, ResNet, GoogLeNet, and InceptionResNet). The extracted features were evaluated using a neural network classi\ufb01er, achieving an accuracy of 88",
    "full_text_length": 54595,
    "chunk_length": 1368
  },
  {
    "chunk_id": 4284,
    "paper_filename": "zuhra_2023_breast_cancer_detection_in_mamography_using_cnn_with_feature_selection.pdf",
    "paper_title": "Zuhra 2023 Breast Cancer Detection In Mamography Using Cnn With Feature Selection",
    "chunk_index": 7,
    "total_chunks": 57,
    "text_content": "BC detection holds great promise in improving patient outcomes and advancing the \ufb01eld of medical imaging diagnostics. This paper provides two signi\ufb01cant contributions to the existing literature. Firstly, it extracts a comprehensive set of features from diverse pre-trained CNNs for different perspectives. Additionally, it incorporates additional features like age to create a feature vector. Secondly, it employs a methodology to reduce feature vector dimensionality by eliminating weak features bas",
    "full_text_length": 54595,
    "chunk_length": 1359
  },
  {
    "chunk_id": 4285,
    "paper_filename": "zuhra_2023_breast_cancer_detection_in_mamography_using_cnn_with_feature_selection.pdf",
    "paper_title": "Zuhra 2023 Breast Cancer Detection In Mamography Using Cnn With Feature Selection",
    "chunk_index": 8,
    "total_chunks": 57,
    "text_content": "results obtained for various datasets. The paper is concluded in Section 5. 2. Materials and Methods 2.1. Datasets A. The main dataset for this project is the radiological society of north america (RSNA) dataset from a recent Kaggle competition [ 22]. The dataset contains 54,713 images in Information 2023 ,14, 410 3 of 14 dicom format from roughly 11,000 patients. For each patient, there are at least four images from different laterality and views. For each subject, two different views CC and ML",
    "full_text_length": 54595,
    "chunk_length": 1218
  },
  {
    "chunk_id": 4286,
    "paper_filename": "zuhra_2023_breast_cancer_detection_in_mamography_using_cnn_with_feature_selection.pdf",
    "paper_title": "Zuhra 2023 Breast Cancer Detection In Mamography Using Cnn With Feature Selection",
    "chunk_index": 9,
    "total_chunks": 57,
    "text_content": "any published research yet. Hence, for comparison purposes, we use two other well-known datasets MIAS and DDSM. This dataset is imbalanced as only 2 percent of the images are from cancer patients, which makes any classi\ufb01cation method biased. To compensate for this, we use all positive cases and only 2320 images from negative cases. Figure 1 depicts two sample images from this dataset for cancer and normal cases. Information 2023 , 14, x FOR PEER REVIEW 3 of 14 2. Materia ls and Methods 2.1. Data",
    "full_text_length": 54595,
    "chunk_length": 1203
  },
  {
    "chunk_id": 4287,
    "paper_filename": "zuhra_2023_breast_cancer_detection_in_mamography_using_cnn_with_feature_selection.pdf",
    "paper_title": "Zuhra 2023 Breast Cancer Detection In Mamography Using Cnn With Feature Selection",
    "chunk_index": 10,
    "total_chunks": 57,
    "text_content": "laterality were provided. The images are of various sizes and formats, including jpeg and jpeg2000, an d different types, such as monochrome -1 and monochrome -2. The dataset provides additional features some of which can be used for classification purposes: age, implant, BIRADS, and density. We base our work on this dataset, but since this dataset is new, it has not been used in any published research yet. Hence, for comparison purposes, we use two other well -known datasets MIAS and DDSM. This",
    "full_text_length": 54595,
    "chunk_length": 1248
  },
  {
    "chunk_id": 4288,
    "paper_filename": "zuhra_2023_breast_cancer_detection_in_mamography_using_cnn_with_feature_selection.pdf",
    "paper_title": "Zuhra 2023 Breast Cancer Detection In Mamography Using Cnn With Feature Selection",
    "chunk_index": 11,
    "total_chunks": 57,
    "text_content": "for the development and evaluation of CAD systems for BC detection. It consists of 322 mammographic images, with each image accompanied by a corresponding ground truth classification of benign or malignant tumors. The dataset is particularly valuable for researchers interested in developing machine learning algorithms for BC detection, as it includes examples of both normal and abnormal mammograms, as well as a range of breast densities and lesion types. Figure 2 depicts two samp le images from ",
    "full_text_length": 54595,
    "chunk_length": 1195
  },
  {
    "chunk_id": 4289,
    "paper_filename": "zuhra_2023_breast_cancer_detection_in_mamography_using_cnn_with_feature_selection.pdf",
    "paper_title": "Zuhra 2023 Breast Cancer Detection In Mamography Using Cnn With Feature Selection",
    "chunk_index": 12,
    "total_chunks": 57,
    "text_content": "by experts. In this research, we do not use the CBIS -DDSM and use the original DDSM data set as we are classifying the images from normal subjects and cancer patients. Figure 3 depicts two sample images from this dataset for cancer and normal cases. Table 1 summarizes these three datasets. (a) (b) Figure 1. These figures show two sample images from the RSNA dataset for (a) a cancerous, and (b) a normal subject. Figure 1. These \ufb01gures show two sample images from the RSNA dataset for ( a) a cance",
    "full_text_length": 54595,
    "chunk_length": 1199
  },
  {
    "chunk_id": 4290,
    "paper_filename": "zuhra_2023_breast_cancer_detection_in_mamography_using_cnn_with_feature_selection.pdf",
    "paper_title": "Zuhra 2023 Breast Cancer Detection In Mamography Using Cnn With Feature Selection",
    "chunk_index": 13,
    "total_chunks": 57,
    "text_content": "for researchers interested in developing machine learning algorithms for BC detection, as it includes examples of both normal and abnormal mammograms, as well as a range of breast densities and lesion types. Figure 2 depicts two sample images from this dataset for cancer and normal cases. C. The digital database for screening mammography (DDSM) [ 24] includes 55,890 im- ages, of which 14% are positive, and the remaining 86% are negative. Images were tiled into 598 \u0002598 tiles, which were then res",
    "full_text_length": 54595,
    "chunk_length": 1156
  },
  {
    "chunk_id": 4291,
    "paper_filename": "zuhra_2023_breast_cancer_detection_in_mamography_using_cnn_with_feature_selection.pdf",
    "paper_title": "Zuhra 2023 Breast Cancer Detection In Mamography Using Cnn With Feature Selection",
    "chunk_index": 14,
    "total_chunks": 57,
    "text_content": "and normal cases. Table 1 summarizes these three datasets. Information 2023 ,14, 410 4 of 14 Information 2023 , 14, x FOR PEER REVIEW 4 of 14 (a) (b) Figure 2. These figures show two sample images from the MIAS dataset for ( a) cancerous, and ( b) normal subjects. (a) (b) Figure 3. These figures show two sample images from the DDSM dataset for (a) cancerous and (b) normal subjects. Table 1. This table shows the description of three datasets. Dataset Number of Images Image Types Image Size RSNA 5",
    "full_text_length": 54595,
    "chunk_length": 1135
  },
  {
    "chunk_id": 4292,
    "paper_filename": "zuhra_2023_breast_cancer_detection_in_mamography_using_cnn_with_feature_selection.pdf",
    "paper_title": "Zuhra 2023 Breast Cancer Detection In Mamography Using Cnn With Feature Selection",
    "chunk_index": 15,
    "total_chunks": 57,
    "text_content": "layer uses a large receptive field to capture low -level features such Figure 2. These \ufb01gures show two sample images from the MIAS dataset for ( a) cancerous, and (b) normal subjects. Information 2023 , 14, x FOR PEER REVIEW 4 of 14 (a) (b) Figure 2. These figures show two sample images from the MIAS dataset for ( a) cancerous, and ( b) normal subjects. (a) (b) Figure 3. These figures show two sample images from the DDSM dataset for (a) cancerous and (b) normal subjects. Table 1. This table show",
    "full_text_length": 54595,
    "chunk_length": 1148
  },
  {
    "chunk_id": 4293,
    "paper_filename": "zuhra_2023_breast_cancer_detection_in_mamography_using_cnn_with_feature_selection.pdf",
    "paper_title": "Zuhra 2023 Breast Cancer Detection In Mamography Using Cnn With Feature Selection",
    "chunk_index": 16,
    "total_chunks": 57,
    "text_content": "of eight layers, including five convolutional layers and three fully connected layers. The first convolut ional layer uses a large receptive field to capture low -level features such Figure 3. These \ufb01gures show two sample images from the DDSM dataset for ( a) cancerous and (b) normal subjects. Table 1. This table shows the description of three datasets. Dataset Number of Images Image Types Image Size RSNA 54,713 Variable Variable MIAS 322 PGM 1024 \u00021024 DDSM 55,890 JPEG 598 \u0002598 2.2. Models A. A",
    "full_text_length": 54595,
    "chunk_length": 1277
  },
  {
    "chunk_id": 4294,
    "paper_filename": "zuhra_2023_breast_cancer_detection_in_mamography_using_cnn_with_feature_selection.pdf",
    "paper_title": "Zuhra 2023 Breast Cancer Detection In Mamography Using Cnn With Feature Selection",
    "chunk_index": 17,
    "total_chunks": 57,
    "text_content": "use smaller receptive \ufb01elds to capture increasingly complex and abstract features. AlexNet was the \ufb01rst deep network to successfully use the recti\ufb01ed linear unit (ReLU) activation functions, which have since become a standard activation function in deep learning. It also used dropout regularization to prevent over\ufb01tting during training. AlexNet\u2019s success on the ImageNet dataset, which contains over one million images, demonstrated the potential of deep neural networks for image recognition tasks",
    "full_text_length": 54595,
    "chunk_length": 1353
  },
  {
    "chunk_id": 4295,
    "paper_filename": "zuhra_2023_breast_cancer_detection_in_mamography_using_cnn_with_feature_selection.pdf",
    "paper_title": "Zuhra 2023 Breast Cancer Detection In Mamography Using Cnn With Feature Selection",
    "chunk_index": 18,
    "total_chunks": 57,
    "text_content": "high-level features. C. Ef\ufb01cientNet [ 27] is a family of deep CNN architectures that were introduced in 2019 and have achieved state-of-the-art performance on a range of computer vision tasks. Ef\ufb01cientNet uses a compound scaling method to simultaneously optimize the depth, width, and resolution of the network, allowing it to achieve high accuracy while maintaining computational ef\ufb01ciency. Ef\ufb01cientNet consists of a backbone network that extracts features from input images and a head network that ",
    "full_text_length": 54595,
    "chunk_length": 1432
  },
  {
    "chunk_id": 4296,
    "paper_filename": "zuhra_2023_breast_cancer_detection_in_mamography_using_cnn_with_feature_selection.pdf",
    "paper_title": "Zuhra 2023 Breast Cancer Detection In Mamography Using Cnn With Feature Selection",
    "chunk_index": 19,
    "total_chunks": 57,
    "text_content": "emphasis on computational ef\ufb01ciency, MobileNet can effectively extract features from mammography images, enabling the detection of subtle patterns or abnormal- ities associated with breast cancer. By utilizing depthwise separable convolutions, MobileNet optimizes memory consumption and computational load, making it ideal for resource-constrained environments. The integration of the ReLU6 activation function further enhances ef\ufb01ciency and compatibility with medical imaging devices. Overall, Mobil",
    "full_text_length": 54595,
    "chunk_length": 1484
  },
  {
    "chunk_id": 4297,
    "paper_filename": "zuhra_2023_breast_cancer_detection_in_mamography_using_cnn_with_feature_selection.pdf",
    "paper_title": "Zuhra 2023 Breast Cancer Detection In Mamography Using Cnn With Feature Selection",
    "chunk_index": 20,
    "total_chunks": 57,
    "text_content": "visual data. 3. Proposed Method In this paper, we propose a method based on the extraction and concatenation of features obtained from various CNN models. The extracted features are then reduced such that only good features are selected and then used for the classi\ufb01cation of normal and cancerous images. Figure 4 illustrates the block diagram of the proposed system. As one can see, the images from different datasets are \ufb01rst preprocessed, and then features are extracted through different CNN mode",
    "full_text_length": 54595,
    "chunk_length": 1290
  },
  {
    "chunk_id": 4298,
    "paper_filename": "zuhra_2023_breast_cancer_detection_in_mamography_using_cnn_with_feature_selection.pdf",
    "paper_title": "Zuhra 2023 Breast Cancer Detection In Mamography Using Cnn With Feature Selection",
    "chunk_index": 21,
    "total_chunks": 57,
    "text_content": "photometric interpretations known as MONOCHROME1 and MONOCHROME2. The former represents grayscale images with ascending pixel values from bright to dark, while the latter represents grayscale images with ascending pixel values from dark to bright. To ensure consistency within the RSNA dataset, we convert all MONOCHROME1 images to MONOCHROME2. In order to standardize the pixel values across the RSNA dataset, intensity normalization is performed. This involves scaling the pixel values to the range",
    "full_text_length": 54595,
    "chunk_length": 1272
  },
  {
    "chunk_id": 4299,
    "paper_filename": "zuhra_2023_breast_cancer_detection_in_mamography_using_cnn_with_feature_selection.pdf",
    "paper_title": "Zuhra 2023 Breast Cancer Detection In Mamography Using Cnn With Feature Selection",
    "chunk_index": 22,
    "total_chunks": 57,
    "text_content": "the region of interest, we initially apply a global thresholding method to the image. Subsequently, we extract the contour of the largest object present in the image, which corresponds to the breast area. Utilizing this contour, we generated a mask that enables us to crop the image and isolate the speci\ufb01c region of interest for further analysis. 3. Image Alignment: In breast cancer datasets, there are two distinct lateral- ity categories: left and right. To enhance consistency and improve accu- ",
    "full_text_length": 54595,
    "chunk_length": 1247
  },
  {
    "chunk_id": 4300,
    "paper_filename": "zuhra_2023_breast_cancer_detection_in_mamography_using_cnn_with_feature_selection.pdf",
    "paper_title": "Zuhra 2023 Breast Cancer Detection In Mamography Using Cnn With Feature Selection",
    "chunk_index": 23,
    "total_chunks": 57,
    "text_content": "model, the features are extracted from the last layer before the last fully connected (FC) layer as the output of the \ufb01nal FC layer has been trained for 1000 classes of the ImageNet dataset, and hence, we skip this layer and extract the features from the last layer before the \ufb01nal FC layer. Table 2 depicts the layer before the \ufb01nal FC layer and the number of features extracted for each CNN model used in this paper. C. Feature concatenation: The 1-dimensional (1D) features extracted in the previo",
    "full_text_length": 54595,
    "chunk_length": 1122
  },
  {
    "chunk_id": 4301,
    "paper_filename": "zuhra_2023_breast_cancer_detection_in_mamography_using_cnn_with_feature_selection.pdf",
    "paper_title": "Zuhra 2023 Breast Cancer Detection In Mamography Using Cnn With Feature Selection",
    "chunk_index": 24,
    "total_chunks": 57,
    "text_content": "for the patient age. Figure 5 depicts the distribution of the age feature provided by the RSNA dataset for both cancer and non-cancer subjects. As can be observed, age can also be considered a valuable feature. We can also simply normalize and add age to our feature vector to have 18,385 features in total. D. Feature selection: The majority of the features are redundant and do not carry any useful information and only increase the complexity of the system. Figure 6 illustrates 2 samples of good ",
    "full_text_length": 54595,
    "chunk_length": 1204
  },
  {
    "chunk_id": 4302,
    "paper_filename": "zuhra_2023_breast_cancer_detection_in_mamography_using_cnn_with_feature_selection.pdf",
    "paper_title": "Zuhra 2023 Breast Cancer Detection In Mamography Using Cnn With Feature Selection",
    "chunk_index": 25,
    "total_chunks": 57,
    "text_content": "features carry useful information, although small, that can improve the performance of classi\ufb01ers used in the next step. To compute mutual information we use the method in [30]. We empirically found a 0.02 threshold gives us the best results. Note that we have also adopted feature selection based on mutual information empirically and Information 2023 ,14, 410 7 of 14 after using various feature selection methods. The number of features for each dataset before and after feature selection is prese",
    "full_text_length": 54595,
    "chunk_length": 1219
  },
  {
    "chunk_id": 4303,
    "paper_filename": "zuhra_2023_breast_cancer_detection_in_mamography_using_cnn_with_feature_selection.pdf",
    "paper_title": "Zuhra 2023 Breast Cancer Detection In Mamography Using Cnn With Feature Selection",
    "chunk_index": 26,
    "total_chunks": 57,
    "text_content": "Additionally, we limit the maximum number of features considered for each tree to 5 and the maximum tree depth to 4. These parameter settings are chosen to optimize the performance of our model and improve the accuracy of breast cancer detection in our X-ray image datasets. In our SVM classi\ufb01er implementation, we utilize a linear kernel and set the regularization parameter \u201cC\u201d to a value of 1. The linear kernel allows us to learn a linear decision boundary, while the \u201cC\u201d parameter balances the t",
    "full_text_length": 54595,
    "chunk_length": 1185
  },
  {
    "chunk_id": 4304,
    "paper_filename": "zuhra_2023_breast_cancer_detection_in_mamography_using_cnn_with_feature_selection.pdf",
    "paper_title": "Zuhra 2023 Breast Cancer Detection In Mamography Using Cnn With Feature Selection",
    "chunk_index": 27,
    "total_chunks": 57,
    "text_content": "FOR PEER REVIEW 7 of 14 Figure 4. Block diagram of the proposed system. Table 2. This table shows the CNN models used in the proposed method along with the layer name where the features have been extracted and the number of features extracted from each model. CNN Models Layer Name 1 Number of Features ResNet50 avg_pool 2048 AlexNet fc8_preflatten 4096 MobileNetSmall Logits 1000 ConvNeXtSmall head_layer 768 EfficientNet avg_pool 1280 1 Layer\u2019s names have been taken from TensorFlow models. C. Feat",
    "full_text_length": 54595,
    "chunk_length": 1172
  },
  {
    "chunk_id": 4305,
    "paper_filename": "zuhra_2023_breast_cancer_detection_in_mamography_using_cnn_with_feature_selection.pdf",
    "paper_title": "Zuhra 2023 Breast Cancer Detection In Mamography Using Cnn With Feature Selection",
    "chunk_index": 28,
    "total_chunks": 57,
    "text_content": "additional useful feature for the patient age. Fig ure 5 depicts the distribution of the age feature provided by the RSNA dataset for both cancer and non -cancer subjects. As can be observed, age can also be considered a valuable feature. We can also simply normalize and add age to our feature vector to hav e 18,385 features in total. D. Feature selection: The majority of the features are redundant and do not carry any useful information and only increase the complexity of the system. Figure 6 i",
    "full_text_length": 54595,
    "chunk_length": 1198
  },
  {
    "chunk_id": 4306,
    "paper_filename": "zuhra_2023_breast_cancer_detection_in_mamography_using_cnn_with_feature_selection.pdf",
    "paper_title": "Zuhra 2023 Breast Cancer Detection In Mamography Using Cnn With Feature Selection",
    "chunk_index": 29,
    "total_chunks": 57,
    "text_content": "obviously different distributions showing that these features carry useful information, although small, that can improve the performance of classifiers used in the next step. To compute mutual information w e use the method in [30]. We empirically found a 0.02 threshold gives Figure 4. Block diagram of the proposed system. Information 2023 , 14, x FOR PEER REVIEW 8 of 14 us the best results. Note that we have also adopted feature selection based on mutual information empirically and after using ",
    "full_text_length": 54595,
    "chunk_length": 1201
  },
  {
    "chunk_id": 4307,
    "paper_filename": "zuhra_2023_breast_cancer_detection_in_mamography_using_cnn_with_feature_selection.pdf",
    "paper_title": "Zuhra 2023 Breast Cancer Detection In Mamography Using Cnn With Feature Selection",
    "chunk_index": 30,
    "total_chunks": 57,
    "text_content": "breast cancer detection. We construct an ensemble of 100 trees, setting the minimum number of samples required to split a node as 2. Additionally, we limit the maxi mum number of features considered for each tree to 5 and the maximum tree depth to 4. These parameter settings are chosen to optimize the performance of our model and improve the accuracy of breast cancer detection in our X-ray image datasets. In our SVM c lassifier implementation, we utilize a linear kernel and set the regularizatio",
    "full_text_length": 54595,
    "chunk_length": 1186
  },
  {
    "chunk_id": 4308,
    "paper_filename": "zuhra_2023_breast_cancer_detection_in_mamography_using_cnn_with_feature_selection.pdf",
    "paper_title": "Zuhra 2023 Breast Cancer Detection In Mamography Using Cnn With Feature Selection",
    "chunk_index": 31,
    "total_chunks": 57,
    "text_content": "and a single -neuron classification layer. For the classification layer, we use a sigmoid activation function that classifies non -cancer cases from cancerous ones. Figure 5. This figure shows the distribution of age for cancer and noncancer subjects in the RSNA dataset. (a) (b) Figure 6. These figures show distributions of (a) a good feature and ( b) a weak feature extracted using a pre -trained CNN model. for cancer and noncancer subjects in the DDSM dataset. The mutual information computed fo",
    "full_text_length": 54595,
    "chunk_length": 1197
  },
  {
    "chunk_id": 4309,
    "paper_filename": "zuhra_2023_breast_cancer_detection_in_mamography_using_cnn_with_feature_selection.pdf",
    "paper_title": "Zuhra 2023 Breast Cancer Detection In Mamography Using Cnn With Feature Selection",
    "chunk_index": 32,
    "total_chunks": 57,
    "text_content": "methods. The number of features fo r each dataset before and after feature selection is presented in Table 3. E. Feature classification: After selecting the best features, we need to classify them. For this purpose, we tried multiple machine learning algorithms such as k -NN, random forest (RF) , SVM , and NN. In our study, we utilize an RF algorithm with specific parameters to enhance breast cancer detection. We construct an ensemble of 100 trees, setting the minimum number of samples required ",
    "full_text_length": 54595,
    "chunk_length": 1156
  },
  {
    "chunk_id": 4310,
    "paper_filename": "zuhra_2023_breast_cancer_detection_in_mamography_using_cnn_with_feature_selection.pdf",
    "paper_title": "Zuhra 2023 Breast Cancer Detection In Mamography Using Cnn With Feature Selection",
    "chunk_index": 33,
    "total_chunks": 57,
    "text_content": "regularization parameter \u201cC\u201d to a value of 1. The linear kernel allows us to learn a linear decision boundary, while the \u201cC\u201d parameter balances the trade -off between training accuracy and the complexity of the decision boundary. In the k -NN classifier, we set k = 5, and for the NN classifier, we used two fully connected (FC) layers with a hidden layer including 96 neurons and a single -neuron classification layer. For the classification layer, we use a sigmoid activation function that classifi",
    "full_text_length": 54595,
    "chunk_length": 1180
  },
  {
    "chunk_id": 4311,
    "paper_filename": "zuhra_2023_breast_cancer_detection_in_mamography_using_cnn_with_feature_selection.pdf",
    "paper_title": "Zuhra 2023 Breast Cancer Detection In Mamography Using Cnn With Feature Selection",
    "chunk_index": 34,
    "total_chunks": 57,
    "text_content": "features is 0.035 and zero, respecti vely. Figure 6. These \ufb01gures show distributions of ( a) a good feature and ( b) a weak feature extracted using a pre-trained CNN model. for cancer and noncancer subjects in the DDSM dataset. The mutual information computed for these two features is 0.035 and zero, respectively. Table 2. This table shows the CNN models used in the proposed method along with the layer name where the features have been extracted and the number of features extracted from each mod",
    "full_text_length": 54595,
    "chunk_length": 1260
  },
  {
    "chunk_id": 4312,
    "paper_filename": "zuhra_2023_breast_cancer_detection_in_mamography_using_cnn_with_feature_selection.pdf",
    "paper_title": "Zuhra 2023 Breast Cancer Detection In Mamography Using Cnn With Feature Selection",
    "chunk_index": 35,
    "total_chunks": 57,
    "text_content": "views for each subject and one additional feature for age. 4. Results and Discussion This section showcases the results obtained from the three datasets introduced in Section 2.1 using the models described in Section 2.2, as well as a combination of all datasets as illustrated in Figure 4. For each dataset, we employed k-fold cross-validation with k = 10. This means that the method was trained and tested 10 times, with 90% of the data allocated for training and 10% for testing in each iteration.",
    "full_text_length": 54595,
    "chunk_length": 1244
  },
  {
    "chunk_id": 4313,
    "paper_filename": "zuhra_2023_breast_cancer_detection_in_mamography_using_cnn_with_feature_selection.pdf",
    "paper_title": "Zuhra 2023 Breast Cancer Detection In Mamography Using Cnn With Feature Selection",
    "chunk_index": 36,
    "total_chunks": 57,
    "text_content": "classi\ufb01er incorrectly classi\ufb01ed the instance with a positive label. In the context of breast abnormality classi\ufb01cation, an FP response corresponds to a type I error according to statisticians. For example, it could refer to a calci\ufb01cation image being classi\ufb01ed as a mass lesion or a benign mass lesion being classi\ufb01ed as a malignant mammogram in the diagnosis. Information 2023 ,14, 410 9 of 14 \u000f True negatives (TN): Instances where the predicted class and actual class are both negative. This indic",
    "full_text_length": 54595,
    "chunk_length": 1279
  },
  {
    "chunk_id": 4314,
    "paper_filename": "zuhra_2023_breast_cancer_detection_in_mamography_using_cnn_with_feature_selection.pdf",
    "paper_title": "Zuhra 2023 Breast Cancer Detection In Mamography Using Cnn With Feature Selection",
    "chunk_index": 37,
    "total_chunks": 57,
    "text_content": "calci\ufb01cation or a malignant mass lesion being classi\ufb01ed as a be- nign mammogram in the diagnosis. Type II errors are particularly signi\ufb01cant in their consequences. \u000f Accuracy: This metric represents the overall number of correctly classi\ufb01ed instances. In the case of the abnormality classi\ufb01er, accuracy signi\ufb01es the correct classi\ufb01ca- tion of image patches containing either mass or calci\ufb01cation. Similarly, accuracy shows the correct classi\ufb01cation of image patches as either malignant or benign in t",
    "full_text_length": 54595,
    "chunk_length": 1381
  },
  {
    "chunk_id": 4315,
    "paper_filename": "zuhra_2023_breast_cancer_detection_in_mamography_using_cnn_with_feature_selection.pdf",
    "paper_title": "Zuhra 2023 Breast Cancer Detection In Mamography Using Cnn With Feature Selection",
    "chunk_index": 38,
    "total_chunks": 57,
    "text_content": "\u000f Precision: This metric re\ufb02ects the proportion of positive predictions that are correctly categorized. It is calculated using the following formula: Pr=TP (TP+FP) \u000f F1 Score: This measure combines the impact of recall and precision using the harmonic mean, giving equal penalties to extreme values. It is commonly calculated using the formula: F\u0000Score =(2\u0002Sn\u0002xPr) (Sn+Pr) 4.2. Performance Evaluation of the Proposed Model for Different Classi\ufb01ers Table 4 presents a comparison of performance metrics",
    "full_text_length": 54595,
    "chunk_length": 1340
  },
  {
    "chunk_id": 4316,
    "paper_filename": "zuhra_2023_breast_cancer_detection_in_mamography_using_cnn_with_feature_selection.pdf",
    "paper_title": "Zuhra 2023 Breast Cancer Detection In Mamography Using Cnn With Feature Selection",
    "chunk_index": 39,
    "total_chunks": 57,
    "text_content": "of the table, one can see that the proposed concatenation scheme, signi\ufb01cantly improves all performance metrics, for instance, the achieved accuracy is 6 percent more than the best CNN model, i.e., Ef\ufb01cientNet. Information 2023 ,14, 410 10 of 14 Table 4. Performance comparison of the proposed method for different CNN models and Concat. Model with the NN classi\ufb01er for RSNA dataset. CNN ModelsAcc Sn Pr AUC F-Score AlexNet 81% 84% 87% 0.82 0.86 Resnet50 84% 90% 86% 0.89 0.88 MobileNetSmall 77% 85% ",
    "full_text_length": 54595,
    "chunk_length": 1221
  },
  {
    "chunk_id": 4317,
    "paper_filename": "zuhra_2023_breast_cancer_detection_in_mamography_using_cnn_with_feature_selection.pdf",
    "paper_title": "Zuhra 2023 Breast Cancer Detection In Mamography Using Cnn With Feature Selection",
    "chunk_index": 40,
    "total_chunks": 57,
    "text_content": "than the accuracy of the same model with the NN classi\ufb01er, and 13 percent lower than the best-performing Ef\ufb01cientNet model with the NN classi\ufb01er. Additionally, the accuracy of the concatenated model is also 14 percent lower compared to the concatenated model with the NN classi\ufb01er. Table 5. Performance comparison of the proposed method for different CNN models and Concat. Model with the kNN classi\ufb01er for RSNA dataset. CNN ModelsAcc Sn Pr AUC F-Score AlexNet 73% 70% 72% 0.70 0.71 Resnet50 72% 75% ",
    "full_text_length": 54595,
    "chunk_length": 1231
  },
  {
    "chunk_id": 4318,
    "paper_filename": "zuhra_2023_breast_cancer_detection_in_mamography_using_cnn_with_feature_selection.pdf",
    "paper_title": "Zuhra 2023 Breast Cancer Detection In Mamography Using Cnn With Feature Selection",
    "chunk_index": 41,
    "total_chunks": 57,
    "text_content": "hibits the most favorable performance metrics, while mobileNetSmall exhibits the least favorable performance. Table 6. Performance comparison of the proposed method for different CNN models and Concat. Model with the RF classi\ufb01er for RSNA dataset. CNN ModelsAcc Sn Pr AUC F-Score AlexNet 71% 67% 69% 0.68 0.68 Resnet50 69% 70% 67% 0.71 0.68 MobileNetSmall 60% 67% 63% 0.64 0.65 ConvNexSmall 62% 69% 65% 0.67 0.67 Ef\ufb01cientNet 73% 74% 70% 0.75 0.72 Concat. Model 78% 79% 77% 0.80 0.78 Table 7 displays ",
    "full_text_length": 54595,
    "chunk_length": 1215
  },
  {
    "chunk_id": 4319,
    "paper_filename": "zuhra_2023_breast_cancer_detection_in_mamography_using_cnn_with_feature_selection.pdf",
    "paper_title": "Zuhra 2023 Breast Cancer Detection In Mamography Using Cnn With Feature Selection",
    "chunk_index": 42,
    "total_chunks": 57,
    "text_content": "14 Table 7. Performance comparison of the proposed method for different CNN models and Concat. Model with the SVM classi\ufb01er for RSNA dataset. CNN ModelsAcc Sn Pr AUC F-Score AlexNet 62% 61% 63% 0.62 0.62 Resnet50 64% 66% 63% 0.65 0.64 MobileNetSmall 60% 63% 59% 0.60 0.61 ConvNexSmall 62% 65% 61% 0.63 0.63 Ef\ufb01cientNet 68% 70% 66% 0.68 0.68 Concat. Model 73% 75% 72% 0.74 0.73 4.3. Comparison of the Proposed System with State-of-the-Art Methods Based on the \ufb01ndings presented in Tables 4\u20137, it is ev",
    "full_text_length": 54595,
    "chunk_length": 1194
  },
  {
    "chunk_id": 4320,
    "paper_filename": "zuhra_2023_breast_cancer_detection_in_mamography_using_cnn_with_feature_selection.pdf",
    "paper_title": "Zuhra 2023 Breast Cancer Detection In Mamography Using Cnn With Feature Selection",
    "chunk_index": 43,
    "total_chunks": 57,
    "text_content": "against existing methods using the MIAS and DDSM datasets and summarized the results in Table 8. Table 8. Performance comparison of our proposed model vs. methods using the MIAS and DDSM datasets. Method Dataset Number of Images ACC Sn Pr SVM & Hough [32] MIAS & InBreast 322&206 86.13% 80.67% 92.81% LQP & SVM [33] MIAS 95 94% NA NA GMM & SVM [34] Mini-MIAS dataset 90 92.5% NA NA KNN [35] Mini-MIAS 120 92% NA NA Voting Classi\ufb01er [36] MIAS 322 85% NA NA CNN-4d [37] Mini-MIAS 547 89.05% 90.63% 83.6",
    "full_text_length": 54595,
    "chunk_length": 1149
  },
  {
    "chunk_id": 4321,
    "paper_filename": "zuhra_2023_breast_cancer_detection_in_mamography_using_cnn_with_feature_selection.pdf",
    "paper_title": "Zuhra 2023 Breast Cancer Detection In Mamography Using Cnn With Feature Selection",
    "chunk_index": 44,
    "total_chunks": 57,
    "text_content": "state-of-the-art algorithms in terms of accuracy and sensitivity across both the MIAS and DDSM datasets. While the method described in [ 32] demon- strated slightly better precision for the MIAS dataset, our algorithm outperformed it in the remaining two performance metrics. 4.4. Cross-Dataset Validation So far, we have trained and tested the proposed method on the same dataset. However, it is crucial to evaluate the ability of a model trained on one dataset to perform well on different datasets",
    "full_text_length": 54595,
    "chunk_length": 1237
  },
  {
    "chunk_id": 4322,
    "paper_filename": "zuhra_2023_breast_cancer_detection_in_mamography_using_cnn_with_feature_selection.pdf",
    "paper_title": "Zuhra 2023 Breast Cancer Detection In Mamography Using Cnn With Feature Selection",
    "chunk_index": 45,
    "total_chunks": 57,
    "text_content": "and tested with different datasets. Train Dataset Test Dataset ACC Sn Pr RSNA MIAS 79.13% 82.67% 80.81% RSNA DDSM 74% 77.50% 76% MIAS RSNA 76.5% 78.80% 78% MIAS DDSM 80.70% 82% 82.80% DDSM RSNA 72% 75.50% 76% DDSM MIAS 79% 80% 79.87% Since the RSNA dataset comprises images of various types and resolutions, cross- validating it with another dataset yields slightly lower performance metrics. Speci\ufb01cally, when the method is trained on either the MIAS or DDSM dataset and tested on RSNA images, the a",
    "full_text_length": 54595,
    "chunk_length": 1305
  },
  {
    "chunk_id": 4323,
    "paper_filename": "zuhra_2023_breast_cancer_detection_in_mamography_using_cnn_with_feature_selection.pdf",
    "paper_title": "Zuhra 2023 Breast Cancer Detection In Mamography Using Cnn With Feature Selection",
    "chunk_index": 46,
    "total_chunks": 57,
    "text_content": "address the accurate diagnosis of breast cancer in mammography images. Our approach involves the extraction and selection of features from multiple pre-trained CNN models, followed by classi\ufb01cation using various machine learning algorithms: kNN, SVM, RF, and NN. The results obtained for different datasets demonstrate the effectiveness of our proposed scheme. Our \ufb01ndings indicate that the NN-based classi\ufb01er yielded the best performance in our experiments. Notably, we achieved impressive accuracie",
    "full_text_length": 54595,
    "chunk_length": 1452
  },
  {
    "chunk_id": 4324,
    "paper_filename": "zuhra_2023_breast_cancer_detection_in_mamography_using_cnn_with_feature_selection.pdf",
    "paper_title": "Zuhra 2023 Breast Cancer Detection In Mamography Using Cnn With Feature Selection",
    "chunk_index": 47,
    "total_chunks": 57,
    "text_content": "system. Lastly, conducting rigorous validation on larger-scale datasets from multiple healthcare institutions would provide more robust evidence of the method\u2019s effectiveness and generalizability. Author Contributions: Z.J. provided, and cleaned the dataset, and implemented the algorithms, and Z.J and E.K. performed experiments and wrote the paper. E.K. edited the paper. All authors have read and agreed to the published version of the manuscript. Funding: The research received no external fundin",
    "full_text_length": 54595,
    "chunk_length": 1362
  },
  {
    "chunk_id": 4325,
    "paper_filename": "zuhra_2023_breast_cancer_detection_in_mamography_using_cnn_with_feature_selection.pdf",
    "paper_title": "Zuhra 2023 Breast Cancer Detection In Mamography Using Cnn With Feature Selection",
    "chunk_index": 48,
    "total_chunks": 57,
    "text_content": "of breast cancer incidence and mortality: A population-based cancer registry data analysis from 2000 to 2020. Cancer Commun. 2021 ,41, 1183\u20131194. [CrossRef] 3. Marks, J.S.; Lee, N.C.; Lawson, H.W.; Henson, R.; Bobo, J.K.; Kaeser, M.K. Implementing recommendations for the early detection of breast and cervical cancer among low-income women. Morb. Mortal. Wkly. Rep. Recomm. Rep. 2000 ,49, 35\u201355. 4. Du-Crow, E. Computer-Aided Detection in Mammography ; The University of Manchester: Manchester, UK, ",
    "full_text_length": 54595,
    "chunk_length": 1394
  },
  {
    "chunk_id": 4326,
    "paper_filename": "zuhra_2023_breast_cancer_detection_in_mamography_using_cnn_with_feature_selection.pdf",
    "paper_title": "Zuhra 2023 Breast Cancer Detection In Mamography Using Cnn With Feature Selection",
    "chunk_index": 49,
    "total_chunks": 57,
    "text_content": "Shi, X.; Liang, C.; Wang, H. Multiview robust graph-based clustering for cancer subtype identi\ufb01cation. IEEE/ACM Trans. Comput. Biol. Bioinform. 2022 ,20, 544\u2013556. [CrossRef] 8. Wang, H.; Jiang, G.; Peng, J.; Deng, R.; Fu, X. Towards Adaptive Consensus Graph: Multi-view Clustering via Graph Collaboration. IEEE Trans. Multimed. 2022 , 1\u201313. [CrossRef] 9. Wang, H.; Wang, Y.; Zhang, Z.; Fu, X.; Zhuo, L.; Xu, M.; Wang, M. Kernelized multiview subspace analysis by self-weighted learning. IEEE Trans. M",
    "full_text_length": 54595,
    "chunk_length": 1353
  },
  {
    "chunk_id": 4327,
    "paper_filename": "zuhra_2023_breast_cancer_detection_in_mamography_using_cnn_with_feature_selection.pdf",
    "paper_title": "Zuhra 2023 Breast Cancer Detection In Mamography Using Cnn With Feature Selection",
    "chunk_index": 50,
    "total_chunks": 57,
    "text_content": "Analysis, applications, and prospects. IEEE Trans. Neural Netw. Learn. Syst. 2022 ,33, 6999\u20137019. [CrossRef] [PubMed] 13. Zuluaga-Gomez, J.; Al Masry, Z.; Benaggoune, K.; Meraghni, S.; Zerhouni, N. A CNN-based methodology for breast cancer diagnosis using thermal images. Comput. Methods Biomech. Biomed. Eng. Imaging Vis. 2021 ,9, 131\u2013145. [CrossRef] 14. Ero \u02d8 glu, Y.; Yildirim, M.; \u00c7inar, A. Convolutional Neural Networks based classification of breast ultrasonography images by hybrid method with",
    "full_text_length": 54595,
    "chunk_length": 1332
  },
  {
    "chunk_id": 4328,
    "paper_filename": "zuhra_2023_breast_cancer_detection_in_mamography_using_cnn_with_feature_selection.pdf",
    "paper_title": "Zuhra 2023 Breast Cancer Detection In Mamography Using Cnn With Feature Selection",
    "chunk_index": 51,
    "total_chunks": 57,
    "text_content": "C.; Li, G.; Jiang, Z.; Heng, P . Weakly supervised 3D deep learning for breast cancer classi\ufb01cation and localization of the lesions in MR images. J. Magn. Reson. Imaging 2019 ,50, 1144\u20131151. [CrossRef] 18. Yurttakal, A.H.; Erbay, H.; Ikizceli, T.; Kara\u00e7avu\u00b8 s, S. Detection of breast cancer via deep convolution neural networks using MRI images. Multimed. Tools Appl. 2019 ,79, 15555\u201315573. [CrossRef] 19. Rahman, A.S.; Belhaouari, S.B.; Bouzerdoum, A.; Baali, H.; Alam, T.; Eldaraa, A.M. Breast mass",
    "full_text_length": 54595,
    "chunk_length": 1406
  },
  {
    "chunk_id": 4329,
    "paper_filename": "zuhra_2023_breast_cancer_detection_in_mamography_using_cnn_with_feature_selection.pdf",
    "paper_title": "Zuhra 2023 Breast Cancer Detection In Mamography Using Cnn With Feature Selection",
    "chunk_index": 52,
    "total_chunks": 57,
    "text_content": "22. Carr, C.; Kitamura, F.; Partridge, G.; Kalpathy-Cramer, J.; Mongan, J.; Andriole, K.; Lavender Vazirabad, M.; Riopel, M.; Ball, R.; Dane, S.; et al. RSNA Screening Mammography Breast Cancer Detection, Kaggle 2022. Available online: https: //kaggle.com/competitions/rsna-breast-cancer-detection (accessed on 1 December 2022). 23. Suckling, J.; Parker, J.; Dance, D.; Astley, S.; Hutt, I.; Boggis, C.; Ricketts, I.; Stamatakis, E.; Cerneaz, N.; Kok, S.; et al. Mammographic Image Analysis Society (",
    "full_text_length": 54595,
    "chunk_length": 1410
  },
  {
    "chunk_id": 4330,
    "paper_filename": "zuhra_2023_breast_cancer_detection_in_mamography_using_cnn_with_feature_selection.pdf",
    "paper_title": "Zuhra 2023 Breast Cancer Detection In Mamography Using Cnn With Feature Selection",
    "chunk_index": 53,
    "total_chunks": 57,
    "text_content": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition 2016, Las Vegas, NV , USA, 27\u201330 June 2016; pp. 770\u2013778. 27. Tan, M.; Le, Q. Ef\ufb01cientnetv2: Smaller models and faster training. In Proceedings of the International Conference on Machine Learning, PMLR, Virtual Event, 1 July 2021; pp. 10096\u201310106. 28. Howard, A.G.; Zhu, M.; Chen, B.; Kalenichenko, D.; Wang, W.; Weyand, T.; Andreetto, M.; Adam, H. Mobilenets: Ef\ufb01cient convolutional neural networks for mobile vision appli",
    "full_text_length": 54595,
    "chunk_length": 1337
  },
  {
    "chunk_id": 4331,
    "paper_filename": "zuhra_2023_breast_cancer_detection_in_mamography_using_cnn_with_feature_selection.pdf",
    "paper_title": "Zuhra 2023 Breast Cancer Detection In Mamography Using Cnn With Feature Selection",
    "chunk_index": 54,
    "total_chunks": 57,
    "text_content": "computer aided breast abnormality diagnosis system. IEEE Access 2022 ,11, 21199\u201321209. [CrossRef] Information 2023 ,14, 410 14 of 14 32. Rampun, A.; Scotney, B.W.; Morrow, P .J.; Wang, H.; Winder, J. Breast density classi\ufb01cation using local quinary patterns with various neighbourhood topologies. J. Imaging 2018 ,4, 14. [CrossRef] 33. Vijayarajeswari, R.; Parthasarathy, P .; Vivekanandan, S.; Basha, A.A. Classi\ufb01cation of mammogram for early detection of breast cancer using SVM classi\ufb01er and Hough",
    "full_text_length": 54595,
    "chunk_length": 1367
  },
  {
    "chunk_id": 4332,
    "paper_filename": "zuhra_2023_breast_cancer_detection_in_mamography_using_cnn_with_feature_selection.pdf",
    "paper_title": "Zuhra 2023 Breast Cancer Detection In Mamography Using Cnn With Feature Selection",
    "chunk_index": 55,
    "total_chunks": 57,
    "text_content": "S.; Rangnekar, R.; Gala, D.; Paul, S.; Kalbande, D. Detection of breast cancer from mammograms using a hybrid approach of deep learning and linear classi\ufb01cation. In Proceedings of the 2018 International Conference on Smart City and Emerging Technology (ICSCET), Mumbai, India, 5 January 2018; pp. 1\u20136. 37. Li, B.; Ge, Y.; Zhao, Y.; Guan, E.; Yan, W. Benign and malignant mammographic image classi\ufb01cation based on convolutional neural networks. In Proceedings of the 2018 10th International Conference",
    "full_text_length": 54595,
    "chunk_length": 1353
  },
  {
    "chunk_id": 4333,
    "paper_filename": "zuhra_2023_breast_cancer_detection_in_mamography_using_cnn_with_feature_selection.pdf",
    "paper_title": "Zuhra 2023 Breast Cancer Detection In Mamography Using Cnn With Feature Selection",
    "chunk_index": 56,
    "total_chunks": 57,
    "text_content": "Deep learning and non-negative matrix factorization in recognition of mammograms. In Proceedings of the Eighth International Conference on Graphic and Image Processing (ICGIP 2016), Tokyo, Japan, 8 February 2017; Volume 10225, pp. 53\u201359. Disclaimer/Publisher\u2019s Note: The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to",
    "full_text_length": 54595,
    "chunk_length": 607
  }
]